epoch 1, iter 5, loss 9.56594, smoothed loss 9.48775, grad norm 0.84528, param norm 65.39430
epoch 1, iter 10, loss 9.48155, smoothed loss 9.48192, grad norm 1.25048, param norm 65.48150
epoch 1, iter 15, loss 9.27466, smoothed loss 9.47689, grad norm 1.24274, param norm 65.56706
epoch 1, iter 20, loss 9.05224, smoothed loss 9.46664, grad norm 1.43521, param norm 65.68195
epoch 1, iter 25, loss 8.69725, smoothed loss 9.43746, grad norm 1.80997, param norm 65.85023
epoch 1, iter 30, loss 8.92908, smoothed loss 9.40513, grad norm 2.57122, param norm 66.03816
epoch 1, iter 35, loss 8.51734, smoothed loss 9.36685, grad norm 2.09049, param norm 66.20326
epoch 1, iter 40, loss 8.40166, smoothed loss 9.31836, grad norm 2.10114, param norm 66.38971
epoch 1, iter 45, loss 7.96162, smoothed loss 9.25844, grad norm 2.04876, param norm 66.58619
epoch 1, iter 50, loss 8.37748, smoothed loss 9.20298, grad norm 2.14335, param norm 66.76685
epoch 1, iter 55, loss 8.60347, smoothed loss 9.15311, grad norm 2.36547, param norm 66.92477
epoch 1, iter 60, loss 8.25105, smoothed loss 9.09915, grad norm 1.76302, param norm 67.07372
epoch 1, iter 65, loss 7.85599, smoothed loss 9.04215, grad norm 1.68576, param norm 67.19906
epoch 1, iter 70, loss 7.81023, smoothed loss 8.98521, grad norm 1.73019, param norm 67.32156
epoch 1, iter 75, loss 7.69250, smoothed loss 8.92837, grad norm 1.80748, param norm 67.45361
epoch 1, iter 80, loss 8.01655, smoothed loss 8.88266, grad norm 1.87851, param norm 67.57132
epoch 1, iter 85, loss 8.00960, smoothed loss 8.83573, grad norm 1.62543, param norm 67.66920
epoch 1, iter 90, loss 7.87683, smoothed loss 8.78435, grad norm 1.87131, param norm 67.80964
epoch 1, iter 95, loss 7.66184, smoothed loss 8.73238, grad norm 2.00680, param norm 67.94031
epoch 1, iter 100, loss 7.99817, smoothed loss 8.69413, grad norm 1.73370, param norm 68.04484
epoch 1, iter 105, loss 7.70917, smoothed loss 8.64631, grad norm 2.21393, param norm 68.16299
epoch 1, iter 110, loss 7.99299, smoothed loss 8.59880, grad norm 1.85719, param norm 68.31291
epoch 1, iter 115, loss 7.59478, smoothed loss 8.55363, grad norm 1.76551, param norm 68.45097
epoch 1, iter 120, loss 7.91601, smoothed loss 8.52535, grad norm 1.95539, param norm 68.56184
epoch 1, iter 125, loss 7.22413, smoothed loss 8.47924, grad norm 1.96015, param norm 68.68202
epoch 1, iter 130, loss 7.24925, smoothed loss 8.43773, grad norm 1.96901, param norm 68.82653
epoch 1, iter 135, loss 7.77115, smoothed loss 8.38870, grad norm 2.17952, param norm 68.96346
epoch 1, iter 140, loss 7.80237, smoothed loss 8.33629, grad norm 2.25320, param norm 69.08756
epoch 1, iter 145, loss 7.77205, smoothed loss 8.30655, grad norm 1.91137, param norm 69.20341
epoch 1, iter 150, loss 7.67538, smoothed loss 8.27583, grad norm 2.32870, param norm 69.33611
epoch 1, iter 155, loss 7.47881, smoothed loss 8.24063, grad norm 2.06610, param norm 69.45943
epoch 1, iter 160, loss 7.51049, smoothed loss 8.19972, grad norm 2.56921, param norm 69.61186
epoch 1, iter 165, loss 7.53855, smoothed loss 8.17020, grad norm 2.58691, param norm 69.76122
epoch 1, iter 170, loss 7.77419, smoothed loss 8.12657, grad norm 2.32789, param norm 69.90572
epoch 1, iter 175, loss 7.18192, smoothed loss 8.08187, grad norm 2.43053, param norm 70.06572
epoch 1, iter 180, loss 7.33599, smoothed loss 8.05104, grad norm 3.24112, param norm 70.22018
epoch 1, iter 185, loss 7.80517, smoothed loss 8.02423, grad norm 2.16331, param norm 70.38813
epoch 1, iter 190, loss 6.79811, smoothed loss 7.98897, grad norm 2.31570, param norm 70.55177
epoch 1, iter 195, loss 7.26097, smoothed loss 7.94504, grad norm 2.72546, param norm 70.71348
epoch 1, iter 200, loss 7.20497, smoothed loss 7.90084, grad norm 2.53434, param norm 70.85113
epoch 1, iter 205, loss 7.08649, smoothed loss 7.85342, grad norm 2.41503, param norm 71.00833
epoch 1, iter 210, loss 6.81664, smoothed loss 7.80362, grad norm 2.71150, param norm 71.19624
epoch 1, iter 215, loss 6.84834, smoothed loss 7.76295, grad norm 2.56679, param norm 71.35206
epoch 1, iter 220, loss 6.55998, smoothed loss 7.71820, grad norm 2.43255, param norm 71.50378
epoch 1, iter 225, loss 6.97539, smoothed loss 7.66431, grad norm 2.72390, param norm 71.68889
epoch 1, iter 230, loss 6.89006, smoothed loss 7.62391, grad norm 3.07809, param norm 71.83989
epoch 1, iter 235, loss 6.59949, smoothed loss 7.58735, grad norm 2.75530, param norm 71.98992
epoch 1, iter 240, loss 6.75316, smoothed loss 7.54823, grad norm 2.38913, param norm 72.13607
epoch 1, iter 245, loss 7.01390, smoothed loss 7.51189, grad norm 2.77396, param norm 72.28653
epoch 1, iter 250, loss 6.96551, smoothed loss 7.48479, grad norm 2.41607, param norm 72.43102
epoch 1, iter 255, loss 7.07071, smoothed loss 7.45016, grad norm 2.60079, param norm 72.57606
epoch 1, iter 260, loss 6.43667, smoothed loss 7.41731, grad norm 2.39088, param norm 72.72526
epoch 1, iter 265, loss 6.88555, smoothed loss 7.38896, grad norm 2.41729, param norm 72.87016
epoch 1, iter 270, loss 6.08038, smoothed loss 7.34821, grad norm 2.49281, param norm 73.03854
epoch 1, iter 275, loss 6.38324, smoothed loss 7.30753, grad norm 2.88250, param norm 73.19904
epoch 1, iter 280, loss 6.57715, smoothed loss 7.28165, grad norm 2.33921, param norm 73.33295
epoch 1, iter 285, loss 6.61073, smoothed loss 7.25486, grad norm 2.56198, param norm 73.45492
epoch 1, iter 290, loss 5.44076, smoothed loss 7.21521, grad norm 2.66664, param norm 73.58649
epoch 1, iter 295, loss 6.46007, smoothed loss 7.18273, grad norm 2.62472, param norm 73.72242
epoch 1, iter 300, loss 6.73159, smoothed loss 7.15402, grad norm 2.79988, param norm 73.85656
epoch 1, iter 305, loss 6.74991, smoothed loss 7.12297, grad norm 2.50310, param norm 73.99657
epoch 1, iter 310, loss 6.05403, smoothed loss 7.08155, grad norm 3.06794, param norm 74.13783
epoch 1, iter 315, loss 6.54640, smoothed loss 7.06338, grad norm 2.58612, param norm 74.26930
epoch 1, iter 320, loss 6.30339, smoothed loss 7.03579, grad norm 2.51003, param norm 74.38345
epoch 1, iter 325, loss 6.13168, smoothed loss 7.00322, grad norm 2.64841, param norm 74.52229
epoch 1, iter 330, loss 6.01217, smoothed loss 6.96869, grad norm 2.73933, param norm 74.66109
epoch 1, iter 335, loss 6.19936, smoothed loss 6.94289, grad norm 2.74998, param norm 74.76746
epoch 1, iter 340, loss 6.65613, smoothed loss 6.91950, grad norm 2.84684, param norm 74.85701
epoch 1, iter 345, loss 6.50457, smoothed loss 6.89152, grad norm 2.73670, param norm 74.94634
epoch 1, iter 350, loss 6.68982, smoothed loss 6.86910, grad norm 2.83508, param norm 75.08413
epoch 1, iter 355, loss 6.54208, smoothed loss 6.84956, grad norm 2.79031, param norm 75.23129
epoch 1, iter 360, loss 5.96566, smoothed loss 6.83036, grad norm 2.52875, param norm 75.34075
epoch 1, iter 365, loss 6.23484, smoothed loss 6.80367, grad norm 3.12320, param norm 75.44026
epoch 1, iter 370, loss 6.07149, smoothed loss 6.77899, grad norm 2.91308, param norm 75.57655
epoch 1, iter 375, loss 5.57069, smoothed loss 6.72952, grad norm 2.87622, param norm 75.72948
epoch 1, iter 380, loss 6.19769, smoothed loss 6.69911, grad norm 2.89161, param norm 75.86842
epoch 1, iter 385, loss 6.08137, smoothed loss 6.67868, grad norm 2.40991, param norm 75.99570
epoch 1, iter 390, loss 5.94218, smoothed loss 6.64825, grad norm 2.56535, param norm 76.12764
epoch 1, iter 395, loss 6.47256, smoothed loss 6.63745, grad norm 2.65250, param norm 76.26901
epoch 1, iter 400, loss 5.64342, smoothed loss 6.60057, grad norm 2.45915, param norm 76.40001
epoch 1, iter 405, loss 6.46677, smoothed loss 6.58638, grad norm 3.51756, param norm 76.53759
epoch 1, iter 410, loss 6.28118, smoothed loss 6.55416, grad norm 3.08509, param norm 76.66142
epoch 1, iter 415, loss 5.74584, smoothed loss 6.53787, grad norm 2.62541, param norm 76.76472
epoch 1, iter 420, loss 5.41601, smoothed loss 6.50220, grad norm 2.53832, param norm 76.89233
epoch 1, iter 425, loss 6.37127, smoothed loss 6.49108, grad norm 3.19627, param norm 77.01186
epoch 1, iter 430, loss 5.42379, smoothed loss 6.46786, grad norm 2.99478, param norm 77.13552
epoch 1, iter 435, loss 6.63012, smoothed loss 6.46016, grad norm 3.49249, param norm 77.26834
epoch 1, iter 440, loss 5.89859, smoothed loss 6.43175, grad norm 2.60760, param norm 77.38426
epoch 1, iter 445, loss 5.92007, smoothed loss 6.41810, grad norm 2.76193, param norm 77.49447
epoch 1, iter 450, loss 6.28627, smoothed loss 6.39985, grad norm 3.04805, param norm 77.60973
epoch 1, iter 455, loss 6.11422, smoothed loss 6.38088, grad norm 3.21001, param norm 77.73136
epoch 1, iter 460, loss 4.45680, smoothed loss 6.34297, grad norm 2.76528, param norm 77.87041
epoch 1, iter 465, loss 4.98032, smoothed loss 6.30822, grad norm 2.98606, param norm 78.00291
epoch 1, iter 470, loss 5.89103, smoothed loss 6.29011, grad norm 3.04672, param norm 78.12026
epoch 1, iter 475, loss 5.91362, smoothed loss 6.28086, grad norm 3.03403, param norm 78.22273
epoch 1, iter 480, loss 6.69992, smoothed loss 6.26471, grad norm 3.64859, param norm 78.34071
epoch 1, iter 485, loss 6.46487, smoothed loss 6.26118, grad norm 3.10320, param norm 78.47097
epoch 1, iter 490, loss 5.57235, smoothed loss 6.26439, grad norm 2.61798, param norm 78.59253
epoch 1, iter 495, loss 5.75269, smoothed loss 6.25101, grad norm 2.61222, param norm 78.72491
epoch 1, iter 500, loss 5.75457, smoothed loss 6.23747, grad norm 2.62170, param norm 78.85561
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 1, Iter 500, dev loss: 5.860887
Calculating Train F1/EM...
F1 train: 1000 examples took 20.87563 seconds [Score: 0.26743]
Exact Match train: 1000 examples took 18.84618 seconds [Score: 0.18600]
Epoch 1, Iter 500, Train F1 score: 0.267429, Train EM score: 0.186000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.59949 seconds [Score: 0.26442]
Exact Match dev: 7118 examples took 125.31513 seconds [Score: 0.16901]
Epoch 1, Iter 500, Dev F1 score: 0.264423, Dev EM score: 0.169008
End of epoch 1
epoch 1, iter 505, loss 4.99073, smoothed loss 6.19718, grad norm 2.83073, param norm 78.98207
epoch 1, iter 510, loss 5.53410, smoothed loss 6.18785, grad norm 2.99823, param norm 79.09963
epoch 1, iter 515, loss 5.43957, smoothed loss 6.17083, grad norm 3.58443, param norm 79.20581
epoch 1, iter 520, loss 5.89994, smoothed loss 6.15947, grad norm 2.82993, param norm 79.33816
epoch 1, iter 525, loss 5.74692, smoothed loss 6.15089, grad norm 3.20339, param norm 79.45601
epoch 1, iter 530, loss 5.42233, smoothed loss 6.12998, grad norm 2.86543, param norm 79.56766
epoch 1, iter 535, loss 6.58518, smoothed loss 6.12923, grad norm 3.16735, param norm 79.67574
epoch 1, iter 540, loss 6.44844, smoothed loss 6.12342, grad norm 3.07818, param norm 79.77697
epoch 1, iter 545, loss 6.33018, smoothed loss 6.11949, grad norm 3.14714, param norm 79.89606
epoch 1, iter 550, loss 5.45053, smoothed loss 6.10914, grad norm 2.80598, param norm 80.01299
epoch 1, iter 555, loss 5.27662, smoothed loss 6.08163, grad norm 2.53423, param norm 80.12810
epoch 1, iter 560, loss 6.08621, smoothed loss 6.07334, grad norm 3.34734, param norm 80.23466
epoch 1, iter 565, loss 5.87543, smoothed loss 6.05818, grad norm 2.96411, param norm 80.34796
epoch 1, iter 570, loss 5.81653, smoothed loss 6.04343, grad norm 2.82206, param norm 80.47202
epoch 1, iter 575, loss 5.95841, smoothed loss 6.02874, grad norm 3.06258, param norm 80.59637
epoch 1, iter 580, loss 5.34990, smoothed loss 6.01295, grad norm 3.27272, param norm 80.69777
epoch 1, iter 585, loss 5.83860, smoothed loss 6.00435, grad norm 2.78640, param norm 80.80193
epoch 1, iter 590, loss 5.59330, smoothed loss 5.99908, grad norm 3.31930, param norm 80.91948
epoch 1, iter 595, loss 5.62328, smoothed loss 5.97723, grad norm 2.91857, param norm 81.04543
epoch 1, iter 600, loss 5.62272, smoothed loss 5.96170, grad norm 3.39995, param norm 81.17038
epoch 1, iter 605, loss 5.78458, smoothed loss 5.96229, grad norm 3.52369, param norm 81.27525
epoch 1, iter 610, loss 5.81129, smoothed loss 5.94597, grad norm 2.91938, param norm 81.38816
epoch 1, iter 615, loss 6.23042, smoothed loss 5.94510, grad norm 3.31359, param norm 81.52494
epoch 1, iter 620, loss 6.00424, smoothed loss 5.93936, grad norm 3.03238, param norm 81.64098
epoch 1, iter 625, loss 5.45080, smoothed loss 5.92381, grad norm 2.61418, param norm 81.74404
epoch 1, iter 630, loss 5.35917, smoothed loss 5.90625, grad norm 2.84980, param norm 81.85215
epoch 1, iter 635, loss 6.07739, smoothed loss 5.89817, grad norm 3.18431, param norm 81.97106
epoch 1, iter 640, loss 5.41170, smoothed loss 5.88636, grad norm 3.00394, param norm 82.09341
epoch 1, iter 645, loss 5.06475, smoothed loss 5.86453, grad norm 3.33500, param norm 82.20621
epoch 1, iter 650, loss 5.85768, smoothed loss 5.86204, grad norm 3.36755, param norm 82.30934
epoch 1, iter 655, loss 4.68047, smoothed loss 5.84652, grad norm 3.05371, param norm 82.41953
epoch 1, iter 660, loss 5.40083, smoothed loss 5.84233, grad norm 3.05730, param norm 82.53385
epoch 1, iter 665, loss 5.80739, smoothed loss 5.83386, grad norm 2.95253, param norm 82.63998
epoch 1, iter 670, loss 6.14865, smoothed loss 5.82395, grad norm 3.31437, param norm 82.75594
epoch 1, iter 675, loss 5.64656, smoothed loss 5.79772, grad norm 3.04550, param norm 82.87659
epoch 1, iter 680, loss 5.09278, smoothed loss 5.79018, grad norm 3.30240, param norm 82.99299
epoch 1, iter 685, loss 6.31254, smoothed loss 5.78691, grad norm 3.40226, param norm 83.09294
epoch 1, iter 690, loss 5.55185, smoothed loss 5.77877, grad norm 2.57178, param norm 83.18867
epoch 1, iter 695, loss 5.56896, smoothed loss 5.77450, grad norm 2.63725, param norm 83.29141
epoch 1, iter 700, loss 5.75737, smoothed loss 5.76497, grad norm 2.93378, param norm 83.40103
epoch 1, iter 705, loss 5.36897, smoothed loss 5.75950, grad norm 3.11741, param norm 83.50691
epoch 1, iter 710, loss 5.60587, smoothed loss 5.74171, grad norm 3.13250, param norm 83.61041
epoch 1, iter 715, loss 5.17380, smoothed loss 5.73187, grad norm 2.98273, param norm 83.71376
epoch 1, iter 720, loss 5.02912, smoothed loss 5.71921, grad norm 3.11417, param norm 83.82243
epoch 1, iter 725, loss 6.09008, smoothed loss 5.71853, grad norm 3.78674, param norm 83.92033
epoch 1, iter 730, loss 5.96522, smoothed loss 5.70728, grad norm 3.01671, param norm 84.01583
epoch 1, iter 735, loss 5.17036, smoothed loss 5.69039, grad norm 3.31588, param norm 84.12875
epoch 1, iter 740, loss 5.48697, smoothed loss 5.68211, grad norm 3.33736, param norm 84.25006
epoch 1, iter 745, loss 4.87707, smoothed loss 5.65726, grad norm 3.11010, param norm 84.36387
epoch 1, iter 750, loss 4.86184, smoothed loss 5.63444, grad norm 3.24083, param norm 84.47892
epoch 1, iter 755, loss 5.40740, smoothed loss 5.62196, grad norm 3.66773, param norm 84.58865
epoch 1, iter 760, loss 5.82582, smoothed loss 5.61067, grad norm 3.42110, param norm 84.68964
epoch 1, iter 765, loss 5.76738, smoothed loss 5.59932, grad norm 2.96434, param norm 84.80654
epoch 1, iter 770, loss 5.38934, smoothed loss 5.59893, grad norm 2.75112, param norm 84.92607
epoch 1, iter 775, loss 6.41129, smoothed loss 5.59702, grad norm 3.89594, param norm 85.05091
epoch 1, iter 780, loss 4.71697, smoothed loss 5.58059, grad norm 2.81128, param norm 85.17164
epoch 1, iter 785, loss 5.27081, smoothed loss 5.57246, grad norm 3.17661, param norm 85.27765
epoch 1, iter 790, loss 6.03758, smoothed loss 5.57169, grad norm 3.32681, param norm 85.38441
epoch 1, iter 795, loss 4.97734, smoothed loss 5.55350, grad norm 3.23885, param norm 85.48491
epoch 1, iter 800, loss 5.04922, smoothed loss 5.54815, grad norm 3.26654, param norm 85.58198
epoch 1, iter 805, loss 5.17099, smoothed loss 5.53019, grad norm 3.15931, param norm 85.67886
epoch 1, iter 810, loss 5.31153, smoothed loss 5.51535, grad norm 3.03687, param norm 85.78026
epoch 1, iter 815, loss 6.11455, smoothed loss 5.51125, grad norm 3.46929, param norm 85.87999
epoch 1, iter 820, loss 4.87665, smoothed loss 5.50120, grad norm 2.78795, param norm 85.97939
epoch 1, iter 825, loss 5.59196, smoothed loss 5.48622, grad norm 3.74590, param norm 86.09634
epoch 1, iter 830, loss 5.61040, smoothed loss 5.48409, grad norm 3.34522, param norm 86.20623
epoch 1, iter 835, loss 4.91252, smoothed loss 5.46769, grad norm 3.57712, param norm 86.31605
epoch 1, iter 840, loss 4.57792, smoothed loss 5.44487, grad norm 3.05657, param norm 86.44164
epoch 1, iter 845, loss 5.56533, smoothed loss 5.44223, grad norm 3.34338, param norm 86.56228
epoch 1, iter 850, loss 5.12790, smoothed loss 5.41115, grad norm 3.43337, param norm 86.67333
epoch 1, iter 855, loss 5.11652, smoothed loss 5.38567, grad norm 3.86536, param norm 86.79298
epoch 1, iter 860, loss 5.22798, smoothed loss 5.38242, grad norm 3.30821, param norm 86.90160
epoch 1, iter 865, loss 4.62246, smoothed loss 5.36981, grad norm 3.36665, param norm 87.01164
epoch 1, iter 870, loss 4.97343, smoothed loss 5.35027, grad norm 3.52797, param norm 87.13483
epoch 1, iter 875, loss 5.49920, smoothed loss 5.34866, grad norm 3.51716, param norm 87.24944
epoch 1, iter 880, loss 4.73082, smoothed loss 5.32855, grad norm 3.55316, param norm 87.35650
epoch 1, iter 885, loss 4.74232, smoothed loss 5.32218, grad norm 3.57704, param norm 87.45666
epoch 1, iter 890, loss 5.39138, smoothed loss 5.32089, grad norm 3.63270, param norm 87.55624
epoch 1, iter 895, loss 4.84826, smoothed loss 5.31402, grad norm 3.19506, param norm 87.66737
epoch 1, iter 900, loss 4.96090, smoothed loss 5.30548, grad norm 3.02824, param norm 87.79268
epoch 1, iter 905, loss 5.37800, smoothed loss 5.31165, grad norm 3.28055, param norm 87.90672
epoch 1, iter 910, loss 5.22483, smoothed loss 5.29112, grad norm 3.44252, param norm 88.01591
epoch 1, iter 915, loss 4.60597, smoothed loss 5.26800, grad norm 3.62643, param norm 88.12211
epoch 1, iter 920, loss 4.53426, smoothed loss 5.25512, grad norm 3.83619, param norm 88.23653
epoch 1, iter 925, loss 4.80629, smoothed loss 5.23831, grad norm 3.34891, param norm 88.35019
epoch 1, iter 930, loss 4.98214, smoothed loss 5.23119, grad norm 3.27742, param norm 88.46284
epoch 1, iter 935, loss 5.42834, smoothed loss 5.22092, grad norm 3.62746, param norm 88.57941
epoch 1, iter 940, loss 5.03946, smoothed loss 5.21816, grad norm 3.07112, param norm 88.70010
epoch 2, iter 945, loss 4.74850, smoothed loss 5.21536, grad norm 3.24900, param norm 88.81175
epoch 2, iter 950, loss 4.07984, smoothed loss 5.18823, grad norm 3.12168, param norm 88.92099
epoch 2, iter 955, loss 4.52218, smoothed loss 5.16504, grad norm 3.48302, param norm 89.04169
epoch 2, iter 960, loss 4.62590, smoothed loss 5.15319, grad norm 3.25216, param norm 89.16220
epoch 2, iter 965, loss 4.55623, smoothed loss 5.13739, grad norm 3.59532, param norm 89.27850
epoch 2, iter 970, loss 4.83282, smoothed loss 5.13308, grad norm 2.91766, param norm 89.38839
epoch 2, iter 975, loss 4.82090, smoothed loss 5.10879, grad norm 3.25913, param norm 89.50286
epoch 2, iter 980, loss 4.56947, smoothed loss 5.07404, grad norm 3.19952, param norm 89.62105
epoch 2, iter 985, loss 5.68629, smoothed loss 5.05252, grad norm 4.70255, param norm 89.73706
epoch 2, iter 990, loss 4.64975, smoothed loss 5.04270, grad norm 3.29041, param norm 89.83096
epoch 2, iter 995, loss 5.06771, smoothed loss 5.04179, grad norm 3.54938, param norm 89.94697
epoch 2, iter 1000, loss 4.82781, smoothed loss 5.02950, grad norm 3.70653, param norm 90.07062
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 2, Iter 1000, dev loss: 4.545556
Calculating Train F1/EM...
F1 train: 1000 examples took 18.62330 seconds [Score: 0.47907]
Exact Match train: 1000 examples took 19.14748 seconds [Score: 0.31800]
Epoch 2, Iter 1000, Train F1 score: 0.479068, Train EM score: 0.318000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.23819 seconds [Score: 0.44053]
Exact Match dev: 7118 examples took 125.48808 seconds [Score: 0.30669]
Epoch 2, Iter 1000, Dev F1 score: 0.440535, Dev EM score: 0.306687
End of epoch 2
epoch 2, iter 1005, loss 5.65844, smoothed loss 5.02812, grad norm 4.10086, param norm 90.19312
epoch 2, iter 1010, loss 4.80406, smoothed loss 5.00565, grad norm 3.49005, param norm 90.29713
epoch 2, iter 1015, loss 4.89088, smoothed loss 5.00173, grad norm 3.24926, param norm 90.39590
epoch 2, iter 1020, loss 4.86470, smoothed loss 4.99451, grad norm 3.94625, param norm 90.50333
epoch 2, iter 1025, loss 4.66185, smoothed loss 4.97988, grad norm 3.48097, param norm 90.61471
epoch 2, iter 1030, loss 4.70341, smoothed loss 4.96669, grad norm 3.14049, param norm 90.72543
epoch 2, iter 1035, loss 4.47266, smoothed loss 4.94242, grad norm 3.20449, param norm 90.82937
epoch 2, iter 1040, loss 5.25333, smoothed loss 4.93494, grad norm 3.90675, param norm 90.92738
epoch 2, iter 1045, loss 4.16091, smoothed loss 4.92496, grad norm 3.66476, param norm 91.02517
epoch 2, iter 1050, loss 4.88382, smoothed loss 4.91169, grad norm 3.70905, param norm 91.13408
epoch 2, iter 1055, loss 4.95860, smoothed loss 4.89108, grad norm 3.79723, param norm 91.25173
epoch 2, iter 1060, loss 4.05596, smoothed loss 4.86595, grad norm 3.43854, param norm 91.34946
epoch 2, iter 1065, loss 4.57602, smoothed loss 4.85118, grad norm 4.21051, param norm 91.45464
epoch 2, iter 1070, loss 4.05003, smoothed loss 4.85354, grad norm 3.32295, param norm 91.55249
epoch 2, iter 1075, loss 4.46279, smoothed loss 4.82895, grad norm 3.29892, param norm 91.65114
epoch 2, iter 1080, loss 5.30485, smoothed loss 4.81523, grad norm 3.57414, param norm 91.73168
epoch 2, iter 1085, loss 4.82825, smoothed loss 4.79805, grad norm 3.48371, param norm 91.80718
epoch 2, iter 1090, loss 3.91350, smoothed loss 4.77307, grad norm 3.86646, param norm 91.90835
epoch 2, iter 1095, loss 4.71460, smoothed loss 4.76528, grad norm 4.22334, param norm 92.01556
epoch 2, iter 1100, loss 4.24971, smoothed loss 4.74281, grad norm 3.21580, param norm 92.11507
epoch 2, iter 1105, loss 4.72684, smoothed loss 4.74127, grad norm 3.26390, param norm 92.21933
epoch 2, iter 1110, loss 3.78473, smoothed loss 4.71609, grad norm 3.26841, param norm 92.32336
epoch 2, iter 1115, loss 4.26323, smoothed loss 4.69910, grad norm 3.47538, param norm 92.42625
epoch 2, iter 1120, loss 3.71330, smoothed loss 4.68834, grad norm 3.48570, param norm 92.52728
epoch 2, iter 1125, loss 4.59612, smoothed loss 4.67866, grad norm 3.27887, param norm 92.62452
epoch 2, iter 1130, loss 4.25857, smoothed loss 4.65404, grad norm 3.15265, param norm 92.72536
epoch 2, iter 1135, loss 4.25382, smoothed loss 4.65968, grad norm 3.07888, param norm 92.83093
epoch 2, iter 1140, loss 4.56768, smoothed loss 4.64349, grad norm 3.41606, param norm 92.93270
epoch 2, iter 1145, loss 3.67237, smoothed loss 4.61258, grad norm 3.61553, param norm 93.03693
epoch 2, iter 1150, loss 3.79655, smoothed loss 4.60202, grad norm 3.52362, param norm 93.12907
epoch 2, iter 1155, loss 3.65480, smoothed loss 4.58694, grad norm 3.37344, param norm 93.20850
epoch 2, iter 1160, loss 4.82186, smoothed loss 4.58260, grad norm 3.51713, param norm 93.28521
epoch 2, iter 1165, loss 4.99070, smoothed loss 4.58206, grad norm 3.81139, param norm 93.36343
epoch 2, iter 1170, loss 4.70646, smoothed loss 4.57675, grad norm 3.53390, param norm 93.46439
epoch 2, iter 1175, loss 3.89157, smoothed loss 4.55943, grad norm 3.06624, param norm 93.56929
epoch 2, iter 1180, loss 4.12370, smoothed loss 4.53248, grad norm 3.38314, param norm 93.66143
epoch 2, iter 1185, loss 5.00332, smoothed loss 4.52905, grad norm 4.08089, param norm 93.73439
epoch 2, iter 1190, loss 4.44926, smoothed loss 4.52913, grad norm 3.54079, param norm 93.80178
epoch 2, iter 1195, loss 4.43166, smoothed loss 4.51615, grad norm 3.71635, param norm 93.89871
epoch 2, iter 1200, loss 5.22759, smoothed loss 4.51595, grad norm 3.84155, param norm 93.99554
epoch 2, iter 1205, loss 3.36271, smoothed loss 4.49214, grad norm 3.03043, param norm 94.07363
epoch 2, iter 1210, loss 4.39580, smoothed loss 4.48284, grad norm 3.49548, param norm 94.17422
epoch 2, iter 1215, loss 4.57685, smoothed loss 4.46067, grad norm 3.93970, param norm 94.27026
epoch 2, iter 1220, loss 4.46633, smoothed loss 4.44928, grad norm 3.25000, param norm 94.33751
epoch 2, iter 1225, loss 4.08994, smoothed loss 4.43661, grad norm 3.16722, param norm 94.40990
epoch 2, iter 1230, loss 3.89806, smoothed loss 4.42823, grad norm 3.61843, param norm 94.50958
epoch 2, iter 1235, loss 4.25341, smoothed loss 4.41561, grad norm 4.04846, param norm 94.60548
epoch 2, iter 1240, loss 3.96992, smoothed loss 4.41770, grad norm 3.09999, param norm 94.68304
epoch 2, iter 1245, loss 3.53267, smoothed loss 4.39637, grad norm 3.11772, param norm 94.75961
epoch 2, iter 1250, loss 4.26391, smoothed loss 4.39107, grad norm 3.12089, param norm 94.84039
epoch 2, iter 1255, loss 4.05558, smoothed loss 4.37624, grad norm 3.28354, param norm 94.92779
epoch 2, iter 1260, loss 4.03978, smoothed loss 4.37389, grad norm 3.50960, param norm 95.01713
epoch 2, iter 1265, loss 5.02253, smoothed loss 4.38139, grad norm 3.70805, param norm 95.10784
epoch 2, iter 1270, loss 4.43165, smoothed loss 4.36577, grad norm 3.35488, param norm 95.19777
epoch 2, iter 1275, loss 3.93841, smoothed loss 4.34756, grad norm 3.28221, param norm 95.28577
epoch 2, iter 1280, loss 4.22917, smoothed loss 4.32322, grad norm 3.66234, param norm 95.37636
epoch 2, iter 1285, loss 4.03870, smoothed loss 4.30784, grad norm 3.60362, param norm 95.47202
epoch 2, iter 1290, loss 2.75488, smoothed loss 4.27980, grad norm 3.23257, param norm 95.56302
epoch 2, iter 1295, loss 3.58095, smoothed loss 4.26188, grad norm 3.28263, param norm 95.64475
epoch 2, iter 1300, loss 3.47383, smoothed loss 4.23801, grad norm 3.84736, param norm 95.71448
epoch 2, iter 1305, loss 3.73933, smoothed loss 4.22519, grad norm 3.34023, param norm 95.79534
epoch 2, iter 1310, loss 4.34670, smoothed loss 4.20523, grad norm 3.77754, param norm 95.88596
epoch 2, iter 1315, loss 3.73054, smoothed loss 4.19520, grad norm 3.93970, param norm 95.97707
epoch 2, iter 1320, loss 4.35323, smoothed loss 4.18680, grad norm 4.27972, param norm 96.04437
epoch 2, iter 1325, loss 4.02057, smoothed loss 4.18227, grad norm 3.15819, param norm 96.11577
epoch 2, iter 1330, loss 3.57258, smoothed loss 4.16426, grad norm 2.97990, param norm 96.18591
epoch 2, iter 1335, loss 3.67976, smoothed loss 4.15875, grad norm 3.31097, param norm 96.26336
epoch 2, iter 1340, loss 3.89974, smoothed loss 4.16223, grad norm 3.38157, param norm 96.34013
epoch 2, iter 1345, loss 3.84234, smoothed loss 4.15200, grad norm 3.22082, param norm 96.42114
epoch 2, iter 1350, loss 5.08283, smoothed loss 4.15092, grad norm 3.85189, param norm 96.48447
epoch 2, iter 1355, loss 3.59745, smoothed loss 4.13914, grad norm 3.32391, param norm 96.54314
epoch 2, iter 1360, loss 4.11484, smoothed loss 4.14539, grad norm 3.48578, param norm 96.61565
epoch 2, iter 1365, loss 4.16550, smoothed loss 4.14212, grad norm 3.13632, param norm 96.68826
epoch 2, iter 1370, loss 3.41184, smoothed loss 4.11647, grad norm 3.13163, param norm 96.75750
epoch 2, iter 1375, loss 3.48349, smoothed loss 4.10445, grad norm 3.01655, param norm 96.83387
epoch 2, iter 1380, loss 4.24790, smoothed loss 4.09980, grad norm 3.50908, param norm 96.89855
epoch 2, iter 1385, loss 4.10091, smoothed loss 4.09029, grad norm 3.69076, param norm 96.97041
epoch 2, iter 1390, loss 4.04284, smoothed loss 4.07358, grad norm 3.44233, param norm 97.05035
epoch 2, iter 1395, loss 4.12080, smoothed loss 4.06079, grad norm 3.22217, param norm 97.12892
epoch 2, iter 1400, loss 4.12028, smoothed loss 4.04729, grad norm 3.76655, param norm 97.19785
epoch 2, iter 1405, loss 3.87825, smoothed loss 4.04611, grad norm 3.35977, param norm 97.26203
epoch 2, iter 1410, loss 4.04309, smoothed loss 4.03937, grad norm 3.01460, param norm 97.32313
epoch 2, iter 1415, loss 4.05835, smoothed loss 4.03142, grad norm 3.36099, param norm 97.39027
epoch 2, iter 1420, loss 3.97368, smoothed loss 4.01981, grad norm 3.43744, param norm 97.45444
epoch 2, iter 1425, loss 4.43632, smoothed loss 4.01949, grad norm 4.42636, param norm 97.52142
epoch 2, iter 1430, loss 3.74064, smoothed loss 4.01627, grad norm 3.51252, param norm 97.59973
epoch 2, iter 1435, loss 4.73485, smoothed loss 4.02901, grad norm 4.93859, param norm 97.67091
epoch 2, iter 1440, loss 3.40824, smoothed loss 4.00649, grad norm 3.11326, param norm 97.74313
epoch 2, iter 1445, loss 3.49775, smoothed loss 3.99417, grad norm 3.33395, param norm 97.82350
epoch 2, iter 1450, loss 3.57786, smoothed loss 3.98938, grad norm 3.27531, param norm 97.90981
epoch 2, iter 1455, loss 3.57619, smoothed loss 3.96408, grad norm 3.39817, param norm 98.01054
epoch 2, iter 1460, loss 3.98732, smoothed loss 3.95787, grad norm 3.42760, param norm 98.09621
epoch 2, iter 1465, loss 4.34961, smoothed loss 3.96502, grad norm 3.82998, param norm 98.16393
epoch 2, iter 1470, loss 3.68601, smoothed loss 3.95743, grad norm 2.78249, param norm 98.22254
epoch 2, iter 1475, loss 3.82974, smoothed loss 3.95615, grad norm 3.37001, param norm 98.29023
epoch 2, iter 1480, loss 4.10815, smoothed loss 3.94967, grad norm 3.53327, param norm 98.38335
epoch 2, iter 1485, loss 3.64580, smoothed loss 3.92672, grad norm 3.35720, param norm 98.47385
epoch 2, iter 1490, loss 3.51334, smoothed loss 3.92639, grad norm 3.50826, param norm 98.55032
epoch 2, iter 1495, loss 4.16411, smoothed loss 3.92979, grad norm 3.37583, param norm 98.62639
epoch 2, iter 1500, loss 3.71421, smoothed loss 3.92561, grad norm 3.49508, param norm 98.70271
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 2, Iter 1500, dev loss: 3.620498
Calculating Train F1/EM...
F1 train: 1000 examples took 18.47541 seconds [Score: 0.57405]
Exact Match train: 1000 examples took 18.35575 seconds [Score: 0.45500]
Epoch 2, Iter 1500, Train F1 score: 0.574047, Train EM score: 0.455000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.70132 seconds [Score: 0.55543]
Exact Match dev: 7118 examples took 124.19505 seconds [Score: 0.40475]
Epoch 2, Iter 1500, Dev F1 score: 0.555428, Dev EM score: 0.404749
End of epoch 2
epoch 2, iter 1505, loss 3.69924, smoothed loss 3.90736, grad norm 3.74598, param norm 98.77489
epoch 2, iter 1510, loss 3.84294, smoothed loss 3.88676, grad norm 3.57129, param norm 98.83740
epoch 2, iter 1515, loss 3.90483, smoothed loss 3.89795, grad norm 3.75951, param norm 98.88001
epoch 2, iter 1520, loss 4.04730, smoothed loss 3.90411, grad norm 3.34433, param norm 98.92779
epoch 2, iter 1525, loss 4.01953, smoothed loss 3.90740, grad norm 3.34340, param norm 98.99226
epoch 2, iter 1530, loss 4.30742, smoothed loss 3.91495, grad norm 3.46150, param norm 99.06158
epoch 2, iter 1535, loss 3.65430, smoothed loss 3.90462, grad norm 3.24109, param norm 99.13276
epoch 2, iter 1540, loss 4.01195, smoothed loss 3.91522, grad norm 2.94802, param norm 99.20901
epoch 2, iter 1545, loss 3.43552, smoothed loss 3.90795, grad norm 3.39531, param norm 99.28197
epoch 2, iter 1550, loss 4.57153, smoothed loss 3.91026, grad norm 4.13922, param norm 99.34913
epoch 2, iter 1555, loss 3.69008, smoothed loss 3.90050, grad norm 3.40445, param norm 99.40657
epoch 2, iter 1560, loss 4.06302, smoothed loss 3.89949, grad norm 3.65063, param norm 99.46633
epoch 2, iter 1565, loss 3.67486, smoothed loss 3.88245, grad norm 3.19736, param norm 99.52745
epoch 2, iter 1570, loss 4.18701, smoothed loss 3.89652, grad norm 3.34567, param norm 99.57766
epoch 2, iter 1575, loss 4.50227, smoothed loss 3.90260, grad norm 3.67307, param norm 99.62771
epoch 2, iter 1580, loss 3.43809, smoothed loss 3.89384, grad norm 3.12390, param norm 99.68733
epoch 2, iter 1585, loss 5.02013, smoothed loss 3.90545, grad norm 4.74396, param norm 99.75657
epoch 2, iter 1590, loss 3.35479, smoothed loss 3.89649, grad norm 3.01705, param norm 99.82197
epoch 2, iter 1595, loss 3.53799, smoothed loss 3.88789, grad norm 3.21818, param norm 99.89237
epoch 2, iter 1600, loss 3.78365, smoothed loss 3.88222, grad norm 3.36944, param norm 99.95612
epoch 2, iter 1605, loss 3.97335, smoothed loss 3.88024, grad norm 3.82503, param norm 100.01537
epoch 2, iter 1610, loss 3.40423, smoothed loss 3.88098, grad norm 3.06744, param norm 100.07039
epoch 2, iter 1615, loss 2.67200, smoothed loss 3.86847, grad norm 2.73101, param norm 100.12859
epoch 2, iter 1620, loss 4.29908, smoothed loss 3.87853, grad norm 3.26813, param norm 100.19787
epoch 2, iter 1625, loss 4.06773, smoothed loss 3.89032, grad norm 3.29648, param norm 100.25787
epoch 2, iter 1630, loss 4.14699, smoothed loss 3.87815, grad norm 3.81905, param norm 100.32348
epoch 2, iter 1635, loss 2.73251, smoothed loss 3.85798, grad norm 3.36870, param norm 100.39414
epoch 2, iter 1640, loss 4.10404, smoothed loss 3.84974, grad norm 3.67041, param norm 100.46588
epoch 2, iter 1645, loss 3.87365, smoothed loss 3.85144, grad norm 3.47607, param norm 100.53349
epoch 2, iter 1650, loss 4.28286, smoothed loss 3.85333, grad norm 3.41065, param norm 100.59750
epoch 2, iter 1655, loss 3.59893, smoothed loss 3.84488, grad norm 3.33720, param norm 100.65695
epoch 2, iter 1660, loss 4.71626, smoothed loss 3.86322, grad norm 3.28885, param norm 100.71777
epoch 2, iter 1665, loss 3.42657, smoothed loss 3.85016, grad norm 3.13249, param norm 100.78117
epoch 2, iter 1670, loss 3.38568, smoothed loss 3.83826, grad norm 3.06834, param norm 100.84641
epoch 2, iter 1675, loss 2.98855, smoothed loss 3.81192, grad norm 3.12450, param norm 100.91410
epoch 2, iter 1680, loss 3.70407, smoothed loss 3.81089, grad norm 3.44953, param norm 100.98887
epoch 2, iter 1685, loss 3.52868, smoothed loss 3.81392, grad norm 3.34857, param norm 101.05340
epoch 2, iter 1690, loss 4.04203, smoothed loss 3.80610, grad norm 3.34624, param norm 101.12059
epoch 2, iter 1695, loss 3.70747, smoothed loss 3.79531, grad norm 3.61099, param norm 101.18775
epoch 2, iter 1700, loss 3.92840, smoothed loss 3.79073, grad norm 3.13891, param norm 101.24731
epoch 2, iter 1705, loss 3.56794, smoothed loss 3.79624, grad norm 3.09823, param norm 101.30305
epoch 2, iter 1710, loss 3.43827, smoothed loss 3.78691, grad norm 3.65005, param norm 101.36316
epoch 2, iter 1715, loss 3.79533, smoothed loss 3.77328, grad norm 3.45862, param norm 101.43011
epoch 2, iter 1720, loss 3.10458, smoothed loss 3.76739, grad norm 2.98824, param norm 101.48914
epoch 2, iter 1725, loss 3.52879, smoothed loss 3.75996, grad norm 3.03283, param norm 101.54324
epoch 2, iter 1730, loss 3.50412, smoothed loss 3.74408, grad norm 3.13168, param norm 101.60178
epoch 2, iter 1735, loss 3.69522, smoothed loss 3.72374, grad norm 3.38193, param norm 101.66354
epoch 2, iter 1740, loss 3.78177, smoothed loss 3.73079, grad norm 3.42701, param norm 101.72592
epoch 2, iter 1745, loss 3.37317, smoothed loss 3.73532, grad norm 3.18584, param norm 101.78780
epoch 2, iter 1750, loss 3.84903, smoothed loss 3.73028, grad norm 3.43058, param norm 101.85278
epoch 2, iter 1755, loss 3.41459, smoothed loss 3.72623, grad norm 3.19915, param norm 101.91767
epoch 2, iter 1760, loss 4.13348, smoothed loss 3.73033, grad norm 3.16034, param norm 101.98523
epoch 2, iter 1765, loss 4.91670, smoothed loss 3.75035, grad norm 4.64018, param norm 102.04368
epoch 2, iter 1770, loss 4.19653, smoothed loss 3.77179, grad norm 3.65940, param norm 102.10375
epoch 2, iter 1775, loss 3.77059, smoothed loss 3.77203, grad norm 3.14156, param norm 102.17104
epoch 2, iter 1780, loss 3.21299, smoothed loss 3.76427, grad norm 3.44307, param norm 102.23775
epoch 2, iter 1785, loss 3.59280, smoothed loss 3.75950, grad norm 3.24093, param norm 102.29907
epoch 2, iter 1790, loss 3.16007, smoothed loss 3.75159, grad norm 2.97307, param norm 102.35442
epoch 2, iter 1795, loss 3.99402, smoothed loss 3.74822, grad norm 3.04253, param norm 102.40910
epoch 2, iter 1800, loss 3.66274, smoothed loss 3.73943, grad norm 3.89594, param norm 102.46411
epoch 2, iter 1805, loss 4.07294, smoothed loss 3.73902, grad norm 3.13735, param norm 102.51912
epoch 2, iter 1810, loss 3.32868, smoothed loss 3.73236, grad norm 2.99369, param norm 102.58204
epoch 2, iter 1815, loss 3.52584, smoothed loss 3.72272, grad norm 3.45698, param norm 102.64860
epoch 2, iter 1820, loss 2.73058, smoothed loss 3.72399, grad norm 3.08683, param norm 102.71283
epoch 2, iter 1825, loss 3.87821, smoothed loss 3.72363, grad norm 3.79830, param norm 102.78477
epoch 2, iter 1830, loss 3.64180, smoothed loss 3.72059, grad norm 3.05506, param norm 102.85229
epoch 2, iter 1835, loss 3.88714, smoothed loss 3.71821, grad norm 3.33728, param norm 102.90888
epoch 2, iter 1840, loss 3.67445, smoothed loss 3.70751, grad norm 3.05894, param norm 102.96419
epoch 2, iter 1845, loss 4.29492, smoothed loss 3.70824, grad norm 3.50243, param norm 103.01811
epoch 2, iter 1850, loss 3.56640, smoothed loss 3.69724, grad norm 2.88151, param norm 103.06978
epoch 2, iter 1855, loss 3.75871, smoothed loss 3.68542, grad norm 3.37020, param norm 103.12119
epoch 2, iter 1860, loss 3.72910, smoothed loss 3.66877, grad norm 3.70598, param norm 103.18279
epoch 2, iter 1865, loss 4.10265, smoothed loss 3.68068, grad norm 3.66302, param norm 103.24514
epoch 2, iter 1870, loss 3.84753, smoothed loss 3.67914, grad norm 3.17140, param norm 103.30331
epoch 2, iter 1875, loss 3.54608, smoothed loss 3.66085, grad norm 3.54623, param norm 103.36145
epoch 2, iter 1880, loss 2.99274, smoothed loss 3.63945, grad norm 3.21104, param norm 103.41756
epoch 2, iter 1885, loss 2.59965, smoothed loss 3.63430, grad norm 2.98758, param norm 103.46828
epoch 3, iter 1890, loss 3.92196, smoothed loss 3.63437, grad norm 3.71900, param norm 103.52731
epoch 3, iter 1895, loss 4.88646, smoothed loss 3.63548, grad norm 4.03732, param norm 103.58318
epoch 3, iter 1900, loss 3.36663, smoothed loss 3.63099, grad norm 3.34730, param norm 103.63828
epoch 3, iter 1905, loss 3.46474, smoothed loss 3.61701, grad norm 3.36996, param norm 103.69995
epoch 3, iter 1910, loss 3.92143, smoothed loss 3.61785, grad norm 3.70695, param norm 103.76279
epoch 3, iter 1915, loss 3.48644, smoothed loss 3.62400, grad norm 3.57010, param norm 103.81643
epoch 3, iter 1920, loss 3.62674, smoothed loss 3.60981, grad norm 3.47419, param norm 103.87237
epoch 3, iter 1925, loss 4.23584, smoothed loss 3.63331, grad norm 3.12975, param norm 103.92874
epoch 3, iter 1930, loss 3.00204, smoothed loss 3.61166, grad norm 2.96487, param norm 103.98593
epoch 3, iter 1935, loss 2.67742, smoothed loss 3.60031, grad norm 3.19387, param norm 104.05927
epoch 3, iter 1940, loss 3.59822, smoothed loss 3.58306, grad norm 3.87915, param norm 104.13411
epoch 3, iter 1945, loss 4.36262, smoothed loss 3.58534, grad norm 3.88486, param norm 104.19738
epoch 3, iter 1950, loss 4.54233, smoothed loss 3.58411, grad norm 3.77849, param norm 104.25800
epoch 3, iter 1955, loss 3.92036, smoothed loss 3.57437, grad norm 3.41073, param norm 104.31364
epoch 3, iter 1960, loss 3.62728, smoothed loss 3.56505, grad norm 3.27987, param norm 104.37404
epoch 3, iter 1965, loss 3.07369, smoothed loss 3.55396, grad norm 3.03047, param norm 104.43522
epoch 3, iter 1970, loss 3.19467, smoothed loss 3.55302, grad norm 3.12375, param norm 104.48934
epoch 3, iter 1975, loss 3.15111, smoothed loss 3.56061, grad norm 2.89392, param norm 104.53793
epoch 3, iter 1980, loss 3.90252, smoothed loss 3.56107, grad norm 3.18471, param norm 104.59956
epoch 3, iter 1985, loss 4.51002, smoothed loss 3.56410, grad norm 3.49509, param norm 104.66356
epoch 3, iter 1990, loss 2.58710, smoothed loss 3.54709, grad norm 2.97314, param norm 104.71597
epoch 3, iter 1995, loss 3.84861, smoothed loss 3.54204, grad norm 3.88140, param norm 104.76639
epoch 3, iter 2000, loss 3.98193, smoothed loss 3.55102, grad norm 3.63676, param norm 104.81933
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 2000, dev loss: 3.348911
Calculating Train F1/EM...
F1 train: 1000 examples took 18.43750 seconds [Score: 0.67806]
Exact Match train: 1000 examples took 18.95250 seconds [Score: 0.52300]
Epoch 3, Iter 2000, Train F1 score: 0.678060, Train EM score: 0.523000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.49748 seconds [Score: 0.59379]
Exact Match dev: 7118 examples took 124.93275 seconds [Score: 0.44226]
Epoch 3, Iter 2000, Dev F1 score: 0.593788, Dev EM score: 0.442259
End of epoch 3
epoch 3, iter 2005, loss 2.96947, smoothed loss 3.55335, grad norm 2.90778, param norm 104.88548
epoch 3, iter 2010, loss 3.90654, smoothed loss 3.56173, grad norm 3.18877, param norm 104.94666
epoch 3, iter 2015, loss 3.10840, smoothed loss 3.55905, grad norm 3.32470, param norm 104.99770
epoch 3, iter 2020, loss 3.20711, smoothed loss 3.56080, grad norm 3.10940, param norm 105.04703
epoch 3, iter 2025, loss 3.37954, smoothed loss 3.55987, grad norm 3.15695, param norm 105.10366
epoch 3, iter 2030, loss 3.54261, smoothed loss 3.56414, grad norm 3.41221, param norm 105.16172
epoch 3, iter 2035, loss 2.69657, smoothed loss 3.55622, grad norm 2.82144, param norm 105.21894
epoch 3, iter 2040, loss 2.97284, smoothed loss 3.55720, grad norm 2.75150, param norm 105.28240
epoch 3, iter 2045, loss 3.84623, smoothed loss 3.55588, grad norm 3.17359, param norm 105.33397
epoch 3, iter 2050, loss 3.73492, smoothed loss 3.56083, grad norm 2.89018, param norm 105.37926
epoch 3, iter 2055, loss 3.72769, smoothed loss 3.56352, grad norm 3.19310, param norm 105.43132
epoch 3, iter 2060, loss 2.84093, smoothed loss 3.53636, grad norm 2.68774, param norm 105.49152
epoch 3, iter 2065, loss 2.67442, smoothed loss 3.53429, grad norm 3.15948, param norm 105.55307
epoch 3, iter 2070, loss 3.19659, smoothed loss 3.52982, grad norm 3.13673, param norm 105.60864
epoch 3, iter 2075, loss 2.77223, smoothed loss 3.52113, grad norm 3.04514, param norm 105.66727
epoch 3, iter 2080, loss 3.71904, smoothed loss 3.53836, grad norm 3.21495, param norm 105.71898
epoch 3, iter 2085, loss 3.58982, smoothed loss 3.52830, grad norm 3.37672, param norm 105.77092
epoch 3, iter 2090, loss 4.65125, smoothed loss 3.53931, grad norm 3.85223, param norm 105.83112
epoch 3, iter 2095, loss 3.12228, smoothed loss 3.51028, grad norm 3.22091, param norm 105.89594
epoch 3, iter 2100, loss 2.95047, smoothed loss 3.49449, grad norm 3.20938, param norm 105.95483
epoch 3, iter 2105, loss 4.65782, smoothed loss 3.50471, grad norm 4.18376, param norm 106.00797
epoch 3, iter 2110, loss 3.44027, smoothed loss 3.50894, grad norm 3.38245, param norm 106.06367
epoch 3, iter 2115, loss 3.22833, smoothed loss 3.48898, grad norm 3.17133, param norm 106.12347
epoch 3, iter 2120, loss 3.53554, smoothed loss 3.49937, grad norm 3.22859, param norm 106.17769
epoch 3, iter 2125, loss 3.80205, smoothed loss 3.50869, grad norm 3.54667, param norm 106.23403
epoch 3, iter 2130, loss 3.22877, smoothed loss 3.50875, grad norm 3.32061, param norm 106.29932
epoch 3, iter 2135, loss 3.47601, smoothed loss 3.51118, grad norm 3.51071, param norm 106.35169
epoch 3, iter 2140, loss 3.20555, smoothed loss 3.51790, grad norm 3.23442, param norm 106.39871
epoch 3, iter 2145, loss 3.96072, smoothed loss 3.52141, grad norm 3.36712, param norm 106.44789
epoch 3, iter 2150, loss 3.65660, smoothed loss 3.53710, grad norm 3.00050, param norm 106.51233
epoch 3, iter 2155, loss 3.87442, smoothed loss 3.54167, grad norm 3.70828, param norm 106.58110
epoch 3, iter 2160, loss 3.03618, smoothed loss 3.53497, grad norm 3.37492, param norm 106.64310
epoch 3, iter 2165, loss 3.32304, smoothed loss 3.54365, grad norm 3.41006, param norm 106.70245
epoch 3, iter 2170, loss 2.91738, smoothed loss 3.53084, grad norm 2.92211, param norm 106.76139
epoch 3, iter 2175, loss 2.86387, smoothed loss 3.52067, grad norm 3.26642, param norm 106.82440
epoch 3, iter 2180, loss 3.86395, smoothed loss 3.51659, grad norm 3.42675, param norm 106.88879
epoch 3, iter 2185, loss 4.42101, smoothed loss 3.50696, grad norm 4.15001, param norm 106.94199
epoch 3, iter 2190, loss 3.18150, smoothed loss 3.49980, grad norm 3.28220, param norm 106.98997
epoch 3, iter 2195, loss 3.18512, smoothed loss 3.50517, grad norm 2.96612, param norm 107.04153
epoch 3, iter 2200, loss 3.44839, smoothed loss 3.50260, grad norm 3.30760, param norm 107.09621
epoch 3, iter 2205, loss 3.50216, smoothed loss 3.51107, grad norm 3.22507, param norm 107.14742
epoch 3, iter 2210, loss 3.37236, smoothed loss 3.50349, grad norm 3.26087, param norm 107.20094
epoch 3, iter 2215, loss 3.04634, smoothed loss 3.49826, grad norm 3.02687, param norm 107.26233
epoch 3, iter 2220, loss 2.73465, smoothed loss 3.49020, grad norm 3.01690, param norm 107.31843
epoch 3, iter 2225, loss 3.77805, smoothed loss 3.48714, grad norm 3.83160, param norm 107.37189
epoch 3, iter 2230, loss 4.27908, smoothed loss 3.48563, grad norm 3.59513, param norm 107.42194
epoch 3, iter 2235, loss 2.71401, smoothed loss 3.47286, grad norm 3.17356, param norm 107.47552
epoch 3, iter 2240, loss 3.12043, smoothed loss 3.47145, grad norm 3.08351, param norm 107.52569
epoch 3, iter 2245, loss 3.70518, smoothed loss 3.45879, grad norm 3.55935, param norm 107.57822
epoch 3, iter 2250, loss 3.97817, smoothed loss 3.45677, grad norm 3.12391, param norm 107.63093
epoch 3, iter 2255, loss 2.96681, smoothed loss 3.43019, grad norm 3.08776, param norm 107.68494
epoch 3, iter 2260, loss 2.52574, smoothed loss 3.41663, grad norm 3.00928, param norm 107.73799
epoch 3, iter 2265, loss 3.83739, smoothed loss 3.41239, grad norm 3.65798, param norm 107.78774
epoch 3, iter 2270, loss 2.95350, smoothed loss 3.40134, grad norm 3.42346, param norm 107.83472
epoch 3, iter 2275, loss 3.17686, smoothed loss 3.40007, grad norm 2.84215, param norm 107.88931
epoch 3, iter 2280, loss 3.33539, smoothed loss 3.40910, grad norm 3.01082, param norm 107.94537
epoch 3, iter 2285, loss 3.19800, smoothed loss 3.40223, grad norm 3.89465, param norm 108.00910
epoch 3, iter 2290, loss 3.22818, smoothed loss 3.39057, grad norm 3.44182, param norm 108.06882
epoch 3, iter 2295, loss 3.24010, smoothed loss 3.37696, grad norm 3.18518, param norm 108.12373
epoch 3, iter 2300, loss 2.99514, smoothed loss 3.35753, grad norm 3.02538, param norm 108.17730
epoch 3, iter 2305, loss 3.29208, smoothed loss 3.34959, grad norm 3.44625, param norm 108.23472
epoch 3, iter 2310, loss 3.63971, smoothed loss 3.34355, grad norm 3.20404, param norm 108.28924
epoch 3, iter 2315, loss 2.75576, smoothed loss 3.33896, grad norm 2.92255, param norm 108.34063
epoch 3, iter 2320, loss 3.35797, smoothed loss 3.33588, grad norm 2.98340, param norm 108.39497
epoch 3, iter 2325, loss 3.82866, smoothed loss 3.35072, grad norm 3.91480, param norm 108.45413
epoch 3, iter 2330, loss 2.90362, smoothed loss 3.33372, grad norm 3.01500, param norm 108.51106
epoch 3, iter 2335, loss 3.20710, smoothed loss 3.34190, grad norm 3.61509, param norm 108.56326
epoch 3, iter 2340, loss 3.26844, smoothed loss 3.34431, grad norm 3.13374, param norm 108.61341
epoch 3, iter 2345, loss 4.30487, smoothed loss 3.35782, grad norm 3.95100, param norm 108.67351
epoch 3, iter 2350, loss 3.17619, smoothed loss 3.35489, grad norm 3.24769, param norm 108.73125
epoch 3, iter 2355, loss 3.27006, smoothed loss 3.36195, grad norm 3.20454, param norm 108.78012
epoch 3, iter 2360, loss 3.63066, smoothed loss 3.36984, grad norm 3.58663, param norm 108.82423
epoch 3, iter 2365, loss 2.95471, smoothed loss 3.37313, grad norm 3.44138, param norm 108.86921
epoch 3, iter 2370, loss 3.74117, smoothed loss 3.38438, grad norm 3.62249, param norm 108.91950
epoch 3, iter 2375, loss 3.43751, smoothed loss 3.40113, grad norm 2.95758, param norm 108.96760
epoch 3, iter 2380, loss 3.51131, smoothed loss 3.40915, grad norm 3.07291, param norm 109.01488
epoch 3, iter 2385, loss 3.11965, smoothed loss 3.41115, grad norm 2.94899, param norm 109.07215
epoch 3, iter 2390, loss 2.77449, smoothed loss 3.40020, grad norm 3.23884, param norm 109.13141
epoch 3, iter 2395, loss 3.07825, smoothed loss 3.39860, grad norm 3.25268, param norm 109.18082
epoch 3, iter 2400, loss 2.73598, smoothed loss 3.38484, grad norm 2.93423, param norm 109.23297
epoch 3, iter 2405, loss 3.21603, smoothed loss 3.37516, grad norm 3.47906, param norm 109.28484
epoch 3, iter 2410, loss 3.55725, smoothed loss 3.37310, grad norm 3.45408, param norm 109.33934
epoch 3, iter 2415, loss 2.59168, smoothed loss 3.35581, grad norm 2.94613, param norm 109.38240
epoch 3, iter 2420, loss 3.40599, smoothed loss 3.35626, grad norm 3.61850, param norm 109.43747
epoch 3, iter 2425, loss 2.94179, smoothed loss 3.34254, grad norm 3.43154, param norm 109.49939
epoch 3, iter 2430, loss 3.66208, smoothed loss 3.34306, grad norm 3.48568, param norm 109.55692
epoch 3, iter 2435, loss 3.52998, smoothed loss 3.33578, grad norm 3.65895, param norm 109.60725
epoch 3, iter 2440, loss 3.35537, smoothed loss 3.33368, grad norm 3.43446, param norm 109.66183
epoch 3, iter 2445, loss 3.11158, smoothed loss 3.33151, grad norm 2.74487, param norm 109.71817
epoch 3, iter 2450, loss 4.06428, smoothed loss 3.33130, grad norm 3.65241, param norm 109.76905
epoch 3, iter 2455, loss 2.99313, smoothed loss 3.33651, grad norm 3.33932, param norm 109.82136
epoch 3, iter 2460, loss 2.96548, smoothed loss 3.33667, grad norm 3.08940, param norm 109.86864
epoch 3, iter 2465, loss 3.55537, smoothed loss 3.34762, grad norm 3.24326, param norm 109.91486
epoch 3, iter 2470, loss 2.66192, smoothed loss 3.34552, grad norm 2.55767, param norm 109.97083
epoch 3, iter 2475, loss 3.11119, smoothed loss 3.33620, grad norm 2.87957, param norm 110.02745
epoch 3, iter 2480, loss 3.12882, smoothed loss 3.31984, grad norm 3.31613, param norm 110.08894
epoch 3, iter 2485, loss 2.55246, smoothed loss 3.31239, grad norm 3.49034, param norm 110.13814
epoch 3, iter 2490, loss 4.54393, smoothed loss 3.33376, grad norm 3.96233, param norm 110.17801
epoch 3, iter 2495, loss 3.92452, smoothed loss 3.32327, grad norm 3.51468, param norm 110.21382
epoch 3, iter 2500, loss 3.61217, smoothed loss 3.31091, grad norm 3.10293, param norm 110.25978
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 2500, dev loss: 3.247155
Calculating Train F1/EM...
F1 train: 1000 examples took 19.03543 seconds [Score: 0.65083]
Exact Match train: 1000 examples took 19.80614 seconds [Score: 0.52300]
Epoch 3, Iter 2500, Train F1 score: 0.650827, Train EM score: 0.523000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 126.46109 seconds [Score: 0.60281]
Exact Match dev: 7118 examples took 126.24495 seconds [Score: 0.45575]
Epoch 3, Iter 2500, Dev F1 score: 0.602806, Dev EM score: 0.455746
End of epoch 3
epoch 3, iter 2505, loss 3.34423, smoothed loss 3.31624, grad norm 2.86680, param norm 110.30785
epoch 3, iter 2510, loss 3.12171, smoothed loss 3.31595, grad norm 2.86761, param norm 110.36232
epoch 3, iter 2515, loss 2.90239, smoothed loss 3.31116, grad norm 3.21926, param norm 110.41260
epoch 3, iter 2520, loss 3.47000, smoothed loss 3.31708, grad norm 3.40961, param norm 110.45805
epoch 3, iter 2525, loss 3.67876, smoothed loss 3.32077, grad norm 2.95874, param norm 110.50642
epoch 3, iter 2530, loss 3.21412, smoothed loss 3.31642, grad norm 3.32394, param norm 110.55236
epoch 3, iter 2535, loss 3.70989, smoothed loss 3.32953, grad norm 3.19597, param norm 110.59978
epoch 3, iter 2540, loss 3.27003, smoothed loss 3.32163, grad norm 3.06619, param norm 110.64922
epoch 3, iter 2545, loss 3.26553, smoothed loss 3.32237, grad norm 3.25042, param norm 110.69411
epoch 3, iter 2550, loss 2.91153, smoothed loss 3.32406, grad norm 2.62038, param norm 110.73622
epoch 3, iter 2555, loss 3.44505, smoothed loss 3.32852, grad norm 3.10738, param norm 110.77959
epoch 3, iter 2560, loss 4.28360, smoothed loss 3.34263, grad norm 4.01599, param norm 110.82874
epoch 3, iter 2565, loss 3.03472, smoothed loss 3.32454, grad norm 3.25492, param norm 110.88345
epoch 3, iter 2570, loss 3.38966, smoothed loss 3.33894, grad norm 3.74255, param norm 110.94095
epoch 3, iter 2575, loss 3.30915, smoothed loss 3.33069, grad norm 3.45503, param norm 111.00317
epoch 3, iter 2580, loss 2.79450, smoothed loss 3.31278, grad norm 2.83731, param norm 111.06594
epoch 3, iter 2585, loss 3.55021, smoothed loss 3.31122, grad norm 3.40814, param norm 111.11907
epoch 3, iter 2590, loss 2.97093, smoothed loss 3.30866, grad norm 3.30077, param norm 111.16574
epoch 3, iter 2595, loss 2.96044, smoothed loss 3.30067, grad norm 3.29341, param norm 111.21030
epoch 3, iter 2600, loss 3.23907, smoothed loss 3.29889, grad norm 3.25664, param norm 111.26936
epoch 3, iter 2605, loss 4.09893, smoothed loss 3.30915, grad norm 4.03368, param norm 111.32221
epoch 3, iter 2610, loss 2.43507, smoothed loss 3.28933, grad norm 2.98923, param norm 111.37704
epoch 3, iter 2615, loss 3.09008, smoothed loss 3.29775, grad norm 3.19157, param norm 111.43252
epoch 3, iter 2620, loss 3.07828, smoothed loss 3.30410, grad norm 2.85723, param norm 111.48682
epoch 3, iter 2625, loss 3.02346, smoothed loss 3.29743, grad norm 3.44750, param norm 111.53903
epoch 3, iter 2630, loss 3.00837, smoothed loss 3.29423, grad norm 3.26021, param norm 111.58552
epoch 3, iter 2635, loss 3.67441, smoothed loss 3.29352, grad norm 3.39871, param norm 111.62960
epoch 3, iter 2640, loss 3.40617, smoothed loss 3.30329, grad norm 3.09191, param norm 111.67970
epoch 3, iter 2645, loss 3.47692, smoothed loss 3.29048, grad norm 3.02053, param norm 111.73117
epoch 3, iter 2650, loss 2.94005, smoothed loss 3.28643, grad norm 3.16104, param norm 111.78195
epoch 3, iter 2655, loss 3.09049, smoothed loss 3.29011, grad norm 3.39033, param norm 111.83308
epoch 3, iter 2660, loss 3.35402, smoothed loss 3.29711, grad norm 3.61064, param norm 111.88268
epoch 3, iter 2665, loss 3.64417, smoothed loss 3.31235, grad norm 3.18925, param norm 111.93222
epoch 3, iter 2670, loss 3.00776, smoothed loss 3.29737, grad norm 3.49921, param norm 111.98239
epoch 3, iter 2675, loss 2.98065, smoothed loss 3.28738, grad norm 2.96032, param norm 112.03523
epoch 3, iter 2680, loss 3.15857, smoothed loss 3.28244, grad norm 3.00086, param norm 112.08714
epoch 3, iter 2685, loss 3.34416, smoothed loss 3.28418, grad norm 3.49460, param norm 112.13662
epoch 3, iter 2690, loss 2.94459, smoothed loss 3.28788, grad norm 3.17160, param norm 112.18285
epoch 3, iter 2695, loss 3.17299, smoothed loss 3.27660, grad norm 3.26724, param norm 112.23677
epoch 3, iter 2700, loss 3.40014, smoothed loss 3.27253, grad norm 3.14929, param norm 112.28988
epoch 3, iter 2705, loss 3.57926, smoothed loss 3.27050, grad norm 4.04906, param norm 112.33540
epoch 3, iter 2710, loss 3.16823, smoothed loss 3.26036, grad norm 3.40296, param norm 112.38785
epoch 3, iter 2715, loss 2.96998, smoothed loss 3.24598, grad norm 3.62859, param norm 112.44482
epoch 3, iter 2720, loss 3.71106, smoothed loss 3.25823, grad norm 3.98688, param norm 112.49308
epoch 3, iter 2725, loss 3.60529, smoothed loss 3.25210, grad norm 3.10628, param norm 112.53863
epoch 3, iter 2730, loss 3.80988, smoothed loss 3.25577, grad norm 3.44407, param norm 112.57818
epoch 3, iter 2735, loss 3.35747, smoothed loss 3.26168, grad norm 3.09074, param norm 112.62469
epoch 3, iter 2740, loss 3.36578, smoothed loss 3.25738, grad norm 3.53173, param norm 112.67431
epoch 3, iter 2745, loss 3.15490, smoothed loss 3.25477, grad norm 3.29737, param norm 112.72597
epoch 3, iter 2750, loss 2.61680, smoothed loss 3.25035, grad norm 3.05720, param norm 112.77109
epoch 3, iter 2755, loss 2.90273, smoothed loss 3.23627, grad norm 2.93310, param norm 112.82439
epoch 3, iter 2760, loss 3.24627, smoothed loss 3.23781, grad norm 3.49132, param norm 112.87269
epoch 3, iter 2765, loss 2.82874, smoothed loss 3.23584, grad norm 3.13139, param norm 112.93060
epoch 3, iter 2770, loss 2.70010, smoothed loss 3.23578, grad norm 3.52645, param norm 112.98538
epoch 3, iter 2775, loss 2.59043, smoothed loss 3.22458, grad norm 3.12895, param norm 113.03809
epoch 3, iter 2780, loss 3.09972, smoothed loss 3.22528, grad norm 3.70233, param norm 113.08876
epoch 3, iter 2785, loss 2.63492, smoothed loss 3.21647, grad norm 3.13693, param norm 113.14050
epoch 3, iter 2790, loss 2.76395, smoothed loss 3.21349, grad norm 3.00825, param norm 113.18593
epoch 3, iter 2795, loss 3.70071, smoothed loss 3.20141, grad norm 3.32170, param norm 113.23335
epoch 3, iter 2800, loss 3.29150, smoothed loss 3.19804, grad norm 3.06237, param norm 113.28820
epoch 3, iter 2805, loss 3.29969, smoothed loss 3.19322, grad norm 3.22305, param norm 113.34081
epoch 3, iter 2810, loss 3.47060, smoothed loss 3.20907, grad norm 3.36130, param norm 113.39155
epoch 3, iter 2815, loss 3.64973, smoothed loss 3.21110, grad norm 3.96692, param norm 113.43587
epoch 3, iter 2820, loss 3.16573, smoothed loss 3.21109, grad norm 3.00851, param norm 113.48256
epoch 3, iter 2825, loss 3.64760, smoothed loss 3.24616, grad norm 3.34682, param norm 113.53187
epoch 3, iter 2830, loss 3.50424, smoothed loss 3.26037, grad norm 3.32038, param norm 113.58620
epoch 4, iter 2835, loss 2.18029, smoothed loss 3.23995, grad norm 2.91438, param norm 113.64430
epoch 4, iter 2840, loss 2.44063, smoothed loss 3.23188, grad norm 2.96920, param norm 113.70021
epoch 4, iter 2845, loss 2.79993, smoothed loss 3.23790, grad norm 3.41362, param norm 113.75062
epoch 4, iter 2850, loss 3.50432, smoothed loss 3.24427, grad norm 3.38294, param norm 113.80437
epoch 4, iter 2855, loss 3.13259, smoothed loss 3.23670, grad norm 3.51192, param norm 113.85417
epoch 4, iter 2860, loss 3.16849, smoothed loss 3.23510, grad norm 3.72904, param norm 113.90696
epoch 4, iter 2865, loss 3.70851, smoothed loss 3.23912, grad norm 3.60683, param norm 113.95765
epoch 4, iter 2870, loss 3.21338, smoothed loss 3.22750, grad norm 3.63966, param norm 114.00200
epoch 4, iter 2875, loss 3.54798, smoothed loss 3.22985, grad norm 3.92621, param norm 114.05011
epoch 4, iter 2880, loss 3.25390, smoothed loss 3.22383, grad norm 3.36070, param norm 114.10080
epoch 4, iter 2885, loss 3.90297, smoothed loss 3.22615, grad norm 3.76055, param norm 114.15136
epoch 4, iter 2890, loss 2.48581, smoothed loss 3.21291, grad norm 2.89602, param norm 114.20102
epoch 4, iter 2895, loss 3.05174, smoothed loss 3.21334, grad norm 3.36910, param norm 114.24743
epoch 4, iter 2900, loss 2.76123, smoothed loss 3.21662, grad norm 2.93269, param norm 114.29846
epoch 4, iter 2905, loss 2.70268, smoothed loss 3.22094, grad norm 2.74737, param norm 114.35471
epoch 4, iter 2910, loss 3.45144, smoothed loss 3.22098, grad norm 2.93800, param norm 114.41110
epoch 4, iter 2915, loss 3.70480, smoothed loss 3.23118, grad norm 3.70453, param norm 114.46191
epoch 4, iter 2920, loss 2.60715, smoothed loss 3.22480, grad norm 3.08332, param norm 114.50852
epoch 4, iter 2925, loss 2.56871, smoothed loss 3.21432, grad norm 3.01061, param norm 114.56064
epoch 4, iter 2930, loss 3.55078, smoothed loss 3.21516, grad norm 3.96194, param norm 114.61411
epoch 4, iter 2935, loss 3.09959, smoothed loss 3.20426, grad norm 3.57345, param norm 114.66906
epoch 4, iter 2940, loss 3.18546, smoothed loss 3.19697, grad norm 3.88480, param norm 114.71532
epoch 4, iter 2945, loss 3.00640, smoothed loss 3.19115, grad norm 3.20820, param norm 114.75503
epoch 4, iter 2950, loss 3.27388, smoothed loss 3.19998, grad norm 3.19376, param norm 114.79679
epoch 4, iter 2955, loss 3.35264, smoothed loss 3.19932, grad norm 3.11586, param norm 114.84818
epoch 4, iter 2960, loss 2.96057, smoothed loss 3.19883, grad norm 3.05253, param norm 114.90505
epoch 4, iter 2965, loss 2.98959, smoothed loss 3.18981, grad norm 3.41363, param norm 114.95853
epoch 4, iter 2970, loss 5.00492, smoothed loss 3.19346, grad norm 5.19053, param norm 115.01219
epoch 4, iter 2975, loss 3.02549, smoothed loss 3.18847, grad norm 3.20999, param norm 115.05981
epoch 4, iter 2980, loss 2.39333, smoothed loss 3.17600, grad norm 3.08722, param norm 115.11175
epoch 4, iter 2985, loss 2.57925, smoothed loss 3.16901, grad norm 3.28818, param norm 115.16300
epoch 4, iter 2990, loss 2.48576, smoothed loss 3.16427, grad norm 3.04096, param norm 115.21030
epoch 4, iter 2995, loss 2.68286, smoothed loss 3.14448, grad norm 3.37278, param norm 115.26215
epoch 4, iter 3000, loss 3.86366, smoothed loss 3.14761, grad norm 4.17259, param norm 115.31638
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 4, Iter 3000, dev loss: 3.152473
Calculating Train F1/EM...
F1 train: 1000 examples took 17.89907 seconds [Score: 0.71222]
Exact Match train: 1000 examples took 19.16583 seconds [Score: 0.58500]
Epoch 4, Iter 3000, Train F1 score: 0.712218, Train EM score: 0.585000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 126.73797 seconds [Score: 0.61889]
Exact Match dev: 7118 examples took 126.36107 seconds [Score: 0.47162]
Epoch 4, Iter 3000, Dev F1 score: 0.618891, Dev EM score: 0.471621
End of epoch 4
epoch 4, iter 3005, loss 2.75656, smoothed loss 3.15024, grad norm 3.07356, param norm 115.36365
epoch 4, iter 3010, loss 2.51883, smoothed loss 3.13649, grad norm 3.42173, param norm 115.41509
epoch 4, iter 3015, loss 3.35215, smoothed loss 3.13781, grad norm 3.49600, param norm 115.47080
epoch 4, iter 3020, loss 3.11002, smoothed loss 3.13193, grad norm 3.21382, param norm 115.52361
epoch 4, iter 3025, loss 3.78627, smoothed loss 3.13412, grad norm 3.46360, param norm 115.57642
epoch 4, iter 3030, loss 3.52086, smoothed loss 3.14062, grad norm 3.65099, param norm 115.62383
epoch 4, iter 3035, loss 3.36117, smoothed loss 3.13889, grad norm 3.34577, param norm 115.67744
epoch 4, iter 3040, loss 3.19818, smoothed loss 3.14070, grad norm 3.11544, param norm 115.73337
epoch 4, iter 3045, loss 3.58907, smoothed loss 3.14986, grad norm 3.17458, param norm 115.78514
epoch 4, iter 3050, loss 3.40997, smoothed loss 3.16337, grad norm 3.29998, param norm 115.83471
epoch 4, iter 3055, loss 2.97305, smoothed loss 3.15544, grad norm 3.39515, param norm 115.89262
epoch 4, iter 3060, loss 2.86295, smoothed loss 3.13953, grad norm 3.65647, param norm 115.95124
epoch 4, iter 3065, loss 3.06266, smoothed loss 3.14272, grad norm 3.61097, param norm 116.00165
epoch 4, iter 3070, loss 2.99187, smoothed loss 3.13820, grad norm 2.99148, param norm 116.04610
epoch 4, iter 3075, loss 3.46104, smoothed loss 3.12859, grad norm 3.75670, param norm 116.08900
epoch 4, iter 3080, loss 3.44778, smoothed loss 3.12288, grad norm 3.16935, param norm 116.14153
epoch 4, iter 3085, loss 2.51650, smoothed loss 3.10692, grad norm 2.74752, param norm 116.19518
epoch 4, iter 3090, loss 3.21228, smoothed loss 3.11480, grad norm 3.81698, param norm 116.24516
epoch 4, iter 3095, loss 3.40277, smoothed loss 3.11322, grad norm 3.52296, param norm 116.28992
epoch 4, iter 3100, loss 3.22914, smoothed loss 3.10860, grad norm 3.60658, param norm 116.33434
epoch 4, iter 3105, loss 3.68034, smoothed loss 3.11915, grad norm 3.58421, param norm 116.38410
epoch 4, iter 3110, loss 3.19554, smoothed loss 3.11906, grad norm 3.12782, param norm 116.43839
epoch 4, iter 3115, loss 2.91533, smoothed loss 3.12213, grad norm 3.18267, param norm 116.49689
epoch 4, iter 3120, loss 3.85739, smoothed loss 3.12809, grad norm 3.48430, param norm 116.55450
epoch 4, iter 3125, loss 3.30437, smoothed loss 3.13473, grad norm 3.25816, param norm 116.60632
epoch 4, iter 3130, loss 3.59763, smoothed loss 3.14457, grad norm 3.69385, param norm 116.66059
epoch 4, iter 3135, loss 3.15512, smoothed loss 3.16302, grad norm 3.48772, param norm 116.71638
epoch 4, iter 3140, loss 3.35007, smoothed loss 3.15910, grad norm 3.61850, param norm 116.77132
epoch 4, iter 3145, loss 3.45091, smoothed loss 3.16437, grad norm 3.23927, param norm 116.81870
epoch 4, iter 3150, loss 3.04603, smoothed loss 3.17278, grad norm 3.46054, param norm 116.87186
epoch 4, iter 3155, loss 3.13469, smoothed loss 3.16630, grad norm 3.91171, param norm 116.92641
epoch 4, iter 3160, loss 2.15597, smoothed loss 3.14723, grad norm 2.93429, param norm 116.96896
epoch 4, iter 3165, loss 3.44837, smoothed loss 3.14322, grad norm 3.74937, param norm 117.01366
epoch 4, iter 3170, loss 3.28194, smoothed loss 3.13979, grad norm 3.36918, param norm 117.06274
epoch 4, iter 3175, loss 2.37242, smoothed loss 3.12732, grad norm 2.99940, param norm 117.10559
epoch 4, iter 3180, loss 3.09499, smoothed loss 3.13126, grad norm 3.29670, param norm 117.14854
epoch 4, iter 3185, loss 3.10139, smoothed loss 3.14276, grad norm 3.46096, param norm 117.19828
epoch 4, iter 3190, loss 3.19309, smoothed loss 3.15379, grad norm 3.32043, param norm 117.25362
epoch 4, iter 3195, loss 2.88363, smoothed loss 3.14446, grad norm 3.61303, param norm 117.29874
epoch 4, iter 3200, loss 3.48031, smoothed loss 3.13729, grad norm 3.47416, param norm 117.35223
epoch 4, iter 3205, loss 2.43327, smoothed loss 3.12102, grad norm 3.09994, param norm 117.39715
epoch 4, iter 3210, loss 2.61058, smoothed loss 3.12426, grad norm 3.02844, param norm 117.43530
epoch 4, iter 3215, loss 2.58635, smoothed loss 3.13502, grad norm 3.13546, param norm 117.47124
epoch 4, iter 3220, loss 3.27583, smoothed loss 3.12853, grad norm 3.50238, param norm 117.51996
epoch 4, iter 3225, loss 2.86340, smoothed loss 3.12038, grad norm 2.93000, param norm 117.57424
epoch 4, iter 3230, loss 2.75820, smoothed loss 3.10926, grad norm 3.07978, param norm 117.61768
epoch 4, iter 3235, loss 2.83010, smoothed loss 3.11856, grad norm 2.89985, param norm 117.65989
epoch 4, iter 3240, loss 2.83983, smoothed loss 3.11374, grad norm 3.08808, param norm 117.70963
epoch 4, iter 3245, loss 2.45743, smoothed loss 3.10269, grad norm 3.40732, param norm 117.76460
epoch 4, iter 3250, loss 2.62398, smoothed loss 3.08863, grad norm 3.13085, param norm 117.81684
epoch 4, iter 3255, loss 2.74687, smoothed loss 3.08795, grad norm 3.09972, param norm 117.86250
epoch 4, iter 3260, loss 3.29550, smoothed loss 3.07116, grad norm 3.45671, param norm 117.90540
epoch 4, iter 3265, loss 2.91873, smoothed loss 3.07348, grad norm 2.94631, param norm 117.94962
epoch 4, iter 3270, loss 2.70874, smoothed loss 3.06789, grad norm 3.07554, param norm 117.99022
epoch 4, iter 3275, loss 2.70446, smoothed loss 3.05818, grad norm 3.27709, param norm 118.03205
epoch 4, iter 3280, loss 3.42649, smoothed loss 3.05806, grad norm 3.53895, param norm 118.07688
epoch 4, iter 3285, loss 2.26138, smoothed loss 3.04188, grad norm 2.86159, param norm 118.12180
epoch 4, iter 3290, loss 3.26558, smoothed loss 3.04729, grad norm 3.57154, param norm 118.16553
epoch 4, iter 3295, loss 3.26121, smoothed loss 3.04149, grad norm 2.99493, param norm 118.21213
epoch 4, iter 3300, loss 2.85713, smoothed loss 3.04242, grad norm 3.40471, param norm 118.25934
epoch 4, iter 3305, loss 3.01034, smoothed loss 3.03287, grad norm 3.12846, param norm 118.30500
epoch 4, iter 3310, loss 2.68308, smoothed loss 3.02791, grad norm 3.04710, param norm 118.35084
epoch 4, iter 3315, loss 3.66652, smoothed loss 3.04360, grad norm 3.49981, param norm 118.38712
epoch 4, iter 3320, loss 3.11188, smoothed loss 3.04322, grad norm 3.75905, param norm 118.42368
epoch 4, iter 3325, loss 3.20445, smoothed loss 3.06139, grad norm 3.35729, param norm 118.47160
epoch 4, iter 3330, loss 2.44146, smoothed loss 3.05474, grad norm 3.32774, param norm 118.52190
epoch 4, iter 3335, loss 2.94345, smoothed loss 3.06036, grad norm 3.38393, param norm 118.56821
epoch 4, iter 3340, loss 2.91151, smoothed loss 3.05898, grad norm 3.82344, param norm 118.61718
epoch 4, iter 3345, loss 2.55197, smoothed loss 3.04437, grad norm 2.95832, param norm 118.67760
epoch 4, iter 3350, loss 2.66550, smoothed loss 3.02474, grad norm 3.32623, param norm 118.73120
epoch 4, iter 3355, loss 2.68880, smoothed loss 3.02822, grad norm 3.21522, param norm 118.77309
epoch 4, iter 3360, loss 3.75202, smoothed loss 3.03305, grad norm 3.98026, param norm 118.81116
epoch 4, iter 3365, loss 2.98345, smoothed loss 3.03146, grad norm 3.20668, param norm 118.85128
epoch 4, iter 3370, loss 3.24030, smoothed loss 3.03022, grad norm 3.34188, param norm 118.89766
epoch 4, iter 3375, loss 2.43089, smoothed loss 3.03086, grad norm 3.36221, param norm 118.94405
epoch 4, iter 3380, loss 2.89110, smoothed loss 3.03251, grad norm 3.25053, param norm 118.99669
epoch 4, iter 3385, loss 3.15915, smoothed loss 3.02449, grad norm 3.66299, param norm 119.04244
epoch 4, iter 3390, loss 2.72214, smoothed loss 3.01974, grad norm 3.30940, param norm 119.08852
epoch 4, iter 3395, loss 2.88644, smoothed loss 3.02105, grad norm 3.41901, param norm 119.13440
epoch 4, iter 3400, loss 2.72555, smoothed loss 3.00591, grad norm 3.31493, param norm 119.17879
epoch 4, iter 3405, loss 3.81432, smoothed loss 3.02385, grad norm 3.85925, param norm 119.22486
epoch 4, iter 3410, loss 2.46942, smoothed loss 3.01173, grad norm 3.07482, param norm 119.26701
epoch 4, iter 3415, loss 2.98219, smoothed loss 3.01343, grad norm 3.31964, param norm 119.31570
epoch 4, iter 3420, loss 3.43828, smoothed loss 3.01018, grad norm 3.09474, param norm 119.36949
epoch 4, iter 3425, loss 2.83844, smoothed loss 3.00741, grad norm 2.93839, param norm 119.41766
epoch 4, iter 3430, loss 3.06165, smoothed loss 3.01504, grad norm 3.37531, param norm 119.45980
epoch 4, iter 3435, loss 3.14863, smoothed loss 3.02123, grad norm 3.36553, param norm 119.50237
epoch 4, iter 3440, loss 3.20012, smoothed loss 3.03109, grad norm 3.38672, param norm 119.54754
epoch 4, iter 3445, loss 2.61381, smoothed loss 3.03142, grad norm 3.08829, param norm 119.58570
epoch 4, iter 3450, loss 2.43680, smoothed loss 3.02772, grad norm 3.33929, param norm 119.62543
epoch 4, iter 3455, loss 3.46271, smoothed loss 3.03389, grad norm 3.59452, param norm 119.66787
epoch 4, iter 3460, loss 2.89958, smoothed loss 3.03765, grad norm 3.14227, param norm 119.71270
epoch 4, iter 3465, loss 2.55647, smoothed loss 3.03107, grad norm 3.10032, param norm 119.75605
epoch 4, iter 3470, loss 3.39651, smoothed loss 3.03284, grad norm 3.48634, param norm 119.79692
epoch 4, iter 3475, loss 2.99136, smoothed loss 3.03066, grad norm 3.17585, param norm 119.83959
epoch 4, iter 3480, loss 3.52658, smoothed loss 3.03548, grad norm 3.28666, param norm 119.88956
epoch 4, iter 3485, loss 3.00417, smoothed loss 3.04311, grad norm 3.21841, param norm 119.94038
epoch 4, iter 3490, loss 3.10651, smoothed loss 3.04206, grad norm 3.08417, param norm 119.99196
epoch 4, iter 3495, loss 3.00016, smoothed loss 3.04458, grad norm 3.10400, param norm 120.04337
epoch 4, iter 3500, loss 2.58183, smoothed loss 3.04350, grad norm 3.18515, param norm 120.08860
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 4, Iter 3500, dev loss: 3.111211
Calculating Train F1/EM...
F1 train: 1000 examples took 17.67315 seconds [Score: 0.71763]
Exact Match train: 1000 examples took 18.29306 seconds [Score: 0.58700]
Epoch 4, Iter 3500, Train F1 score: 0.717632, Train EM score: 0.587000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 122.33864 seconds [Score: 0.62817]
Exact Match dev: 7118 examples took 121.93023 seconds [Score: 0.47570]
Epoch 4, Iter 3500, Dev F1 score: 0.628166, Dev EM score: 0.475695
End of epoch 4
epoch 4, iter 3505, loss 3.06147, smoothed loss 3.04186, grad norm 3.31296, param norm 120.13649
epoch 4, iter 3510, loss 3.05807, smoothed loss 3.03898, grad norm 3.77462, param norm 120.18665
epoch 4, iter 3515, loss 2.82323, smoothed loss 3.03642, grad norm 3.17484, param norm 120.23360
epoch 4, iter 3520, loss 3.28926, smoothed loss 3.04563, grad norm 3.62712, param norm 120.27769
epoch 4, iter 3525, loss 2.96766, smoothed loss 3.03651, grad norm 3.16788, param norm 120.32310
epoch 4, iter 3530, loss 2.22476, smoothed loss 3.03500, grad norm 2.62634, param norm 120.36297
epoch 4, iter 3535, loss 3.53899, smoothed loss 3.03407, grad norm 3.75597, param norm 120.40086
epoch 4, iter 3540, loss 3.33958, smoothed loss 3.04002, grad norm 3.31566, param norm 120.44483
epoch 4, iter 3545, loss 3.47152, smoothed loss 3.04911, grad norm 3.47783, param norm 120.49035
epoch 4, iter 3550, loss 3.34015, smoothed loss 3.04454, grad norm 3.75354, param norm 120.53314
epoch 4, iter 3555, loss 2.94734, smoothed loss 3.03596, grad norm 3.14595, param norm 120.58144
epoch 4, iter 3560, loss 2.64019, smoothed loss 3.02319, grad norm 3.37168, param norm 120.62386
epoch 4, iter 3565, loss 2.86548, smoothed loss 3.01895, grad norm 3.40048, param norm 120.66630
epoch 4, iter 3570, loss 3.22496, smoothed loss 3.01759, grad norm 3.15076, param norm 120.71449
epoch 4, iter 3575, loss 3.43083, smoothed loss 3.00985, grad norm 3.43228, param norm 120.75968
epoch 4, iter 3580, loss 3.08708, smoothed loss 3.00645, grad norm 3.72196, param norm 120.80854
epoch 4, iter 3585, loss 2.99143, smoothed loss 2.99687, grad norm 3.43938, param norm 120.85725
epoch 4, iter 3590, loss 3.71724, smoothed loss 2.99980, grad norm 3.72375, param norm 120.90372
epoch 4, iter 3595, loss 2.75962, smoothed loss 3.01447, grad norm 2.95016, param norm 120.94798
epoch 4, iter 3600, loss 3.28568, smoothed loss 3.02502, grad norm 3.30103, param norm 120.98799
epoch 4, iter 3605, loss 2.84839, smoothed loss 3.02500, grad norm 3.01658, param norm 121.02917
epoch 4, iter 3610, loss 2.68895, smoothed loss 3.01663, grad norm 2.93224, param norm 121.07540
epoch 4, iter 3615, loss 3.34590, smoothed loss 3.02138, grad norm 3.88007, param norm 121.12058
epoch 4, iter 3620, loss 2.59896, smoothed loss 3.00730, grad norm 3.18999, param norm 121.16444
epoch 4, iter 3625, loss 2.65728, smoothed loss 3.00677, grad norm 3.12539, param norm 121.20962
epoch 4, iter 3630, loss 3.13234, smoothed loss 3.00214, grad norm 3.22365, param norm 121.25479
epoch 4, iter 3635, loss 2.65596, smoothed loss 3.00231, grad norm 3.31843, param norm 121.29978
epoch 4, iter 3640, loss 2.81432, smoothed loss 2.99463, grad norm 3.38361, param norm 121.34804
epoch 4, iter 3645, loss 2.99155, smoothed loss 2.99343, grad norm 2.97706, param norm 121.39717
epoch 4, iter 3650, loss 2.99168, smoothed loss 2.99016, grad norm 3.32208, param norm 121.44322
epoch 4, iter 3655, loss 2.54328, smoothed loss 2.97408, grad norm 2.75522, param norm 121.48540
epoch 4, iter 3660, loss 3.06751, smoothed loss 2.96567, grad norm 3.53104, param norm 121.52949
epoch 4, iter 3665, loss 3.04232, smoothed loss 2.96009, grad norm 3.64822, param norm 121.57976
epoch 4, iter 3670, loss 2.62260, smoothed loss 2.96386, grad norm 2.92609, param norm 121.62456
epoch 4, iter 3675, loss 2.35986, smoothed loss 2.95495, grad norm 3.15592, param norm 121.66566
epoch 4, iter 3680, loss 2.92793, smoothed loss 2.94669, grad norm 3.37079, param norm 121.71238
epoch 4, iter 3685, loss 2.62711, smoothed loss 2.94512, grad norm 2.97728, param norm 121.75962
epoch 4, iter 3690, loss 3.11632, smoothed loss 2.94663, grad norm 2.84428, param norm 121.80473
epoch 4, iter 3695, loss 3.40105, smoothed loss 2.95066, grad norm 3.47671, param norm 121.85196
epoch 4, iter 3700, loss 3.37244, smoothed loss 2.94881, grad norm 3.62733, param norm 121.89727
epoch 4, iter 3705, loss 3.61952, smoothed loss 2.96250, grad norm 3.32983, param norm 121.93697
epoch 4, iter 3710, loss 3.13922, smoothed loss 2.95457, grad norm 3.47886, param norm 121.97444
epoch 4, iter 3715, loss 3.00467, smoothed loss 2.95793, grad norm 3.15507, param norm 122.01485
epoch 4, iter 3720, loss 2.78898, smoothed loss 2.95606, grad norm 2.99602, param norm 122.06192
epoch 4, iter 3725, loss 2.52115, smoothed loss 2.96376, grad norm 3.18126, param norm 122.11308
epoch 4, iter 3730, loss 2.57111, smoothed loss 2.95842, grad norm 2.94269, param norm 122.16045
epoch 4, iter 3735, loss 3.26225, smoothed loss 2.96380, grad norm 3.34857, param norm 122.20966
epoch 4, iter 3740, loss 2.88033, smoothed loss 2.95632, grad norm 3.65534, param norm 122.25267
epoch 4, iter 3745, loss 2.49306, smoothed loss 2.95113, grad norm 3.17025, param norm 122.29612
epoch 4, iter 3750, loss 3.28377, smoothed loss 2.96047, grad norm 3.42087, param norm 122.33730
epoch 4, iter 3755, loss 3.35289, smoothed loss 2.97554, grad norm 3.36295, param norm 122.38178
epoch 4, iter 3760, loss 2.96398, smoothed loss 2.97518, grad norm 3.08580, param norm 122.42651
epoch 4, iter 3765, loss 3.51145, smoothed loss 2.95984, grad norm 3.62766, param norm 122.47462
epoch 4, iter 3770, loss 2.93136, smoothed loss 2.95831, grad norm 3.53451, param norm 122.52177
epoch 4, iter 3775, loss 3.25615, smoothed loss 2.95719, grad norm 3.66058, param norm 122.56672
epoch 5, iter 3780, loss 2.96173, smoothed loss 2.94618, grad norm 3.32329, param norm 122.61533
epoch 5, iter 3785, loss 2.96976, smoothed loss 2.95704, grad norm 3.18975, param norm 122.66339
epoch 5, iter 3790, loss 2.70092, smoothed loss 2.95706, grad norm 3.39575, param norm 122.71276
epoch 5, iter 3795, loss 2.69367, smoothed loss 2.94620, grad norm 3.15781, param norm 122.75828
epoch 5, iter 3800, loss 2.56518, smoothed loss 2.93737, grad norm 3.24591, param norm 122.80201
epoch 5, iter 3805, loss 2.46753, smoothed loss 2.92483, grad norm 3.19255, param norm 122.84425
epoch 5, iter 3810, loss 2.95491, smoothed loss 2.91822, grad norm 3.55399, param norm 122.88609
epoch 5, iter 3815, loss 2.81447, smoothed loss 2.92443, grad norm 3.73577, param norm 122.93181
epoch 5, iter 3820, loss 3.13694, smoothed loss 2.92882, grad norm 3.34192, param norm 122.97409
epoch 5, iter 3825, loss 2.49780, smoothed loss 2.91684, grad norm 2.81343, param norm 123.01738
epoch 5, iter 3830, loss 3.04073, smoothed loss 2.90721, grad norm 3.84995, param norm 123.06108
epoch 5, iter 3835, loss 3.05163, smoothed loss 2.92038, grad norm 3.73517, param norm 123.10917
epoch 5, iter 3840, loss 2.71037, smoothed loss 2.91982, grad norm 3.61336, param norm 123.15970
epoch 5, iter 3845, loss 2.88872, smoothed loss 2.93018, grad norm 3.32013, param norm 123.21432
epoch 5, iter 3850, loss 2.79187, smoothed loss 2.91888, grad norm 3.59484, param norm 123.26473
epoch 5, iter 3855, loss 3.28966, smoothed loss 2.92947, grad norm 3.63603, param norm 123.30811
epoch 5, iter 3860, loss 3.51095, smoothed loss 2.94281, grad norm 3.52596, param norm 123.35283
epoch 5, iter 3865, loss 3.10509, smoothed loss 2.95450, grad norm 3.69260, param norm 123.40032
epoch 5, iter 3870, loss 3.29835, smoothed loss 2.95859, grad norm 3.36472, param norm 123.45148
epoch 5, iter 3875, loss 2.66361, smoothed loss 2.95273, grad norm 2.82790, param norm 123.50056
epoch 5, iter 3880, loss 3.63798, smoothed loss 2.97066, grad norm 3.69143, param norm 123.54392
epoch 5, iter 3885, loss 2.67106, smoothed loss 2.96872, grad norm 3.38722, param norm 123.58591
epoch 5, iter 3890, loss 2.92510, smoothed loss 2.95819, grad norm 3.18464, param norm 123.63873
epoch 5, iter 3895, loss 3.25611, smoothed loss 2.94566, grad norm 3.56670, param norm 123.68722
epoch 5, iter 3900, loss 2.52862, smoothed loss 2.94946, grad norm 3.12950, param norm 123.72769
epoch 5, iter 3905, loss 3.19778, smoothed loss 2.94346, grad norm 3.46243, param norm 123.76521
epoch 5, iter 3910, loss 2.91566, smoothed loss 2.93553, grad norm 3.12972, param norm 123.80820
epoch 5, iter 3915, loss 3.25791, smoothed loss 2.92940, grad norm 3.73451, param norm 123.85625
epoch 5, iter 3920, loss 2.14211, smoothed loss 2.91103, grad norm 2.97327, param norm 123.91215
epoch 5, iter 3925, loss 2.81834, smoothed loss 2.90565, grad norm 3.06795, param norm 123.96120
epoch 5, iter 3930, loss 2.08891, smoothed loss 2.90533, grad norm 3.06470, param norm 124.00761
epoch 5, iter 3935, loss 3.09197, smoothed loss 2.90636, grad norm 3.49559, param norm 124.05143
epoch 5, iter 3940, loss 2.89184, smoothed loss 2.91335, grad norm 3.26101, param norm 124.10054
epoch 5, iter 3945, loss 2.57962, smoothed loss 2.90601, grad norm 3.17426, param norm 124.14912
epoch 5, iter 3950, loss 2.95487, smoothed loss 2.89647, grad norm 3.72712, param norm 124.19397
epoch 5, iter 3955, loss 2.25909, smoothed loss 2.89187, grad norm 2.93852, param norm 124.23637
epoch 5, iter 3960, loss 2.57855, smoothed loss 2.88370, grad norm 3.48678, param norm 124.28400
epoch 5, iter 3965, loss 2.83167, smoothed loss 2.88575, grad norm 3.31052, param norm 124.33278
epoch 5, iter 3970, loss 2.94256, smoothed loss 2.87889, grad norm 3.16814, param norm 124.37840
epoch 5, iter 3975, loss 2.67847, smoothed loss 2.87385, grad norm 3.21694, param norm 124.42309
epoch 5, iter 3980, loss 2.47302, smoothed loss 2.86736, grad norm 3.21053, param norm 124.46769
epoch 5, iter 3985, loss 2.29122, smoothed loss 2.87096, grad norm 3.00825, param norm 124.51125
epoch 5, iter 3990, loss 2.42578, smoothed loss 2.85124, grad norm 3.38369, param norm 124.55272
epoch 5, iter 3995, loss 3.34631, smoothed loss 2.87030, grad norm 3.24769, param norm 124.59242
epoch 5, iter 4000, loss 2.65410, smoothed loss 2.86545, grad norm 2.91292, param norm 124.63408
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 4000, dev loss: 3.078208
Calculating Train F1/EM...
F1 train: 1000 examples took 18.85940 seconds [Score: 0.75444]
Exact Match train: 1000 examples took 18.79092 seconds [Score: 0.62400]
Epoch 5, Iter 4000, Train F1 score: 0.754437, Train EM score: 0.624000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 123.44966 seconds [Score: 0.63223]
Exact Match dev: 7118 examples took 123.51662 seconds [Score: 0.48117]
Epoch 5, Iter 4000, Dev F1 score: 0.632228, Dev EM score: 0.481174
End of epoch 5
epoch 5, iter 4005, loss 2.80112, smoothed loss 2.85491, grad norm 3.02551, param norm 124.68135
epoch 5, iter 4010, loss 3.07371, smoothed loss 2.86223, grad norm 3.41880, param norm 124.72905
epoch 5, iter 4015, loss 2.23876, smoothed loss 2.85999, grad norm 3.05786, param norm 124.77797
epoch 5, iter 4020, loss 2.96697, smoothed loss 2.85978, grad norm 3.40941, param norm 124.82838
epoch 5, iter 4025, loss 2.71403, smoothed loss 2.86465, grad norm 3.44480, param norm 124.88041
epoch 5, iter 4030, loss 2.99833, smoothed loss 2.88499, grad norm 3.63086, param norm 124.92966
epoch 5, iter 4035, loss 2.53997, smoothed loss 2.88172, grad norm 2.68656, param norm 124.97137
epoch 5, iter 4040, loss 2.68900, smoothed loss 2.87971, grad norm 3.17141, param norm 125.01593
epoch 5, iter 4045, loss 2.50328, smoothed loss 2.89151, grad norm 3.31228, param norm 125.06606
epoch 5, iter 4050, loss 2.57374, smoothed loss 2.88644, grad norm 3.28163, param norm 125.11167
epoch 5, iter 4055, loss 2.43629, smoothed loss 2.88242, grad norm 3.19523, param norm 125.15945
epoch 5, iter 4060, loss 2.41171, smoothed loss 2.89004, grad norm 3.12313, param norm 125.20750
epoch 5, iter 4065, loss 2.74053, smoothed loss 2.89705, grad norm 3.03858, param norm 125.25333
epoch 5, iter 4070, loss 2.27899, smoothed loss 2.89206, grad norm 3.18124, param norm 125.29928
epoch 5, iter 4075, loss 2.28322, smoothed loss 2.88158, grad norm 3.21513, param norm 125.34540
epoch 5, iter 4080, loss 2.71412, smoothed loss 2.87905, grad norm 3.52426, param norm 125.39080
epoch 5, iter 4085, loss 3.26397, smoothed loss 2.90168, grad norm 3.48814, param norm 125.43098
epoch 5, iter 4090, loss 2.72265, smoothed loss 2.90747, grad norm 3.03831, param norm 125.47395
epoch 5, iter 4095, loss 3.04394, smoothed loss 2.91303, grad norm 3.34460, param norm 125.51743
epoch 5, iter 4100, loss 3.24403, smoothed loss 2.91373, grad norm 3.69691, param norm 125.56491
epoch 5, iter 4105, loss 2.92098, smoothed loss 2.90167, grad norm 3.24313, param norm 125.61951
epoch 5, iter 4110, loss 2.18951, smoothed loss 2.88040, grad norm 2.69078, param norm 125.66728
epoch 5, iter 4115, loss 3.24673, smoothed loss 2.89162, grad norm 3.96009, param norm 125.70869
epoch 5, iter 4120, loss 2.71922, smoothed loss 2.88247, grad norm 3.39902, param norm 125.75128
epoch 5, iter 4125, loss 2.60493, smoothed loss 2.88553, grad norm 3.15377, param norm 125.79565
epoch 5, iter 4130, loss 2.27046, smoothed loss 2.87066, grad norm 3.09194, param norm 125.83878
epoch 5, iter 4135, loss 2.24944, smoothed loss 2.85730, grad norm 3.29821, param norm 125.87794
epoch 5, iter 4140, loss 3.80786, smoothed loss 2.85604, grad norm 4.17012, param norm 125.91479
epoch 5, iter 4145, loss 2.73883, smoothed loss 2.85879, grad norm 3.75726, param norm 125.94753
epoch 5, iter 4150, loss 2.71505, smoothed loss 2.85031, grad norm 3.31300, param norm 125.98716
epoch 5, iter 4155, loss 2.07063, smoothed loss 2.84608, grad norm 3.73547, param norm 126.02817
epoch 5, iter 4160, loss 3.21861, smoothed loss 2.85063, grad norm 3.80289, param norm 126.07217
epoch 5, iter 4165, loss 2.32668, smoothed loss 2.83394, grad norm 3.01329, param norm 126.11469
epoch 5, iter 4170, loss 3.19347, smoothed loss 2.82719, grad norm 3.51145, param norm 126.16360
epoch 5, iter 4175, loss 2.68676, smoothed loss 2.83439, grad norm 3.47511, param norm 126.21298
epoch 5, iter 4180, loss 3.47234, smoothed loss 2.84783, grad norm 3.77757, param norm 126.26346
epoch 5, iter 4185, loss 2.22484, smoothed loss 2.83897, grad norm 3.07512, param norm 126.31399
epoch 5, iter 4190, loss 2.34728, smoothed loss 2.83102, grad norm 2.99772, param norm 126.36744
epoch 5, iter 4195, loss 3.00902, smoothed loss 2.83992, grad norm 3.57701, param norm 126.41323
epoch 5, iter 4200, loss 3.34361, smoothed loss 2.84749, grad norm 3.52250, param norm 126.46005
epoch 5, iter 4205, loss 3.18854, smoothed loss 2.85572, grad norm 3.64379, param norm 126.50698
epoch 5, iter 4210, loss 2.72758, smoothed loss 2.84215, grad norm 3.39922, param norm 126.55217
epoch 5, iter 4215, loss 2.64515, smoothed loss 2.83896, grad norm 3.20855, param norm 126.59810
epoch 5, iter 4220, loss 2.79232, smoothed loss 2.83876, grad norm 3.36749, param norm 126.64478
epoch 5, iter 4225, loss 2.94855, smoothed loss 2.84186, grad norm 3.18203, param norm 126.68530
epoch 5, iter 4230, loss 2.22192, smoothed loss 2.83671, grad norm 3.02344, param norm 126.72585
epoch 5, iter 4235, loss 3.22962, smoothed loss 2.85130, grad norm 3.77685, param norm 126.77105
epoch 5, iter 4240, loss 2.19446, smoothed loss 2.83735, grad norm 3.13039, param norm 126.81014
epoch 5, iter 4245, loss 2.59392, smoothed loss 2.82560, grad norm 3.48750, param norm 126.84945
epoch 5, iter 4250, loss 2.84927, smoothed loss 2.82306, grad norm 3.72721, param norm 126.88682
epoch 5, iter 4255, loss 2.47643, smoothed loss 2.83224, grad norm 2.95947, param norm 126.92542
epoch 5, iter 4260, loss 2.74958, smoothed loss 2.83353, grad norm 3.06446, param norm 126.96479
epoch 5, iter 4265, loss 3.15735, smoothed loss 2.83367, grad norm 3.13559, param norm 127.00491
epoch 5, iter 4270, loss 3.02077, smoothed loss 2.82864, grad norm 3.32409, param norm 127.03891
epoch 5, iter 4275, loss 2.48449, smoothed loss 2.84130, grad norm 2.75885, param norm 127.07332
epoch 5, iter 4280, loss 3.03378, smoothed loss 2.85113, grad norm 4.30396, param norm 127.11051
epoch 5, iter 4285, loss 3.30129, smoothed loss 2.85896, grad norm 3.74938, param norm 127.15756
epoch 5, iter 4290, loss 2.22149, smoothed loss 2.84911, grad norm 3.05089, param norm 127.20609
epoch 5, iter 4295, loss 2.72119, smoothed loss 2.84027, grad norm 3.23986, param norm 127.25341
epoch 5, iter 4300, loss 2.07519, smoothed loss 2.82535, grad norm 3.40173, param norm 127.30270
epoch 5, iter 4305, loss 2.84993, smoothed loss 2.82501, grad norm 3.23328, param norm 127.34567
epoch 5, iter 4310, loss 2.81112, smoothed loss 2.82330, grad norm 3.23433, param norm 127.38557
epoch 5, iter 4315, loss 2.56721, smoothed loss 2.81439, grad norm 2.97277, param norm 127.43329
epoch 5, iter 4320, loss 3.67326, smoothed loss 2.82836, grad norm 3.68460, param norm 127.47711
epoch 5, iter 4325, loss 2.95171, smoothed loss 2.83818, grad norm 3.57523, param norm 127.52657
epoch 5, iter 4330, loss 2.38133, smoothed loss 2.81727, grad norm 3.23902, param norm 127.57558
epoch 5, iter 4335, loss 3.10078, smoothed loss 2.82233, grad norm 3.60825, param norm 127.61806
epoch 5, iter 4340, loss 2.87975, smoothed loss 2.82210, grad norm 3.71219, param norm 127.65813
epoch 5, iter 4345, loss 3.14973, smoothed loss 2.82722, grad norm 3.20374, param norm 127.70040
epoch 5, iter 4350, loss 2.72952, smoothed loss 2.83723, grad norm 3.50686, param norm 127.73849
epoch 5, iter 4355, loss 2.39341, smoothed loss 2.82981, grad norm 3.19575, param norm 127.77579
epoch 5, iter 4360, loss 2.56410, smoothed loss 2.83282, grad norm 3.58260, param norm 127.81243
epoch 5, iter 4365, loss 2.51368, smoothed loss 2.82531, grad norm 3.02188, param norm 127.85264
epoch 5, iter 4370, loss 2.83203, smoothed loss 2.81715, grad norm 3.81888, param norm 127.89014
epoch 5, iter 4375, loss 2.75189, smoothed loss 2.80800, grad norm 3.39533, param norm 127.93313
epoch 5, iter 4380, loss 2.12298, smoothed loss 2.79961, grad norm 2.96531, param norm 127.97793
epoch 5, iter 4385, loss 2.62179, smoothed loss 2.80089, grad norm 3.00914, param norm 128.02142
epoch 5, iter 4390, loss 3.36480, smoothed loss 2.81035, grad norm 3.71715, param norm 128.06323
epoch 5, iter 4395, loss 3.39977, smoothed loss 2.81451, grad norm 3.55624, param norm 128.10329
epoch 5, iter 4400, loss 3.02920, smoothed loss 2.81618, grad norm 3.21897, param norm 128.14668
epoch 5, iter 4405, loss 3.21749, smoothed loss 2.80248, grad norm 3.31670, param norm 128.18974
epoch 5, iter 4410, loss 2.74814, smoothed loss 2.80871, grad norm 3.17496, param norm 128.23154
epoch 5, iter 4415, loss 2.80572, smoothed loss 2.81499, grad norm 3.51670, param norm 128.27214
epoch 5, iter 4420, loss 3.31805, smoothed loss 2.82719, grad norm 3.37693, param norm 128.31709
epoch 5, iter 4425, loss 2.33275, smoothed loss 2.81180, grad norm 3.19264, param norm 128.36688
epoch 5, iter 4430, loss 3.74562, smoothed loss 2.81734, grad norm 4.33043, param norm 128.41054
epoch 5, iter 4435, loss 3.03152, smoothed loss 2.81527, grad norm 3.44602, param norm 128.45020
epoch 5, iter 4440, loss 2.62588, smoothed loss 2.80880, grad norm 3.21291, param norm 128.49026
epoch 5, iter 4445, loss 3.54982, smoothed loss 2.81185, grad norm 3.98000, param norm 128.53064
epoch 5, iter 4450, loss 2.56263, smoothed loss 2.81093, grad norm 3.01724, param norm 128.57381
epoch 5, iter 4455, loss 2.64608, smoothed loss 2.81237, grad norm 3.22143, param norm 128.61653
epoch 5, iter 4460, loss 3.01289, smoothed loss 2.80355, grad norm 3.62653, param norm 128.66045
epoch 5, iter 4465, loss 2.50930, smoothed loss 2.79497, grad norm 3.22925, param norm 128.70380
epoch 5, iter 4470, loss 3.24769, smoothed loss 2.79073, grad norm 3.63963, param norm 128.75136
epoch 5, iter 4475, loss 2.68106, smoothed loss 2.80430, grad norm 3.12181, param norm 128.79637
epoch 5, iter 4480, loss 2.94021, smoothed loss 2.80186, grad norm 3.35780, param norm 128.83804
epoch 5, iter 4485, loss 2.05403, smoothed loss 2.80547, grad norm 2.78634, param norm 128.88380
epoch 5, iter 4490, loss 2.91374, smoothed loss 2.81300, grad norm 3.28809, param norm 128.92972
epoch 5, iter 4495, loss 2.94277, smoothed loss 2.81742, grad norm 3.28458, param norm 128.98111
epoch 5, iter 4500, loss 2.31344, smoothed loss 2.81282, grad norm 3.09432, param norm 129.03345
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 4500, dev loss: 3.031997
Calculating Train F1/EM...
F1 train: 1000 examples took 18.34853 seconds [Score: 0.71674]
Exact Match train: 1000 examples took 19.11335 seconds [Score: 0.58400]
Epoch 5, Iter 4500, Train F1 score: 0.716736, Train EM score: 0.584000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.80440 seconds [Score: 0.63880]
Exact Match dev: 7118 examples took 124.15221 seconds [Score: 0.49059]
Epoch 5, Iter 4500, Dev F1 score: 0.638803, Dev EM score: 0.490587
End of epoch 5
epoch 5, iter 4505, loss 2.67007, smoothed loss 2.81101, grad norm 3.64651, param norm 129.08142
epoch 5, iter 4510, loss 3.03852, smoothed loss 2.82458, grad norm 3.92817, param norm 129.12308
epoch 5, iter 4515, loss 3.08393, smoothed loss 2.81348, grad norm 3.60074, param norm 129.16379
epoch 5, iter 4520, loss 2.67200, smoothed loss 2.80416, grad norm 3.23348, param norm 129.20647
epoch 5, iter 4525, loss 2.62902, smoothed loss 2.80377, grad norm 3.15125, param norm 129.24136
epoch 5, iter 4530, loss 3.27528, smoothed loss 2.81315, grad norm 3.60494, param norm 129.28136
epoch 5, iter 4535, loss 2.88514, smoothed loss 2.81594, grad norm 3.62067, param norm 129.31848
epoch 5, iter 4540, loss 3.08728, smoothed loss 2.82392, grad norm 3.27605, param norm 129.35599
epoch 5, iter 4545, loss 2.69996, smoothed loss 2.81430, grad norm 3.15930, param norm 129.39355
epoch 5, iter 4550, loss 2.24115, smoothed loss 2.81498, grad norm 3.33269, param norm 129.43118
epoch 5, iter 4555, loss 2.62624, smoothed loss 2.82246, grad norm 3.52945, param norm 129.47639
epoch 5, iter 4560, loss 2.45278, smoothed loss 2.81302, grad norm 3.15694, param norm 129.52220
epoch 5, iter 4565, loss 2.46681, smoothed loss 2.79988, grad norm 3.09784, param norm 129.56541
epoch 5, iter 4570, loss 3.13891, smoothed loss 2.79911, grad norm 3.25722, param norm 129.60236
epoch 5, iter 4575, loss 2.48890, smoothed loss 2.79939, grad norm 3.24201, param norm 129.63437
epoch 5, iter 4580, loss 2.35398, smoothed loss 2.79925, grad norm 3.13367, param norm 129.66573
epoch 5, iter 4585, loss 2.68662, smoothed loss 2.79720, grad norm 3.01527, param norm 129.70282
epoch 5, iter 4590, loss 2.92705, smoothed loss 2.79168, grad norm 3.32487, param norm 129.74704
epoch 5, iter 4595, loss 2.86349, smoothed loss 2.79507, grad norm 3.99695, param norm 129.79675
epoch 5, iter 4600, loss 3.10715, smoothed loss 2.78666, grad norm 3.41566, param norm 129.84386
epoch 5, iter 4605, loss 2.55006, smoothed loss 2.76694, grad norm 3.62113, param norm 129.88628
epoch 5, iter 4610, loss 2.94073, smoothed loss 2.76546, grad norm 3.59066, param norm 129.92426
epoch 5, iter 4615, loss 2.56257, smoothed loss 2.76814, grad norm 3.29552, param norm 129.96611
epoch 5, iter 4620, loss 2.61995, smoothed loss 2.75677, grad norm 3.29461, param norm 130.01326
epoch 5, iter 4625, loss 2.65054, smoothed loss 2.75681, grad norm 3.59326, param norm 130.06284
epoch 5, iter 4630, loss 2.42654, smoothed loss 2.74964, grad norm 3.01533, param norm 130.11586
epoch 5, iter 4635, loss 3.23178, smoothed loss 2.76090, grad norm 3.82548, param norm 130.16286
epoch 5, iter 4640, loss 2.86779, smoothed loss 2.75593, grad norm 3.49963, param norm 130.20409
epoch 5, iter 4645, loss 3.00285, smoothed loss 2.75827, grad norm 3.71069, param norm 130.24405
epoch 5, iter 4650, loss 2.93804, smoothed loss 2.75429, grad norm 3.14235, param norm 130.28036
epoch 5, iter 4655, loss 3.22097, smoothed loss 2.75038, grad norm 4.26596, param norm 130.32294
epoch 5, iter 4660, loss 2.91661, smoothed loss 2.74990, grad norm 3.55359, param norm 130.36816
epoch 5, iter 4665, loss 2.22729, smoothed loss 2.74373, grad norm 2.64905, param norm 130.41379
epoch 5, iter 4670, loss 3.41184, smoothed loss 2.75505, grad norm 4.05764, param norm 130.45872
epoch 5, iter 4675, loss 2.82590, smoothed loss 2.76114, grad norm 3.91823, param norm 130.49837
epoch 5, iter 4680, loss 2.89999, smoothed loss 2.76663, grad norm 3.04111, param norm 130.54059
epoch 5, iter 4685, loss 2.25277, smoothed loss 2.75696, grad norm 3.02821, param norm 130.58342
epoch 5, iter 4690, loss 2.86396, smoothed loss 2.75664, grad norm 3.47347, param norm 130.62589
epoch 5, iter 4695, loss 2.98270, smoothed loss 2.75753, grad norm 3.32205, param norm 130.66931
epoch 5, iter 4700, loss 3.01146, smoothed loss 2.76278, grad norm 4.01678, param norm 130.70894
epoch 5, iter 4705, loss 2.14645, smoothed loss 2.76159, grad norm 3.69358, param norm 130.74770
epoch 5, iter 4710, loss 2.09143, smoothed loss 2.75969, grad norm 2.77192, param norm 130.79089
epoch 5, iter 4715, loss 2.90133, smoothed loss 2.75748, grad norm 3.90921, param norm 130.83253
epoch 5, iter 4720, loss 3.16836, smoothed loss 2.77504, grad norm 3.70954, param norm 130.87143
epoch 6, iter 4725, loss 3.11640, smoothed loss 2.77428, grad norm 3.38618, param norm 130.91341
epoch 6, iter 4730, loss 3.58149, smoothed loss 2.77261, grad norm 3.47053, param norm 130.95952
epoch 6, iter 4735, loss 2.62443, smoothed loss 2.76280, grad norm 3.28893, param norm 131.00479
epoch 6, iter 4740, loss 3.04132, smoothed loss 2.75228, grad norm 3.69646, param norm 131.04881
epoch 6, iter 4745, loss 2.37945, smoothed loss 2.74418, grad norm 3.32238, param norm 131.09410
epoch 6, iter 4750, loss 2.51126, smoothed loss 2.74588, grad norm 3.22462, param norm 131.13773
epoch 6, iter 4755, loss 3.18575, smoothed loss 2.74488, grad norm 3.64763, param norm 131.18039
epoch 6, iter 4760, loss 2.01154, smoothed loss 2.74734, grad norm 2.83537, param norm 131.21968
epoch 6, iter 4765, loss 2.43287, smoothed loss 2.75344, grad norm 3.29488, param norm 131.26068
epoch 6, iter 4770, loss 2.42572, smoothed loss 2.75568, grad norm 3.08001, param norm 131.30278
epoch 6, iter 4775, loss 2.59379, smoothed loss 2.76373, grad norm 3.16461, param norm 131.34373
epoch 6, iter 4780, loss 2.36526, smoothed loss 2.76207, grad norm 3.20784, param norm 131.38309
epoch 6, iter 4785, loss 3.33068, smoothed loss 2.76646, grad norm 4.10424, param norm 131.41850
epoch 6, iter 4790, loss 2.72034, smoothed loss 2.77123, grad norm 3.54945, param norm 131.45377
epoch 6, iter 4795, loss 2.65725, smoothed loss 2.76921, grad norm 2.92501, param norm 131.49503
epoch 6, iter 4800, loss 2.85236, smoothed loss 2.75418, grad norm 3.77170, param norm 131.53482
epoch 6, iter 4805, loss 2.72140, smoothed loss 2.75951, grad norm 3.39758, param norm 131.57523
epoch 6, iter 4810, loss 2.73773, smoothed loss 2.74658, grad norm 2.98844, param norm 131.61674
epoch 6, iter 4815, loss 3.20648, smoothed loss 2.73901, grad norm 4.00113, param norm 131.65807
epoch 6, iter 4820, loss 2.61027, smoothed loss 2.74826, grad norm 3.07833, param norm 131.70177
epoch 6, iter 4825, loss 2.70471, smoothed loss 2.75091, grad norm 3.36849, param norm 131.74745
epoch 6, iter 4830, loss 2.83726, smoothed loss 2.75152, grad norm 3.75879, param norm 131.79190
epoch 6, iter 4835, loss 2.48020, smoothed loss 2.75551, grad norm 3.20645, param norm 131.83203
epoch 6, iter 4840, loss 2.79240, smoothed loss 2.75952, grad norm 3.32035, param norm 131.86993
epoch 6, iter 4845, loss 2.67540, smoothed loss 2.76086, grad norm 3.15560, param norm 131.91519
epoch 6, iter 4850, loss 2.75847, smoothed loss 2.75090, grad norm 3.11231, param norm 131.96292
epoch 6, iter 4855, loss 2.10410, smoothed loss 2.74720, grad norm 3.34194, param norm 132.00635
epoch 6, iter 4860, loss 3.29012, smoothed loss 2.75558, grad norm 3.52231, param norm 132.04898
epoch 6, iter 4865, loss 2.35859, smoothed loss 2.75186, grad norm 3.53403, param norm 132.08554
epoch 6, iter 4870, loss 2.70491, smoothed loss 2.73921, grad norm 3.94705, param norm 132.12225
epoch 6, iter 4875, loss 2.37830, smoothed loss 2.73527, grad norm 3.55786, param norm 132.16444
epoch 6, iter 4880, loss 2.12387, smoothed loss 2.73688, grad norm 3.22854, param norm 132.20959
epoch 6, iter 4885, loss 2.78797, smoothed loss 2.72765, grad norm 3.51771, param norm 132.25555
epoch 6, iter 4890, loss 3.14908, smoothed loss 2.73000, grad norm 3.80590, param norm 132.29488
epoch 6, iter 4895, loss 2.88350, smoothed loss 2.72771, grad norm 3.53438, param norm 132.33124
epoch 6, iter 4900, loss 2.71035, smoothed loss 2.72499, grad norm 2.87897, param norm 132.37007
epoch 6, iter 4905, loss 2.72652, smoothed loss 2.72103, grad norm 3.16465, param norm 132.41183
epoch 6, iter 4910, loss 2.92068, smoothed loss 2.72294, grad norm 3.74371, param norm 132.45433
epoch 6, iter 4915, loss 3.57580, smoothed loss 2.74228, grad norm 3.58550, param norm 132.49963
epoch 6, iter 4920, loss 2.57261, smoothed loss 2.74277, grad norm 3.27615, param norm 132.54161
epoch 6, iter 4925, loss 2.42890, smoothed loss 2.73995, grad norm 3.07462, param norm 132.58456
epoch 6, iter 4930, loss 3.24065, smoothed loss 2.73247, grad norm 3.91706, param norm 132.62889
epoch 6, iter 4935, loss 2.36583, smoothed loss 2.73244, grad norm 3.10063, param norm 132.67471
epoch 6, iter 4940, loss 3.22021, smoothed loss 2.72183, grad norm 3.75267, param norm 132.72014
epoch 6, iter 4945, loss 3.23866, smoothed loss 2.71863, grad norm 4.21775, param norm 132.76097
epoch 6, iter 4950, loss 2.46627, smoothed loss 2.71894, grad norm 3.79195, param norm 132.79997
epoch 6, iter 4955, loss 2.44601, smoothed loss 2.71860, grad norm 3.37714, param norm 132.83936
epoch 6, iter 4960, loss 2.50687, smoothed loss 2.71246, grad norm 4.23375, param norm 132.88054
epoch 6, iter 4965, loss 3.07100, smoothed loss 2.72416, grad norm 3.43341, param norm 132.92075
epoch 6, iter 4970, loss 2.93058, smoothed loss 2.73264, grad norm 3.39282, param norm 132.95976
epoch 6, iter 4975, loss 3.18923, smoothed loss 2.73822, grad norm 3.34228, param norm 133.00066
epoch 6, iter 4980, loss 2.64009, smoothed loss 2.74330, grad norm 3.57885, param norm 133.04420
epoch 6, iter 4985, loss 2.74747, smoothed loss 2.74007, grad norm 3.68740, param norm 133.08955
epoch 6, iter 4990, loss 2.60918, smoothed loss 2.73654, grad norm 3.62682, param norm 133.13954
epoch 6, iter 4995, loss 3.17780, smoothed loss 2.74303, grad norm 3.64416, param norm 133.18590
epoch 6, iter 5000, loss 2.51157, smoothed loss 2.74925, grad norm 3.08927, param norm 133.22446
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 6, Iter 5000, dev loss: 3.072563
Calculating Train F1/EM...
F1 train: 1000 examples took 18.23636 seconds [Score: 0.76985]
Exact Match train: 1000 examples took 18.49882 seconds [Score: 0.63600]
Epoch 6, Iter 5000, Train F1 score: 0.769853, Train EM score: 0.636000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.84169 seconds [Score: 0.64134]
Exact Match dev: 7118 examples took 124.73781 seconds [Score: 0.48806]
Epoch 6, Iter 5000, Dev F1 score: 0.641341, Dev EM score: 0.488058
End of epoch 6
epoch 6, iter 5005, loss 2.96815, smoothed loss 2.75322, grad norm 3.41968, param norm 133.25835
epoch 6, iter 5010, loss 2.52597, smoothed loss 2.75309, grad norm 3.34556, param norm 133.29625
epoch 6, iter 5015, loss 2.78129, smoothed loss 2.75592, grad norm 3.17720, param norm 133.34113
epoch 6, iter 5020, loss 2.16931, smoothed loss 2.74592, grad norm 3.19638, param norm 133.38220
epoch 6, iter 5025, loss 3.27937, smoothed loss 2.75450, grad norm 3.65809, param norm 133.42012
epoch 6, iter 5030, loss 2.68298, smoothed loss 2.75607, grad norm 3.62387, param norm 133.46178
epoch 6, iter 5035, loss 2.83443, smoothed loss 2.75422, grad norm 3.57901, param norm 133.50850
epoch 6, iter 5040, loss 3.67120, smoothed loss 2.76789, grad norm 4.62667, param norm 133.55594
epoch 6, iter 5045, loss 2.64515, smoothed loss 2.75520, grad norm 3.44913, param norm 133.60036
epoch 6, iter 5050, loss 3.05102, smoothed loss 2.74509, grad norm 3.65414, param norm 133.64507
epoch 6, iter 5055, loss 3.00318, smoothed loss 2.74610, grad norm 3.37919, param norm 133.68742
epoch 6, iter 5060, loss 2.13598, smoothed loss 2.72838, grad norm 3.28350, param norm 133.72992
epoch 6, iter 5065, loss 2.59879, smoothed loss 2.72469, grad norm 3.87534, param norm 133.77783
epoch 6, iter 5070, loss 2.84615, smoothed loss 2.72492, grad norm 3.60100, param norm 133.83102
epoch 6, iter 5075, loss 2.47441, smoothed loss 2.71860, grad norm 3.31887, param norm 133.87883
epoch 6, iter 5080, loss 2.45904, smoothed loss 2.70484, grad norm 3.35936, param norm 133.92592
epoch 6, iter 5085, loss 2.43942, smoothed loss 2.68630, grad norm 3.49413, param norm 133.96729
epoch 6, iter 5090, loss 2.79491, smoothed loss 2.68090, grad norm 3.41255, param norm 134.00940
epoch 6, iter 5095, loss 2.60559, smoothed loss 2.69071, grad norm 3.35522, param norm 134.05078
epoch 6, iter 5100, loss 2.64986, smoothed loss 2.68504, grad norm 3.44816, param norm 134.09651
epoch 6, iter 5105, loss 2.31192, smoothed loss 2.67820, grad norm 3.25120, param norm 134.14412
epoch 6, iter 5110, loss 2.43276, smoothed loss 2.66503, grad norm 2.84951, param norm 134.18663
epoch 6, iter 5115, loss 2.72416, smoothed loss 2.66719, grad norm 4.13860, param norm 134.22401
epoch 6, iter 5120, loss 2.62964, smoothed loss 2.65275, grad norm 3.63105, param norm 134.26100
epoch 6, iter 5125, loss 2.37011, smoothed loss 2.64472, grad norm 3.17806, param norm 134.30624
epoch 6, iter 5130, loss 2.13096, smoothed loss 2.63413, grad norm 3.05379, param norm 134.35040
epoch 6, iter 5135, loss 2.46949, smoothed loss 2.63602, grad norm 3.11782, param norm 134.39296
epoch 6, iter 5140, loss 2.28073, smoothed loss 2.62787, grad norm 3.29909, param norm 134.43651
epoch 6, iter 5145, loss 2.62343, smoothed loss 2.62861, grad norm 3.50447, param norm 134.48244
epoch 6, iter 5150, loss 2.13283, smoothed loss 2.63312, grad norm 3.21734, param norm 134.52963
epoch 6, iter 5155, loss 2.38769, smoothed loss 2.63165, grad norm 3.17038, param norm 134.57478
epoch 6, iter 5160, loss 2.33595, smoothed loss 2.63196, grad norm 3.25381, param norm 134.61821
epoch 6, iter 5165, loss 2.32238, smoothed loss 2.64427, grad norm 3.44117, param norm 134.65892
epoch 6, iter 5170, loss 3.08041, smoothed loss 2.64541, grad norm 3.12925, param norm 134.70027
epoch 6, iter 5175, loss 2.32599, smoothed loss 2.64451, grad norm 3.50616, param norm 134.73903
epoch 6, iter 5180, loss 2.64426, smoothed loss 2.65137, grad norm 3.38241, param norm 134.77776
epoch 6, iter 5185, loss 2.33352, smoothed loss 2.64894, grad norm 3.43883, param norm 134.81558
epoch 6, iter 5190, loss 3.21051, smoothed loss 2.65133, grad norm 4.79044, param norm 134.84952
epoch 6, iter 5195, loss 2.32831, smoothed loss 2.65659, grad norm 2.74647, param norm 134.88135
epoch 6, iter 5200, loss 2.59295, smoothed loss 2.65978, grad norm 3.37227, param norm 134.91573
epoch 6, iter 5205, loss 2.80984, smoothed loss 2.67158, grad norm 3.71209, param norm 134.95335
epoch 6, iter 5210, loss 2.35803, smoothed loss 2.66963, grad norm 3.61988, param norm 134.99164
epoch 6, iter 5215, loss 2.16809, smoothed loss 2.65859, grad norm 3.29107, param norm 135.03687
epoch 6, iter 5220, loss 2.71984, smoothed loss 2.65521, grad norm 3.71954, param norm 135.08470
epoch 6, iter 5225, loss 3.00980, smoothed loss 2.66197, grad norm 3.81777, param norm 135.12352
epoch 6, iter 5230, loss 2.68535, smoothed loss 2.66876, grad norm 3.37510, param norm 135.15952
epoch 6, iter 5235, loss 3.11914, smoothed loss 2.66943, grad norm 3.94009, param norm 135.19055
epoch 6, iter 5240, loss 2.61463, smoothed loss 2.67704, grad norm 3.76210, param norm 135.22589
epoch 6, iter 5245, loss 2.73039, smoothed loss 2.68657, grad norm 3.82362, param norm 135.26639
epoch 6, iter 5250, loss 2.49467, smoothed loss 2.68061, grad norm 3.14342, param norm 135.30609
epoch 6, iter 5255, loss 3.13277, smoothed loss 2.69749, grad norm 3.43533, param norm 135.34744
epoch 6, iter 5260, loss 2.62489, smoothed loss 2.69188, grad norm 3.18989, param norm 135.39124
epoch 6, iter 5265, loss 3.36326, smoothed loss 2.69779, grad norm 4.00468, param norm 135.43509
epoch 6, iter 5270, loss 3.02755, smoothed loss 2.70713, grad norm 4.04556, param norm 135.47617
epoch 6, iter 5275, loss 2.41486, smoothed loss 2.69995, grad norm 3.02213, param norm 135.52191
epoch 6, iter 5280, loss 3.38440, smoothed loss 2.69681, grad norm 3.80913, param norm 135.56299
epoch 6, iter 5285, loss 2.82903, smoothed loss 2.68928, grad norm 3.91101, param norm 135.59749
epoch 6, iter 5290, loss 3.10197, smoothed loss 2.68064, grad norm 4.03546, param norm 135.63213
epoch 6, iter 5295, loss 2.32597, smoothed loss 2.67140, grad norm 3.00944, param norm 135.66815
epoch 6, iter 5300, loss 3.12466, smoothed loss 2.67001, grad norm 3.61160, param norm 135.70613
epoch 6, iter 5305, loss 2.27315, smoothed loss 2.66583, grad norm 3.01428, param norm 135.74649
epoch 6, iter 5310, loss 2.72858, smoothed loss 2.67318, grad norm 3.82370, param norm 135.78850
epoch 6, iter 5315, loss 2.75060, smoothed loss 2.66858, grad norm 3.71556, param norm 135.82504
epoch 6, iter 5320, loss 2.65133, smoothed loss 2.65955, grad norm 3.64402, param norm 135.86157
epoch 6, iter 5325, loss 2.49759, smoothed loss 2.64220, grad norm 3.56192, param norm 135.90211
epoch 6, iter 5330, loss 3.28736, smoothed loss 2.65815, grad norm 3.98298, param norm 135.93915
epoch 6, iter 5335, loss 3.11528, smoothed loss 2.65142, grad norm 3.19459, param norm 135.97046
epoch 6, iter 5340, loss 2.22111, smoothed loss 2.63947, grad norm 2.90190, param norm 136.00240
epoch 6, iter 5345, loss 2.49148, smoothed loss 2.63798, grad norm 2.95652, param norm 136.04031
epoch 6, iter 5350, loss 3.03020, smoothed loss 2.65569, grad norm 3.80026, param norm 136.08141
epoch 6, iter 5355, loss 2.34799, smoothed loss 2.64971, grad norm 3.10501, param norm 136.12361
epoch 6, iter 5360, loss 2.06088, smoothed loss 2.65221, grad norm 3.35833, param norm 136.16066
epoch 6, iter 5365, loss 2.78368, smoothed loss 2.66610, grad norm 4.06907, param norm 136.19893
epoch 6, iter 5370, loss 1.90071, smoothed loss 2.66024, grad norm 2.63098, param norm 136.23907
epoch 6, iter 5375, loss 3.35271, smoothed loss 2.65487, grad norm 3.86897, param norm 136.28104
epoch 6, iter 5380, loss 2.60956, smoothed loss 2.65868, grad norm 3.63729, param norm 136.31937
epoch 6, iter 5385, loss 3.08008, smoothed loss 2.66196, grad norm 3.66302, param norm 136.36049
epoch 6, iter 5390, loss 2.10255, smoothed loss 2.66042, grad norm 3.12703, param norm 136.40067
epoch 6, iter 5395, loss 3.22215, smoothed loss 2.66629, grad norm 3.99346, param norm 136.44144
epoch 6, iter 5400, loss 2.28777, smoothed loss 2.65155, grad norm 3.39959, param norm 136.48389
epoch 6, iter 5405, loss 2.07118, smoothed loss 2.65187, grad norm 3.15244, param norm 136.52487
epoch 6, iter 5410, loss 2.75936, smoothed loss 2.66465, grad norm 4.29392, param norm 136.56752
epoch 6, iter 5415, loss 2.58322, smoothed loss 2.66145, grad norm 3.36397, param norm 136.60628
epoch 6, iter 5420, loss 2.41854, smoothed loss 2.64503, grad norm 2.94170, param norm 136.64331
epoch 6, iter 5425, loss 2.03142, smoothed loss 2.64142, grad norm 3.18385, param norm 136.67851
epoch 6, iter 5430, loss 2.34995, smoothed loss 2.64982, grad norm 3.63964, param norm 136.71756
epoch 6, iter 5435, loss 2.99240, smoothed loss 2.64375, grad norm 4.01077, param norm 136.75957
epoch 6, iter 5440, loss 3.06051, smoothed loss 2.65287, grad norm 3.67287, param norm 136.80116
epoch 6, iter 5445, loss 2.48641, smoothed loss 2.66231, grad norm 3.32443, param norm 136.83731
epoch 6, iter 5450, loss 2.27087, smoothed loss 2.65931, grad norm 3.13250, param norm 136.87666
epoch 6, iter 5455, loss 2.52874, smoothed loss 2.64658, grad norm 3.02431, param norm 136.91692
epoch 6, iter 5460, loss 2.24859, smoothed loss 2.63752, grad norm 3.16152, param norm 136.95616
epoch 6, iter 5465, loss 2.99937, smoothed loss 2.63683, grad norm 3.96123, param norm 136.99545
epoch 6, iter 5470, loss 1.83500, smoothed loss 2.63356, grad norm 2.85576, param norm 137.03470
epoch 6, iter 5475, loss 3.22753, smoothed loss 2.63257, grad norm 4.13383, param norm 137.07726
epoch 6, iter 5480, loss 3.08220, smoothed loss 2.62669, grad norm 4.14254, param norm 137.12265
epoch 6, iter 5485, loss 2.86013, smoothed loss 2.61897, grad norm 3.30902, param norm 137.17068
epoch 6, iter 5490, loss 2.96695, smoothed loss 2.63467, grad norm 3.80218, param norm 137.20979
epoch 6, iter 5495, loss 2.59188, smoothed loss 2.62937, grad norm 3.12512, param norm 137.24393
epoch 6, iter 5500, loss 2.77037, smoothed loss 2.63222, grad norm 3.30098, param norm 137.27798
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 6, Iter 5500, dev loss: 3.032568
Calculating Train F1/EM...
F1 train: 1000 examples took 18.49887 seconds [Score: 0.75553]
Exact Match train: 1000 examples took 18.44168 seconds [Score: 0.60300]
Epoch 6, Iter 5500, Train F1 score: 0.755534, Train EM score: 0.603000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.69002 seconds [Score: 0.64154]
Exact Match dev: 7118 examples took 124.78607 seconds [Score: 0.49396]
Epoch 6, Iter 5500, Dev F1 score: 0.641538, Dev EM score: 0.493959
End of epoch 6
epoch 6, iter 5505, loss 2.94978, smoothed loss 2.65082, grad norm 4.05119, param norm 137.31264
epoch 6, iter 5510, loss 2.45484, smoothed loss 2.65137, grad norm 3.08378, param norm 137.34738
epoch 6, iter 5515, loss 2.41688, smoothed loss 2.64344, grad norm 3.76722, param norm 137.38959
epoch 6, iter 5520, loss 2.96784, smoothed loss 2.65322, grad norm 3.42032, param norm 137.43272
epoch 6, iter 5525, loss 2.64677, smoothed loss 2.66039, grad norm 3.93500, param norm 137.47391
epoch 6, iter 5530, loss 2.20225, smoothed loss 2.65222, grad norm 3.51603, param norm 137.51308
epoch 6, iter 5535, loss 2.23871, smoothed loss 2.63996, grad norm 4.13090, param norm 137.54776
epoch 6, iter 5540, loss 2.57877, smoothed loss 2.63222, grad norm 3.72987, param norm 137.58786
epoch 6, iter 5545, loss 2.49802, smoothed loss 2.62923, grad norm 3.78981, param norm 137.62634
epoch 6, iter 5550, loss 2.86861, smoothed loss 2.63627, grad norm 3.65882, param norm 137.66710
epoch 6, iter 5555, loss 2.63262, smoothed loss 2.64304, grad norm 3.62156, param norm 137.70528
epoch 6, iter 5560, loss 2.68388, smoothed loss 2.64673, grad norm 3.50165, param norm 137.74019
epoch 6, iter 5565, loss 2.36367, smoothed loss 2.64072, grad norm 3.16284, param norm 137.77461
epoch 6, iter 5570, loss 2.31618, smoothed loss 2.64051, grad norm 2.97384, param norm 137.81076
epoch 6, iter 5575, loss 2.98235, smoothed loss 2.64159, grad norm 3.88351, param norm 137.85382
epoch 6, iter 5580, loss 3.22533, smoothed loss 2.64044, grad norm 4.13545, param norm 137.89879
epoch 6, iter 5585, loss 2.41686, smoothed loss 2.63941, grad norm 3.22438, param norm 137.94193
epoch 6, iter 5590, loss 3.01712, smoothed loss 2.63179, grad norm 3.91459, param norm 137.98886
epoch 6, iter 5595, loss 2.35763, smoothed loss 2.61845, grad norm 3.49967, param norm 138.03438
epoch 6, iter 5600, loss 3.09513, smoothed loss 2.61857, grad norm 4.02438, param norm 138.08618
epoch 6, iter 5605, loss 2.12070, smoothed loss 2.60610, grad norm 3.11408, param norm 138.13480
epoch 6, iter 5610, loss 2.14877, smoothed loss 2.60995, grad norm 3.71092, param norm 138.17863
epoch 6, iter 5615, loss 2.18920, smoothed loss 2.61252, grad norm 3.22301, param norm 138.21799
epoch 6, iter 5620, loss 2.35449, smoothed loss 2.59459, grad norm 3.00700, param norm 138.25665
epoch 6, iter 5625, loss 2.09658, smoothed loss 2.58651, grad norm 3.31080, param norm 138.29341
epoch 6, iter 5630, loss 2.46868, smoothed loss 2.59083, grad norm 4.14085, param norm 138.33095
epoch 6, iter 5635, loss 3.24824, smoothed loss 2.58859, grad norm 3.99360, param norm 138.37308
epoch 6, iter 5640, loss 2.97811, smoothed loss 2.59894, grad norm 3.78460, param norm 138.41035
epoch 6, iter 5645, loss 2.65414, smoothed loss 2.60737, grad norm 3.60521, param norm 138.44633
epoch 6, iter 5650, loss 2.11288, smoothed loss 2.59906, grad norm 3.03637, param norm 138.48344
epoch 6, iter 5655, loss 2.60072, smoothed loss 2.60019, grad norm 3.67838, param norm 138.52318
epoch 6, iter 5660, loss 2.74248, smoothed loss 2.60936, grad norm 3.19710, param norm 138.57043
epoch 7, iter 5665, loss 2.28722, smoothed loss 2.61771, grad norm 3.05681, param norm 138.62021
epoch 7, iter 5670, loss 2.57414, smoothed loss 2.61927, grad norm 3.17858, param norm 138.66162
epoch 7, iter 5675, loss 3.09361, smoothed loss 2.62464, grad norm 3.31923, param norm 138.69934
epoch 7, iter 5680, loss 2.57695, smoothed loss 2.61458, grad norm 2.68388, param norm 138.73199
epoch 7, iter 5685, loss 2.93652, smoothed loss 2.60970, grad norm 3.74957, param norm 138.76715
epoch 7, iter 5690, loss 2.78782, smoothed loss 2.61465, grad norm 3.80662, param norm 138.80632
epoch 7, iter 5695, loss 2.21442, smoothed loss 2.61003, grad norm 3.10244, param norm 138.84402
epoch 7, iter 5700, loss 2.45927, smoothed loss 2.61127, grad norm 3.26076, param norm 138.87967
epoch 7, iter 5705, loss 2.32123, smoothed loss 2.60716, grad norm 3.38739, param norm 138.91049
epoch 7, iter 5710, loss 1.93006, smoothed loss 2.59498, grad norm 2.87398, param norm 138.94958
epoch 7, iter 5715, loss 3.43297, smoothed loss 2.60455, grad norm 4.17138, param norm 138.99155
epoch 7, iter 5720, loss 2.95010, smoothed loss 2.61442, grad norm 3.58308, param norm 139.03180
epoch 7, iter 5725, loss 3.56423, smoothed loss 2.61832, grad norm 3.95280, param norm 139.06650
epoch 7, iter 5730, loss 2.79024, smoothed loss 2.60701, grad norm 3.71433, param norm 139.10530
epoch 7, iter 5735, loss 2.89743, smoothed loss 2.59738, grad norm 3.96178, param norm 139.14638
epoch 7, iter 5740, loss 3.02164, smoothed loss 2.59170, grad norm 4.40483, param norm 139.18204
epoch 7, iter 5745, loss 2.85419, smoothed loss 2.60540, grad norm 3.93953, param norm 139.22018
epoch 7, iter 5750, loss 2.91525, smoothed loss 2.60867, grad norm 3.52076, param norm 139.25821
epoch 7, iter 5755, loss 2.74854, smoothed loss 2.59031, grad norm 3.21277, param norm 139.29889
epoch 7, iter 5760, loss 2.05431, smoothed loss 2.57257, grad norm 2.88246, param norm 139.33673
epoch 7, iter 5765, loss 2.91883, smoothed loss 2.55896, grad norm 3.61110, param norm 139.37143
epoch 7, iter 5770, loss 2.72715, smoothed loss 2.57107, grad norm 3.95195, param norm 139.40701
epoch 7, iter 5775, loss 2.79085, smoothed loss 2.57142, grad norm 4.00367, param norm 139.44800
epoch 7, iter 5780, loss 3.19285, smoothed loss 2.57575, grad norm 4.12083, param norm 139.48764
epoch 7, iter 5785, loss 3.21891, smoothed loss 2.59153, grad norm 4.33296, param norm 139.52826
epoch 7, iter 5790, loss 2.77374, smoothed loss 2.59021, grad norm 3.55600, param norm 139.56783
epoch 7, iter 5795, loss 3.11848, smoothed loss 2.58238, grad norm 3.71753, param norm 139.60547
epoch 7, iter 5800, loss 1.83358, smoothed loss 2.57153, grad norm 2.77543, param norm 139.64140
epoch 7, iter 5805, loss 2.51248, smoothed loss 2.57415, grad norm 3.40375, param norm 139.67873
epoch 7, iter 5810, loss 2.78911, smoothed loss 2.58018, grad norm 3.28706, param norm 139.71480
epoch 7, iter 5815, loss 2.57586, smoothed loss 2.57885, grad norm 3.25633, param norm 139.75508
epoch 7, iter 5820, loss 3.10435, smoothed loss 2.58224, grad norm 3.88279, param norm 139.79530
epoch 7, iter 5825, loss 2.49486, smoothed loss 2.59084, grad norm 3.31881, param norm 139.83173
epoch 7, iter 5830, loss 2.60904, smoothed loss 2.58834, grad norm 3.26483, param norm 139.87555
epoch 7, iter 5835, loss 2.32864, smoothed loss 2.58741, grad norm 3.21086, param norm 139.91533
epoch 7, iter 5840, loss 2.10039, smoothed loss 2.58188, grad norm 3.44262, param norm 139.95076
epoch 7, iter 5845, loss 2.87042, smoothed loss 2.59705, grad norm 3.59459, param norm 139.98695
epoch 7, iter 5850, loss 3.09166, smoothed loss 2.60415, grad norm 3.48469, param norm 140.02641
epoch 7, iter 5855, loss 2.46797, smoothed loss 2.61198, grad norm 3.44606, param norm 140.06802
epoch 7, iter 5860, loss 2.17109, smoothed loss 2.60000, grad norm 3.06065, param norm 140.11066
epoch 7, iter 5865, loss 2.19924, smoothed loss 2.59340, grad norm 3.19093, param norm 140.15213
epoch 7, iter 5870, loss 3.36163, smoothed loss 2.59764, grad norm 4.01553, param norm 140.18829
epoch 7, iter 5875, loss 2.41014, smoothed loss 2.59908, grad norm 3.18771, param norm 140.22345
epoch 7, iter 5880, loss 2.14108, smoothed loss 2.59205, grad norm 3.66138, param norm 140.26241
epoch 7, iter 5885, loss 2.97519, smoothed loss 2.59163, grad norm 3.48830, param norm 140.30072
epoch 7, iter 5890, loss 2.21365, smoothed loss 2.58329, grad norm 3.82809, param norm 140.33780
epoch 7, iter 5895, loss 2.48006, smoothed loss 2.58863, grad norm 3.65352, param norm 140.37263
epoch 7, iter 5900, loss 3.55352, smoothed loss 2.60473, grad norm 4.36304, param norm 140.40904
epoch 7, iter 5905, loss 2.76487, smoothed loss 2.60181, grad norm 3.31521, param norm 140.45039
epoch 7, iter 5910, loss 2.45812, smoothed loss 2.59487, grad norm 3.57493, param norm 140.49425
epoch 7, iter 5915, loss 3.08888, smoothed loss 2.59170, grad norm 3.57854, param norm 140.54288
epoch 7, iter 5920, loss 2.78363, smoothed loss 2.59866, grad norm 3.39964, param norm 140.58559
epoch 7, iter 5925, loss 2.41810, smoothed loss 2.59777, grad norm 3.64463, param norm 140.62318
epoch 7, iter 5930, loss 2.55886, smoothed loss 2.58996, grad norm 3.53828, param norm 140.66118
epoch 7, iter 5935, loss 2.92848, smoothed loss 2.59781, grad norm 4.14449, param norm 140.70132
epoch 7, iter 5940, loss 2.03839, smoothed loss 2.57786, grad norm 3.26951, param norm 140.74666
epoch 7, iter 5945, loss 2.22673, smoothed loss 2.56806, grad norm 3.56806, param norm 140.79245
epoch 7, iter 5950, loss 2.41413, smoothed loss 2.56413, grad norm 3.31811, param norm 140.83701
epoch 7, iter 5955, loss 1.90035, smoothed loss 2.56409, grad norm 3.03095, param norm 140.87782
epoch 7, iter 5960, loss 2.28542, smoothed loss 2.56465, grad norm 3.10773, param norm 140.91367
epoch 7, iter 5965, loss 3.04115, smoothed loss 2.57299, grad norm 3.77259, param norm 140.94905
epoch 7, iter 5970, loss 3.20066, smoothed loss 2.57289, grad norm 3.84596, param norm 140.98627
epoch 7, iter 5975, loss 2.18890, smoothed loss 2.57759, grad norm 3.16505, param norm 141.02852
epoch 7, iter 5980, loss 2.24091, smoothed loss 2.58415, grad norm 3.01533, param norm 141.06816
epoch 7, iter 5985, loss 2.56441, smoothed loss 2.57447, grad norm 3.04013, param norm 141.10512
epoch 7, iter 5990, loss 2.70936, smoothed loss 2.56877, grad norm 4.38442, param norm 141.13930
epoch 7, iter 5995, loss 2.31806, smoothed loss 2.56948, grad norm 3.06921, param norm 141.18787
epoch 7, iter 6000, loss 2.13389, smoothed loss 2.56400, grad norm 3.10560, param norm 141.23441
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 6000, dev loss: 3.058075
Calculating Train F1/EM...
F1 train: 1000 examples took 18.81087 seconds [Score: 0.81819]
Exact Match train: 1000 examples took 17.94755 seconds [Score: 0.65900]
Epoch 7, Iter 6000, Train F1 score: 0.818187, Train EM score: 0.659000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.19384 seconds [Score: 0.64032]
Exact Match dev: 7118 examples took 123.55224 seconds [Score: 0.49185]
Epoch 7, Iter 6000, Dev F1 score: 0.640317, Dev EM score: 0.491852
End of epoch 7
epoch 7, iter 6005, loss 2.59532, smoothed loss 2.55615, grad norm 3.70634, param norm 141.27673
epoch 7, iter 6010, loss 2.57136, smoothed loss 2.54383, grad norm 3.69814, param norm 141.31941
epoch 7, iter 6015, loss 2.48529, smoothed loss 2.53774, grad norm 3.46600, param norm 141.35704
epoch 7, iter 6020, loss 2.92191, smoothed loss 2.53131, grad norm 3.89993, param norm 141.39429
epoch 7, iter 6025, loss 2.46772, smoothed loss 2.54850, grad norm 3.52487, param norm 141.43523
epoch 7, iter 6030, loss 2.46467, smoothed loss 2.54480, grad norm 3.35297, param norm 141.47789
epoch 7, iter 6035, loss 2.02157, smoothed loss 2.53789, grad norm 3.12037, param norm 141.52135
epoch 7, iter 6040, loss 2.35692, smoothed loss 2.53226, grad norm 3.19782, param norm 141.56239
epoch 7, iter 6045, loss 2.99098, smoothed loss 2.54550, grad norm 4.76291, param norm 141.59801
epoch 7, iter 6050, loss 2.94111, smoothed loss 2.54144, grad norm 3.65770, param norm 141.63130
epoch 7, iter 6055, loss 2.23267, smoothed loss 2.55168, grad norm 3.60509, param norm 141.66559
epoch 7, iter 6060, loss 2.49387, smoothed loss 2.56116, grad norm 3.77807, param norm 141.69850
epoch 7, iter 6065, loss 2.61932, smoothed loss 2.56197, grad norm 3.87501, param norm 141.73346
epoch 7, iter 6070, loss 2.50226, smoothed loss 2.56159, grad norm 3.70406, param norm 141.76788
epoch 7, iter 6075, loss 2.18396, smoothed loss 2.55423, grad norm 3.03369, param norm 141.80145
epoch 7, iter 6080, loss 2.32453, smoothed loss 2.54322, grad norm 3.44086, param norm 141.83672
epoch 7, iter 6085, loss 2.00615, smoothed loss 2.53528, grad norm 3.26492, param norm 141.87347
epoch 7, iter 6090, loss 2.60841, smoothed loss 2.53384, grad norm 3.46330, param norm 141.91380
epoch 7, iter 6095, loss 2.73704, smoothed loss 2.52820, grad norm 3.76498, param norm 141.95631
epoch 7, iter 6100, loss 2.59075, smoothed loss 2.52860, grad norm 4.03921, param norm 141.99484
epoch 7, iter 6105, loss 2.60840, smoothed loss 2.52087, grad norm 3.50493, param norm 142.02930
epoch 7, iter 6110, loss 2.42126, smoothed loss 2.51523, grad norm 3.37653, param norm 142.06793
epoch 7, iter 6115, loss 1.61704, smoothed loss 2.49956, grad norm 3.07512, param norm 142.10803
epoch 7, iter 6120, loss 2.82759, smoothed loss 2.51072, grad norm 3.91200, param norm 142.14807
epoch 7, iter 6125, loss 3.30432, smoothed loss 2.50676, grad norm 4.11136, param norm 142.18875
epoch 7, iter 6130, loss 2.76625, smoothed loss 2.51097, grad norm 3.54076, param norm 142.22617
epoch 7, iter 6135, loss 2.04721, smoothed loss 2.49714, grad norm 2.78118, param norm 142.26402
epoch 7, iter 6140, loss 3.26208, smoothed loss 2.49657, grad norm 4.23548, param norm 142.30135
epoch 7, iter 6145, loss 2.65923, smoothed loss 2.51309, grad norm 3.67224, param norm 142.33391
epoch 7, iter 6150, loss 3.42602, smoothed loss 2.52162, grad norm 3.37933, param norm 142.36467
epoch 7, iter 6155, loss 2.08448, smoothed loss 2.51504, grad norm 3.73531, param norm 142.39774
epoch 7, iter 6160, loss 1.98583, smoothed loss 2.51363, grad norm 3.10015, param norm 142.43365
epoch 7, iter 6165, loss 1.94688, smoothed loss 2.50071, grad norm 2.85630, param norm 142.46977
epoch 7, iter 6170, loss 2.30655, smoothed loss 2.49865, grad norm 3.08505, param norm 142.50089
epoch 7, iter 6175, loss 2.17597, smoothed loss 2.49485, grad norm 3.69194, param norm 142.53685
epoch 7, iter 6180, loss 2.77095, smoothed loss 2.50102, grad norm 3.92032, param norm 142.56999
epoch 7, iter 6185, loss 2.74860, smoothed loss 2.49584, grad norm 4.19802, param norm 142.60233
epoch 7, iter 6190, loss 1.78081, smoothed loss 2.48135, grad norm 3.08631, param norm 142.63628
epoch 7, iter 6195, loss 2.23792, smoothed loss 2.49349, grad norm 3.30456, param norm 142.66777
epoch 7, iter 6200, loss 2.13839, smoothed loss 2.48465, grad norm 3.35252, param norm 142.70166
epoch 7, iter 6205, loss 2.70095, smoothed loss 2.48752, grad norm 3.68269, param norm 142.73993
epoch 7, iter 6210, loss 2.32621, smoothed loss 2.49397, grad norm 3.23903, param norm 142.77716
epoch 7, iter 6215, loss 2.11696, smoothed loss 2.49289, grad norm 3.50515, param norm 142.81508
epoch 7, iter 6220, loss 2.54051, smoothed loss 2.49422, grad norm 3.59874, param norm 142.85242
epoch 7, iter 6225, loss 2.65888, smoothed loss 2.49651, grad norm 3.85977, param norm 142.89215
epoch 7, iter 6230, loss 2.45566, smoothed loss 2.48967, grad norm 3.49123, param norm 142.92926
epoch 7, iter 6235, loss 2.95271, smoothed loss 2.50374, grad norm 3.61629, param norm 142.96683
epoch 7, iter 6240, loss 2.50158, smoothed loss 2.50399, grad norm 3.22345, param norm 143.00438
epoch 7, iter 6245, loss 2.04526, smoothed loss 2.50563, grad norm 2.60127, param norm 143.03917
epoch 7, iter 6250, loss 2.56724, smoothed loss 2.50218, grad norm 3.79502, param norm 143.07745
epoch 7, iter 6255, loss 2.88573, smoothed loss 2.50290, grad norm 3.39568, param norm 143.11931
epoch 7, iter 6260, loss 2.82345, smoothed loss 2.49926, grad norm 4.18331, param norm 143.15858
epoch 7, iter 6265, loss 2.48568, smoothed loss 2.49642, grad norm 3.00642, param norm 143.19473
epoch 7, iter 6270, loss 2.81386, smoothed loss 2.50373, grad norm 3.49106, param norm 143.22156
epoch 7, iter 6275, loss 2.13129, smoothed loss 2.50322, grad norm 3.46803, param norm 143.24725
epoch 7, iter 6280, loss 3.41939, smoothed loss 2.50653, grad norm 4.21044, param norm 143.28368
epoch 7, iter 6285, loss 2.37191, smoothed loss 2.50647, grad norm 3.75053, param norm 143.32274
epoch 7, iter 6290, loss 2.67279, smoothed loss 2.51328, grad norm 3.52626, param norm 143.36009
epoch 7, iter 6295, loss 2.39916, smoothed loss 2.51713, grad norm 3.71775, param norm 143.39445
epoch 7, iter 6300, loss 3.03110, smoothed loss 2.51553, grad norm 3.70220, param norm 143.42519
epoch 7, iter 6305, loss 2.54759, smoothed loss 2.52294, grad norm 3.66356, param norm 143.46153
epoch 7, iter 6310, loss 2.31444, smoothed loss 2.52178, grad norm 3.19964, param norm 143.50006
epoch 7, iter 6315, loss 2.05632, smoothed loss 2.52162, grad norm 3.76970, param norm 143.53798
epoch 7, iter 6320, loss 2.61079, smoothed loss 2.52457, grad norm 3.38621, param norm 143.57571
epoch 7, iter 6325, loss 2.49489, smoothed loss 2.53224, grad norm 3.09687, param norm 143.61031
epoch 7, iter 6330, loss 2.15479, smoothed loss 2.52042, grad norm 3.11650, param norm 143.64771
epoch 7, iter 6335, loss 2.52079, smoothed loss 2.50834, grad norm 3.65688, param norm 143.68733
epoch 7, iter 6340, loss 3.04368, smoothed loss 2.51307, grad norm 4.72405, param norm 143.72992
epoch 7, iter 6345, loss 2.45740, smoothed loss 2.52385, grad norm 3.27957, param norm 143.76758
epoch 7, iter 6350, loss 2.48632, smoothed loss 2.53355, grad norm 3.35352, param norm 143.80135
epoch 7, iter 6355, loss 2.34251, smoothed loss 2.53117, grad norm 2.94944, param norm 143.83504
epoch 7, iter 6360, loss 2.54861, smoothed loss 2.53999, grad norm 4.00043, param norm 143.86954
epoch 7, iter 6365, loss 2.90806, smoothed loss 2.53819, grad norm 3.58797, param norm 143.90646
epoch 7, iter 6370, loss 2.16946, smoothed loss 2.53555, grad norm 3.40532, param norm 143.94240
epoch 7, iter 6375, loss 2.04297, smoothed loss 2.52896, grad norm 3.06053, param norm 143.97989
epoch 7, iter 6380, loss 2.54445, smoothed loss 2.53276, grad norm 3.95152, param norm 144.01288
epoch 7, iter 6385, loss 2.84343, smoothed loss 2.53718, grad norm 3.97087, param norm 144.04520
epoch 7, iter 6390, loss 2.34281, smoothed loss 2.54422, grad norm 3.22989, param norm 144.08025
epoch 7, iter 6395, loss 2.61137, smoothed loss 2.54434, grad norm 3.26733, param norm 144.11166
epoch 7, iter 6400, loss 2.68555, smoothed loss 2.53901, grad norm 3.80076, param norm 144.14371
epoch 7, iter 6405, loss 2.06292, smoothed loss 2.52819, grad norm 3.19205, param norm 144.17821
epoch 7, iter 6410, loss 2.31775, smoothed loss 2.53296, grad norm 3.30221, param norm 144.21370
epoch 7, iter 6415, loss 2.79521, smoothed loss 2.52432, grad norm 3.27390, param norm 144.24719
epoch 7, iter 6420, loss 2.64159, smoothed loss 2.52630, grad norm 3.83251, param norm 144.28050
epoch 7, iter 6425, loss 2.24794, smoothed loss 2.52484, grad norm 3.40002, param norm 144.31723
epoch 7, iter 6430, loss 2.20090, smoothed loss 2.51423, grad norm 3.10582, param norm 144.35249
epoch 7, iter 6435, loss 2.39040, smoothed loss 2.51479, grad norm 3.32595, param norm 144.38528
epoch 7, iter 6440, loss 2.58471, smoothed loss 2.51668, grad norm 3.68993, param norm 144.41455
epoch 7, iter 6445, loss 2.53372, smoothed loss 2.51090, grad norm 3.25933, param norm 144.44385
epoch 7, iter 6450, loss 2.58868, smoothed loss 2.50869, grad norm 3.60236, param norm 144.47546
epoch 7, iter 6455, loss 1.93465, smoothed loss 2.49573, grad norm 3.24622, param norm 144.51154
epoch 7, iter 6460, loss 2.22800, smoothed loss 2.49453, grad norm 3.80477, param norm 144.54945
epoch 7, iter 6465, loss 2.87453, smoothed loss 2.48275, grad norm 4.40423, param norm 144.58269
epoch 7, iter 6470, loss 1.74127, smoothed loss 2.47587, grad norm 2.85426, param norm 144.61382
epoch 7, iter 6475, loss 2.55335, smoothed loss 2.48131, grad norm 3.87916, param norm 144.64914
epoch 7, iter 6480, loss 2.44907, smoothed loss 2.48247, grad norm 3.90293, param norm 144.68765
epoch 7, iter 6485, loss 2.11701, smoothed loss 2.46794, grad norm 3.41051, param norm 144.72899
epoch 7, iter 6490, loss 1.79194, smoothed loss 2.44723, grad norm 3.16738, param norm 144.77075
epoch 7, iter 6495, loss 2.79736, smoothed loss 2.45913, grad norm 3.68564, param norm 144.80429
epoch 7, iter 6500, loss 2.59840, smoothed loss 2.46573, grad norm 3.45879, param norm 144.83092
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 6500, dev loss: 3.033989
Calculating Train F1/EM...
F1 train: 1000 examples took 17.77856 seconds [Score: 0.78785]
Exact Match train: 1000 examples took 18.92951 seconds [Score: 0.64300]
Epoch 7, Iter 6500, Train F1 score: 0.787846, Train EM score: 0.643000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.60790 seconds [Score: 0.64437]
Exact Match dev: 7118 examples took 125.07009 seconds [Score: 0.49298]
Epoch 7, Iter 6500, Dev F1 score: 0.644371, Dev EM score: 0.492976
End of epoch 7
epoch 7, iter 6505, loss 2.36242, smoothed loss 2.46468, grad norm 3.67961, param norm 144.86165
epoch 7, iter 6510, loss 2.09174, smoothed loss 2.46443, grad norm 3.43223, param norm 144.90048
epoch 7, iter 6515, loss 2.40239, smoothed loss 2.46788, grad norm 2.97036, param norm 144.94078
epoch 7, iter 6520, loss 2.45056, smoothed loss 2.45327, grad norm 3.81904, param norm 144.97597
epoch 7, iter 6525, loss 2.18810, smoothed loss 2.45563, grad norm 3.57670, param norm 145.01410
epoch 7, iter 6530, loss 2.58506, smoothed loss 2.46435, grad norm 3.51858, param norm 145.04770
epoch 7, iter 6535, loss 2.98483, smoothed loss 2.47182, grad norm 3.75012, param norm 145.08037
epoch 7, iter 6540, loss 2.29210, smoothed loss 2.46869, grad norm 3.53094, param norm 145.11182
epoch 7, iter 6545, loss 1.79990, smoothed loss 2.47510, grad norm 2.85553, param norm 145.15038
epoch 7, iter 6550, loss 2.79263, smoothed loss 2.47707, grad norm 3.73862, param norm 145.19286
epoch 7, iter 6555, loss 2.18035, smoothed loss 2.46783, grad norm 2.94119, param norm 145.23088
epoch 7, iter 6560, loss 2.25744, smoothed loss 2.46728, grad norm 3.29380, param norm 145.26735
epoch 7, iter 6565, loss 2.26191, smoothed loss 2.46265, grad norm 3.50977, param norm 145.30171
epoch 7, iter 6570, loss 2.45873, smoothed loss 2.46250, grad norm 3.62599, param norm 145.33704
epoch 7, iter 6575, loss 2.34804, smoothed loss 2.46280, grad norm 3.99803, param norm 145.37512
epoch 7, iter 6580, loss 1.83464, smoothed loss 2.46341, grad norm 3.90657, param norm 145.41383
epoch 7, iter 6585, loss 2.87526, smoothed loss 2.46599, grad norm 3.68564, param norm 145.45486
epoch 7, iter 6590, loss 2.16335, smoothed loss 2.45960, grad norm 3.27130, param norm 145.49518
epoch 7, iter 6595, loss 2.27332, smoothed loss 2.44352, grad norm 3.36161, param norm 145.53278
epoch 7, iter 6600, loss 2.15385, smoothed loss 2.43492, grad norm 3.33255, param norm 145.56654
epoch 7, iter 6605, loss 2.66013, smoothed loss 2.44218, grad norm 3.54588, param norm 145.60110
epoch 8, iter 6610, loss 2.91409, smoothed loss 2.44940, grad norm 3.83301, param norm 145.63472
epoch 8, iter 6615, loss 2.47196, smoothed loss 2.44501, grad norm 3.90246, param norm 145.66800
epoch 8, iter 6620, loss 2.96444, smoothed loss 2.45441, grad norm 3.89588, param norm 145.70230
epoch 8, iter 6625, loss 3.06394, smoothed loss 2.45721, grad norm 3.92906, param norm 145.73859
epoch 8, iter 6630, loss 2.10700, smoothed loss 2.46462, grad norm 3.36423, param norm 145.78122
epoch 8, iter 6635, loss 2.59937, smoothed loss 2.45151, grad norm 3.25875, param norm 145.82288
epoch 8, iter 6640, loss 2.02427, smoothed loss 2.45366, grad norm 3.58724, param norm 145.86349
epoch 8, iter 6645, loss 2.62380, smoothed loss 2.45117, grad norm 3.67564, param norm 145.90379
epoch 8, iter 6650, loss 2.32503, smoothed loss 2.44556, grad norm 3.31332, param norm 145.93655
epoch 8, iter 6655, loss 2.57272, smoothed loss 2.45148, grad norm 3.70128, param norm 145.96844
epoch 8, iter 6660, loss 2.32145, smoothed loss 2.44934, grad norm 3.66458, param norm 146.00594
epoch 8, iter 6665, loss 2.54838, smoothed loss 2.45655, grad norm 3.45619, param norm 146.04485
epoch 8, iter 6670, loss 2.78511, smoothed loss 2.45495, grad norm 4.13158, param norm 146.08775
epoch 8, iter 6675, loss 2.14138, smoothed loss 2.43886, grad norm 3.19897, param norm 146.13000
epoch 8, iter 6680, loss 1.90270, smoothed loss 2.43416, grad norm 3.74468, param norm 146.16750
epoch 8, iter 6685, loss 2.50385, smoothed loss 2.44415, grad norm 4.02882, param norm 146.20566
epoch 8, iter 6690, loss 2.01888, smoothed loss 2.43440, grad norm 2.96558, param norm 146.24559
epoch 8, iter 6695, loss 2.88843, smoothed loss 2.43957, grad norm 3.92265, param norm 146.28593
epoch 8, iter 6700, loss 2.09708, smoothed loss 2.44369, grad norm 3.09368, param norm 146.32550
epoch 8, iter 6705, loss 2.29568, smoothed loss 2.44799, grad norm 3.04776, param norm 146.36464
epoch 8, iter 6710, loss 2.17813, smoothed loss 2.44935, grad norm 3.16176, param norm 146.40097
epoch 8, iter 6715, loss 2.17119, smoothed loss 2.42842, grad norm 3.50181, param norm 146.43814
epoch 8, iter 6720, loss 2.67722, smoothed loss 2.44367, grad norm 4.01677, param norm 146.47638
epoch 8, iter 6725, loss 2.46353, smoothed loss 2.44076, grad norm 3.92584, param norm 146.51640
epoch 8, iter 6730, loss 2.04125, smoothed loss 2.42305, grad norm 3.53889, param norm 146.55669
epoch 8, iter 6735, loss 2.31057, smoothed loss 2.42678, grad norm 3.45017, param norm 146.59840
epoch 8, iter 6740, loss 2.19736, smoothed loss 2.42543, grad norm 3.24560, param norm 146.63664
epoch 8, iter 6745, loss 2.67484, smoothed loss 2.42339, grad norm 3.60781, param norm 146.67203
epoch 8, iter 6750, loss 2.31402, smoothed loss 2.43516, grad norm 3.54171, param norm 146.70422
epoch 8, iter 6755, loss 2.44382, smoothed loss 2.44165, grad norm 3.79408, param norm 146.74263
epoch 8, iter 6760, loss 2.83584, smoothed loss 2.44768, grad norm 4.01679, param norm 146.78113
epoch 8, iter 6765, loss 2.04065, smoothed loss 2.43921, grad norm 3.38595, param norm 146.81720
epoch 8, iter 6770, loss 2.37862, smoothed loss 2.45133, grad norm 3.83103, param norm 146.85193
epoch 8, iter 6775, loss 2.27694, smoothed loss 2.45610, grad norm 3.58193, param norm 146.88297
epoch 8, iter 6780, loss 2.51984, smoothed loss 2.45794, grad norm 4.17838, param norm 146.91841
epoch 8, iter 6785, loss 2.40430, smoothed loss 2.45099, grad norm 3.32772, param norm 146.95889
epoch 8, iter 6790, loss 2.55053, smoothed loss 2.44311, grad norm 3.48110, param norm 146.99603
epoch 8, iter 6795, loss 2.43937, smoothed loss 2.44526, grad norm 3.69182, param norm 147.03395
epoch 8, iter 6800, loss 2.75905, smoothed loss 2.45329, grad norm 3.83530, param norm 147.07100
epoch 8, iter 6805, loss 2.89107, smoothed loss 2.45737, grad norm 4.23512, param norm 147.11026
epoch 8, iter 6810, loss 2.28116, smoothed loss 2.45415, grad norm 3.69999, param norm 147.15407
epoch 8, iter 6815, loss 2.69238, smoothed loss 2.45027, grad norm 3.75468, param norm 147.19246
epoch 8, iter 6820, loss 2.50636, smoothed loss 2.45517, grad norm 3.56202, param norm 147.22801
epoch 8, iter 6825, loss 2.45091, smoothed loss 2.45533, grad norm 3.64244, param norm 147.26248
epoch 8, iter 6830, loss 2.83731, smoothed loss 2.45555, grad norm 3.71390, param norm 147.29916
epoch 8, iter 6835, loss 2.32707, smoothed loss 2.44927, grad norm 3.52576, param norm 147.34016
epoch 8, iter 6840, loss 2.95490, smoothed loss 2.45635, grad norm 4.53711, param norm 147.37796
epoch 8, iter 6845, loss 2.16946, smoothed loss 2.45985, grad norm 3.23427, param norm 147.41266
epoch 8, iter 6850, loss 2.45969, smoothed loss 2.45699, grad norm 3.64941, param norm 147.44815
epoch 8, iter 6855, loss 2.48973, smoothed loss 2.46041, grad norm 3.16096, param norm 147.48888
epoch 8, iter 6860, loss 3.01328, smoothed loss 2.46388, grad norm 4.17478, param norm 147.53065
epoch 8, iter 6865, loss 1.82036, smoothed loss 2.45793, grad norm 3.15250, param norm 147.56870
epoch 8, iter 6870, loss 2.72394, smoothed loss 2.46042, grad norm 3.81355, param norm 147.60719
epoch 8, iter 6875, loss 2.38878, smoothed loss 2.45560, grad norm 3.63313, param norm 147.64566
epoch 8, iter 6880, loss 2.44512, smoothed loss 2.45380, grad norm 3.59936, param norm 147.68338
epoch 8, iter 6885, loss 2.79012, smoothed loss 2.45713, grad norm 4.00905, param norm 147.72151
epoch 8, iter 6890, loss 3.20599, smoothed loss 2.46602, grad norm 4.39264, param norm 147.75581
epoch 8, iter 6895, loss 2.37515, smoothed loss 2.46011, grad norm 3.84382, param norm 147.79152
epoch 8, iter 6900, loss 2.11550, smoothed loss 2.45066, grad norm 3.17033, param norm 147.82532
epoch 8, iter 6905, loss 2.38433, smoothed loss 2.44866, grad norm 3.65507, param norm 147.85703
epoch 8, iter 6910, loss 1.90783, smoothed loss 2.44750, grad norm 3.27516, param norm 147.88850
epoch 8, iter 6915, loss 2.65709, smoothed loss 2.45529, grad norm 3.54360, param norm 147.92097
epoch 8, iter 6920, loss 2.54641, smoothed loss 2.46441, grad norm 3.76496, param norm 147.95428
epoch 8, iter 6925, loss 2.14781, smoothed loss 2.46343, grad norm 4.08480, param norm 147.99007
epoch 8, iter 6930, loss 2.49803, smoothed loss 2.45280, grad norm 4.00239, param norm 148.02849
epoch 8, iter 6935, loss 2.37176, smoothed loss 2.43709, grad norm 3.68488, param norm 148.07079
epoch 8, iter 6940, loss 2.18616, smoothed loss 2.43539, grad norm 3.53857, param norm 148.10966
epoch 8, iter 6945, loss 2.00596, smoothed loss 2.44043, grad norm 3.20285, param norm 148.14877
epoch 8, iter 6950, loss 2.65890, smoothed loss 2.45435, grad norm 3.52084, param norm 148.18724
epoch 8, iter 6955, loss 2.79153, smoothed loss 2.45991, grad norm 3.81243, param norm 148.22289
epoch 8, iter 6960, loss 1.97945, smoothed loss 2.45204, grad norm 3.30329, param norm 148.26056
epoch 8, iter 6965, loss 2.48031, smoothed loss 2.43832, grad norm 3.56502, param norm 148.29765
epoch 8, iter 6970, loss 2.12988, smoothed loss 2.43015, grad norm 3.61990, param norm 148.33168
epoch 8, iter 6975, loss 2.35680, smoothed loss 2.42653, grad norm 4.04842, param norm 148.36084
epoch 8, iter 6980, loss 2.18457, smoothed loss 2.41480, grad norm 3.31800, param norm 148.39432
epoch 8, iter 6985, loss 2.33922, smoothed loss 2.41214, grad norm 3.76049, param norm 148.43489
epoch 8, iter 6990, loss 1.98856, smoothed loss 2.41499, grad norm 3.38343, param norm 148.47763
epoch 8, iter 6995, loss 2.22651, smoothed loss 2.41671, grad norm 3.49522, param norm 148.51613
epoch 8, iter 7000, loss 2.12177, smoothed loss 2.40821, grad norm 3.30941, param norm 148.55321
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 8, Iter 7000, dev loss: 3.110446
Calculating Train F1/EM...
F1 train: 1000 examples took 19.15331 seconds [Score: 0.82410]
Exact Match train: 1000 examples took 18.99837 seconds [Score: 0.66600]
Epoch 8, Iter 7000, Train F1 score: 0.824099, Train EM score: 0.666000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.91886 seconds [Score: 0.64670]
Exact Match dev: 7118 examples took 125.42503 seconds [Score: 0.49733]
Epoch 8, Iter 7000, Dev F1 score: 0.646702, Dev EM score: 0.497331
End of epoch 8
epoch 8, iter 7005, loss 2.22147, smoothed loss 2.40326, grad norm 3.37453, param norm 148.58664
epoch 8, iter 7010, loss 2.23312, smoothed loss 2.39990, grad norm 3.33962, param norm 148.62007
epoch 8, iter 7015, loss 2.26148, smoothed loss 2.39992, grad norm 3.32321, param norm 148.65504
epoch 8, iter 7020, loss 2.82905, smoothed loss 2.39968, grad norm 4.29826, param norm 148.68912
epoch 8, iter 7025, loss 2.35873, smoothed loss 2.40346, grad norm 3.49620, param norm 148.72157
epoch 8, iter 7030, loss 2.02904, smoothed loss 2.38990, grad norm 3.77988, param norm 148.75603
epoch 8, iter 7035, loss 2.01696, smoothed loss 2.39426, grad norm 3.10909, param norm 148.78993
epoch 8, iter 7040, loss 2.35913, smoothed loss 2.38951, grad norm 3.91830, param norm 148.82358
epoch 8, iter 7045, loss 1.38492, smoothed loss 2.37726, grad norm 3.36655, param norm 148.86334
epoch 8, iter 7050, loss 2.54982, smoothed loss 2.37008, grad norm 3.67742, param norm 148.90193
epoch 8, iter 7055, loss 2.47350, smoothed loss 2.37899, grad norm 3.61269, param norm 148.93483
epoch 8, iter 7060, loss 2.48442, smoothed loss 2.38052, grad norm 3.79616, param norm 148.96640
epoch 8, iter 7065, loss 2.84965, smoothed loss 2.39102, grad norm 3.88329, param norm 149.00047
epoch 8, iter 7070, loss 1.94896, smoothed loss 2.39117, grad norm 3.05789, param norm 149.03207
epoch 8, iter 7075, loss 2.89211, smoothed loss 2.40217, grad norm 3.86169, param norm 149.06456
epoch 8, iter 7080, loss 2.71534, smoothed loss 2.40211, grad norm 4.03839, param norm 149.09433
epoch 8, iter 7085, loss 2.42459, smoothed loss 2.40287, grad norm 3.59300, param norm 149.12326
epoch 8, iter 7090, loss 2.74843, smoothed loss 2.40621, grad norm 3.77350, param norm 149.15244
epoch 8, iter 7095, loss 2.19070, smoothed loss 2.39343, grad norm 3.35187, param norm 149.18839
epoch 8, iter 7100, loss 2.33522, smoothed loss 2.39105, grad norm 3.47185, param norm 149.22452
epoch 8, iter 7105, loss 2.74809, smoothed loss 2.38998, grad norm 4.13113, param norm 149.25894
epoch 8, iter 7110, loss 1.82089, smoothed loss 2.38216, grad norm 3.15965, param norm 149.28784
epoch 8, iter 7115, loss 2.39437, smoothed loss 2.37980, grad norm 3.73642, param norm 149.31563
epoch 8, iter 7120, loss 3.00744, smoothed loss 2.37584, grad norm 4.12498, param norm 149.34665
epoch 8, iter 7125, loss 2.13515, smoothed loss 2.38140, grad norm 3.65512, param norm 149.37720
epoch 8, iter 7130, loss 2.20456, smoothed loss 2.38750, grad norm 3.48747, param norm 149.41109
epoch 8, iter 7135, loss 2.79086, smoothed loss 2.38878, grad norm 3.98685, param norm 149.44995
epoch 8, iter 7140, loss 2.19673, smoothed loss 2.37608, grad norm 3.40697, param norm 149.48407
epoch 8, iter 7145, loss 2.52449, smoothed loss 2.38326, grad norm 3.96254, param norm 149.51878
epoch 8, iter 7150, loss 1.85295, smoothed loss 2.39492, grad norm 3.15782, param norm 149.55165
epoch 8, iter 7155, loss 2.28132, smoothed loss 2.40082, grad norm 3.09790, param norm 149.58453
epoch 8, iter 7160, loss 2.17536, smoothed loss 2.40583, grad norm 3.01939, param norm 149.61710
epoch 8, iter 7165, loss 2.21834, smoothed loss 2.41396, grad norm 3.33028, param norm 149.64926
epoch 8, iter 7170, loss 2.38046, smoothed loss 2.42099, grad norm 3.45809, param norm 149.68207
epoch 8, iter 7175, loss 2.73761, smoothed loss 2.43117, grad norm 3.73812, param norm 149.71904
epoch 8, iter 7180, loss 2.69961, smoothed loss 2.43042, grad norm 4.03687, param norm 149.75580
epoch 8, iter 7185, loss 2.45396, smoothed loss 2.44249, grad norm 3.46815, param norm 149.79030
epoch 8, iter 7190, loss 2.72975, smoothed loss 2.43706, grad norm 3.23772, param norm 149.82280
epoch 8, iter 7195, loss 2.46009, smoothed loss 2.43736, grad norm 3.45866, param norm 149.85298
epoch 8, iter 7200, loss 3.09558, smoothed loss 2.44084, grad norm 3.69838, param norm 149.88205
epoch 8, iter 7205, loss 2.34971, smoothed loss 2.42738, grad norm 3.58425, param norm 149.91182
epoch 8, iter 7210, loss 2.28057, smoothed loss 2.42689, grad norm 3.68573, param norm 149.93983
epoch 8, iter 7215, loss 2.97242, smoothed loss 2.42728, grad norm 4.17274, param norm 149.96820
epoch 8, iter 7220, loss 2.47549, smoothed loss 2.42860, grad norm 3.47520, param norm 150.00166
epoch 8, iter 7225, loss 1.88103, smoothed loss 2.41729, grad norm 3.93415, param norm 150.03296
epoch 8, iter 7230, loss 1.72465, smoothed loss 2.40447, grad norm 3.16092, param norm 150.06364
epoch 8, iter 7235, loss 2.85248, smoothed loss 2.41672, grad norm 4.25048, param norm 150.09558
epoch 8, iter 7240, loss 1.86979, smoothed loss 2.40681, grad norm 3.12781, param norm 150.12593
epoch 8, iter 7245, loss 2.07627, smoothed loss 2.40916, grad norm 3.66378, param norm 150.15720
epoch 8, iter 7250, loss 2.63076, smoothed loss 2.41012, grad norm 3.96598, param norm 150.19252
epoch 8, iter 7255, loss 1.91560, smoothed loss 2.39929, grad norm 3.53742, param norm 150.22688
epoch 8, iter 7260, loss 2.12994, smoothed loss 2.40226, grad norm 3.58141, param norm 150.25867
epoch 8, iter 7265, loss 2.55196, smoothed loss 2.41175, grad norm 3.78074, param norm 150.29173
epoch 8, iter 7270, loss 2.67138, smoothed loss 2.40075, grad norm 4.13830, param norm 150.32484
epoch 8, iter 7275, loss 2.67048, smoothed loss 2.38377, grad norm 3.49042, param norm 150.36121
epoch 8, iter 7280, loss 2.09571, smoothed loss 2.37778, grad norm 3.60925, param norm 150.39833
epoch 8, iter 7285, loss 2.11063, smoothed loss 2.37427, grad norm 3.34498, param norm 150.43285
epoch 8, iter 7290, loss 2.58265, smoothed loss 2.38113, grad norm 3.60617, param norm 150.46700
epoch 8, iter 7295, loss 2.12473, smoothed loss 2.38046, grad norm 3.47377, param norm 150.49825
epoch 8, iter 7300, loss 2.14863, smoothed loss 2.37720, grad norm 3.32444, param norm 150.52916
epoch 8, iter 7305, loss 2.33277, smoothed loss 2.38159, grad norm 3.52808, param norm 150.56125
epoch 8, iter 7310, loss 2.55911, smoothed loss 2.38702, grad norm 3.70294, param norm 150.59644
epoch 8, iter 7315, loss 2.29420, smoothed loss 2.38443, grad norm 3.93522, param norm 150.63512
epoch 8, iter 7320, loss 3.27273, smoothed loss 2.39460, grad norm 4.26215, param norm 150.67224
epoch 8, iter 7325, loss 2.46726, smoothed loss 2.39965, grad norm 3.57958, param norm 150.71010
epoch 8, iter 7330, loss 3.12483, smoothed loss 2.39893, grad norm 4.32306, param norm 150.74669
epoch 8, iter 7335, loss 2.02279, smoothed loss 2.39479, grad norm 3.42419, param norm 150.78349
epoch 8, iter 7340, loss 2.54639, smoothed loss 2.39827, grad norm 4.01992, param norm 150.81883
epoch 8, iter 7345, loss 2.65933, smoothed loss 2.39541, grad norm 4.28249, param norm 150.85014
epoch 8, iter 7350, loss 2.94726, smoothed loss 2.39981, grad norm 4.36770, param norm 150.88158
epoch 8, iter 7355, loss 2.25408, smoothed loss 2.39785, grad norm 3.36149, param norm 150.91595
epoch 8, iter 7360, loss 2.59366, smoothed loss 2.39721, grad norm 3.66196, param norm 150.95114
epoch 8, iter 7365, loss 2.29229, smoothed loss 2.40728, grad norm 3.05149, param norm 150.98340
epoch 8, iter 7370, loss 1.90558, smoothed loss 2.41227, grad norm 3.03441, param norm 151.01573
epoch 8, iter 7375, loss 2.50356, smoothed loss 2.41439, grad norm 3.64486, param norm 151.04819
epoch 8, iter 7380, loss 2.52983, smoothed loss 2.41072, grad norm 3.72055, param norm 151.07689
epoch 8, iter 7385, loss 2.18868, smoothed loss 2.39915, grad norm 3.76799, param norm 151.10550
epoch 8, iter 7390, loss 2.62963, smoothed loss 2.39727, grad norm 3.57216, param norm 151.13498
epoch 8, iter 7395, loss 2.08100, smoothed loss 2.39041, grad norm 3.46161, param norm 151.16385
epoch 8, iter 7400, loss 1.93351, smoothed loss 2.37844, grad norm 3.95228, param norm 151.19868
epoch 8, iter 7405, loss 2.48040, smoothed loss 2.38683, grad norm 4.05272, param norm 151.23056
epoch 8, iter 7410, loss 2.01211, smoothed loss 2.38846, grad norm 3.37073, param norm 151.26247
epoch 8, iter 7415, loss 2.86707, smoothed loss 2.40574, grad norm 3.71220, param norm 151.29079
epoch 8, iter 7420, loss 2.07231, smoothed loss 2.39279, grad norm 3.68134, param norm 151.32082
epoch 8, iter 7425, loss 2.66990, smoothed loss 2.39785, grad norm 3.57283, param norm 151.35692
epoch 8, iter 7430, loss 2.51006, smoothed loss 2.40598, grad norm 4.21482, param norm 151.39207
epoch 8, iter 7435, loss 2.36941, smoothed loss 2.40341, grad norm 3.70161, param norm 151.42632
epoch 8, iter 7440, loss 2.69562, smoothed loss 2.40631, grad norm 3.72278, param norm 151.46477
epoch 8, iter 7445, loss 2.36526, smoothed loss 2.39737, grad norm 3.86559, param norm 151.50049
epoch 8, iter 7450, loss 2.03187, smoothed loss 2.39223, grad norm 3.57848, param norm 151.53578
epoch 8, iter 7455, loss 2.28252, smoothed loss 2.39345, grad norm 3.52101, param norm 151.57140
epoch 8, iter 7460, loss 2.58633, smoothed loss 2.40012, grad norm 3.90846, param norm 151.60608
epoch 8, iter 7465, loss 1.86271, smoothed loss 2.39100, grad norm 3.42125, param norm 151.63544
epoch 8, iter 7470, loss 2.27760, smoothed loss 2.38743, grad norm 3.91756, param norm 151.66164
epoch 8, iter 7475, loss 2.02864, smoothed loss 2.38299, grad norm 3.71446, param norm 151.68922
epoch 8, iter 7480, loss 1.80522, smoothed loss 2.37508, grad norm 3.04947, param norm 151.71800
epoch 8, iter 7485, loss 2.17073, smoothed loss 2.35964, grad norm 3.52147, param norm 151.74614
epoch 8, iter 7490, loss 2.92337, smoothed loss 2.35807, grad norm 3.80010, param norm 151.77856
epoch 8, iter 7495, loss 2.09797, smoothed loss 2.35797, grad norm 3.51718, param norm 151.81175
epoch 8, iter 7500, loss 2.04402, smoothed loss 2.33968, grad norm 3.29957, param norm 151.84599
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 8, Iter 7500, dev loss: 3.061874
Calculating Train F1/EM...
F1 train: 1000 examples took 18.36110 seconds [Score: 0.79815]
Exact Match train: 1000 examples took 19.39572 seconds [Score: 0.68600]
Epoch 8, Iter 7500, Train F1 score: 0.798149, Train EM score: 0.686000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.76724 seconds [Score: 0.64839]
Exact Match dev: 7118 examples took 125.53995 seconds [Score: 0.50126]
Epoch 8, Iter 7500, Dev F1 score: 0.648390, Dev EM score: 0.501264
End of epoch 8
epoch 8, iter 7505, loss 1.89506, smoothed loss 2.34105, grad norm 3.02288, param norm 151.88342
epoch 8, iter 7510, loss 2.48872, smoothed loss 2.34666, grad norm 4.27383, param norm 151.92014
epoch 8, iter 7515, loss 2.06439, smoothed loss 2.33392, grad norm 3.09875, param norm 151.95653
epoch 8, iter 7520, loss 2.59169, smoothed loss 2.33328, grad norm 3.73997, param norm 151.98763
epoch 8, iter 7525, loss 2.26525, smoothed loss 2.33437, grad norm 3.57060, param norm 152.01910
epoch 8, iter 7530, loss 1.98898, smoothed loss 2.33262, grad norm 3.11440, param norm 152.05101
epoch 8, iter 7535, loss 2.32686, smoothed loss 2.33301, grad norm 4.10934, param norm 152.08328
epoch 8, iter 7540, loss 1.85719, smoothed loss 2.32571, grad norm 3.01756, param norm 152.11775
epoch 8, iter 7545, loss 2.16633, smoothed loss 2.31665, grad norm 4.06951, param norm 152.15118
epoch 8, iter 7550, loss 2.40247, smoothed loss 2.31752, grad norm 3.66748, param norm 152.18518
epoch 9, iter 7555, loss 2.52972, smoothed loss 2.32138, grad norm 3.99584, param norm 152.21748
epoch 9, iter 7560, loss 2.52365, smoothed loss 2.31037, grad norm 3.30796, param norm 152.25383
epoch 9, iter 7565, loss 2.34545, smoothed loss 2.30871, grad norm 3.22681, param norm 152.28951
epoch 9, iter 7570, loss 2.53171, smoothed loss 2.30500, grad norm 4.13277, param norm 152.32414
epoch 9, iter 7575, loss 3.32714, smoothed loss 2.29316, grad norm 4.69463, param norm 152.35788
epoch 9, iter 7580, loss 2.50357, smoothed loss 2.29374, grad norm 4.12572, param norm 152.39250
epoch 9, iter 7585, loss 1.94516, smoothed loss 2.29025, grad norm 3.68171, param norm 152.42813
epoch 9, iter 7590, loss 2.05219, smoothed loss 2.28286, grad norm 3.21755, param norm 152.46487
epoch 9, iter 7595, loss 2.52100, smoothed loss 2.28678, grad norm 3.98145, param norm 152.49989
epoch 9, iter 7600, loss 1.97859, smoothed loss 2.28891, grad norm 2.97329, param norm 152.53636
epoch 9, iter 7605, loss 2.32250, smoothed loss 2.29047, grad norm 3.55505, param norm 152.57280
epoch 9, iter 7610, loss 2.24548, smoothed loss 2.28496, grad norm 3.65223, param norm 152.60648
epoch 9, iter 7615, loss 2.71443, smoothed loss 2.28944, grad norm 3.85064, param norm 152.63921
epoch 9, iter 7620, loss 2.25831, smoothed loss 2.29591, grad norm 3.32153, param norm 152.66858
epoch 9, iter 7625, loss 2.11881, smoothed loss 2.30108, grad norm 3.34274, param norm 152.69907
epoch 9, iter 7630, loss 2.62256, smoothed loss 2.29862, grad norm 4.05089, param norm 152.73233
epoch 9, iter 7635, loss 2.74822, smoothed loss 2.31814, grad norm 4.13686, param norm 152.76424
epoch 9, iter 7640, loss 2.78709, smoothed loss 2.31471, grad norm 4.00244, param norm 152.79662
epoch 9, iter 7645, loss 2.44679, smoothed loss 2.31362, grad norm 3.52894, param norm 152.83102
epoch 9, iter 7650, loss 1.89716, smoothed loss 2.31725, grad norm 3.04233, param norm 152.86620
epoch 9, iter 7655, loss 2.25810, smoothed loss 2.32107, grad norm 3.98878, param norm 152.90062
epoch 9, iter 7660, loss 2.38108, smoothed loss 2.32092, grad norm 3.80551, param norm 152.93307
epoch 9, iter 7665, loss 2.56129, smoothed loss 2.33354, grad norm 4.05280, param norm 152.96529
epoch 9, iter 7670, loss 1.74630, smoothed loss 2.32614, grad norm 3.43042, param norm 152.99733
epoch 9, iter 7675, loss 2.00402, smoothed loss 2.32186, grad norm 3.48125, param norm 153.02913
epoch 9, iter 7680, loss 2.11607, smoothed loss 2.33043, grad norm 3.13656, param norm 153.05852
epoch 9, iter 7685, loss 2.05570, smoothed loss 2.33200, grad norm 3.38274, param norm 153.09119
epoch 9, iter 7690, loss 2.14752, smoothed loss 2.32901, grad norm 3.12286, param norm 153.12804
epoch 9, iter 7695, loss 2.01390, smoothed loss 2.32284, grad norm 3.63930, param norm 153.16458
epoch 9, iter 7700, loss 2.99507, smoothed loss 2.33309, grad norm 4.05581, param norm 153.19995
epoch 9, iter 7705, loss 2.24788, smoothed loss 2.33446, grad norm 3.64102, param norm 153.23512
epoch 9, iter 7710, loss 2.07275, smoothed loss 2.32744, grad norm 4.12235, param norm 153.27174
epoch 9, iter 7715, loss 2.11853, smoothed loss 2.32674, grad norm 3.96003, param norm 153.30705
epoch 9, iter 7720, loss 2.60055, smoothed loss 2.34023, grad norm 3.40356, param norm 153.33920
epoch 9, iter 7725, loss 2.44308, smoothed loss 2.34579, grad norm 4.00497, param norm 153.36769
epoch 9, iter 7730, loss 2.02868, smoothed loss 2.35432, grad norm 3.72906, param norm 153.40009
epoch 9, iter 7735, loss 2.61718, smoothed loss 2.36085, grad norm 3.68968, param norm 153.43459
epoch 9, iter 7740, loss 2.60454, smoothed loss 2.36753, grad norm 3.85050, param norm 153.46727
epoch 9, iter 7745, loss 2.16736, smoothed loss 2.36350, grad norm 3.55747, param norm 153.49924
epoch 9, iter 7750, loss 2.08951, smoothed loss 2.35907, grad norm 3.50450, param norm 153.52731
epoch 9, iter 7755, loss 2.21779, smoothed loss 2.35313, grad norm 3.73208, param norm 153.55232
epoch 9, iter 7760, loss 2.49559, smoothed loss 2.36233, grad norm 3.70650, param norm 153.58199
epoch 9, iter 7765, loss 1.62755, smoothed loss 2.34459, grad norm 3.32383, param norm 153.61829
epoch 9, iter 7770, loss 2.25413, smoothed loss 2.35919, grad norm 3.45301, param norm 153.65695
epoch 9, iter 7775, loss 2.72351, smoothed loss 2.36402, grad norm 4.29496, param norm 153.69765
epoch 9, iter 7780, loss 1.80697, smoothed loss 2.34607, grad norm 3.31997, param norm 153.73857
epoch 9, iter 7785, loss 3.47014, smoothed loss 2.35183, grad norm 5.41497, param norm 153.77760
epoch 9, iter 7790, loss 3.09294, smoothed loss 2.34945, grad norm 4.20814, param norm 153.81012
epoch 9, iter 7795, loss 2.13774, smoothed loss 2.35192, grad norm 3.23469, param norm 153.83931
epoch 9, iter 7800, loss 1.95755, smoothed loss 2.34241, grad norm 3.67069, param norm 153.87239
epoch 9, iter 7805, loss 1.90716, smoothed loss 2.33682, grad norm 3.48123, param norm 153.90550
epoch 9, iter 7810, loss 2.34446, smoothed loss 2.33618, grad norm 3.52232, param norm 153.93904
epoch 9, iter 7815, loss 2.01349, smoothed loss 2.33126, grad norm 3.55447, param norm 153.96967
epoch 9, iter 7820, loss 2.85007, smoothed loss 2.33866, grad norm 4.23934, param norm 154.00197
epoch 9, iter 7825, loss 2.64762, smoothed loss 2.34352, grad norm 3.63310, param norm 154.03752
epoch 9, iter 7830, loss 2.30109, smoothed loss 2.35245, grad norm 3.46056, param norm 154.07277
epoch 9, iter 7835, loss 1.91561, smoothed loss 2.33659, grad norm 2.87527, param norm 154.10690
epoch 9, iter 7840, loss 2.76353, smoothed loss 2.33505, grad norm 3.92483, param norm 154.13821
epoch 9, iter 7845, loss 2.48928, smoothed loss 2.32899, grad norm 3.80464, param norm 154.16840
epoch 9, iter 7850, loss 1.84995, smoothed loss 2.33116, grad norm 3.87370, param norm 154.20093
epoch 9, iter 7855, loss 2.20815, smoothed loss 2.33719, grad norm 3.65249, param norm 154.23611
epoch 9, iter 7860, loss 1.90875, smoothed loss 2.33184, grad norm 3.57378, param norm 154.26833
epoch 9, iter 7865, loss 2.04363, smoothed loss 2.34307, grad norm 3.14944, param norm 154.30074
epoch 9, iter 7870, loss 2.50918, smoothed loss 2.33817, grad norm 3.80937, param norm 154.33772
epoch 9, iter 7875, loss 2.19987, smoothed loss 2.33074, grad norm 4.19900, param norm 154.37833
epoch 9, iter 7880, loss 2.64457, smoothed loss 2.32071, grad norm 4.23631, param norm 154.41595
epoch 9, iter 7885, loss 1.70600, smoothed loss 2.31129, grad norm 3.30767, param norm 154.45273
epoch 9, iter 7890, loss 2.19064, smoothed loss 2.30305, grad norm 3.96475, param norm 154.48889
epoch 9, iter 7895, loss 2.35197, smoothed loss 2.30925, grad norm 3.77921, param norm 154.52516
epoch 9, iter 7900, loss 2.18029, smoothed loss 2.30759, grad norm 3.33511, param norm 154.56364
epoch 9, iter 7905, loss 1.82749, smoothed loss 2.29980, grad norm 3.38821, param norm 154.59914
epoch 9, iter 7910, loss 1.79395, smoothed loss 2.29799, grad norm 3.46666, param norm 154.63573
epoch 9, iter 7915, loss 2.11693, smoothed loss 2.29841, grad norm 3.90712, param norm 154.67647
epoch 9, iter 7920, loss 2.20315, smoothed loss 2.29880, grad norm 3.89893, param norm 154.71594
epoch 9, iter 7925, loss 2.54108, smoothed loss 2.30356, grad norm 3.87691, param norm 154.75247
epoch 9, iter 7930, loss 2.47723, smoothed loss 2.30966, grad norm 3.10754, param norm 154.78694
epoch 9, iter 7935, loss 2.38681, smoothed loss 2.31064, grad norm 4.01897, param norm 154.82115
epoch 9, iter 7940, loss 2.26313, smoothed loss 2.30703, grad norm 3.33276, param norm 154.85475
epoch 9, iter 7945, loss 2.16681, smoothed loss 2.30085, grad norm 3.49847, param norm 154.88702
epoch 9, iter 7950, loss 1.91284, smoothed loss 2.30262, grad norm 2.90297, param norm 154.91689
epoch 9, iter 7955, loss 2.58215, smoothed loss 2.30377, grad norm 3.75911, param norm 154.94754
epoch 9, iter 7960, loss 3.03527, smoothed loss 2.29494, grad norm 4.89849, param norm 154.98224
epoch 9, iter 7965, loss 2.35249, smoothed loss 2.29322, grad norm 4.35125, param norm 155.01224
epoch 9, iter 7970, loss 2.56181, smoothed loss 2.30320, grad norm 3.84900, param norm 155.03992
epoch 9, iter 7975, loss 2.07032, smoothed loss 2.29581, grad norm 3.88568, param norm 155.06834
epoch 9, iter 7980, loss 2.34247, smoothed loss 2.29360, grad norm 4.40756, param norm 155.09641
epoch 9, iter 7985, loss 1.48431, smoothed loss 2.28117, grad norm 2.76221, param norm 155.12775
epoch 9, iter 7990, loss 2.40332, smoothed loss 2.29184, grad norm 3.68047, param norm 155.15709
epoch 9, iter 7995, loss 1.98691, smoothed loss 2.28738, grad norm 3.57942, param norm 155.18462
epoch 9, iter 8000, loss 2.13284, smoothed loss 2.27412, grad norm 3.02863, param norm 155.21558
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 9, Iter 8000, dev loss: 3.067425
Calculating Train F1/EM...
F1 train: 1000 examples took 18.32643 seconds [Score: 0.82924]
Exact Match train: 1000 examples took 18.74132 seconds [Score: 0.68800]
Epoch 9, Iter 8000, Train F1 score: 0.829239, Train EM score: 0.688000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.94302 seconds [Score: 0.65127]
Exact Match dev: 7118 examples took 125.22351 seconds [Score: 0.49803]
Epoch 9, Iter 8000, Dev F1 score: 0.651273, Dev EM score: 0.498033
End of epoch 9
epoch 9, iter 8005, loss 2.06474, smoothed loss 2.26876, grad norm 3.68262, param norm 155.24785
epoch 9, iter 8010, loss 2.55556, smoothed loss 2.26785, grad norm 3.82434, param norm 155.28065
epoch 9, iter 8015, loss 2.25391, smoothed loss 2.25665, grad norm 3.33530, param norm 155.31306
epoch 9, iter 8020, loss 2.01348, smoothed loss 2.24623, grad norm 3.42366, param norm 155.34042
epoch 9, iter 8025, loss 1.92987, smoothed loss 2.25133, grad norm 3.49687, param norm 155.37074
epoch 9, iter 8030, loss 2.55212, smoothed loss 2.25872, grad norm 4.31012, param norm 155.40497
epoch 9, iter 8035, loss 2.38443, smoothed loss 2.26332, grad norm 3.98481, param norm 155.43579
epoch 9, iter 8040, loss 2.74103, smoothed loss 2.27792, grad norm 3.79737, param norm 155.46838
epoch 9, iter 8045, loss 1.98689, smoothed loss 2.27971, grad norm 3.22516, param norm 155.50157
epoch 9, iter 8050, loss 2.65202, smoothed loss 2.29246, grad norm 3.61725, param norm 155.53116
epoch 9, iter 8055, loss 2.95790, smoothed loss 2.29856, grad norm 4.04428, param norm 155.56061
epoch 9, iter 8060, loss 2.35485, smoothed loss 2.29847, grad norm 3.78590, param norm 155.59169
epoch 9, iter 8065, loss 2.62643, smoothed loss 2.29191, grad norm 3.98988, param norm 155.62199
epoch 9, iter 8070, loss 3.06808, smoothed loss 2.30219, grad norm 4.41030, param norm 155.65298
epoch 9, iter 8075, loss 2.33200, smoothed loss 2.30485, grad norm 3.76985, param norm 155.67863
epoch 9, iter 8080, loss 1.93977, smoothed loss 2.30250, grad norm 3.83284, param norm 155.70529
epoch 9, iter 8085, loss 2.17859, smoothed loss 2.30580, grad norm 3.68228, param norm 155.73900
epoch 9, iter 8090, loss 2.11624, smoothed loss 2.30373, grad norm 3.73840, param norm 155.77553
epoch 9, iter 8095, loss 1.91239, smoothed loss 2.30448, grad norm 3.66190, param norm 155.81114
epoch 9, iter 8100, loss 1.99916, smoothed loss 2.29442, grad norm 3.93029, param norm 155.84621
epoch 9, iter 8105, loss 2.06249, smoothed loss 2.28085, grad norm 3.66505, param norm 155.88071
epoch 9, iter 8110, loss 2.28144, smoothed loss 2.27431, grad norm 3.89103, param norm 155.90894
epoch 9, iter 8115, loss 3.17180, smoothed loss 2.28658, grad norm 4.34031, param norm 155.93663
epoch 9, iter 8120, loss 2.44996, smoothed loss 2.28489, grad norm 3.66110, param norm 155.96411
epoch 9, iter 8125, loss 3.04629, smoothed loss 2.29775, grad norm 4.11218, param norm 155.99173
epoch 9, iter 8130, loss 2.09426, smoothed loss 2.30539, grad norm 3.76419, param norm 156.01886
epoch 9, iter 8135, loss 2.64638, smoothed loss 2.30732, grad norm 4.00147, param norm 156.05051
epoch 9, iter 8140, loss 2.64064, smoothed loss 2.30109, grad norm 4.55177, param norm 156.08272
epoch 9, iter 8145, loss 2.51980, smoothed loss 2.30588, grad norm 3.68792, param norm 156.11218
epoch 9, iter 8150, loss 2.45877, smoothed loss 2.30777, grad norm 3.63673, param norm 156.14204
epoch 9, iter 8155, loss 2.59083, smoothed loss 2.29543, grad norm 3.95372, param norm 156.17546
epoch 9, iter 8160, loss 2.48017, smoothed loss 2.30466, grad norm 3.69472, param norm 156.20728
epoch 9, iter 8165, loss 2.20211, smoothed loss 2.29555, grad norm 3.67556, param norm 156.23886
epoch 9, iter 8170, loss 2.15392, smoothed loss 2.29323, grad norm 3.19660, param norm 156.27307
epoch 9, iter 8175, loss 2.20311, smoothed loss 2.30438, grad norm 3.13777, param norm 156.30356
epoch 9, iter 8180, loss 2.32104, smoothed loss 2.30195, grad norm 3.86760, param norm 156.33249
epoch 9, iter 8185, loss 2.58830, smoothed loss 2.30111, grad norm 3.66887, param norm 156.36105
epoch 9, iter 8190, loss 2.21838, smoothed loss 2.29622, grad norm 3.52658, param norm 156.38678
epoch 9, iter 8195, loss 2.55441, smoothed loss 2.29768, grad norm 3.97809, param norm 156.41325
epoch 9, iter 8200, loss 2.83677, smoothed loss 2.30633, grad norm 4.81202, param norm 156.44122
epoch 9, iter 8205, loss 2.95377, smoothed loss 2.31000, grad norm 4.12292, param norm 156.47099
epoch 9, iter 8210, loss 2.49321, smoothed loss 2.30980, grad norm 3.61166, param norm 156.49870
epoch 9, iter 8215, loss 3.12001, smoothed loss 2.31618, grad norm 4.19838, param norm 156.53049
epoch 9, iter 8220, loss 1.87629, smoothed loss 2.30513, grad norm 3.47704, param norm 156.56223
epoch 9, iter 8225, loss 2.45742, smoothed loss 2.29460, grad norm 4.13574, param norm 156.59608
epoch 9, iter 8230, loss 1.98519, smoothed loss 2.28677, grad norm 3.54171, param norm 156.62413
epoch 9, iter 8235, loss 2.52688, smoothed loss 2.28878, grad norm 3.56658, param norm 156.65315
epoch 9, iter 8240, loss 2.24378, smoothed loss 2.28356, grad norm 3.23814, param norm 156.68195
epoch 9, iter 8245, loss 1.54705, smoothed loss 2.28613, grad norm 3.08259, param norm 156.71318
epoch 9, iter 8250, loss 2.39399, smoothed loss 2.28877, grad norm 3.56962, param norm 156.74591
epoch 9, iter 8255, loss 2.67518, smoothed loss 2.29900, grad norm 3.98886, param norm 156.77631
epoch 9, iter 8260, loss 2.11368, smoothed loss 2.28735, grad norm 3.69178, param norm 156.80466
epoch 9, iter 8265, loss 2.81755, smoothed loss 2.29504, grad norm 3.92541, param norm 156.83510
epoch 9, iter 8270, loss 2.30530, smoothed loss 2.29315, grad norm 3.44944, param norm 156.86357
epoch 9, iter 8275, loss 2.55932, smoothed loss 2.28967, grad norm 4.18024, param norm 156.89415
epoch 9, iter 8280, loss 2.51899, smoothed loss 2.29140, grad norm 4.53246, param norm 156.92552
epoch 9, iter 8285, loss 2.09326, smoothed loss 2.29241, grad norm 4.06264, param norm 156.95624
epoch 9, iter 8290, loss 2.37714, smoothed loss 2.29545, grad norm 3.98114, param norm 156.98726
epoch 9, iter 8295, loss 2.29945, smoothed loss 2.29046, grad norm 3.63587, param norm 157.01668
epoch 9, iter 8300, loss 2.96367, smoothed loss 2.30061, grad norm 3.83803, param norm 157.04697
epoch 9, iter 8305, loss 2.23332, smoothed loss 2.31353, grad norm 3.65908, param norm 157.08253
epoch 9, iter 8310, loss 2.18850, smoothed loss 2.32135, grad norm 3.34407, param norm 157.11784
epoch 9, iter 8315, loss 2.51620, smoothed loss 2.31982, grad norm 3.47454, param norm 157.15022
epoch 9, iter 8320, loss 2.58851, smoothed loss 2.32855, grad norm 3.97973, param norm 157.18034
epoch 9, iter 8325, loss 1.77447, smoothed loss 2.32987, grad norm 3.37473, param norm 157.21118
epoch 9, iter 8330, loss 2.25850, smoothed loss 2.33300, grad norm 3.24266, param norm 157.24258
epoch 9, iter 8335, loss 1.79304, smoothed loss 2.32415, grad norm 3.11359, param norm 157.27502
epoch 9, iter 8340, loss 2.99030, smoothed loss 2.32528, grad norm 4.09859, param norm 157.30598
epoch 9, iter 8345, loss 2.37119, smoothed loss 2.33046, grad norm 3.78764, param norm 157.33655
epoch 9, iter 8350, loss 1.97542, smoothed loss 2.32776, grad norm 3.41219, param norm 157.36847
epoch 9, iter 8355, loss 2.53075, smoothed loss 2.32762, grad norm 3.99002, param norm 157.40309
epoch 9, iter 8360, loss 1.84674, smoothed loss 2.30872, grad norm 3.11490, param norm 157.43614
epoch 9, iter 8365, loss 2.34334, smoothed loss 2.30736, grad norm 4.34209, param norm 157.46619
epoch 9, iter 8370, loss 2.30030, smoothed loss 2.32234, grad norm 4.17165, param norm 157.49449
epoch 9, iter 8375, loss 2.57582, smoothed loss 2.32093, grad norm 3.60863, param norm 157.52628
epoch 9, iter 8380, loss 2.33873, smoothed loss 2.30539, grad norm 3.24063, param norm 157.56218
epoch 9, iter 8385, loss 2.00289, smoothed loss 2.30360, grad norm 3.51542, param norm 157.59558
epoch 9, iter 8390, loss 2.82431, smoothed loss 2.30096, grad norm 4.71787, param norm 157.62851
epoch 9, iter 8395, loss 1.93126, smoothed loss 2.28084, grad norm 3.19385, param norm 157.65521
epoch 9, iter 8400, loss 1.94451, smoothed loss 2.27159, grad norm 3.20477, param norm 157.68121
epoch 9, iter 8405, loss 2.12672, smoothed loss 2.26249, grad norm 4.21092, param norm 157.70804
epoch 9, iter 8410, loss 2.01173, smoothed loss 2.27242, grad norm 3.89587, param norm 157.73418
epoch 9, iter 8415, loss 2.35612, smoothed loss 2.28333, grad norm 3.84616, param norm 157.75975
epoch 9, iter 8420, loss 2.47534, smoothed loss 2.28218, grad norm 3.61219, param norm 157.78995
epoch 9, iter 8425, loss 2.24747, smoothed loss 2.28116, grad norm 3.77123, param norm 157.82730
epoch 9, iter 8430, loss 1.79655, smoothed loss 2.27701, grad norm 2.87735, param norm 157.86674
epoch 9, iter 8435, loss 1.79970, smoothed loss 2.27220, grad norm 3.39395, param norm 157.90254
epoch 9, iter 8440, loss 2.47722, smoothed loss 2.26447, grad norm 3.45776, param norm 157.93584
epoch 9, iter 8445, loss 2.14411, smoothed loss 2.26081, grad norm 3.86991, param norm 157.96782
epoch 9, iter 8450, loss 2.19708, smoothed loss 2.27269, grad norm 3.79661, param norm 157.99538
epoch 9, iter 8455, loss 2.39361, smoothed loss 2.26406, grad norm 3.87966, param norm 158.02592
epoch 9, iter 8460, loss 2.56542, smoothed loss 2.27519, grad norm 4.35552, param norm 158.05740
epoch 9, iter 8465, loss 2.18820, smoothed loss 2.26486, grad norm 3.20812, param norm 158.08789
epoch 9, iter 8470, loss 2.04952, smoothed loss 2.25477, grad norm 3.70206, param norm 158.11606
epoch 9, iter 8475, loss 2.10776, smoothed loss 2.25538, grad norm 3.75049, param norm 158.14645
epoch 9, iter 8480, loss 2.14609, smoothed loss 2.24533, grad norm 3.90225, param norm 158.17331
epoch 9, iter 8485, loss 2.53188, smoothed loss 2.24831, grad norm 4.31629, param norm 158.20253
epoch 9, iter 8490, loss 2.01506, smoothed loss 2.24769, grad norm 3.51731, param norm 158.22958
epoch 9, iter 8495, loss 2.67989, smoothed loss 2.26109, grad norm 4.82448, param norm 158.25911
epoch 10, iter 8500, loss 1.83377, smoothed loss 2.25805, grad norm 3.47088, param norm 158.28993
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 8500, dev loss: 3.134152
Calculating Train F1/EM...
F1 train: 1000 examples took 18.79552 seconds [Score: 0.80534]
Exact Match train: 1000 examples took 18.86560 seconds [Score: 0.67800]
Epoch 10, Iter 8500, Train F1 score: 0.805335, Train EM score: 0.678000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.97228 seconds [Score: 0.65196]
Exact Match dev: 7118 examples took 124.73717 seconds [Score: 0.50450]
Epoch 10, Iter 8500, Dev F1 score: 0.651964, Dev EM score: 0.504496
End of epoch 10
epoch 10, iter 8505, loss 2.53815, smoothed loss 2.25794, grad norm 4.67233, param norm 158.32716
epoch 10, iter 8510, loss 1.95672, smoothed loss 2.25605, grad norm 3.60991, param norm 158.36963
epoch 10, iter 8515, loss 2.71672, smoothed loss 2.25655, grad norm 3.98268, param norm 158.41103
epoch 10, iter 8520, loss 2.03633, smoothed loss 2.25783, grad norm 3.39151, param norm 158.44496
epoch 10, iter 8525, loss 2.61528, smoothed loss 2.25476, grad norm 3.96246, param norm 158.47356
epoch 10, iter 8530, loss 2.09027, smoothed loss 2.24202, grad norm 3.54823, param norm 158.50211
epoch 10, iter 8535, loss 2.00013, smoothed loss 2.26105, grad norm 3.29580, param norm 158.53021
epoch 10, iter 8540, loss 1.66778, smoothed loss 2.24006, grad norm 3.54034, param norm 158.56142
epoch 10, iter 8545, loss 2.19598, smoothed loss 2.22665, grad norm 3.37123, param norm 158.59084
epoch 10, iter 8550, loss 1.78366, smoothed loss 2.21782, grad norm 3.56809, param norm 158.61760
epoch 10, iter 8555, loss 2.70872, smoothed loss 2.21712, grad norm 3.99577, param norm 158.64632
epoch 10, iter 8560, loss 1.77678, smoothed loss 2.21001, grad norm 3.74948, param norm 158.67467
epoch 10, iter 8565, loss 2.17628, smoothed loss 2.20678, grad norm 3.60700, param norm 158.70422
epoch 10, iter 8570, loss 2.80942, smoothed loss 2.22540, grad norm 4.30861, param norm 158.72923
epoch 10, iter 8575, loss 2.58006, smoothed loss 2.23459, grad norm 4.55922, param norm 158.75638
epoch 10, iter 8580, loss 2.47884, smoothed loss 2.22630, grad norm 3.93917, param norm 158.78824
epoch 10, iter 8585, loss 2.01557, smoothed loss 2.21385, grad norm 3.48405, param norm 158.82236
epoch 10, iter 8590, loss 2.78337, smoothed loss 2.20856, grad norm 4.83768, param norm 158.85608
epoch 10, iter 8595, loss 2.41118, smoothed loss 2.21912, grad norm 4.07298, param norm 158.89192
epoch 10, iter 8600, loss 2.51134, smoothed loss 2.22670, grad norm 3.83780, param norm 158.92690
epoch 10, iter 8605, loss 1.68651, smoothed loss 2.23117, grad norm 2.93547, param norm 158.96259
epoch 10, iter 8610, loss 2.06900, smoothed loss 2.24153, grad norm 3.41476, param norm 158.99759
epoch 10, iter 8615, loss 1.76100, smoothed loss 2.24921, grad norm 3.20971, param norm 159.03242
epoch 10, iter 8620, loss 2.14972, smoothed loss 2.24977, grad norm 3.54525, param norm 159.06671
epoch 10, iter 8625, loss 2.60532, smoothed loss 2.25178, grad norm 3.73769, param norm 159.09926
epoch 10, iter 8630, loss 1.93302, smoothed loss 2.25841, grad norm 3.67994, param norm 159.12820
epoch 10, iter 8635, loss 2.06993, smoothed loss 2.25628, grad norm 3.21888, param norm 159.15886
epoch 10, iter 8640, loss 2.75609, smoothed loss 2.26076, grad norm 3.84871, param norm 159.18846
epoch 10, iter 8645, loss 1.94666, smoothed loss 2.24679, grad norm 3.26539, param norm 159.21826
epoch 10, iter 8650, loss 2.50573, smoothed loss 2.24727, grad norm 4.43415, param norm 159.24977
epoch 10, iter 8655, loss 1.82573, smoothed loss 2.23577, grad norm 3.36543, param norm 159.28545
epoch 10, iter 8660, loss 2.41222, smoothed loss 2.23096, grad norm 3.80065, param norm 159.31924
epoch 10, iter 8665, loss 2.44604, smoothed loss 2.22691, grad norm 3.88999, param norm 159.34869
epoch 10, iter 8670, loss 2.37942, smoothed loss 2.22804, grad norm 3.59810, param norm 159.37984
epoch 10, iter 8675, loss 1.95242, smoothed loss 2.23067, grad norm 3.65400, param norm 159.41113
epoch 10, iter 8680, loss 2.08663, smoothed loss 2.23077, grad norm 3.57659, param norm 159.44206
epoch 10, iter 8685, loss 2.03251, smoothed loss 2.22739, grad norm 3.62731, param norm 159.47296
epoch 10, iter 8690, loss 2.37357, smoothed loss 2.22909, grad norm 3.73860, param norm 159.50374
epoch 10, iter 8695, loss 1.93471, smoothed loss 2.22909, grad norm 3.78040, param norm 159.53275
epoch 10, iter 8700, loss 2.49884, smoothed loss 2.23792, grad norm 4.22407, param norm 159.56340
epoch 10, iter 8705, loss 2.40012, smoothed loss 2.23266, grad norm 4.03996, param norm 159.59654
epoch 10, iter 8710, loss 1.83422, smoothed loss 2.23408, grad norm 3.94165, param norm 159.63249
epoch 10, iter 8715, loss 1.96982, smoothed loss 2.23574, grad norm 3.79202, param norm 159.66595
epoch 10, iter 8720, loss 1.89445, smoothed loss 2.23464, grad norm 3.16173, param norm 159.69821
epoch 10, iter 8725, loss 2.40195, smoothed loss 2.24735, grad norm 3.22297, param norm 159.72849
epoch 10, iter 8730, loss 2.46549, smoothed loss 2.26229, grad norm 4.48099, param norm 159.75912
epoch 10, iter 8735, loss 2.38978, smoothed loss 2.26595, grad norm 3.73674, param norm 159.79265
epoch 10, iter 8740, loss 1.85493, smoothed loss 2.26236, grad norm 3.73395, param norm 159.82582
epoch 10, iter 8745, loss 1.98262, smoothed loss 2.27145, grad norm 3.43053, param norm 159.85922
epoch 10, iter 8750, loss 1.87317, smoothed loss 2.26257, grad norm 3.50321, param norm 159.89267
epoch 10, iter 8755, loss 2.49788, smoothed loss 2.26800, grad norm 4.04406, param norm 159.92491
epoch 10, iter 8760, loss 2.28264, smoothed loss 2.25760, grad norm 3.82349, param norm 159.95624
epoch 10, iter 8765, loss 2.61060, smoothed loss 2.25389, grad norm 4.12209, param norm 159.98570
epoch 10, iter 8770, loss 2.52302, smoothed loss 2.25640, grad norm 3.66068, param norm 160.01279
epoch 10, iter 8775, loss 2.36152, smoothed loss 2.25212, grad norm 3.55626, param norm 160.04166
epoch 10, iter 8780, loss 1.91121, smoothed loss 2.24288, grad norm 3.66395, param norm 160.07271
epoch 10, iter 8785, loss 3.32501, smoothed loss 2.24570, grad norm 5.67132, param norm 160.10413
epoch 10, iter 8790, loss 2.73802, smoothed loss 2.25102, grad norm 4.41846, param norm 160.13376
epoch 10, iter 8795, loss 1.92571, smoothed loss 2.25098, grad norm 3.06737, param norm 160.16118
epoch 10, iter 8800, loss 1.89725, smoothed loss 2.24701, grad norm 3.46211, param norm 160.18872
epoch 10, iter 8805, loss 1.88984, smoothed loss 2.24326, grad norm 3.39363, param norm 160.21347
epoch 10, iter 8810, loss 2.37603, smoothed loss 2.24181, grad norm 3.66969, param norm 160.24158
epoch 10, iter 8815, loss 1.93249, smoothed loss 2.24150, grad norm 3.63917, param norm 160.27365
epoch 10, iter 8820, loss 2.87217, smoothed loss 2.24261, grad norm 5.13071, param norm 160.30763
epoch 10, iter 8825, loss 2.06473, smoothed loss 2.23674, grad norm 3.70794, param norm 160.34279
epoch 10, iter 8830, loss 1.82414, smoothed loss 2.22229, grad norm 3.63330, param norm 160.37895
epoch 10, iter 8835, loss 2.36128, smoothed loss 2.21118, grad norm 4.18619, param norm 160.41298
epoch 10, iter 8840, loss 2.82133, smoothed loss 2.22607, grad norm 4.39829, param norm 160.44289
epoch 10, iter 8845, loss 2.27220, smoothed loss 2.22173, grad norm 4.47498, param norm 160.46928
epoch 10, iter 8850, loss 2.13668, smoothed loss 2.21861, grad norm 4.07020, param norm 160.49773
epoch 10, iter 8855, loss 2.24250, smoothed loss 2.21371, grad norm 3.61330, param norm 160.53094
epoch 10, iter 8860, loss 2.20307, smoothed loss 2.20989, grad norm 4.54399, param norm 160.56116
epoch 10, iter 8865, loss 2.57089, smoothed loss 2.21470, grad norm 3.87903, param norm 160.59306
epoch 10, iter 8870, loss 2.14237, smoothed loss 2.20713, grad norm 3.36780, param norm 160.62482
epoch 10, iter 8875, loss 2.06524, smoothed loss 2.20213, grad norm 3.70753, param norm 160.65474
epoch 10, iter 8880, loss 2.16805, smoothed loss 2.19600, grad norm 3.32585, param norm 160.68370
epoch 10, iter 8885, loss 1.78871, smoothed loss 2.19464, grad norm 3.68563, param norm 160.71068
epoch 10, iter 8890, loss 2.63977, smoothed loss 2.19759, grad norm 4.15575, param norm 160.73662
epoch 10, iter 8895, loss 2.43063, smoothed loss 2.19167, grad norm 3.93135, param norm 160.76521
epoch 10, iter 8900, loss 2.22495, smoothed loss 2.19130, grad norm 4.03652, param norm 160.79646
epoch 10, iter 8905, loss 1.97591, smoothed loss 2.19918, grad norm 3.36750, param norm 160.83101
epoch 10, iter 8910, loss 2.83373, smoothed loss 2.20017, grad norm 3.92754, param norm 160.86752
epoch 10, iter 8915, loss 2.13252, smoothed loss 2.19326, grad norm 3.53097, param norm 160.90150
epoch 10, iter 8920, loss 2.41153, smoothed loss 2.19390, grad norm 3.98653, param norm 160.93616
epoch 10, iter 8925, loss 1.69609, smoothed loss 2.18979, grad norm 3.46006, param norm 160.96933
epoch 10, iter 8930, loss 1.59721, smoothed loss 2.18204, grad norm 3.73436, param norm 161.00334
epoch 10, iter 8935, loss 1.64132, smoothed loss 2.19608, grad norm 3.11062, param norm 161.03467
epoch 10, iter 8940, loss 2.53553, smoothed loss 2.19345, grad norm 4.52219, param norm 161.06622
epoch 10, iter 8945, loss 2.14195, smoothed loss 2.19447, grad norm 3.78539, param norm 161.09741
epoch 10, iter 8950, loss 2.33111, smoothed loss 2.19921, grad norm 4.14312, param norm 161.12848
epoch 10, iter 8955, loss 2.34338, smoothed loss 2.19588, grad norm 3.78740, param norm 161.15814
epoch 10, iter 8960, loss 1.87332, smoothed loss 2.19496, grad norm 3.82273, param norm 161.18504
epoch 10, iter 8965, loss 2.03004, smoothed loss 2.18959, grad norm 4.21278, param norm 161.21263
epoch 10, iter 8970, loss 2.30842, smoothed loss 2.18170, grad norm 3.90131, param norm 161.24336
epoch 10, iter 8975, loss 2.12582, smoothed loss 2.17798, grad norm 4.06514, param norm 161.27365
epoch 10, iter 8980, loss 2.67498, smoothed loss 2.18677, grad norm 4.37873, param norm 161.30423
epoch 10, iter 8985, loss 2.27340, smoothed loss 2.18195, grad norm 4.32552, param norm 161.33302
epoch 10, iter 8990, loss 1.73010, smoothed loss 2.17198, grad norm 3.64829, param norm 161.36520
epoch 10, iter 8995, loss 1.97449, smoothed loss 2.17038, grad norm 4.16939, param norm 161.39764
epoch 10, iter 9000, loss 2.05272, smoothed loss 2.17663, grad norm 3.38869, param norm 161.42784
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 9000, dev loss: 3.126870
Calculating Train F1/EM...
F1 train: 1000 examples took 18.66033 seconds [Score: 0.82181]
Exact Match train: 1000 examples took 18.66805 seconds [Score: 0.71700]
Epoch 10, Iter 9000, Train F1 score: 0.821806, Train EM score: 0.717000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.76509 seconds [Score: 0.65379]
Exact Match dev: 7118 examples took 124.77600 seconds [Score: 0.50351]
Epoch 10, Iter 9000, Dev F1 score: 0.653786, Dev EM score: 0.503512
End of epoch 10
epoch 10, iter 9005, loss 2.21930, smoothed loss 2.18145, grad norm 4.35756, param norm 161.45625
epoch 10, iter 9010, loss 1.99327, smoothed loss 2.19006, grad norm 3.12936, param norm 161.48282
epoch 10, iter 9015, loss 2.02502, smoothed loss 2.18399, grad norm 3.48693, param norm 161.50844
epoch 10, iter 9020, loss 1.60100, smoothed loss 2.18243, grad norm 2.98358, param norm 161.53734
epoch 10, iter 9025, loss 2.36946, smoothed loss 2.18747, grad norm 3.82792, param norm 161.56618
epoch 10, iter 9030, loss 2.25798, smoothed loss 2.17817, grad norm 4.08381, param norm 161.59438
epoch 10, iter 9035, loss 2.56660, smoothed loss 2.16736, grad norm 4.52210, param norm 161.62563
epoch 10, iter 9040, loss 1.90858, smoothed loss 2.16947, grad norm 3.27351, param norm 161.65308
epoch 10, iter 9045, loss 1.83877, smoothed loss 2.15933, grad norm 3.23730, param norm 161.68333
epoch 10, iter 9050, loss 2.19692, smoothed loss 2.18687, grad norm 3.66395, param norm 161.70856
epoch 10, iter 9055, loss 2.79581, smoothed loss 2.19055, grad norm 4.06746, param norm 161.73418
epoch 10, iter 9060, loss 2.23759, smoothed loss 2.19544, grad norm 4.00039, param norm 161.76131
epoch 10, iter 9065, loss 2.25342, smoothed loss 2.19217, grad norm 3.85729, param norm 161.79306
epoch 10, iter 9070, loss 2.53020, smoothed loss 2.20201, grad norm 3.99262, param norm 161.82411
epoch 10, iter 9075, loss 2.04631, smoothed loss 2.20197, grad norm 3.56918, param norm 161.85217
epoch 10, iter 9080, loss 2.54195, smoothed loss 2.21043, grad norm 4.01690, param norm 161.87953
epoch 10, iter 9085, loss 1.85585, smoothed loss 2.21620, grad norm 3.42817, param norm 161.90544
epoch 10, iter 9090, loss 2.12057, smoothed loss 2.20762, grad norm 4.01904, param norm 161.93333
epoch 10, iter 9095, loss 2.42528, smoothed loss 2.19640, grad norm 3.97228, param norm 161.96179
epoch 10, iter 9100, loss 2.32174, smoothed loss 2.19792, grad norm 3.96879, param norm 161.99063
epoch 10, iter 9105, loss 2.17172, smoothed loss 2.20089, grad norm 3.36545, param norm 162.01863
epoch 10, iter 9110, loss 2.21045, smoothed loss 2.21920, grad norm 3.35095, param norm 162.04387
epoch 10, iter 9115, loss 2.06804, smoothed loss 2.22212, grad norm 2.83239, param norm 162.06908
epoch 10, iter 9120, loss 2.12001, smoothed loss 2.21899, grad norm 3.07563, param norm 162.09645
epoch 10, iter 9125, loss 2.38138, smoothed loss 2.22093, grad norm 3.75697, param norm 162.12537
epoch 10, iter 9130, loss 1.85946, smoothed loss 2.21781, grad norm 3.53491, param norm 162.15379
epoch 10, iter 9135, loss 2.86364, smoothed loss 2.22404, grad norm 4.37467, param norm 162.18063
epoch 10, iter 9140, loss 2.40190, smoothed loss 2.21529, grad norm 3.97980, param norm 162.20639
epoch 10, iter 9145, loss 2.05598, smoothed loss 2.20651, grad norm 3.50641, param norm 162.23859
epoch 10, iter 9150, loss 2.28269, smoothed loss 2.21491, grad norm 4.18449, param norm 162.26837
epoch 10, iter 9155, loss 2.01430, smoothed loss 2.21884, grad norm 4.06498, param norm 162.30237
epoch 10, iter 9160, loss 2.78776, smoothed loss 2.22666, grad norm 4.56464, param norm 162.33842
epoch 10, iter 9165, loss 2.19807, smoothed loss 2.22691, grad norm 3.78415, param norm 162.36823
epoch 10, iter 9170, loss 1.85705, smoothed loss 2.21487, grad norm 2.94656, param norm 162.39423
epoch 10, iter 9175, loss 2.53774, smoothed loss 2.22314, grad norm 4.10939, param norm 162.42282
epoch 10, iter 9180, loss 2.43714, smoothed loss 2.21807, grad norm 3.70734, param norm 162.45113
epoch 10, iter 9185, loss 2.06983, smoothed loss 2.21676, grad norm 3.56614, param norm 162.48222
epoch 10, iter 9190, loss 2.22465, smoothed loss 2.21244, grad norm 3.48837, param norm 162.51379
epoch 10, iter 9195, loss 1.94673, smoothed loss 2.21233, grad norm 3.93373, param norm 162.54672
epoch 10, iter 9200, loss 2.00907, smoothed loss 2.21244, grad norm 3.54197, param norm 162.57837
epoch 10, iter 9205, loss 2.67627, smoothed loss 2.21837, grad norm 4.16639, param norm 162.61032
epoch 10, iter 9210, loss 2.18878, smoothed loss 2.20602, grad norm 3.81473, param norm 162.64453
epoch 10, iter 9215, loss 1.97625, smoothed loss 2.20593, grad norm 3.46567, param norm 162.67543
epoch 10, iter 9220, loss 2.11611, smoothed loss 2.19957, grad norm 3.96326, param norm 162.70882
epoch 10, iter 9225, loss 2.03666, smoothed loss 2.18836, grad norm 3.59128, param norm 162.74251
epoch 10, iter 9230, loss 1.93902, smoothed loss 2.19562, grad norm 3.54442, param norm 162.77374
epoch 10, iter 9235, loss 2.22870, smoothed loss 2.19760, grad norm 3.50138, param norm 162.80536
epoch 10, iter 9240, loss 3.06078, smoothed loss 2.20429, grad norm 4.34103, param norm 162.83376
epoch 10, iter 9245, loss 2.03072, smoothed loss 2.20587, grad norm 3.32860, param norm 162.86217
epoch 10, iter 9250, loss 2.58157, smoothed loss 2.21503, grad norm 3.72892, param norm 162.89415
epoch 10, iter 9255, loss 2.63876, smoothed loss 2.21867, grad norm 3.71231, param norm 162.92514
epoch 10, iter 9260, loss 2.70068, smoothed loss 2.21923, grad norm 4.35773, param norm 162.95253
epoch 10, iter 9265, loss 2.26913, smoothed loss 2.22047, grad norm 3.68847, param norm 162.97998
epoch 10, iter 9270, loss 2.19572, smoothed loss 2.20942, grad norm 4.12486, param norm 163.00800
epoch 10, iter 9275, loss 2.37322, smoothed loss 2.21427, grad norm 4.25019, param norm 163.04143
epoch 10, iter 9280, loss 2.39579, smoothed loss 2.21213, grad norm 4.09018, param norm 163.07425
epoch 10, iter 9285, loss 2.79047, smoothed loss 2.21707, grad norm 3.95288, param norm 163.10532
epoch 10, iter 9290, loss 1.95362, smoothed loss 2.20460, grad norm 3.48177, param norm 163.13176
epoch 10, iter 9295, loss 2.85748, smoothed loss 2.21606, grad norm 4.80314, param norm 163.15828
epoch 10, iter 9300, loss 2.29278, smoothed loss 2.21775, grad norm 4.27739, param norm 163.18600
epoch 10, iter 9305, loss 2.50214, smoothed loss 2.22278, grad norm 4.46091, param norm 163.21811
epoch 10, iter 9310, loss 1.81361, smoothed loss 2.22715, grad norm 3.52926, param norm 163.24925
epoch 10, iter 9315, loss 2.48271, smoothed loss 2.22297, grad norm 3.61193, param norm 163.27817
epoch 10, iter 9320, loss 2.05591, smoothed loss 2.22044, grad norm 4.16540, param norm 163.30440
epoch 10, iter 9325, loss 2.17017, smoothed loss 2.22207, grad norm 3.62177, param norm 163.33340
epoch 10, iter 9330, loss 2.15977, smoothed loss 2.22206, grad norm 4.16789, param norm 163.36269
epoch 10, iter 9335, loss 1.76252, smoothed loss 2.22012, grad norm 3.34477, param norm 163.39125
epoch 10, iter 9340, loss 1.95762, smoothed loss 2.20813, grad norm 3.12127, param norm 163.41733
epoch 10, iter 9345, loss 1.90920, smoothed loss 2.19618, grad norm 3.68625, param norm 163.44781
epoch 10, iter 9350, loss 2.04305, smoothed loss 2.19077, grad norm 3.86610, param norm 163.47870
epoch 10, iter 9355, loss 2.30439, smoothed loss 2.18413, grad norm 3.95851, param norm 163.51427
epoch 10, iter 9360, loss 1.81938, smoothed loss 2.18011, grad norm 3.63950, param norm 163.55077
epoch 10, iter 9365, loss 1.47876, smoothed loss 2.17051, grad norm 2.96689, param norm 163.58441
epoch 10, iter 9370, loss 2.06912, smoothed loss 2.16639, grad norm 3.74924, param norm 163.61304
epoch 10, iter 9375, loss 1.62605, smoothed loss 2.15593, grad norm 3.37061, param norm 163.64104
epoch 10, iter 9380, loss 1.98074, smoothed loss 2.15251, grad norm 4.04621, param norm 163.66998
epoch 10, iter 9385, loss 2.43787, smoothed loss 2.15224, grad norm 4.18205, param norm 163.70036
epoch 10, iter 9390, loss 1.81423, smoothed loss 2.14547, grad norm 4.03524, param norm 163.72754
epoch 10, iter 9395, loss 2.13203, smoothed loss 2.14820, grad norm 3.57586, param norm 163.75351
epoch 10, iter 9400, loss 1.99830, smoothed loss 2.14988, grad norm 4.64531, param norm 163.78073
epoch 10, iter 9405, loss 1.99719, smoothed loss 2.14429, grad norm 3.65850, param norm 163.81058
epoch 10, iter 9410, loss 2.22631, smoothed loss 2.14481, grad norm 3.86129, param norm 163.84003
epoch 10, iter 9415, loss 2.08778, smoothed loss 2.14527, grad norm 4.56035, param norm 163.86980
epoch 10, iter 9420, loss 2.11102, smoothed loss 2.14520, grad norm 3.88448, param norm 163.90186
epoch 10, iter 9425, loss 1.81624, smoothed loss 2.14745, grad norm 3.24836, param norm 163.93385
epoch 10, iter 9430, loss 1.62309, smoothed loss 2.14181, grad norm 3.26182, param norm 163.96692
epoch 10, iter 9435, loss 2.62140, smoothed loss 2.14777, grad norm 4.25003, param norm 163.99901
epoch 10, iter 9440, loss 3.07387, smoothed loss 2.15379, grad norm 4.70234, param norm 164.02486
epoch 11, iter 9445, loss 2.52794, smoothed loss 2.15940, grad norm 3.96189, param norm 164.04787
epoch 11, iter 9450, loss 2.14141, smoothed loss 2.15814, grad norm 4.04711, param norm 164.07486
epoch 11, iter 9455, loss 1.63830, smoothed loss 2.14670, grad norm 3.62095, param norm 164.10632
epoch 11, iter 9460, loss 3.04637, smoothed loss 2.15898, grad norm 4.88333, param norm 164.14008
epoch 11, iter 9465, loss 2.17925, smoothed loss 2.16674, grad norm 3.78781, param norm 164.17056
epoch 11, iter 9470, loss 2.24636, smoothed loss 2.15297, grad norm 4.10497, param norm 164.20029
epoch 11, iter 9475, loss 1.96976, smoothed loss 2.14817, grad norm 3.82575, param norm 164.23022
epoch 11, iter 9480, loss 2.45238, smoothed loss 2.15494, grad norm 5.02085, param norm 164.25887
epoch 11, iter 9485, loss 2.41725, smoothed loss 2.16746, grad norm 4.24665, param norm 164.28479
epoch 11, iter 9490, loss 1.69862, smoothed loss 2.15666, grad norm 3.34709, param norm 164.31256
epoch 11, iter 9495, loss 2.32194, smoothed loss 2.16092, grad norm 3.67291, param norm 164.33951
epoch 11, iter 9500, loss 2.18094, smoothed loss 2.15685, grad norm 3.84973, param norm 164.36700
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 11, Iter 9500, dev loss: 3.124367
Calculating Train F1/EM...
F1 train: 1000 examples took 18.31106 seconds [Score: 0.82738]
Exact Match train: 1000 examples took 18.26465 seconds [Score: 0.69200]
Epoch 11, Iter 9500, Train F1 score: 0.827376, Train EM score: 0.692000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 126.20797 seconds [Score: 0.65081]
Exact Match dev: 7118 examples took 125.44478 seconds [Score: 0.49944]
Epoch 11, Iter 9500, Dev F1 score: 0.650808, Dev EM score: 0.499438
End of epoch 11
epoch 11, iter 9505, loss 2.13479, smoothed loss 2.14381, grad norm 3.55025, param norm 164.39632
epoch 11, iter 9510, loss 2.54195, smoothed loss 2.13742, grad norm 4.53494, param norm 164.42897
epoch 11, iter 9515, loss 1.53411, smoothed loss 2.13329, grad norm 3.18965, param norm 164.45634
epoch 11, iter 9520, loss 1.47382, smoothed loss 2.12342, grad norm 3.20819, param norm 164.48495
epoch 11, iter 9525, loss 2.26691, smoothed loss 2.12459, grad norm 3.94169, param norm 164.51526
epoch 11, iter 9530, loss 2.17266, smoothed loss 2.12307, grad norm 3.66162, param norm 164.54599
epoch 11, iter 9535, loss 1.90118, smoothed loss 2.12332, grad norm 3.48049, param norm 164.57477
epoch 11, iter 9540, loss 1.95077, smoothed loss 2.12994, grad norm 3.98068, param norm 164.60536
epoch 11, iter 9545, loss 1.69943, smoothed loss 2.13041, grad norm 3.70851, param norm 164.63660
epoch 11, iter 9550, loss 2.05359, smoothed loss 2.13621, grad norm 3.55644, param norm 164.66574
epoch 11, iter 9555, loss 2.15781, smoothed loss 2.13969, grad norm 4.09447, param norm 164.69313
epoch 11, iter 9560, loss 2.16036, smoothed loss 2.13040, grad norm 3.96440, param norm 164.72099
epoch 11, iter 9565, loss 1.99519, smoothed loss 2.12215, grad norm 3.42879, param norm 164.74731
epoch 11, iter 9570, loss 2.57918, smoothed loss 2.13921, grad norm 4.44954, param norm 164.77144
epoch 11, iter 9575, loss 1.64952, smoothed loss 2.13159, grad norm 3.01035, param norm 164.79733
epoch 11, iter 9580, loss 1.96009, smoothed loss 2.13283, grad norm 3.63372, param norm 164.82741
epoch 11, iter 9585, loss 1.99043, smoothed loss 2.13746, grad norm 3.72678, param norm 164.85893
epoch 11, iter 9590, loss 2.43471, smoothed loss 2.14274, grad norm 4.77142, param norm 164.89134
epoch 11, iter 9595, loss 2.23963, smoothed loss 2.14858, grad norm 3.98681, param norm 164.92534
epoch 11, iter 9600, loss 2.09548, smoothed loss 2.14559, grad norm 3.82766, param norm 164.95497
epoch 11, iter 9605, loss 1.75757, smoothed loss 2.14379, grad norm 3.61650, param norm 164.98582
epoch 11, iter 9610, loss 2.27995, smoothed loss 2.14557, grad norm 3.97653, param norm 165.01320
epoch 11, iter 9615, loss 2.15777, smoothed loss 2.14549, grad norm 3.70547, param norm 165.04082
epoch 11, iter 9620, loss 2.44111, smoothed loss 2.15079, grad norm 3.82840, param norm 165.07178
epoch 11, iter 9625, loss 1.83443, smoothed loss 2.15081, grad norm 4.00938, param norm 165.10388
epoch 11, iter 9630, loss 2.18897, smoothed loss 2.15235, grad norm 3.73423, param norm 165.13425
epoch 11, iter 9635, loss 1.77258, smoothed loss 2.15242, grad norm 3.36699, param norm 165.16650
epoch 11, iter 9640, loss 1.95060, smoothed loss 2.15581, grad norm 4.15074, param norm 165.20125
epoch 11, iter 9645, loss 2.24735, smoothed loss 2.16419, grad norm 3.98636, param norm 165.23245
epoch 11, iter 9650, loss 1.69491, smoothed loss 2.14707, grad norm 3.15194, param norm 165.26135
epoch 11, iter 9655, loss 2.06208, smoothed loss 2.14585, grad norm 3.61703, param norm 165.28792
epoch 11, iter 9660, loss 1.89794, smoothed loss 2.14202, grad norm 3.99023, param norm 165.31232
epoch 11, iter 9665, loss 2.20969, smoothed loss 2.14910, grad norm 4.01125, param norm 165.34175
epoch 11, iter 9670, loss 2.04611, smoothed loss 2.15347, grad norm 3.89246, param norm 165.37398
epoch 11, iter 9675, loss 1.95212, smoothed loss 2.14114, grad norm 3.79410, param norm 165.40846
epoch 11, iter 9680, loss 2.02985, smoothed loss 2.14430, grad norm 4.02606, param norm 165.43916
epoch 11, iter 9685, loss 2.60065, smoothed loss 2.15623, grad norm 4.56332, param norm 165.46869
epoch 11, iter 9690, loss 2.07352, smoothed loss 2.15325, grad norm 3.67589, param norm 165.49898
epoch 11, iter 9695, loss 2.37224, smoothed loss 2.16550, grad norm 3.87765, param norm 165.52821
epoch 11, iter 9700, loss 2.12586, smoothed loss 2.15454, grad norm 3.70649, param norm 165.56062
epoch 11, iter 9705, loss 1.81949, smoothed loss 2.15391, grad norm 3.57605, param norm 165.59085
epoch 11, iter 9710, loss 2.93628, smoothed loss 2.16951, grad norm 4.22150, param norm 165.62144
epoch 11, iter 9715, loss 2.13606, smoothed loss 2.17982, grad norm 3.85727, param norm 165.65387
epoch 11, iter 9720, loss 2.33488, smoothed loss 2.17500, grad norm 3.91000, param norm 165.68356
epoch 11, iter 9725, loss 2.02628, smoothed loss 2.17352, grad norm 3.91305, param norm 165.71249
epoch 11, iter 9730, loss 1.89437, smoothed loss 2.17864, grad norm 3.89446, param norm 165.73985
epoch 11, iter 9735, loss 1.85148, smoothed loss 2.17541, grad norm 3.53558, param norm 165.76552
epoch 11, iter 9740, loss 2.12606, smoothed loss 2.16738, grad norm 4.03369, param norm 165.79474
epoch 11, iter 9745, loss 2.09759, smoothed loss 2.16028, grad norm 4.00474, param norm 165.82430
epoch 11, iter 9750, loss 2.07710, smoothed loss 2.15943, grad norm 3.42978, param norm 165.85068
epoch 11, iter 9755, loss 2.41992, smoothed loss 2.15856, grad norm 4.30977, param norm 165.87650
epoch 11, iter 9760, loss 2.71380, smoothed loss 2.16994, grad norm 4.94064, param norm 165.90149
epoch 11, iter 9765, loss 2.42626, smoothed loss 2.16819, grad norm 4.17037, param norm 165.92757
epoch 11, iter 9770, loss 2.15065, smoothed loss 2.15969, grad norm 3.54117, param norm 165.95441
epoch 11, iter 9775, loss 2.12194, smoothed loss 2.15688, grad norm 3.38427, param norm 165.98175
epoch 11, iter 9780, loss 2.02467, smoothed loss 2.15204, grad norm 3.78051, param norm 166.01242
epoch 11, iter 9785, loss 2.16239, smoothed loss 2.15229, grad norm 4.42732, param norm 166.04276
epoch 11, iter 9790, loss 2.36408, smoothed loss 2.15985, grad norm 4.30602, param norm 166.06987
epoch 11, iter 9795, loss 1.55674, smoothed loss 2.14832, grad norm 3.42423, param norm 166.09666
epoch 11, iter 9800, loss 2.22345, smoothed loss 2.14527, grad norm 3.92550, param norm 166.12477
epoch 11, iter 9805, loss 2.12305, smoothed loss 2.13834, grad norm 4.12196, param norm 166.15559
epoch 11, iter 9810, loss 1.80243, smoothed loss 2.13882, grad norm 3.75765, param norm 166.18460
epoch 11, iter 9815, loss 1.67740, smoothed loss 2.13904, grad norm 3.80114, param norm 166.21384
epoch 11, iter 9820, loss 1.84655, smoothed loss 2.13025, grad norm 3.40845, param norm 166.24524
epoch 11, iter 9825, loss 1.46468, smoothed loss 2.11053, grad norm 3.28587, param norm 166.27603
epoch 11, iter 9830, loss 1.93742, smoothed loss 2.10932, grad norm 3.40281, param norm 166.30511
epoch 11, iter 9835, loss 1.54620, smoothed loss 2.09790, grad norm 3.29307, param norm 166.33125
epoch 11, iter 9840, loss 2.06685, smoothed loss 2.10860, grad norm 3.58923, param norm 166.35556
epoch 11, iter 9845, loss 1.57161, smoothed loss 2.10848, grad norm 3.12526, param norm 166.38136
epoch 11, iter 9850, loss 2.21439, smoothed loss 2.11139, grad norm 4.06185, param norm 166.40790
epoch 11, iter 9855, loss 2.17869, smoothed loss 2.12029, grad norm 3.96279, param norm 166.43333
epoch 11, iter 9860, loss 1.65959, smoothed loss 2.12111, grad norm 3.05779, param norm 166.46130
epoch 11, iter 9865, loss 1.58968, smoothed loss 2.11183, grad norm 2.53811, param norm 166.49040
epoch 11, iter 9870, loss 2.15704, smoothed loss 2.10172, grad norm 4.95825, param norm 166.51990
epoch 11, iter 9875, loss 1.74141, smoothed loss 2.10909, grad norm 3.38212, param norm 166.54668
epoch 11, iter 9880, loss 1.82453, smoothed loss 2.10010, grad norm 3.27234, param norm 166.57457
epoch 11, iter 9885, loss 2.54945, smoothed loss 2.09280, grad norm 5.20772, param norm 166.60121
epoch 11, iter 9890, loss 2.60130, smoothed loss 2.10309, grad norm 5.12762, param norm 166.63020
epoch 11, iter 9895, loss 2.35178, smoothed loss 2.11095, grad norm 3.80186, param norm 166.66096
epoch 11, iter 9900, loss 2.06371, smoothed loss 2.12108, grad norm 3.57974, param norm 166.69276
epoch 11, iter 9905, loss 1.76063, smoothed loss 2.12608, grad norm 3.33584, param norm 166.72580
epoch 11, iter 9910, loss 2.23091, smoothed loss 2.12832, grad norm 3.66645, param norm 166.75760
epoch 11, iter 9915, loss 1.78806, smoothed loss 2.13005, grad norm 3.62619, param norm 166.78502
epoch 11, iter 9920, loss 1.54867, smoothed loss 2.11636, grad norm 3.07395, param norm 166.81096
epoch 11, iter 9925, loss 2.62143, smoothed loss 2.12887, grad norm 4.78505, param norm 166.83864
epoch 11, iter 9930, loss 1.88728, smoothed loss 2.11978, grad norm 3.28713, param norm 166.86626
epoch 11, iter 9935, loss 1.61366, smoothed loss 2.13248, grad norm 3.41449, param norm 166.89217
epoch 11, iter 9940, loss 2.07659, smoothed loss 2.12068, grad norm 3.37657, param norm 166.91859
epoch 11, iter 9945, loss 1.93958, smoothed loss 2.12719, grad norm 3.67635, param norm 166.94243
epoch 11, iter 9950, loss 2.56345, smoothed loss 2.13151, grad norm 4.10301, param norm 166.96780
epoch 11, iter 9955, loss 1.96259, smoothed loss 2.12946, grad norm 3.38071, param norm 166.99677
epoch 11, iter 9960, loss 2.29443, smoothed loss 2.13775, grad norm 3.57968, param norm 167.02576
epoch 11, iter 9965, loss 2.25034, smoothed loss 2.14527, grad norm 3.80638, param norm 167.05051
epoch 11, iter 9970, loss 1.77956, smoothed loss 2.13762, grad norm 3.86508, param norm 167.07362
epoch 11, iter 9975, loss 1.98391, smoothed loss 2.13925, grad norm 3.49887, param norm 167.09833
epoch 11, iter 9980, loss 1.80690, smoothed loss 2.13450, grad norm 3.59011, param norm 167.12250
epoch 11, iter 9985, loss 2.31647, smoothed loss 2.14073, grad norm 4.51128, param norm 167.14536
epoch 11, iter 9990, loss 2.28709, smoothed loss 2.15014, grad norm 4.22055, param norm 167.17035
epoch 11, iter 9995, loss 2.10091, smoothed loss 2.15853, grad norm 4.02351, param norm 167.19748
epoch 11, iter 10000, loss 1.96555, smoothed loss 2.15118, grad norm 3.45382, param norm 167.22662
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 11, Iter 10000, dev loss: 3.126340
Calculating Train F1/EM...
F1 train: 1000 examples took 18.66049 seconds [Score: 0.84149]
Exact Match train: 1000 examples took 18.70322 seconds [Score: 0.70200]
Epoch 11, Iter 10000, Train F1 score: 0.841491, Train EM score: 0.702000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 125.31439 seconds [Score: 0.65316]
Exact Match dev: 7118 examples took 124.41515 seconds [Score: 0.50379]
Epoch 11, Iter 10000, Dev F1 score: 0.653157, Dev EM score: 0.503793
End of epoch 11
epoch 11, iter 10005, loss 2.28660, smoothed loss 2.14926, grad norm 3.83733, param norm 167.25307
epoch 11, iter 10010, loss 2.36324, smoothed loss 2.15426, grad norm 3.65802, param norm 167.27953
epoch 11, iter 10015, loss 1.65235, smoothed loss 2.15372, grad norm 3.79776, param norm 167.30939
epoch 11, iter 10020, loss 2.83088, smoothed loss 2.14897, grad norm 5.04178, param norm 167.34044
epoch 11, iter 10025, loss 1.82596, smoothed loss 2.14167, grad norm 3.58715, param norm 167.37053
epoch 11, iter 10030, loss 2.18958, smoothed loss 2.13281, grad norm 3.95763, param norm 167.39738
epoch 11, iter 10035, loss 1.85759, smoothed loss 2.13156, grad norm 3.76083, param norm 167.42332
epoch 11, iter 10040, loss 1.86087, smoothed loss 2.12329, grad norm 3.42533, param norm 167.44914
epoch 11, iter 10045, loss 2.97557, smoothed loss 2.12799, grad norm 4.30506, param norm 167.47565
epoch 11, iter 10050, loss 3.03696, smoothed loss 2.13324, grad norm 4.25312, param norm 167.50203
epoch 11, iter 10055, loss 2.10796, smoothed loss 2.14338, grad norm 3.70989, param norm 167.52916
epoch 11, iter 10060, loss 2.33049, smoothed loss 2.14378, grad norm 3.74829, param norm 167.55659
epoch 11, iter 10065, loss 2.29244, smoothed loss 2.13542, grad norm 4.09047, param norm 167.58444
epoch 11, iter 10070, loss 1.67914, smoothed loss 2.12449, grad norm 3.49512, param norm 167.60823
epoch 11, iter 10075, loss 2.35201, smoothed loss 2.12144, grad norm 4.86821, param norm 167.62985
epoch 11, iter 10080, loss 2.11706, smoothed loss 2.12459, grad norm 3.99634, param norm 167.65723
epoch 11, iter 10085, loss 2.48245, smoothed loss 2.12532, grad norm 3.96452, param norm 167.68648
epoch 11, iter 10090, loss 2.09688, smoothed loss 2.11857, grad norm 3.78191, param norm 167.71448
epoch 11, iter 10095, loss 2.30976, smoothed loss 2.12330, grad norm 4.18017, param norm 167.74355
epoch 11, iter 10100, loss 2.12061, smoothed loss 2.12102, grad norm 4.07535, param norm 167.77374
epoch 11, iter 10105, loss 1.80102, smoothed loss 2.12028, grad norm 3.35423, param norm 167.80260
epoch 11, iter 10110, loss 1.73202, smoothed loss 2.11817, grad norm 3.37036, param norm 167.83362
epoch 11, iter 10115, loss 1.69232, smoothed loss 2.10901, grad norm 3.24655, param norm 167.86368
epoch 11, iter 10120, loss 2.43731, smoothed loss 2.10727, grad norm 4.70159, param norm 167.89273
epoch 11, iter 10125, loss 1.97520, smoothed loss 2.10522, grad norm 3.75214, param norm 167.91977
epoch 11, iter 10130, loss 2.73921, smoothed loss 2.11138, grad norm 4.90041, param norm 167.95004
epoch 11, iter 10135, loss 2.23460, smoothed loss 2.11818, grad norm 4.01048, param norm 167.98074
epoch 11, iter 10140, loss 1.98393, smoothed loss 2.12563, grad norm 4.21663, param norm 168.00970
epoch 11, iter 10145, loss 2.47498, smoothed loss 2.12805, grad norm 4.21267, param norm 168.04120
epoch 11, iter 10150, loss 1.98933, smoothed loss 2.11949, grad norm 3.43791, param norm 168.07124
epoch 11, iter 10155, loss 2.33828, smoothed loss 2.12762, grad norm 3.77816, param norm 168.10123
epoch 11, iter 10160, loss 2.35729, smoothed loss 2.12548, grad norm 4.08048, param norm 168.12881
epoch 11, iter 10165, loss 1.45342, smoothed loss 2.11998, grad norm 3.39469, param norm 168.15329
epoch 11, iter 10170, loss 2.48948, smoothed loss 2.13414, grad norm 4.26333, param norm 168.18295
epoch 11, iter 10175, loss 2.22786, smoothed loss 2.13736, grad norm 4.00077, param norm 168.21243
epoch 11, iter 10180, loss 2.37630, smoothed loss 2.14264, grad norm 4.13112, param norm 168.24051
epoch 11, iter 10185, loss 1.97531, smoothed loss 2.13499, grad norm 3.77250, param norm 168.26616
epoch 11, iter 10190, loss 2.08137, smoothed loss 2.13265, grad norm 3.49307, param norm 168.29231
epoch 11, iter 10195, loss 1.70425, smoothed loss 2.13105, grad norm 3.02912, param norm 168.31844
epoch 11, iter 10200, loss 2.34396, smoothed loss 2.12695, grad norm 3.75997, param norm 168.34755
epoch 11, iter 10205, loss 2.26133, smoothed loss 2.13182, grad norm 4.47748, param norm 168.37517
epoch 11, iter 10210, loss 2.16532, smoothed loss 2.13008, grad norm 3.98928, param norm 168.40810
epoch 11, iter 10215, loss 2.26024, smoothed loss 2.13264, grad norm 3.87935, param norm 168.43962
epoch 11, iter 10220, loss 2.45855, smoothed loss 2.14619, grad norm 4.47793, param norm 168.46999
epoch 11, iter 10225, loss 2.51818, smoothed loss 2.14290, grad norm 4.24397, param norm 168.49960
epoch 11, iter 10230, loss 2.34363, smoothed loss 2.14806, grad norm 4.34610, param norm 168.52882
epoch 11, iter 10235, loss 2.39706, smoothed loss 2.15588, grad norm 3.95670, param norm 168.55667
epoch 11, iter 10240, loss 2.36830, smoothed loss 2.15067, grad norm 4.48515, param norm 168.58574
epoch 11, iter 10245, loss 1.36580, smoothed loss 2.14371, grad norm 3.62280, param norm 168.61363
epoch 11, iter 10250, loss 1.96740, smoothed loss 2.13589, grad norm 3.43635, param norm 168.64693
epoch 11, iter 10255, loss 2.56191, smoothed loss 2.14171, grad norm 4.26223, param norm 168.67883
epoch 11, iter 10260, loss 2.23952, smoothed loss 2.14584, grad norm 5.07298, param norm 168.70891
epoch 11, iter 10265, loss 1.60914, smoothed loss 2.12958, grad norm 3.62761, param norm 168.73540
epoch 11, iter 10270, loss 1.77140, smoothed loss 2.11367, grad norm 3.73112, param norm 168.76164
epoch 11, iter 10275, loss 1.70836, smoothed loss 2.10142, grad norm 3.66767, param norm 168.78720
epoch 11, iter 10280, loss 1.65556, smoothed loss 2.09961, grad norm 4.04315, param norm 168.81085
epoch 11, iter 10285, loss 1.88904, smoothed loss 2.08600, grad norm 3.87032, param norm 168.83672
epoch 11, iter 10290, loss 1.98094, smoothed loss 2.08706, grad norm 3.79715, param norm 168.86475
epoch 11, iter 10295, loss 2.84245, smoothed loss 2.09630, grad norm 4.44337, param norm 168.89452
epoch 11, iter 10300, loss 1.97056, smoothed loss 2.09704, grad norm 3.95442, param norm 168.92741
epoch 11, iter 10305, loss 1.85946, smoothed loss 2.09499, grad norm 3.07310, param norm 168.96202
epoch 11, iter 10310, loss 2.07823, smoothed loss 2.10086, grad norm 4.52777, param norm 168.99390
epoch 11, iter 10315, loss 1.75521, smoothed loss 2.09014, grad norm 4.21037, param norm 169.02588
epoch 11, iter 10320, loss 1.89219, smoothed loss 2.09294, grad norm 3.68136, param norm 169.05684
epoch 11, iter 10325, loss 2.32950, smoothed loss 2.09546, grad norm 3.92431, param norm 169.08572
epoch 11, iter 10330, loss 2.12068, smoothed loss 2.09019, grad norm 4.43599, param norm 169.11531
epoch 11, iter 10335, loss 2.76860, smoothed loss 2.09334, grad norm 5.12646, param norm 169.14494
epoch 11, iter 10340, loss 1.98891, smoothed loss 2.08778, grad norm 3.67199, param norm 169.17413
epoch 11, iter 10345, loss 1.63536, smoothed loss 2.09214, grad norm 3.41880, param norm 169.20190
epoch 11, iter 10350, loss 2.00378, smoothed loss 2.09762, grad norm 3.79748, param norm 169.22551
epoch 11, iter 10355, loss 2.11194, smoothed loss 2.09677, grad norm 3.75957, param norm 169.25026
epoch 11, iter 10360, loss 2.09400, smoothed loss 2.09674, grad norm 4.00265, param norm 169.27504
epoch 11, iter 10365, loss 2.38925, smoothed loss 2.09862, grad norm 3.92388, param norm 169.30147
epoch 11, iter 10370, loss 2.79496, smoothed loss 2.09020, grad norm 4.82429, param norm 169.33070
epoch 11, iter 10375, loss 2.55415, smoothed loss 2.08918, grad norm 4.61875, param norm 169.35971
epoch 11, iter 10380, loss 2.36520, smoothed loss 2.10359, grad norm 4.22583, param norm 169.38989
epoch 12, iter 10385, loss 1.91900, smoothed loss 2.09821, grad norm 4.32596, param norm 169.42287
epoch 12, iter 10390, loss 1.81530, smoothed loss 2.08942, grad norm 3.93651, param norm 169.45624
epoch 12, iter 10395, loss 1.94255, smoothed loss 2.07692, grad norm 3.86073, param norm 169.48637
epoch 12, iter 10400, loss 2.02523, smoothed loss 2.08698, grad norm 4.08268, param norm 169.51454
epoch 12, iter 10405, loss 2.26773, smoothed loss 2.08936, grad norm 3.66805, param norm 169.54343
epoch 12, iter 10410, loss 1.89290, smoothed loss 2.08369, grad norm 4.15756, param norm 169.57504
epoch 12, iter 10415, loss 2.43556, smoothed loss 2.09240, grad norm 3.86084, param norm 169.60449
epoch 12, iter 10420, loss 2.30440, smoothed loss 2.09806, grad norm 4.15641, param norm 169.63261
epoch 12, iter 10425, loss 1.51988, smoothed loss 2.09394, grad norm 3.06831, param norm 169.65973
epoch 12, iter 10430, loss 2.10769, smoothed loss 2.08805, grad norm 3.70405, param norm 169.68767
epoch 12, iter 10435, loss 1.99052, smoothed loss 2.08051, grad norm 3.88423, param norm 169.71553
epoch 12, iter 10440, loss 2.25563, smoothed loss 2.08421, grad norm 3.76587, param norm 169.74527
epoch 12, iter 10445, loss 1.94246, smoothed loss 2.08075, grad norm 4.17182, param norm 169.77309
epoch 12, iter 10450, loss 2.15441, smoothed loss 2.07505, grad norm 4.43363, param norm 169.80511
epoch 12, iter 10455, loss 2.41114, smoothed loss 2.07601, grad norm 4.50791, param norm 169.83755
epoch 12, iter 10460, loss 1.69036, smoothed loss 2.07617, grad norm 3.69110, param norm 169.86455
epoch 12, iter 10465, loss 1.75366, smoothed loss 2.06712, grad norm 3.39838, param norm 169.89391
epoch 12, iter 10470, loss 1.73942, smoothed loss 2.07444, grad norm 3.67085, param norm 169.92332
epoch 12, iter 10475, loss 2.15012, smoothed loss 2.06909, grad norm 4.22497, param norm 169.95448
epoch 12, iter 10480, loss 2.01348, smoothed loss 2.06760, grad norm 4.19846, param norm 169.98567
epoch 12, iter 10485, loss 2.24300, smoothed loss 2.06613, grad norm 4.38985, param norm 170.01828
epoch 12, iter 10490, loss 1.99397, smoothed loss 2.06472, grad norm 3.75030, param norm 170.04933
epoch 12, iter 10495, loss 1.73243, smoothed loss 2.06346, grad norm 3.90845, param norm 170.07683
epoch 12, iter 10500, loss 2.41444, smoothed loss 2.05745, grad norm 4.45938, param norm 170.10851
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 12, Iter 10500, dev loss: 3.214390
Calculating Train F1/EM...
F1 train: 1000 examples took 17.52800 seconds [Score: 0.89254]
Exact Match train: 1000 examples took 18.17609 seconds [Score: 0.74500]
Epoch 12, Iter 10500, Train F1 score: 0.892537, Train EM score: 0.745000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.42963 seconds [Score: 0.64623]
Exact Match dev: 7118 examples took 124.46892 seconds [Score: 0.49649]
Epoch 12, Iter 10500, Dev F1 score: 0.646229, Dev EM score: 0.496488
End of epoch 12
epoch 12, iter 10505, loss 2.04908, smoothed loss 2.03869, grad norm 3.92219, param norm 170.14185
epoch 12, iter 10510, loss 1.92691, smoothed loss 2.03995, grad norm 3.72184, param norm 170.17328
epoch 12, iter 10515, loss 2.31585, smoothed loss 2.03537, grad norm 4.30179, param norm 170.20242
epoch 12, iter 10520, loss 2.42987, smoothed loss 2.04792, grad norm 4.02818, param norm 170.22652
epoch 12, iter 10525, loss 1.76372, smoothed loss 2.04873, grad norm 3.85603, param norm 170.25180
epoch 12, iter 10530, loss 2.92038, smoothed loss 2.06317, grad norm 4.28876, param norm 170.27861
epoch 12, iter 10535, loss 2.16710, smoothed loss 2.07186, grad norm 4.59908, param norm 170.30417
epoch 12, iter 10540, loss 1.39774, smoothed loss 2.06535, grad norm 3.16978, param norm 170.32846
epoch 12, iter 10545, loss 2.09018, smoothed loss 2.06551, grad norm 4.05396, param norm 170.35385
epoch 12, iter 10550, loss 2.39856, smoothed loss 2.07239, grad norm 5.22702, param norm 170.38112
epoch 12, iter 10555, loss 2.64699, smoothed loss 2.07108, grad norm 4.46412, param norm 170.41353
epoch 12, iter 10560, loss 1.64371, smoothed loss 2.06127, grad norm 3.70566, param norm 170.44441
epoch 12, iter 10565, loss 2.00796, smoothed loss 2.07143, grad norm 3.58129, param norm 170.47133
epoch 12, iter 10570, loss 2.07666, smoothed loss 2.06978, grad norm 3.61888, param norm 170.49809
epoch 12, iter 10575, loss 1.74876, smoothed loss 2.06401, grad norm 3.40519, param norm 170.52528
epoch 12, iter 10580, loss 2.18995, smoothed loss 2.06202, grad norm 3.80932, param norm 170.55603
epoch 12, iter 10585, loss 1.75670, smoothed loss 2.04748, grad norm 3.69892, param norm 170.58411
epoch 12, iter 10590, loss 2.47764, smoothed loss 2.05059, grad norm 4.58932, param norm 170.61154
epoch 12, iter 10595, loss 1.73713, smoothed loss 2.05502, grad norm 3.13887, param norm 170.63892
epoch 12, iter 10600, loss 2.51165, smoothed loss 2.06487, grad norm 4.35384, param norm 170.66571
epoch 12, iter 10605, loss 2.06879, smoothed loss 2.06634, grad norm 3.94474, param norm 170.69154
epoch 12, iter 10610, loss 1.68820, smoothed loss 2.06254, grad norm 3.45896, param norm 170.71959
epoch 12, iter 10615, loss 2.52400, smoothed loss 2.07623, grad norm 3.95238, param norm 170.74683
epoch 12, iter 10620, loss 1.98504, smoothed loss 2.08448, grad norm 3.49060, param norm 170.77650
epoch 12, iter 10625, loss 1.92474, smoothed loss 2.08474, grad norm 3.35588, param norm 170.80856
epoch 12, iter 10630, loss 1.92986, smoothed loss 2.08957, grad norm 4.05828, param norm 170.83878
epoch 12, iter 10635, loss 1.93183, smoothed loss 2.07931, grad norm 3.15956, param norm 170.87029
epoch 12, iter 10640, loss 2.39094, smoothed loss 2.08057, grad norm 4.72441, param norm 170.90105
epoch 12, iter 10645, loss 2.20693, smoothed loss 2.07750, grad norm 4.20369, param norm 170.92856
epoch 12, iter 10650, loss 2.39211, smoothed loss 2.08560, grad norm 4.36753, param norm 170.95345
epoch 12, iter 10655, loss 2.64821, smoothed loss 2.09362, grad norm 4.91592, param norm 170.97658
epoch 12, iter 10660, loss 2.03717, smoothed loss 2.08723, grad norm 4.36840, param norm 171.00375
epoch 12, iter 10665, loss 2.24253, smoothed loss 2.08691, grad norm 4.15556, param norm 171.03255
epoch 12, iter 10670, loss 2.23396, smoothed loss 2.06911, grad norm 3.67716, param norm 171.06149
epoch 12, iter 10675, loss 2.34598, smoothed loss 2.07285, grad norm 4.09679, param norm 171.08807
epoch 12, iter 10680, loss 1.78533, smoothed loss 2.07463, grad norm 3.45297, param norm 171.11526
epoch 12, iter 10685, loss 2.01065, smoothed loss 2.07555, grad norm 3.88471, param norm 171.14613
epoch 12, iter 10690, loss 2.36081, smoothed loss 2.08646, grad norm 4.26697, param norm 171.17725
epoch 12, iter 10695, loss 2.38279, smoothed loss 2.09711, grad norm 4.51055, param norm 171.20862
epoch 12, iter 10700, loss 2.14655, smoothed loss 2.09305, grad norm 4.26470, param norm 171.23611
epoch 12, iter 10705, loss 1.86064, smoothed loss 2.09181, grad norm 3.96307, param norm 171.26363
epoch 12, iter 10710, loss 1.86131, smoothed loss 2.09236, grad norm 4.04809, param norm 171.29324
epoch 12, iter 10715, loss 2.08238, smoothed loss 2.10341, grad norm 3.58710, param norm 171.32245
epoch 12, iter 10720, loss 1.85407, smoothed loss 2.10095, grad norm 4.07238, param norm 171.35214
epoch 12, iter 10725, loss 2.44590, smoothed loss 2.10930, grad norm 4.30696, param norm 171.38191
epoch 12, iter 10730, loss 2.48813, smoothed loss 2.11493, grad norm 4.09183, param norm 171.41312
epoch 12, iter 10735, loss 2.03603, smoothed loss 2.11316, grad norm 3.76197, param norm 171.44421
epoch 12, iter 10740, loss 1.31746, smoothed loss 2.10519, grad norm 3.00221, param norm 171.47121
epoch 12, iter 10745, loss 2.07921, smoothed loss 2.10199, grad norm 3.79640, param norm 171.49391
epoch 12, iter 10750, loss 1.99081, smoothed loss 2.09698, grad norm 4.22508, param norm 171.51587
epoch 12, iter 10755, loss 2.45441, smoothed loss 2.10113, grad norm 4.43394, param norm 171.54184
epoch 12, iter 10760, loss 1.84312, smoothed loss 2.09148, grad norm 3.54251, param norm 171.57141
epoch 12, iter 10765, loss 2.00057, smoothed loss 2.09239, grad norm 4.80217, param norm 171.60054
epoch 12, iter 10770, loss 2.07788, smoothed loss 2.08605, grad norm 4.06922, param norm 171.63029
epoch 12, iter 10775, loss 1.99987, smoothed loss 2.08567, grad norm 3.82886, param norm 171.65915
epoch 12, iter 10780, loss 2.13577, smoothed loss 2.07577, grad norm 4.18608, param norm 171.68861
epoch 12, iter 10785, loss 1.54463, smoothed loss 2.06161, grad norm 4.01586, param norm 171.71950
epoch 12, iter 10790, loss 2.48940, smoothed loss 2.06368, grad norm 4.76218, param norm 171.74640
epoch 12, iter 10795, loss 1.74350, smoothed loss 2.05567, grad norm 3.00911, param norm 171.77182
epoch 12, iter 10800, loss 1.98262, smoothed loss 2.05698, grad norm 3.91625, param norm 171.79800
epoch 12, iter 10805, loss 1.67829, smoothed loss 2.04473, grad norm 3.33399, param norm 171.82640
epoch 12, iter 10810, loss 2.75808, smoothed loss 2.05326, grad norm 4.30911, param norm 171.85071
epoch 12, iter 10815, loss 1.86332, smoothed loss 2.03972, grad norm 3.95187, param norm 171.87373
epoch 12, iter 10820, loss 2.04382, smoothed loss 2.03388, grad norm 4.14777, param norm 171.89659
epoch 12, iter 10825, loss 2.25976, smoothed loss 2.02889, grad norm 4.16703, param norm 171.91840
epoch 12, iter 10830, loss 2.04109, smoothed loss 2.02286, grad norm 3.51746, param norm 171.94180
epoch 12, iter 10835, loss 1.36026, smoothed loss 2.01495, grad norm 3.19368, param norm 171.96283
epoch 12, iter 10840, loss 2.40475, smoothed loss 2.02476, grad norm 4.83690, param norm 171.98643
epoch 12, iter 10845, loss 1.73072, smoothed loss 2.01909, grad norm 4.03797, param norm 172.01270
epoch 12, iter 10850, loss 2.01400, smoothed loss 2.01078, grad norm 4.37610, param norm 172.04041
epoch 12, iter 10855, loss 2.29907, smoothed loss 2.01122, grad norm 3.85317, param norm 172.06871
epoch 12, iter 10860, loss 1.85259, smoothed loss 2.01657, grad norm 3.48871, param norm 172.09631
epoch 12, iter 10865, loss 1.51897, smoothed loss 2.01306, grad norm 3.11087, param norm 172.12256
epoch 12, iter 10870, loss 2.33574, smoothed loss 2.02684, grad norm 4.29115, param norm 172.15068
epoch 12, iter 10875, loss 2.11303, smoothed loss 2.03430, grad norm 4.42549, param norm 172.17752
epoch 12, iter 10880, loss 1.42575, smoothed loss 2.02725, grad norm 3.25776, param norm 172.20213
epoch 12, iter 10885, loss 1.61582, smoothed loss 2.02712, grad norm 3.66574, param norm 172.22705
epoch 12, iter 10890, loss 2.53725, smoothed loss 2.04728, grad norm 4.66020, param norm 172.24902
epoch 12, iter 10895, loss 1.63792, smoothed loss 2.04129, grad norm 3.61273, param norm 172.27275
epoch 12, iter 10900, loss 1.59425, smoothed loss 2.03839, grad norm 3.29760, param norm 172.30064
epoch 12, iter 10905, loss 2.42413, smoothed loss 2.04437, grad norm 4.11580, param norm 172.33052
epoch 12, iter 10910, loss 1.65635, smoothed loss 2.03023, grad norm 3.24602, param norm 172.35904
epoch 12, iter 10915, loss 2.61085, smoothed loss 2.03050, grad norm 4.50593, param norm 172.38464
epoch 12, iter 10920, loss 1.96787, smoothed loss 2.02293, grad norm 3.38777, param norm 172.40926
epoch 12, iter 10925, loss 1.77287, smoothed loss 2.01871, grad norm 3.98968, param norm 172.43161
epoch 12, iter 10930, loss 2.16609, smoothed loss 2.02114, grad norm 4.21304, param norm 172.45151
epoch 12, iter 10935, loss 1.94065, smoothed loss 2.01235, grad norm 3.80353, param norm 172.47485
epoch 12, iter 10940, loss 1.84141, smoothed loss 2.03271, grad norm 3.72685, param norm 172.50056
epoch 12, iter 10945, loss 1.64214, smoothed loss 2.04063, grad norm 3.19336, param norm 172.52631
epoch 12, iter 10950, loss 2.16861, smoothed loss 2.04341, grad norm 3.42516, param norm 172.55321
epoch 12, iter 10955, loss 1.94920, smoothed loss 2.03900, grad norm 3.43418, param norm 172.58261
epoch 12, iter 10960, loss 2.15849, smoothed loss 2.03960, grad norm 4.13568, param norm 172.61031
epoch 12, iter 10965, loss 1.85003, smoothed loss 2.03070, grad norm 3.36174, param norm 172.63861
epoch 12, iter 10970, loss 1.71443, smoothed loss 2.03146, grad norm 3.83185, param norm 172.66374
epoch 12, iter 10975, loss 1.98481, smoothed loss 2.04133, grad norm 3.64361, param norm 172.68715
epoch 12, iter 10980, loss 1.86539, smoothed loss 2.04682, grad norm 3.16910, param norm 172.71078
epoch 12, iter 10985, loss 2.12861, smoothed loss 2.06245, grad norm 3.77434, param norm 172.73448
epoch 12, iter 10990, loss 2.06571, smoothed loss 2.06130, grad norm 3.74807, param norm 172.75856
epoch 12, iter 10995, loss 2.04674, smoothed loss 2.06702, grad norm 3.73141, param norm 172.78119
epoch 12, iter 11000, loss 2.24857, smoothed loss 2.06785, grad norm 3.82213, param norm 172.80574
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 12, Iter 11000, dev loss: 3.137923
Calculating Train F1/EM...
F1 train: 1000 examples took 18.88837 seconds [Score: 0.86378]
Exact Match train: 1000 examples took 18.60780 seconds [Score: 0.73000]
Epoch 12, Iter 11000, Train F1 score: 0.863776, Train EM score: 0.730000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 123.87711 seconds [Score: 0.65413]
Exact Match dev: 7118 examples took 123.60621 seconds [Score: 0.50464]
Epoch 12, Iter 11000, Dev F1 score: 0.654126, Dev EM score: 0.504636
End of epoch 12
epoch 12, iter 11005, loss 2.38816, smoothed loss 2.07225, grad norm 3.95563, param norm 172.83275
epoch 12, iter 11010, loss 2.48552, smoothed loss 2.07700, grad norm 4.40340, param norm 172.85747
epoch 12, iter 11015, loss 2.16307, smoothed loss 2.08281, grad norm 4.16164, param norm 172.88040
epoch 12, iter 11020, loss 1.67396, smoothed loss 2.07591, grad norm 4.13885, param norm 172.90442
epoch 12, iter 11025, loss 1.90558, smoothed loss 2.07572, grad norm 3.28191, param norm 172.93106
epoch 12, iter 11030, loss 1.96955, smoothed loss 2.06452, grad norm 3.49760, param norm 172.95877
epoch 12, iter 11035, loss 2.21264, smoothed loss 2.06976, grad norm 3.94168, param norm 172.98254
epoch 12, iter 11040, loss 2.21844, smoothed loss 2.06617, grad norm 4.01329, param norm 173.00533
epoch 12, iter 11045, loss 1.43467, smoothed loss 2.06457, grad norm 3.13527, param norm 173.02675
epoch 12, iter 11050, loss 2.45912, smoothed loss 2.06646, grad norm 4.13741, param norm 173.04755
epoch 12, iter 11055, loss 2.07471, smoothed loss 2.06070, grad norm 4.20138, param norm 173.07008
epoch 12, iter 11060, loss 1.47953, smoothed loss 2.05715, grad norm 3.37014, param norm 173.09610
epoch 12, iter 11065, loss 2.10012, smoothed loss 2.05126, grad norm 4.04301, param norm 173.12285
epoch 12, iter 11070, loss 1.86183, smoothed loss 2.04905, grad norm 4.28432, param norm 173.14888
epoch 12, iter 11075, loss 2.40080, smoothed loss 2.05687, grad norm 4.14343, param norm 173.17360
epoch 12, iter 11080, loss 1.91483, smoothed loss 2.06427, grad norm 3.70880, param norm 173.20073
epoch 12, iter 11085, loss 2.42116, smoothed loss 2.06422, grad norm 5.10500, param norm 173.22987
epoch 12, iter 11090, loss 2.57526, smoothed loss 2.06304, grad norm 3.88272, param norm 173.25787
epoch 12, iter 11095, loss 2.10553, smoothed loss 2.06605, grad norm 3.42673, param norm 173.28249
epoch 12, iter 11100, loss 2.03549, smoothed loss 2.06870, grad norm 3.43389, param norm 173.30750
epoch 12, iter 11105, loss 2.21611, smoothed loss 2.06605, grad norm 3.92906, param norm 173.33148
epoch 12, iter 11110, loss 2.05031, smoothed loss 2.06859, grad norm 3.70322, param norm 173.35510
epoch 12, iter 11115, loss 2.11806, smoothed loss 2.06137, grad norm 4.29874, param norm 173.38177
epoch 12, iter 11120, loss 2.12973, smoothed loss 2.05761, grad norm 3.63376, param norm 173.40851
epoch 12, iter 11125, loss 1.86495, smoothed loss 2.05069, grad norm 3.47050, param norm 173.43430
epoch 12, iter 11130, loss 2.30982, smoothed loss 2.04744, grad norm 4.86042, param norm 173.45905
epoch 12, iter 11135, loss 2.07471, smoothed loss 2.03964, grad norm 4.62039, param norm 173.48227
epoch 12, iter 11140, loss 2.05185, smoothed loss 2.04298, grad norm 4.09279, param norm 173.50722
epoch 12, iter 11145, loss 2.06929, smoothed loss 2.04318, grad norm 4.10297, param norm 173.53403
epoch 12, iter 11150, loss 2.36264, smoothed loss 2.06128, grad norm 4.90862, param norm 173.56007
epoch 12, iter 11155, loss 2.96002, smoothed loss 2.07071, grad norm 4.52693, param norm 173.58424
epoch 12, iter 11160, loss 2.06387, smoothed loss 2.07606, grad norm 3.82364, param norm 173.61136
epoch 12, iter 11165, loss 2.16571, smoothed loss 2.08571, grad norm 4.26191, param norm 173.63692
epoch 12, iter 11170, loss 2.47582, smoothed loss 2.08317, grad norm 4.23747, param norm 173.66426
epoch 12, iter 11175, loss 2.00960, smoothed loss 2.07782, grad norm 4.71512, param norm 173.69090
epoch 12, iter 11180, loss 2.01621, smoothed loss 2.07302, grad norm 4.13659, param norm 173.71623
epoch 12, iter 11185, loss 2.08651, smoothed loss 2.06638, grad norm 3.99350, param norm 173.74396
epoch 12, iter 11190, loss 1.90404, smoothed loss 2.06819, grad norm 3.83908, param norm 173.77075
epoch 12, iter 11195, loss 1.79624, smoothed loss 2.07242, grad norm 4.06345, param norm 173.79733
epoch 12, iter 11200, loss 2.20143, smoothed loss 2.06850, grad norm 3.98702, param norm 173.82454
epoch 12, iter 11205, loss 2.69011, smoothed loss 2.07195, grad norm 4.49339, param norm 173.84900
epoch 12, iter 11210, loss 1.38389, smoothed loss 2.06886, grad norm 3.34997, param norm 173.87206
epoch 12, iter 11215, loss 2.07628, smoothed loss 2.06734, grad norm 3.98416, param norm 173.89685
epoch 12, iter 11220, loss 1.93608, smoothed loss 2.06703, grad norm 3.77568, param norm 173.92133
epoch 12, iter 11225, loss 2.43132, smoothed loss 2.07072, grad norm 4.86428, param norm 173.94629
epoch 12, iter 11230, loss 2.19668, smoothed loss 2.07148, grad norm 4.08435, param norm 173.97147
epoch 12, iter 11235, loss 1.63555, smoothed loss 2.05950, grad norm 3.87080, param norm 174.00076
epoch 12, iter 11240, loss 1.91599, smoothed loss 2.05753, grad norm 3.48841, param norm 174.03047
epoch 12, iter 11245, loss 2.04517, smoothed loss 2.04916, grad norm 3.22139, param norm 174.05736
epoch 12, iter 11250, loss 2.21116, smoothed loss 2.04434, grad norm 3.79157, param norm 174.08682
epoch 12, iter 11255, loss 1.95980, smoothed loss 2.05184, grad norm 3.87944, param norm 174.11293
epoch 12, iter 11260, loss 1.65188, smoothed loss 2.04527, grad norm 4.08821, param norm 174.13690
epoch 12, iter 11265, loss 2.31429, smoothed loss 2.05693, grad norm 3.85149, param norm 174.16206
epoch 12, iter 11270, loss 1.95567, smoothed loss 2.05801, grad norm 3.56116, param norm 174.18916
epoch 12, iter 11275, loss 1.90661, smoothed loss 2.05576, grad norm 3.81204, param norm 174.21693
epoch 12, iter 11280, loss 1.55418, smoothed loss 2.04444, grad norm 4.06764, param norm 174.24217
epoch 12, iter 11285, loss 2.61382, smoothed loss 2.04222, grad norm 4.90563, param norm 174.26852
epoch 12, iter 11290, loss 1.89875, smoothed loss 2.03461, grad norm 3.56528, param norm 174.29379
epoch 12, iter 11295, loss 2.42353, smoothed loss 2.02953, grad norm 3.97434, param norm 174.32001
epoch 12, iter 11300, loss 2.87163, smoothed loss 2.03421, grad norm 4.98599, param norm 174.34856
epoch 12, iter 11305, loss 1.51970, smoothed loss 2.02695, grad norm 3.52036, param norm 174.37413
epoch 12, iter 11310, loss 1.35787, smoothed loss 2.01871, grad norm 3.34940, param norm 174.40033
epoch 12, iter 11315, loss 2.05191, smoothed loss 2.02231, grad norm 4.44866, param norm 174.42485
epoch 12, iter 11320, loss 1.90153, smoothed loss 2.02591, grad norm 4.17393, param norm 174.45131
epoch 12, iter 11325, loss 1.97148, smoothed loss 2.02518, grad norm 3.37094, param norm 174.47655
epoch 13, iter 11330, loss 2.33593, smoothed loss 2.01913, grad norm 3.93925, param norm 174.50168
epoch 13, iter 11335, loss 1.34246, smoothed loss 2.01115, grad norm 3.37316, param norm 174.52783
epoch 13, iter 11340, loss 2.13710, smoothed loss 2.01530, grad norm 4.05426, param norm 174.55524
epoch 13, iter 11345, loss 2.05226, smoothed loss 2.01029, grad norm 4.65922, param norm 174.58081
epoch 13, iter 11350, loss 2.07262, smoothed loss 1.99647, grad norm 3.93253, param norm 174.60745
epoch 13, iter 11355, loss 2.36446, smoothed loss 1.99522, grad norm 4.69116, param norm 174.63457
epoch 13, iter 11360, loss 1.88486, smoothed loss 1.99260, grad norm 3.93737, param norm 174.66014
epoch 13, iter 11365, loss 2.01041, smoothed loss 1.98907, grad norm 4.27172, param norm 174.68695
epoch 13, iter 11370, loss 2.11967, smoothed loss 1.99512, grad norm 3.71730, param norm 174.71089
epoch 13, iter 11375, loss 2.02695, smoothed loss 1.99388, grad norm 3.69360, param norm 174.73602
epoch 13, iter 11380, loss 1.32534, smoothed loss 1.98699, grad norm 3.40002, param norm 174.76460
epoch 13, iter 11385, loss 2.40638, smoothed loss 2.00170, grad norm 4.22214, param norm 174.79210
epoch 13, iter 11390, loss 2.01487, smoothed loss 1.99838, grad norm 4.24508, param norm 174.81931
epoch 13, iter 11395, loss 1.99788, smoothed loss 2.00314, grad norm 4.09524, param norm 174.84978
epoch 13, iter 11400, loss 1.92078, smoothed loss 1.98933, grad norm 4.06330, param norm 174.87990
epoch 13, iter 11405, loss 1.39230, smoothed loss 1.98598, grad norm 3.14805, param norm 174.90544
epoch 13, iter 11410, loss 2.31229, smoothed loss 1.99426, grad norm 4.11372, param norm 174.93025
epoch 13, iter 11415, loss 1.91452, smoothed loss 1.98395, grad norm 3.36597, param norm 174.95641
epoch 13, iter 11420, loss 1.93053, smoothed loss 1.97514, grad norm 3.88481, param norm 174.98083
epoch 13, iter 11425, loss 1.76710, smoothed loss 1.97976, grad norm 3.62867, param norm 175.00523
epoch 13, iter 11430, loss 1.71298, smoothed loss 1.97728, grad norm 3.65336, param norm 175.03102
epoch 13, iter 11435, loss 2.07536, smoothed loss 1.98002, grad norm 3.99849, param norm 175.06020
epoch 13, iter 11440, loss 1.75590, smoothed loss 1.98809, grad norm 3.79010, param norm 175.08499
epoch 13, iter 11445, loss 1.62894, smoothed loss 1.98491, grad norm 3.88549, param norm 175.11067
epoch 13, iter 11450, loss 2.19079, smoothed loss 1.99752, grad norm 4.01370, param norm 175.13728
epoch 13, iter 11455, loss 1.76998, smoothed loss 1.99357, grad norm 3.26678, param norm 175.16589
epoch 13, iter 11460, loss 1.86070, smoothed loss 1.99439, grad norm 4.14623, param norm 175.19292
epoch 13, iter 11465, loss 1.92049, smoothed loss 2.00283, grad norm 3.35538, param norm 175.21884
epoch 13, iter 11470, loss 1.95187, smoothed loss 2.00104, grad norm 3.56071, param norm 175.24059
epoch 13, iter 11475, loss 1.63468, smoothed loss 1.99720, grad norm 3.36767, param norm 175.26256
epoch 13, iter 11480, loss 2.28679, smoothed loss 1.99915, grad norm 4.50200, param norm 175.28632
epoch 13, iter 11485, loss 1.98999, smoothed loss 1.99807, grad norm 3.91376, param norm 175.31012
epoch 13, iter 11490, loss 2.35928, smoothed loss 2.00446, grad norm 4.08889, param norm 175.33652
epoch 13, iter 11495, loss 1.49241, smoothed loss 2.00309, grad norm 3.49249, param norm 175.35968
epoch 13, iter 11500, loss 1.70477, smoothed loss 1.99615, grad norm 4.23934, param norm 175.38428
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 13, Iter 11500, dev loss: 3.214932
Calculating Train F1/EM...
F1 train: 1000 examples took 18.74347 seconds [Score: 0.88916]
Exact Match train: 1000 examples took 18.49101 seconds [Score: 0.77000]
Epoch 13, Iter 11500, Train F1 score: 0.889156, Train EM score: 0.770000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 124.10969 seconds [Score: 0.65188]
Exact Match dev: 7118 examples took 123.56350 seconds [Score: 0.49817]
Epoch 13, Iter 11500, Dev F1 score: 0.651885, Dev EM score: 0.498174
End of epoch 13
epoch 13, iter 11505, loss 2.02535, smoothed loss 2.00222, grad norm 4.18332, param norm 175.41090
epoch 13, iter 11510, loss 2.72499, smoothed loss 2.00629, grad norm 4.34191, param norm 175.43857
epoch 13, iter 11515, loss 2.03442, smoothed loss 2.01367, grad norm 4.47777, param norm 175.46451
epoch 13, iter 11520, loss 2.40569, smoothed loss 2.02057, grad norm 4.40339, param norm 175.49077
epoch 13, iter 11525, loss 2.40343, smoothed loss 2.03121, grad norm 3.86683, param norm 175.51743
epoch 13, iter 11530, loss 1.67039, smoothed loss 2.03569, grad norm 3.00007, param norm 175.54529
epoch 13, iter 11535, loss 2.13859, smoothed loss 2.03890, grad norm 3.97365, param norm 175.57402
epoch 13, iter 11540, loss 1.67562, smoothed loss 2.03694, grad norm 3.40303, param norm 175.60307
epoch 13, iter 11545, loss 2.20166, smoothed loss 2.03758, grad norm 4.19848, param norm 175.63155
epoch 13, iter 11550, loss 1.77158, smoothed loss 2.03459, grad norm 3.49849, param norm 175.66032
epoch 13, iter 11555, loss 1.76951, smoothed loss 2.03378, grad norm 4.00133, param norm 175.68858
epoch 13, iter 11560, loss 2.11087, smoothed loss 2.03134, grad norm 3.93345, param norm 175.71469
epoch 13, iter 11565, loss 1.65844, smoothed loss 2.02917, grad norm 3.13802, param norm 175.74214
epoch 13, iter 11570, loss 1.85567, smoothed loss 2.02102, grad norm 3.71815, param norm 175.76894
epoch 13, iter 11575, loss 1.70327, smoothed loss 2.01297, grad norm 4.13032, param norm 175.79428
epoch 13, iter 11580, loss 1.60673, smoothed loss 1.99869, grad norm 3.50399, param norm 175.82085
epoch 13, iter 11585, loss 1.60602, smoothed loss 2.00798, grad norm 4.36500, param norm 175.84718
epoch 13, iter 11590, loss 2.17196, smoothed loss 2.01242, grad norm 4.02426, param norm 175.87381
epoch 13, iter 11595, loss 2.55132, smoothed loss 2.01750, grad norm 4.65406, param norm 175.90211
epoch 13, iter 11600, loss 1.94801, smoothed loss 2.02194, grad norm 3.63744, param norm 175.92874
epoch 13, iter 11605, loss 2.44818, smoothed loss 2.03529, grad norm 3.62350, param norm 175.95631
epoch 13, iter 11610, loss 2.02521, smoothed loss 2.03199, grad norm 3.68842, param norm 175.98541
epoch 13, iter 11615, loss 1.66540, smoothed loss 2.02000, grad norm 3.24365, param norm 176.01358
epoch 13, iter 11620, loss 2.79608, smoothed loss 2.01898, grad norm 4.70874, param norm 176.04045
epoch 13, iter 11625, loss 2.17138, smoothed loss 2.00967, grad norm 4.39597, param norm 176.06396
epoch 13, iter 11630, loss 2.41608, smoothed loss 2.02385, grad norm 4.32836, param norm 176.08673
epoch 13, iter 11635, loss 2.16270, smoothed loss 2.02364, grad norm 4.13670, param norm 176.10838
epoch 13, iter 11640, loss 1.52240, smoothed loss 2.01299, grad norm 3.20619, param norm 176.13330
epoch 13, iter 11645, loss 2.79143, smoothed loss 2.01719, grad norm 5.09348, param norm 176.15791
epoch 13, iter 11650, loss 2.56966, smoothed loss 2.01442, grad norm 4.26300, param norm 176.18147
epoch 13, iter 11655, loss 2.52488, smoothed loss 2.01875, grad norm 4.35384, param norm 176.20367
epoch 13, iter 11660, loss 1.62231, smoothed loss 2.01237, grad norm 3.41335, param norm 176.22798
epoch 13, iter 11665, loss 2.39260, smoothed loss 2.01816, grad norm 5.75193, param norm 176.25444
epoch 13, iter 11670, loss 2.10838, smoothed loss 2.01766, grad norm 4.17779, param norm 176.27744
epoch 13, iter 11675, loss 1.99144, smoothed loss 2.02786, grad norm 3.89975, param norm 176.30048
epoch 13, iter 11680, loss 2.50179, smoothed loss 2.02951, grad norm 4.37687, param norm 176.32703
epoch 13, iter 11685, loss 2.01737, smoothed loss 2.02307, grad norm 3.46072, param norm 176.35562
epoch 13, iter 11690, loss 2.33555, smoothed loss 2.02034, grad norm 5.21427, param norm 176.38380
epoch 13, iter 11695, loss 1.76475, smoothed loss 2.00888, grad norm 3.86038, param norm 176.40974
epoch 13, iter 11700, loss 2.23679, smoothed loss 1.99837, grad norm 4.92768, param norm 176.43318
epoch 13, iter 11705, loss 1.79136, smoothed loss 1.99965, grad norm 4.08962, param norm 176.45483
epoch 13, iter 11710, loss 1.85873, smoothed loss 1.99232, grad norm 3.63405, param norm 176.47818
epoch 13, iter 11715, loss 1.91922, smoothed loss 1.98852, grad norm 4.32247, param norm 176.50339
epoch 13, iter 11720, loss 2.03017, smoothed loss 1.98020, grad norm 3.71923, param norm 176.52669
epoch 13, iter 11725, loss 2.05532, smoothed loss 1.97922, grad norm 3.79227, param norm 176.54974
epoch 13, iter 11730, loss 2.33245, smoothed loss 1.98074, grad norm 3.90348, param norm 176.57658
epoch 13, iter 11735, loss 2.44191, smoothed loss 1.98919, grad norm 4.82405, param norm 176.60022
epoch 13, iter 11740, loss 1.97434, smoothed loss 1.99245, grad norm 4.33791, param norm 176.62421
epoch 13, iter 11745, loss 2.10496, smoothed loss 1.98646, grad norm 4.55755, param norm 176.64996
epoch 13, iter 11750, loss 2.31673, smoothed loss 1.99451, grad norm 4.15858, param norm 176.67641
epoch 13, iter 11755, loss 1.88610, smoothed loss 1.99523, grad norm 3.83291, param norm 176.70091
epoch 13, iter 11760, loss 1.92881, smoothed loss 1.98967, grad norm 4.31336, param norm 176.72661
epoch 13, iter 11765, loss 1.81688, smoothed loss 1.99051, grad norm 3.81338, param norm 176.75247
epoch 13, iter 11770, loss 1.67680, smoothed loss 1.98328, grad norm 3.37454, param norm 176.77705
epoch 13, iter 11775, loss 1.84802, smoothed loss 1.97278, grad norm 3.88750, param norm 176.80212
epoch 13, iter 11780, loss 1.60691, smoothed loss 1.96154, grad norm 4.23979, param norm 176.82515
epoch 13, iter 11785, loss 1.67250, smoothed loss 1.95628, grad norm 3.86837, param norm 176.85011
epoch 13, iter 11790, loss 2.14748, smoothed loss 1.95450, grad norm 3.89047, param norm 176.87532
epoch 13, iter 11795, loss 2.02785, smoothed loss 1.94860, grad norm 3.74609, param norm 176.89688
epoch 13, iter 11800, loss 1.24399, smoothed loss 1.93238, grad norm 3.47332, param norm 176.91765
epoch 13, iter 11805, loss 2.63792, smoothed loss 1.94209, grad norm 5.30367, param norm 176.94342
epoch 13, iter 11810, loss 2.47544, smoothed loss 1.94987, grad norm 4.25626, param norm 176.96893
epoch 13, iter 11815, loss 1.76123, smoothed loss 1.95137, grad norm 3.58955, param norm 176.99210
epoch 13, iter 11820, loss 2.42423, smoothed loss 1.96042, grad norm 4.14297, param norm 177.01567
epoch 13, iter 11825, loss 1.75129, smoothed loss 1.96076, grad norm 4.10779, param norm 177.04172
epoch 13, iter 11830, loss 1.89071, smoothed loss 1.96456, grad norm 3.69368, param norm 177.06671
epoch 13, iter 11835, loss 1.74133, smoothed loss 1.96636, grad norm 3.39496, param norm 177.09337
epoch 13, iter 11840, loss 1.59924, smoothed loss 1.96215, grad norm 3.65116, param norm 177.11868
epoch 13, iter 11845, loss 2.18578, smoothed loss 1.96423, grad norm 4.04537, param norm 177.14253
epoch 13, iter 11850, loss 1.95150, smoothed loss 1.94575, grad norm 3.89892, param norm 177.16432
epoch 13, iter 11855, loss 1.78875, smoothed loss 1.95949, grad norm 4.34922, param norm 177.18694
epoch 13, iter 11860, loss 2.17781, smoothed loss 1.96941, grad norm 3.93795, param norm 177.20859
epoch 13, iter 11865, loss 1.55575, smoothed loss 1.97329, grad norm 3.45836, param norm 177.23010
epoch 13, iter 11870, loss 1.83832, smoothed loss 1.96714, grad norm 3.93012, param norm 177.25266
epoch 13, iter 11875, loss 2.50428, smoothed loss 1.95106, grad norm 4.24488, param norm 177.27377
epoch 13, iter 11880, loss 1.92064, smoothed loss 1.94719, grad norm 3.29620, param norm 177.29657
epoch 13, iter 11885, loss 1.90679, smoothed loss 1.94705, grad norm 4.13176, param norm 177.32295
epoch 13, iter 11890, loss 2.06250, smoothed loss 1.95404, grad norm 4.03328, param norm 177.35089
epoch 13, iter 11895, loss 1.80304, smoothed loss 1.96354, grad norm 3.44649, param norm 177.37743
epoch 13, iter 11900, loss 2.10937, smoothed loss 1.96913, grad norm 3.76607, param norm 177.40498
epoch 13, iter 11905, loss 1.64935, smoothed loss 1.96293, grad norm 3.62993, param norm 177.43222
epoch 13, iter 11910, loss 1.90993, smoothed loss 1.96757, grad norm 4.37227, param norm 177.45857
epoch 13, iter 11915, loss 1.97970, smoothed loss 1.96375, grad norm 3.59894, param norm 177.48364
epoch 13, iter 11920, loss 2.35522, smoothed loss 1.97117, grad norm 3.85201, param norm 177.50789
epoch 13, iter 11925, loss 1.67745, smoothed loss 1.96818, grad norm 3.82326, param norm 177.53622
epoch 13, iter 11930, loss 2.19457, smoothed loss 1.97145, grad norm 4.02945, param norm 177.56679
epoch 13, iter 11935, loss 2.61516, smoothed loss 1.97688, grad norm 5.07481, param norm 177.59926
epoch 13, iter 11940, loss 2.16929, smoothed loss 1.99052, grad norm 4.41186, param norm 177.62660
epoch 13, iter 11945, loss 1.75495, smoothed loss 1.99922, grad norm 3.83223, param norm 177.65120
epoch 13, iter 11950, loss 1.81853, smoothed loss 1.99069, grad norm 3.40822, param norm 177.67387
epoch 13, iter 11955, loss 1.58887, smoothed loss 1.99428, grad norm 4.21548, param norm 177.69640
epoch 13, iter 11960, loss 2.04010, smoothed loss 1.99123, grad norm 3.73794, param norm 177.72171
epoch 13, iter 11965, loss 2.61802, smoothed loss 1.99275, grad norm 4.36893, param norm 177.74934
epoch 13, iter 11970, loss 2.02766, smoothed loss 2.00154, grad norm 3.64379, param norm 177.77515
epoch 13, iter 11975, loss 2.07351, smoothed loss 2.00363, grad norm 3.93862, param norm 177.80333
epoch 13, iter 11980, loss 2.56358, smoothed loss 2.00803, grad norm 5.23956, param norm 177.83009
epoch 13, iter 11985, loss 1.62952, smoothed loss 2.01186, grad norm 4.11261, param norm 177.85468
epoch 13, iter 11990, loss 2.82192, smoothed loss 2.01573, grad norm 4.09580, param norm 177.87846
epoch 13, iter 11995, loss 1.67036, smoothed loss 2.01904, grad norm 3.51921, param norm 177.90065
epoch 13, iter 12000, loss 1.99165, smoothed loss 2.01604, grad norm 3.59219, param norm 177.92149
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 13, Iter 12000, dev loss: 3.163583
Calculating Train F1/EM...
F1 train: 1000 examples took 18.52257 seconds [Score: 0.85563]
Exact Match train: 1000 examples took 18.40062 seconds [Score: 0.75100]
Epoch 13, Iter 12000, Train F1 score: 0.855627, Train EM score: 0.751000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 121.91597 seconds [Score: 0.65465]
Exact Match dev: 7118 examples took 120.98305 seconds [Score: 0.50295]
Epoch 13, Iter 12000, Dev F1 score: 0.654653, Dev EM score: 0.502950
End of epoch 13
epoch 13, iter 12005, loss 2.26642, smoothed loss 2.01177, grad norm 4.47918, param norm 177.94388
epoch 13, iter 12010, loss 2.11743, smoothed loss 2.01692, grad norm 4.22753, param norm 177.96980
epoch 13, iter 12015, loss 1.78923, smoothed loss 2.01252, grad norm 3.49720, param norm 177.99695
epoch 13, iter 12020, loss 2.08760, smoothed loss 2.00887, grad norm 4.28614, param norm 178.02347
epoch 13, iter 12025, loss 2.26814, smoothed loss 2.00854, grad norm 3.90017, param norm 178.04735
epoch 13, iter 12030, loss 1.66195, smoothed loss 2.01625, grad norm 3.89746, param norm 178.07153
epoch 13, iter 12035, loss 2.29490, smoothed loss 2.02447, grad norm 4.90507, param norm 178.09438
epoch 13, iter 12040, loss 2.15885, smoothed loss 2.01530, grad norm 4.26577, param norm 178.11671
epoch 13, iter 12045, loss 2.01622, smoothed loss 2.01702, grad norm 4.14074, param norm 178.14153
epoch 13, iter 12050, loss 1.72843, smoothed loss 2.00978, grad norm 3.82991, param norm 178.16591
epoch 13, iter 12055, loss 2.08537, smoothed loss 2.00899, grad norm 3.78199, param norm 178.19034
epoch 13, iter 12060, loss 1.83427, smoothed loss 2.01951, grad norm 4.07519, param norm 178.21591
epoch 13, iter 12065, loss 1.56209, smoothed loss 2.00705, grad norm 3.59177, param norm 178.24049
epoch 13, iter 12070, loss 1.65012, smoothed loss 2.00002, grad norm 4.28692, param norm 178.26515
epoch 13, iter 12075, loss 2.34296, smoothed loss 1.99089, grad norm 4.11167, param norm 178.29118
epoch 13, iter 12080, loss 1.80764, smoothed loss 1.99089, grad norm 3.86375, param norm 178.31703
epoch 13, iter 12085, loss 2.69631, smoothed loss 2.00699, grad norm 5.07200, param norm 178.34348
epoch 13, iter 12090, loss 2.26360, smoothed loss 2.00525, grad norm 4.62907, param norm 178.36823
epoch 13, iter 12095, loss 2.19275, smoothed loss 2.01567, grad norm 3.88737, param norm 178.39304
epoch 13, iter 12100, loss 2.54771, smoothed loss 2.02432, grad norm 4.48245, param norm 178.41905
epoch 13, iter 12105, loss 2.49504, smoothed loss 2.02414, grad norm 4.21176, param norm 178.44315
epoch 13, iter 12110, loss 1.96476, smoothed loss 2.01448, grad norm 3.86509, param norm 178.46959
epoch 13, iter 12115, loss 2.62590, smoothed loss 2.02437, grad norm 5.42269, param norm 178.49550
epoch 13, iter 12120, loss 2.53989, smoothed loss 2.01570, grad norm 4.45761, param norm 178.51715
epoch 13, iter 12125, loss 1.83253, smoothed loss 2.00799, grad norm 3.50309, param norm 178.54152
epoch 13, iter 12130, loss 2.10822, smoothed loss 2.01799, grad norm 4.15695, param norm 178.56664
epoch 13, iter 12135, loss 1.84555, smoothed loss 2.01056, grad norm 4.02173, param norm 178.59236
epoch 13, iter 12140, loss 1.93457, smoothed loss 2.00597, grad norm 4.10546, param norm 178.61485
epoch 13, iter 12145, loss 2.18096, smoothed loss 2.01622, grad norm 4.01357, param norm 178.63666
epoch 13, iter 12150, loss 1.91292, smoothed loss 2.01243, grad norm 4.15802, param norm 178.65990
epoch 13, iter 12155, loss 2.12865, smoothed loss 2.00851, grad norm 4.07568, param norm 178.68576
epoch 13, iter 12160, loss 1.74192, smoothed loss 1.99999, grad norm 3.35168, param norm 178.71501
epoch 13, iter 12165, loss 2.12015, smoothed loss 2.00355, grad norm 3.85370, param norm 178.74649
epoch 13, iter 12170, loss 2.00230, smoothed loss 1.99813, grad norm 3.65497, param norm 178.77669
epoch 13, iter 12175, loss 1.88559, smoothed loss 1.99325, grad norm 4.01639, param norm 178.80275
epoch 13, iter 12180, loss 2.05768, smoothed loss 1.99692, grad norm 4.91071, param norm 178.82881
epoch 13, iter 12185, loss 1.70801, smoothed loss 1.99308, grad norm 4.84691, param norm 178.85403
epoch 13, iter 12190, loss 1.63792, smoothed loss 1.98412, grad norm 3.46405, param norm 178.87982
epoch 13, iter 12195, loss 2.08787, smoothed loss 1.98243, grad norm 4.63903, param norm 178.90457
epoch 13, iter 12200, loss 2.03867, smoothed loss 1.97538, grad norm 3.75264, param norm 178.92876
epoch 13, iter 12205, loss 2.20316, smoothed loss 1.97936, grad norm 3.77390, param norm 178.95232
epoch 13, iter 12210, loss 1.65484, smoothed loss 1.96272, grad norm 3.96408, param norm 178.97354
epoch 13, iter 12215, loss 2.05899, smoothed loss 1.96121, grad norm 4.49218, param norm 178.99655
epoch 13, iter 12220, loss 2.16258, smoothed loss 1.96427, grad norm 4.13213, param norm 179.01950
epoch 13, iter 12225, loss 1.46442, smoothed loss 1.95975, grad norm 3.35323, param norm 179.04414
epoch 13, iter 12230, loss 2.73317, smoothed loss 1.96723, grad norm 5.10926, param norm 179.07045
epoch 13, iter 12235, loss 2.27688, smoothed loss 1.96790, grad norm 4.53797, param norm 179.09483
epoch 13, iter 12240, loss 1.97244, smoothed loss 1.96800, grad norm 4.33307, param norm 179.11754
epoch 13, iter 12245, loss 1.70616, smoothed loss 1.97920, grad norm 3.63307, param norm 179.14069
epoch 13, iter 12250, loss 2.25875, smoothed loss 1.98516, grad norm 4.71684, param norm 179.16194
epoch 13, iter 12255, loss 1.39156, smoothed loss 1.97417, grad norm 3.37644, param norm 179.18713
epoch 13, iter 12260, loss 2.36831, smoothed loss 1.96954, grad norm 4.84266, param norm 179.21559
epoch 13, iter 12265, loss 1.51743, smoothed loss 1.95784, grad norm 4.06132, param norm 179.24200
epoch 13, iter 12270, loss 1.83722, smoothed loss 1.95908, grad norm 4.08563, param norm 179.26599
epoch 14, iter 12275, loss 2.11005, smoothed loss 1.96953, grad norm 3.95484, param norm 179.29187
epoch 14, iter 12280, loss 2.21265, smoothed loss 1.97818, grad norm 4.29166, param norm 179.31670
epoch 14, iter 12285, loss 1.41565, smoothed loss 1.96221, grad norm 3.97979, param norm 179.34195
epoch 14, iter 12290, loss 2.11846, smoothed loss 1.96405, grad norm 4.18277, param norm 179.36824
epoch 14, iter 12295, loss 1.41931, smoothed loss 1.96803, grad norm 3.78107, param norm 179.39426
epoch 14, iter 12300, loss 1.91940, smoothed loss 1.96447, grad norm 4.10251, param norm 179.41982
epoch 14, iter 12305, loss 1.70818, smoothed loss 1.96800, grad norm 3.37197, param norm 179.44377
epoch 14, iter 12310, loss 1.50187, smoothed loss 1.96753, grad norm 3.98963, param norm 179.46744
epoch 14, iter 12315, loss 2.47880, smoothed loss 1.97136, grad norm 4.25748, param norm 179.49217
epoch 14, iter 12320, loss 1.57664, smoothed loss 1.96960, grad norm 4.06933, param norm 179.51840
epoch 14, iter 12325, loss 1.82196, smoothed loss 1.97281, grad norm 3.81198, param norm 179.54311
epoch 14, iter 12330, loss 1.80899, smoothed loss 1.98163, grad norm 3.92691, param norm 179.57018
epoch 14, iter 12335, loss 1.68850, smoothed loss 1.98298, grad norm 3.88740, param norm 179.59793
epoch 14, iter 12340, loss 2.14971, smoothed loss 1.97926, grad norm 4.15469, param norm 179.62413
epoch 14, iter 12345, loss 2.44279, smoothed loss 1.97530, grad norm 4.16065, param norm 179.65057
epoch 14, iter 12350, loss 1.94431, smoothed loss 1.97942, grad norm 3.58438, param norm 179.67352
epoch 14, iter 12355, loss 2.23601, smoothed loss 1.97715, grad norm 3.96228, param norm 179.69540
epoch 14, iter 12360, loss 1.90582, smoothed loss 1.97000, grad norm 3.74998, param norm 179.71756
epoch 14, iter 12365, loss 2.17126, smoothed loss 1.96734, grad norm 3.89340, param norm 179.74100
epoch 14, iter 12370, loss 1.89486, smoothed loss 1.96228, grad norm 4.19223, param norm 179.76266
epoch 14, iter 12375, loss 1.74458, smoothed loss 1.96050, grad norm 4.12798, param norm 179.78470
epoch 14, iter 12380, loss 2.08695, smoothed loss 1.97370, grad norm 3.90996, param norm 179.80484
epoch 14, iter 12385, loss 1.39732, smoothed loss 1.96158, grad norm 3.87102, param norm 179.82480
epoch 14, iter 12390, loss 1.88954, smoothed loss 1.95735, grad norm 5.27636, param norm 179.85065
epoch 14, iter 12395, loss 1.95055, smoothed loss 1.93857, grad norm 4.07274, param norm 179.87860
epoch 14, iter 12400, loss 1.78549, smoothed loss 1.93364, grad norm 4.22403, param norm 179.90627
epoch 14, iter 12405, loss 1.35347, smoothed loss 1.92292, grad norm 3.64671, param norm 179.93439
epoch 14, iter 12410, loss 2.07547, smoothed loss 1.92450, grad norm 4.26447, param norm 179.96161
epoch 14, iter 12415, loss 2.11657, smoothed loss 1.91425, grad norm 4.26238, param norm 179.98953
epoch 14, iter 12420, loss 2.05142, smoothed loss 1.91712, grad norm 4.05562, param norm 180.01543
epoch 14, iter 12425, loss 2.48697, smoothed loss 1.91797, grad norm 4.81891, param norm 180.03957
epoch 14, iter 12430, loss 2.00486, smoothed loss 1.92551, grad norm 3.90005, param norm 180.06195
epoch 14, iter 12435, loss 1.78382, smoothed loss 1.94129, grad norm 4.22341, param norm 180.08105
epoch 14, iter 12440, loss 1.93455, smoothed loss 1.94013, grad norm 3.81934, param norm 180.10196
epoch 14, iter 12445, loss 1.88044, smoothed loss 1.93558, grad norm 3.53634, param norm 180.12306
epoch 14, iter 12450, loss 2.06946, smoothed loss 1.93775, grad norm 4.31917, param norm 180.14233
epoch 14, iter 12455, loss 1.72969, smoothed loss 1.93481, grad norm 3.15989, param norm 180.15974
epoch 14, iter 12460, loss 2.41954, smoothed loss 1.93635, grad norm 5.00037, param norm 180.17783
epoch 14, iter 12465, loss 1.74530, smoothed loss 1.92392, grad norm 3.57238, param norm 180.19608
epoch 14, iter 12470, loss 1.95357, smoothed loss 1.92968, grad norm 4.39749, param norm 180.21878
epoch 14, iter 12475, loss 2.46661, smoothed loss 1.93905, grad norm 4.89960, param norm 180.24420
epoch 14, iter 12480, loss 2.93750, smoothed loss 1.95143, grad norm 5.33530, param norm 180.26910
epoch 14, iter 12485, loss 1.59981, smoothed loss 1.94542, grad norm 3.49182, param norm 180.29341
epoch 14, iter 12490, loss 1.39315, smoothed loss 1.94388, grad norm 3.25608, param norm 180.32188
epoch 14, iter 12495, loss 1.73349, smoothed loss 1.94882, grad norm 3.46959, param norm 180.34952
epoch 14, iter 12500, loss 2.51026, smoothed loss 1.96314, grad norm 4.89471, param norm 180.37254
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 14, Iter 12500, dev loss: 3.266020
Calculating Train F1/EM...
F1 train: 1000 examples took 17.77116 seconds [Score: 0.89409]
Exact Match train: 1000 examples took 17.73215 seconds [Score: 0.74600]
Epoch 14, Iter 12500, Train F1 score: 0.894093, Train EM score: 0.746000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 122.51283 seconds [Score: 0.64661]
Exact Match dev: 7118 examples took 122.32500 seconds [Score: 0.50042]
Epoch 14, Iter 12500, Dev F1 score: 0.646612, Dev EM score: 0.500421
End of epoch 14
epoch 14, iter 12505, loss 1.86396, smoothed loss 1.96585, grad norm 4.39003, param norm 180.39262
epoch 14, iter 12510, loss 2.10512, smoothed loss 1.97042, grad norm 4.36196, param norm 180.41626
epoch 14, iter 12515, loss 2.43891, smoothed loss 1.97601, grad norm 4.00540, param norm 180.44196
epoch 14, iter 12520, loss 2.51474, smoothed loss 1.97804, grad norm 4.15671, param norm 180.46741
epoch 14, iter 12525, loss 2.41639, smoothed loss 1.98013, grad norm 4.05379, param norm 180.49280
epoch 14, iter 12530, loss 2.06943, smoothed loss 1.96563, grad norm 3.63684, param norm 180.51817
epoch 14, iter 12535, loss 1.61704, smoothed loss 1.96136, grad norm 3.62877, param norm 180.54541
epoch 14, iter 12540, loss 1.69811, smoothed loss 1.94984, grad norm 4.44282, param norm 180.57324
epoch 14, iter 12545, loss 1.49235, smoothed loss 1.95819, grad norm 4.08317, param norm 180.59723
epoch 14, iter 12550, loss 1.75962, smoothed loss 1.95355, grad norm 3.52398, param norm 180.62282
epoch 14, iter 12555, loss 1.98311, smoothed loss 1.95368, grad norm 4.62143, param norm 180.65125
epoch 14, iter 12560, loss 2.22837, smoothed loss 1.97266, grad norm 4.48299, param norm 180.67647
epoch 14, iter 12565, loss 2.07696, smoothed loss 1.96808, grad norm 3.85495, param norm 180.70146
epoch 14, iter 12570, loss 2.26978, smoothed loss 1.97474, grad norm 4.77797, param norm 180.72874
epoch 14, iter 12575, loss 1.73235, smoothed loss 1.97441, grad norm 4.04919, param norm 180.75662
epoch 14, iter 12580, loss 1.92648, smoothed loss 1.96832, grad norm 3.71333, param norm 180.78412
epoch 14, iter 12585, loss 2.39798, smoothed loss 1.97992, grad norm 4.27152, param norm 180.81039
epoch 14, iter 12590, loss 1.71668, smoothed loss 1.97953, grad norm 3.73173, param norm 180.83617
epoch 14, iter 12595, loss 1.81809, smoothed loss 1.96197, grad norm 3.70337, param norm 180.86420
epoch 14, iter 12600, loss 1.95443, smoothed loss 1.95361, grad norm 4.75413, param norm 180.89301
epoch 14, iter 12605, loss 1.70492, smoothed loss 1.94636, grad norm 3.71776, param norm 180.92189
epoch 14, iter 12610, loss 1.87458, smoothed loss 1.94580, grad norm 3.90251, param norm 180.95064
epoch 14, iter 12615, loss 1.99102, smoothed loss 1.95070, grad norm 4.02755, param norm 180.97961
epoch 14, iter 12620, loss 1.99734, smoothed loss 1.95424, grad norm 4.52297, param norm 181.00301
epoch 14, iter 12625, loss 2.02087, smoothed loss 1.96140, grad norm 4.06727, param norm 181.02350
epoch 14, iter 12630, loss 2.53119, smoothed loss 1.96081, grad norm 5.04437, param norm 181.04623
epoch 14, iter 12635, loss 2.25336, smoothed loss 1.96902, grad norm 4.09370, param norm 181.06976
epoch 14, iter 12640, loss 2.04233, smoothed loss 1.97878, grad norm 4.40045, param norm 181.09271
epoch 14, iter 12645, loss 1.73027, smoothed loss 1.97169, grad norm 3.28549, param norm 181.11502
epoch 14, iter 12650, loss 1.78038, smoothed loss 1.96408, grad norm 3.86164, param norm 181.13371
epoch 14, iter 12655, loss 1.86991, smoothed loss 1.95910, grad norm 4.67141, param norm 181.15594
epoch 14, iter 12660, loss 2.11379, smoothed loss 1.96144, grad norm 3.99526, param norm 181.18446
epoch 14, iter 12665, loss 1.84941, smoothed loss 1.95993, grad norm 4.46942, param norm 181.21410
epoch 14, iter 12670, loss 1.94951, smoothed loss 1.94784, grad norm 3.80082, param norm 181.24384
epoch 14, iter 12675, loss 1.63165, smoothed loss 1.94259, grad norm 3.30422, param norm 181.27151
epoch 14, iter 12680, loss 1.74444, smoothed loss 1.94164, grad norm 3.27144, param norm 181.29494
epoch 14, iter 12685, loss 1.51646, smoothed loss 1.93323, grad norm 4.51850, param norm 181.31491
epoch 14, iter 12690, loss 2.72412, smoothed loss 1.94303, grad norm 4.92880, param norm 181.33507
epoch 14, iter 12695, loss 2.09793, smoothed loss 1.94679, grad norm 4.64289, param norm 181.35944
epoch 14, iter 12700, loss 2.06171, smoothed loss 1.94953, grad norm 3.83671, param norm 181.38478
epoch 14, iter 12705, loss 2.35809, smoothed loss 1.95797, grad norm 4.01003, param norm 181.41203
epoch 14, iter 12710, loss 2.04839, smoothed loss 1.95543, grad norm 4.49655, param norm 181.44052
epoch 14, iter 12715, loss 2.00843, smoothed loss 1.94759, grad norm 4.01594, param norm 181.47014
epoch 14, iter 12720, loss 1.75300, smoothed loss 1.93366, grad norm 3.47631, param norm 181.49648
epoch 14, iter 12725, loss 2.03596, smoothed loss 1.93644, grad norm 4.12653, param norm 181.51740
epoch 14, iter 12730, loss 2.05189, smoothed loss 1.93236, grad norm 4.14402, param norm 181.53491
epoch 14, iter 12735, loss 1.69086, smoothed loss 1.92528, grad norm 4.31373, param norm 181.55389
epoch 14, iter 12740, loss 1.60893, smoothed loss 1.92356, grad norm 3.61901, param norm 181.57558
epoch 14, iter 12745, loss 2.00056, smoothed loss 1.93447, grad norm 3.89658, param norm 181.59996
epoch 14, iter 12750, loss 1.57226, smoothed loss 1.93553, grad norm 3.87508, param norm 181.62241
epoch 14, iter 12755, loss 2.10703, smoothed loss 1.94421, grad norm 3.73761, param norm 181.64394
epoch 14, iter 12760, loss 2.66326, smoothed loss 1.94755, grad norm 4.67189, param norm 181.66316
epoch 14, iter 12765, loss 2.09125, smoothed loss 1.95934, grad norm 4.20365, param norm 181.68367
epoch 14, iter 12770, loss 1.78288, smoothed loss 1.95306, grad norm 4.13035, param norm 181.70946
epoch 14, iter 12775, loss 1.97307, smoothed loss 1.94160, grad norm 3.49061, param norm 181.73599
epoch 14, iter 12780, loss 1.92712, smoothed loss 1.92967, grad norm 3.77716, param norm 181.75912
epoch 14, iter 12785, loss 2.35901, smoothed loss 1.94045, grad norm 4.62236, param norm 181.77776
epoch 14, iter 12790, loss 1.74583, smoothed loss 1.92691, grad norm 3.64164, param norm 181.79962
epoch 14, iter 12795, loss 1.54006, smoothed loss 1.93168, grad norm 3.72161, param norm 181.82469
epoch 14, iter 12800, loss 1.68278, smoothed loss 1.92990, grad norm 3.92622, param norm 181.85074
epoch 14, iter 12805, loss 1.33092, smoothed loss 1.92848, grad norm 4.12655, param norm 181.87602
epoch 14, iter 12810, loss 1.83785, smoothed loss 1.94156, grad norm 3.87640, param norm 181.89775
epoch 14, iter 12815, loss 1.96398, smoothed loss 1.93764, grad norm 3.59505, param norm 181.92085
epoch 14, iter 12820, loss 1.73677, smoothed loss 1.93564, grad norm 3.77622, param norm 181.94524
epoch 14, iter 12825, loss 1.70623, smoothed loss 1.93564, grad norm 3.59296, param norm 181.96794
epoch 14, iter 12830, loss 1.82709, smoothed loss 1.94891, grad norm 3.56513, param norm 181.99110
epoch 14, iter 12835, loss 1.72171, smoothed loss 1.95088, grad norm 3.58194, param norm 182.01517
epoch 14, iter 12840, loss 1.86746, smoothed loss 1.95036, grad norm 3.65844, param norm 182.04077
epoch 14, iter 12845, loss 1.68219, smoothed loss 1.94785, grad norm 3.99031, param norm 182.06590
epoch 14, iter 12850, loss 1.70610, smoothed loss 1.94662, grad norm 3.72732, param norm 182.09088
epoch 14, iter 12855, loss 1.85121, smoothed loss 1.94646, grad norm 4.28769, param norm 182.11304
epoch 14, iter 12860, loss 1.65305, smoothed loss 1.94617, grad norm 3.71201, param norm 182.13425
epoch 14, iter 12865, loss 1.60717, smoothed loss 1.95355, grad norm 3.25363, param norm 182.15343
epoch 14, iter 12870, loss 2.10698, smoothed loss 1.95174, grad norm 4.45999, param norm 182.17496
epoch 14, iter 12875, loss 1.98215, smoothed loss 1.94571, grad norm 4.17475, param norm 182.19823
epoch 14, iter 12880, loss 2.12498, smoothed loss 1.95918, grad norm 4.10934, param norm 182.22321
epoch 14, iter 12885, loss 1.98024, smoothed loss 1.95790, grad norm 4.58501, param norm 182.24939
epoch 14, iter 12890, loss 2.05926, smoothed loss 1.95696, grad norm 4.42750, param norm 182.27432
epoch 14, iter 12895, loss 2.04101, smoothed loss 1.95650, grad norm 4.39905, param norm 182.30025
epoch 14, iter 12900, loss 1.97081, smoothed loss 1.95735, grad norm 3.64735, param norm 182.32407
epoch 14, iter 12905, loss 1.57114, smoothed loss 1.96175, grad norm 4.01162, param norm 182.34897
epoch 14, iter 12910, loss 1.77793, smoothed loss 1.96113, grad norm 4.06647, param norm 182.37061
epoch 14, iter 12915, loss 2.05465, smoothed loss 1.96685, grad norm 3.86937, param norm 182.39290
epoch 14, iter 12920, loss 1.87946, smoothed loss 1.96853, grad norm 3.96123, param norm 182.41496
epoch 14, iter 12925, loss 1.99392, smoothed loss 1.96750, grad norm 3.92440, param norm 182.43689
epoch 14, iter 12930, loss 1.58825, smoothed loss 1.97059, grad norm 3.66352, param norm 182.46056
epoch 14, iter 12935, loss 1.63948, smoothed loss 1.96989, grad norm 3.49581, param norm 182.48607
epoch 14, iter 12940, loss 2.17550, smoothed loss 1.96556, grad norm 4.29134, param norm 182.51021
epoch 14, iter 12945, loss 2.52106, smoothed loss 1.97935, grad norm 4.27163, param norm 182.53194
epoch 14, iter 12950, loss 2.23800, smoothed loss 1.98205, grad norm 4.39741, param norm 182.55525
epoch 14, iter 12955, loss 1.57594, smoothed loss 1.98032, grad norm 3.79262, param norm 182.58145
epoch 14, iter 12960, loss 1.60109, smoothed loss 1.96565, grad norm 3.54780, param norm 182.60779
epoch 14, iter 12965, loss 1.76436, smoothed loss 1.96813, grad norm 3.40637, param norm 182.63301
epoch 14, iter 12970, loss 1.85160, smoothed loss 1.96966, grad norm 3.52088, param norm 182.65518
epoch 14, iter 12975, loss 1.67249, smoothed loss 1.96614, grad norm 3.91500, param norm 182.67717
epoch 14, iter 12980, loss 2.10728, smoothed loss 1.96334, grad norm 4.10637, param norm 182.70076
epoch 14, iter 12985, loss 1.59402, smoothed loss 1.96393, grad norm 3.70606, param norm 182.71962
epoch 14, iter 12990, loss 1.93392, smoothed loss 1.95555, grad norm 3.70987, param norm 182.74048
epoch 14, iter 12995, loss 2.32734, smoothed loss 1.95851, grad norm 4.60414, param norm 182.76480
epoch 14, iter 13000, loss 1.91932, smoothed loss 1.96796, grad norm 3.99082, param norm 182.79314
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 14, Iter 13000, dev loss: 3.247399
Calculating Train F1/EM...
F1 train: 1000 examples took 18.36300 seconds [Score: 0.88053]
Exact Match train: 1000 examples took 17.90038 seconds [Score: 0.73500]
Epoch 14, Iter 13000, Train F1 score: 0.880531, Train EM score: 0.735000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 123.49514 seconds [Score: 0.64375]
Exact Match dev: 7118 examples took 122.93453 seconds [Score: 0.49227]
Epoch 14, Iter 13000, Dev F1 score: 0.643747, Dev EM score: 0.492273
End of epoch 14
epoch 14, iter 13005, loss 2.09241, smoothed loss 1.97103, grad norm 3.62769, param norm 182.82135
epoch 14, iter 13010, loss 1.68276, smoothed loss 1.97782, grad norm 3.57808, param norm 182.84970
epoch 14, iter 13015, loss 1.71884, smoothed loss 1.97079, grad norm 3.64239, param norm 182.87883
epoch 14, iter 13020, loss 2.08261, smoothed loss 1.96970, grad norm 3.96021, param norm 182.90660
epoch 14, iter 13025, loss 2.35830, smoothed loss 1.97035, grad norm 4.46171, param norm 182.93155
epoch 14, iter 13030, loss 1.73893, smoothed loss 1.98716, grad norm 4.29919, param norm 182.95329
epoch 14, iter 13035, loss 1.56410, smoothed loss 1.98701, grad norm 3.90160, param norm 182.97722
epoch 14, iter 13040, loss 1.63854, smoothed loss 1.98427, grad norm 3.53520, param norm 183.00113
epoch 14, iter 13045, loss 1.49974, smoothed loss 1.97791, grad norm 3.81752, param norm 183.02686
epoch 14, iter 13050, loss 2.21824, smoothed loss 1.97257, grad norm 4.10210, param norm 183.05042
epoch 14, iter 13055, loss 2.41094, smoothed loss 1.97027, grad norm 4.14954, param norm 183.07309
epoch 14, iter 13060, loss 2.03975, smoothed loss 1.97310, grad norm 3.88152, param norm 183.09753
epoch 14, iter 13065, loss 1.73062, smoothed loss 1.96711, grad norm 3.99457, param norm 183.12077
epoch 14, iter 13070, loss 2.13212, smoothed loss 1.96272, grad norm 4.19997, param norm 183.14432
epoch 14, iter 13075, loss 2.11322, smoothed loss 1.96101, grad norm 4.31354, param norm 183.16757
epoch 14, iter 13080, loss 1.98232, smoothed loss 1.95748, grad norm 4.74420, param norm 183.18979
epoch 14, iter 13085, loss 1.63599, smoothed loss 1.94642, grad norm 4.35338, param norm 183.21303
epoch 14, iter 13090, loss 1.64482, smoothed loss 1.93471, grad norm 4.03312, param norm 183.23740
epoch 14, iter 13095, loss 1.17109, smoothed loss 1.92710, grad norm 3.32796, param norm 183.26230
epoch 14, iter 13100, loss 1.31384, smoothed loss 1.91797, grad norm 3.00717, param norm 183.28932
epoch 14, iter 13105, loss 1.93746, smoothed loss 1.92112, grad norm 5.02814, param norm 183.31467
epoch 14, iter 13110, loss 1.20708, smoothed loss 1.92133, grad norm 3.61451, param norm 183.33737
epoch 14, iter 13115, loss 1.06136, smoothed loss 1.91204, grad norm 3.15739, param norm 183.36140
epoch 14, iter 13120, loss 1.59163, smoothed loss 1.90583, grad norm 4.51855, param norm 183.38429
epoch 14, iter 13125, loss 2.14701, smoothed loss 1.89973, grad norm 4.22468, param norm 183.40826
epoch 14, iter 13130, loss 1.75781, smoothed loss 1.90280, grad norm 3.97934, param norm 183.43216
epoch 14, iter 13135, loss 2.14216, smoothed loss 1.91394, grad norm 4.14002, param norm 183.45555
epoch 14, iter 13140, loss 2.47713, smoothed loss 1.92238, grad norm 4.80994, param norm 183.48016
epoch 14, iter 13145, loss 1.83450, smoothed loss 1.91790, grad norm 3.78698, param norm 183.50427
epoch 14, iter 13150, loss 1.48286, smoothed loss 1.91814, grad norm 3.83074, param norm 183.52792
epoch 14, iter 13155, loss 2.01555, smoothed loss 1.91832, grad norm 4.70130, param norm 183.55072
epoch 14, iter 13160, loss 2.15139, smoothed loss 1.92171, grad norm 4.51269, param norm 183.57448
epoch 14, iter 13165, loss 2.13526, smoothed loss 1.93318, grad norm 5.13643, param norm 183.60164
epoch 14, iter 13170, loss 1.43382, smoothed loss 1.93226, grad norm 3.34822, param norm 183.62982
epoch 14, iter 13175, loss 1.63816, smoothed loss 1.92591, grad norm 3.77839, param norm 183.65668
epoch 14, iter 13180, loss 1.95886, smoothed loss 1.92658, grad norm 4.51046, param norm 183.68277
epoch 14, iter 13185, loss 1.74072, smoothed loss 1.91697, grad norm 3.89811, param norm 183.70694
epoch 14, iter 13190, loss 1.71502, smoothed loss 1.92073, grad norm 3.87233, param norm 183.73053
epoch 14, iter 13195, loss 1.93224, smoothed loss 1.92223, grad norm 3.76943, param norm 183.75418
epoch 14, iter 13200, loss 2.03861, smoothed loss 1.92926, grad norm 3.98433, param norm 183.77689
epoch 14, iter 13205, loss 2.06226, smoothed loss 1.92149, grad norm 4.86524, param norm 183.80099
epoch 14, iter 13210, loss 1.41832, smoothed loss 1.91897, grad norm 3.82305, param norm 183.82658
epoch 14, iter 13215, loss 2.08386, smoothed loss 1.91527, grad norm 5.04697, param norm 183.85239
epoch 15, iter 13220, loss 2.04879, smoothed loss 1.91728, grad norm 4.18382, param norm 183.87941
epoch 15, iter 13225, loss 1.94360, smoothed loss 1.90928, grad norm 4.52192, param norm 183.90605
epoch 15, iter 13230, loss 1.63398, smoothed loss 1.90051, grad norm 3.84827, param norm 183.93057
epoch 15, iter 13235, loss 2.06446, smoothed loss 1.91053, grad norm 5.22032, param norm 183.95358
epoch 15, iter 13240, loss 2.02413, smoothed loss 1.91953, grad norm 4.20846, param norm 183.97758
epoch 15, iter 13245, loss 1.98163, smoothed loss 1.92604, grad norm 4.16904, param norm 184.00351
epoch 15, iter 13250, loss 1.85742, smoothed loss 1.92329, grad norm 4.22835, param norm 184.03136
epoch 15, iter 13255, loss 2.08148, smoothed loss 1.92483, grad norm 3.78226, param norm 184.05838
epoch 15, iter 13260, loss 1.94344, smoothed loss 1.91602, grad norm 3.92864, param norm 184.08279
epoch 15, iter 13265, loss 1.99574, smoothed loss 1.91582, grad norm 4.25561, param norm 184.10469
epoch 15, iter 13270, loss 2.16085, smoothed loss 1.92461, grad norm 4.77465, param norm 184.12735
epoch 15, iter 13275, loss 1.78356, smoothed loss 1.91797, grad norm 4.03883, param norm 184.14748
epoch 15, iter 13280, loss 1.64617, smoothed loss 1.91319, grad norm 3.52381, param norm 184.17101
epoch 15, iter 13285, loss 1.99105, smoothed loss 1.90391, grad norm 3.74499, param norm 184.19478
epoch 15, iter 13290, loss 1.64974, smoothed loss 1.89791, grad norm 3.65061, param norm 184.21709
epoch 15, iter 13295, loss 1.21856, smoothed loss 1.89787, grad norm 2.83887, param norm 184.23659
epoch 15, iter 13300, loss 2.18013, smoothed loss 1.90368, grad norm 4.55381, param norm 184.25647
epoch 15, iter 13305, loss 2.42807, smoothed loss 1.90051, grad norm 4.90848, param norm 184.27785
epoch 15, iter 13310, loss 1.75020, smoothed loss 1.89581, grad norm 3.80496, param norm 184.30049
epoch 15, iter 13315, loss 1.40886, smoothed loss 1.89849, grad norm 3.49777, param norm 184.32465
epoch 15, iter 13320, loss 1.63357, smoothed loss 1.88843, grad norm 3.82360, param norm 184.35017
epoch 15, iter 13325, loss 1.55000, smoothed loss 1.88546, grad norm 3.70621, param norm 184.37367
epoch 15, iter 13330, loss 1.50500, smoothed loss 1.86731, grad norm 2.98410, param norm 184.39806
epoch 15, iter 13335, loss 1.44646, smoothed loss 1.86182, grad norm 4.04900, param norm 184.42039
epoch 15, iter 13340, loss 2.07099, smoothed loss 1.87364, grad norm 3.60643, param norm 184.44142
epoch 15, iter 13345, loss 2.12156, smoothed loss 1.88392, grad norm 4.32925, param norm 184.46083
epoch 15, iter 13350, loss 1.97716, smoothed loss 1.88545, grad norm 4.39624, param norm 184.48146
epoch 15, iter 13355, loss 1.96403, smoothed loss 1.88480, grad norm 3.47607, param norm 184.50558
epoch 15, iter 13360, loss 2.04343, smoothed loss 1.89534, grad norm 4.02136, param norm 184.52895
epoch 15, iter 13365, loss 1.86248, smoothed loss 1.89850, grad norm 4.86431, param norm 184.55333
epoch 15, iter 13370, loss 1.46811, smoothed loss 1.89474, grad norm 3.37601, param norm 184.57799
epoch 15, iter 13375, loss 1.70591, smoothed loss 1.90361, grad norm 4.50405, param norm 184.60033
epoch 15, iter 13380, loss 1.47330, smoothed loss 1.89187, grad norm 3.32741, param norm 184.62302
epoch 15, iter 13385, loss 2.66409, smoothed loss 1.90640, grad norm 4.49484, param norm 184.64507
epoch 15, iter 13390, loss 2.19146, smoothed loss 1.91583, grad norm 3.99744, param norm 184.66768
epoch 15, iter 13395, loss 2.38236, smoothed loss 1.91947, grad norm 4.56068, param norm 184.69101
epoch 15, iter 13400, loss 2.32589, smoothed loss 1.92281, grad norm 4.95963, param norm 184.71677
epoch 15, iter 13405, loss 2.06470, smoothed loss 1.92559, grad norm 4.28514, param norm 184.74144
epoch 15, iter 13410, loss 2.48328, smoothed loss 1.92745, grad norm 4.89654, param norm 184.76619
epoch 15, iter 13415, loss 1.69078, smoothed loss 1.92728, grad norm 3.90809, param norm 184.79010
epoch 15, iter 13420, loss 1.79258, smoothed loss 1.92857, grad norm 3.63110, param norm 184.81334
epoch 15, iter 13425, loss 1.71293, smoothed loss 1.93595, grad norm 3.69983, param norm 184.83504
epoch 15, iter 13430, loss 2.30070, smoothed loss 1.93606, grad norm 4.59556, param norm 184.85616
epoch 15, iter 13435, loss 2.06273, smoothed loss 1.94664, grad norm 4.79518, param norm 184.87523
epoch 15, iter 13440, loss 1.81862, smoothed loss 1.94949, grad norm 3.79198, param norm 184.89639
epoch 15, iter 13445, loss 2.15011, smoothed loss 1.94421, grad norm 4.03452, param norm 184.92128
epoch 15, iter 13450, loss 2.16157, smoothed loss 1.94333, grad norm 4.51586, param norm 184.94484
epoch 15, iter 13455, loss 2.17737, smoothed loss 1.94734, grad norm 4.23703, param norm 184.97011
epoch 15, iter 13460, loss 1.59539, smoothed loss 1.94349, grad norm 3.82014, param norm 184.99959
epoch 15, iter 13465, loss 1.83592, smoothed loss 1.94408, grad norm 4.33164, param norm 185.02896
epoch 15, iter 13470, loss 1.87451, smoothed loss 1.94721, grad norm 4.03075, param norm 185.05769
epoch 15, iter 13475, loss 1.99957, smoothed loss 1.95139, grad norm 3.90170, param norm 185.08655
epoch 15, iter 13480, loss 1.56160, smoothed loss 1.93793, grad norm 4.24815, param norm 185.11043
epoch 15, iter 13485, loss 1.56776, smoothed loss 1.93180, grad norm 3.45134, param norm 185.13268
epoch 15, iter 13490, loss 1.68925, smoothed loss 1.93182, grad norm 4.55850, param norm 185.15543
epoch 15, iter 13495, loss 1.50195, smoothed loss 1.93096, grad norm 3.82991, param norm 185.17834
epoch 15, iter 13500, loss 1.61404, smoothed loss 1.92470, grad norm 3.55523, param norm 185.20087
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 15, Iter 13500, dev loss: 3.242184
Calculating Train F1/EM...
F1 train: 1000 examples took 18.42109 seconds [Score: 0.89272]
Exact Match train: 1000 examples took 18.25484 seconds [Score: 0.77300]
Epoch 15, Iter 13500, Train F1 score: 0.892724, Train EM score: 0.773000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 122.21500 seconds [Score: 0.64447]
Exact Match dev: 7118 examples took 122.45582 seconds [Score: 0.49466]
Epoch 15, Iter 13500, Dev F1 score: 0.644475, Dev EM score: 0.494661
End of epoch 15
epoch 15, iter 13505, loss 1.28593, smoothed loss 1.92529, grad norm 2.94714, param norm 185.22299
epoch 15, iter 13510, loss 1.46313, smoothed loss 1.91503, grad norm 3.38515, param norm 185.24623
epoch 15, iter 13515, loss 1.54193, smoothed loss 1.90397, grad norm 3.62943, param norm 185.26787
epoch 15, iter 13520, loss 2.24740, smoothed loss 1.90894, grad norm 4.38912, param norm 185.28925
epoch 15, iter 13525, loss 1.45851, smoothed loss 1.89769, grad norm 3.35246, param norm 185.31204
epoch 15, iter 13530, loss 1.47325, smoothed loss 1.89065, grad norm 3.41848, param norm 185.33310
epoch 15, iter 13535, loss 2.28657, smoothed loss 1.88864, grad norm 4.75815, param norm 185.35782
epoch 15, iter 13540, loss 2.39609, smoothed loss 1.89787, grad norm 4.94876, param norm 185.38098
epoch 15, iter 13545, loss 1.94683, smoothed loss 1.89759, grad norm 4.18727, param norm 185.40689
epoch 15, iter 13550, loss 1.59002, smoothed loss 1.89452, grad norm 3.56224, param norm 185.43471
epoch 15, iter 13555, loss 2.09900, smoothed loss 1.90810, grad norm 4.44833, param norm 185.45877
epoch 15, iter 13560, loss 1.93027, smoothed loss 1.90015, grad norm 3.85792, param norm 185.48146
epoch 15, iter 13565, loss 1.64801, smoothed loss 1.90563, grad norm 3.39279, param norm 185.50246
epoch 15, iter 13570, loss 1.60879, smoothed loss 1.89921, grad norm 3.01597, param norm 185.52629
epoch 15, iter 13575, loss 1.59102, smoothed loss 1.89407, grad norm 3.80155, param norm 185.55046
epoch 15, iter 13580, loss 2.36466, smoothed loss 1.89056, grad norm 4.59129, param norm 185.57298
epoch 15, iter 13585, loss 1.94801, smoothed loss 1.88831, grad norm 4.25650, param norm 185.59563
epoch 15, iter 13590, loss 2.25811, smoothed loss 1.88296, grad norm 4.72719, param norm 185.61734
epoch 15, iter 13595, loss 1.82444, smoothed loss 1.89182, grad norm 3.72314, param norm 185.63791
epoch 15, iter 13600, loss 1.33451, smoothed loss 1.89013, grad norm 3.34222, param norm 185.66118
epoch 15, iter 13605, loss 1.38566, smoothed loss 1.88949, grad norm 2.93576, param norm 185.68779
epoch 15, iter 13610, loss 1.68579, smoothed loss 1.88569, grad norm 3.85911, param norm 185.71346
epoch 15, iter 13615, loss 2.06807, smoothed loss 1.88662, grad norm 4.67745, param norm 185.73596
epoch 15, iter 13620, loss 2.01379, smoothed loss 1.89170, grad norm 4.40718, param norm 185.75943
epoch 15, iter 13625, loss 1.80082, smoothed loss 1.89689, grad norm 3.83787, param norm 185.78058
epoch 15, iter 13630, loss 2.12813, smoothed loss 1.89015, grad norm 4.03561, param norm 185.80225
epoch 15, iter 13635, loss 2.45984, smoothed loss 1.89944, grad norm 4.91542, param norm 185.82430
epoch 15, iter 13640, loss 1.60568, smoothed loss 1.89211, grad norm 3.97939, param norm 185.84538
epoch 15, iter 13645, loss 1.71988, smoothed loss 1.89534, grad norm 4.49079, param norm 185.86772
epoch 15, iter 13650, loss 1.61180, smoothed loss 1.89584, grad norm 4.10457, param norm 185.89114
epoch 15, iter 13655, loss 1.64467, smoothed loss 1.88821, grad norm 3.96048, param norm 185.91878
epoch 15, iter 13660, loss 1.69502, smoothed loss 1.88756, grad norm 3.57277, param norm 185.94591
epoch 15, iter 13665, loss 2.41608, smoothed loss 1.89475, grad norm 4.67168, param norm 185.97067
epoch 15, iter 13670, loss 2.26224, smoothed loss 1.90284, grad norm 4.18006, param norm 185.99452
epoch 15, iter 13675, loss 1.48371, smoothed loss 1.90017, grad norm 3.50283, param norm 186.01550
epoch 15, iter 13680, loss 1.94671, smoothed loss 1.89921, grad norm 4.52548, param norm 186.03787
epoch 15, iter 13685, loss 1.74533, smoothed loss 1.88780, grad norm 4.18901, param norm 186.06131
epoch 15, iter 13690, loss 1.81516, smoothed loss 1.88143, grad norm 3.96225, param norm 186.08195
epoch 15, iter 13695, loss 2.41831, smoothed loss 1.88375, grad norm 4.62950, param norm 186.10190
epoch 15, iter 13700, loss 1.71378, smoothed loss 1.89215, grad norm 4.22540, param norm 186.12395
epoch 15, iter 13705, loss 2.34067, smoothed loss 1.89344, grad norm 4.41525, param norm 186.14566
epoch 15, iter 13710, loss 2.20464, smoothed loss 1.90030, grad norm 4.65158, param norm 186.16684
epoch 15, iter 13715, loss 2.26800, smoothed loss 1.90794, grad norm 3.96970, param norm 186.19034
epoch 15, iter 13720, loss 1.86373, smoothed loss 1.90604, grad norm 3.79814, param norm 186.21353
epoch 15, iter 13725, loss 2.00682, smoothed loss 1.90503, grad norm 3.98689, param norm 186.23264
epoch 15, iter 13730, loss 2.17124, smoothed loss 1.91175, grad norm 4.48287, param norm 186.25241
epoch 15, iter 13735, loss 2.02073, smoothed loss 1.91736, grad norm 4.68156, param norm 186.27298
epoch 15, iter 13740, loss 2.41062, smoothed loss 1.91961, grad norm 4.92200, param norm 186.29439
epoch 15, iter 13745, loss 1.93139, smoothed loss 1.90899, grad norm 4.08480, param norm 186.31761
epoch 15, iter 13750, loss 1.98455, smoothed loss 1.90097, grad norm 4.54841, param norm 186.33948
epoch 15, iter 13755, loss 2.14210, smoothed loss 1.90119, grad norm 4.21190, param norm 186.35831
epoch 15, iter 13760, loss 2.66887, smoothed loss 1.90878, grad norm 5.31087, param norm 186.37665
epoch 15, iter 13765, loss 1.45945, smoothed loss 1.90106, grad norm 3.28865, param norm 186.39742
epoch 15, iter 13770, loss 1.87537, smoothed loss 1.90577, grad norm 4.26878, param norm 186.42015
epoch 15, iter 13775, loss 1.94661, smoothed loss 1.91253, grad norm 3.96230, param norm 186.44559
