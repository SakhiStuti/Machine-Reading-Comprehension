[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
Loading devset:
100% 48/48 [00:04<00:00, 10.19it/s]
Number of triples ignored due to token mapping problems:  3212
Number of triples ignored due to unalignment with tokenization problems:  240
Number of triples ignored due to span alignment problems:  0
Processed examples: 7118 out of 10570
Loading trainset:
100% 442/442 [00:39<00:00, 11.31it/s]
Number of triples ignored due to token mapping problems:  28669
Number of triples ignored due to unalignment with tokenization problems:  1760
Number of triples ignored due to span alignment problems:  7
Processed examples: 57163 out of 87599
100% 400000/400000 [00:12<00:00, 31783.85it/s]
Finished processing GloVe vectors
Mode Running:
SpanMode:  False
CharCNN:  True
Highway:  False
Bidaf:  True
Model Params Initialised
HyperParameters Initialised
epochs:  20
learning rate:  0.0008
keep_prob:  0.75
Learning Initialised
Model Save parameters Initialised
Char CNN parameters Initialised
Placeholders Defined
Embed Layer Defined
Char Embed Layer Defined
RNN encoder Layer Defined
Bidaf Layer Defined
Output Layer Defined
Loss Defined
Finished initialization of model
Training Network
2018-11-26 04:46:17.550705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-26 04:46:17.551176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-11-26 04:46:17.551229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-26 04:46:18.481444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-26 04:46:18.481508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-26 04:46:18.481541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-26 04:46:18.481937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 5, loss 9.63686, smoothed loss 9.67376, grad norm 2.13088, param norm 55.53208
INFO:root:epoch 1, iter 10, loss 9.49886, smoothed loss 9.66758, grad norm 1.93804, param norm 55.62448
INFO:root:epoch 1, iter 15, loss 9.54654, smoothed loss 9.66092, grad norm 1.81226, param norm 55.71336
INFO:root:epoch 1, iter 20, loss 9.44220, smoothed loss 9.65431, grad norm 1.90972, param norm 55.79464
INFO:root:epoch 1, iter 25, loss 9.50121, smoothed loss 9.64711, grad norm 1.94458, param norm 55.86986
INFO:root:epoch 1, iter 30, loss 9.17493, smoothed loss 9.63313, grad norm 2.11175, param norm 55.94187
INFO:root:epoch 1, iter 35, loss 9.64893, smoothed loss 9.62291, grad norm 2.66579, param norm 56.01429
INFO:root:epoch 1, iter 40, loss 9.30192, smoothed loss 9.60677, grad norm 2.10527, param norm 56.08542
INFO:root:epoch 1, iter 45, loss 9.44866, smoothed loss 9.59181, grad norm 2.21902, param norm 56.15901
INFO:root:epoch 1, iter 50, loss 9.25713, smoothed loss 9.56876, grad norm 2.41184, param norm 56.23701
INFO:root:epoch 1, iter 55, loss 9.10222, smoothed loss 9.54143, grad norm 2.54949, param norm 56.31840
INFO:root:epoch 1, iter 60, loss 8.86151, smoothed loss 9.51053, grad norm 3.01391, param norm 56.40543
INFO:root:epoch 1, iter 65, loss 8.64250, smoothed loss 9.46974, grad norm 3.18039, param norm 56.49693
INFO:root:epoch 1, iter 70, loss 8.75417, smoothed loss 9.43290, grad norm 3.00151, param norm 56.59562
INFO:root:epoch 1, iter 75, loss 8.84659, smoothed loss 9.40381, grad norm 3.01753, param norm 56.69059
INFO:root:epoch 1, iter 80, loss 8.68948, smoothed loss 9.37054, grad norm 2.83193, param norm 56.78391
INFO:root:epoch 1, iter 85, loss 8.54998, smoothed loss 9.33091, grad norm 2.83113, param norm 56.88192
INFO:root:epoch 1, iter 90, loss 8.22747, smoothed loss 9.28524, grad norm 2.90560, param norm 56.97913
INFO:root:epoch 1, iter 95, loss 8.18042, smoothed loss 9.24792, grad norm 3.31187, param norm 57.07611
INFO:root:epoch 1, iter 100, loss 8.46519, smoothed loss 9.21382, grad norm 3.14091, param norm 57.16936
INFO:root:epoch 1, iter 105, loss 8.03506, smoothed loss 9.16855, grad norm 3.14812, param norm 57.25848
INFO:root:epoch 1, iter 110, loss 8.19684, smoothed loss 9.12752, grad norm 2.56064, param norm 57.34628
INFO:root:epoch 1, iter 115, loss 8.02039, smoothed loss 9.07684, grad norm 2.88706, param norm 57.43243
INFO:root:epoch 1, iter 120, loss 8.00037, smoothed loss 9.03621, grad norm 2.97361, param norm 57.51963
INFO:root:epoch 1, iter 125, loss 8.46832, smoothed loss 9.00097, grad norm 2.39194, param norm 57.59480
INFO:root:epoch 1, iter 130, loss 8.27521, smoothed loss 8.96818, grad norm 2.35580, param norm 57.66876
INFO:root:epoch 1, iter 135, loss 8.15179, smoothed loss 8.93296, grad norm 2.47406, param norm 57.74548
INFO:root:epoch 1, iter 140, loss 7.93436, smoothed loss 8.89169, grad norm 2.54898, param norm 57.82219
INFO:root:epoch 1, iter 145, loss 8.35678, smoothed loss 8.85260, grad norm 2.44496, param norm 57.89172
INFO:root:epoch 1, iter 150, loss 7.90033, smoothed loss 8.81616, grad norm 2.50000, param norm 57.95662
INFO:root:epoch 1, iter 155, loss 8.12403, smoothed loss 8.79148, grad norm 2.17306, param norm 58.02767
INFO:root:epoch 1, iter 160, loss 8.49765, smoothed loss 8.75925, grad norm 2.72739, param norm 58.10039
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 165, loss 7.92448, smoothed loss 8.72646, grad norm 2.40652, param norm 58.16491
INFO:root:epoch 1, iter 170, loss 8.42326, smoothed loss 8.70559, grad norm 2.36426, param norm 58.22910
INFO:root:epoch 1, iter 175, loss 8.10546, smoothed loss 8.67658, grad norm 2.46197, param norm 58.29597
INFO:root:epoch 1, iter 180, loss 8.33786, smoothed loss 8.64153, grad norm 2.89578, param norm 58.36330
INFO:root:epoch 1, iter 185, loss 7.73734, smoothed loss 8.61017, grad norm 2.71604, param norm 58.42578
INFO:root:epoch 1, iter 190, loss 8.34819, smoothed loss 8.58789, grad norm 2.52307, param norm 58.49678
INFO:root:epoch 1, iter 195, loss 8.30511, smoothed loss 8.57040, grad norm 2.25711, param norm 58.56077
INFO:root:epoch 1, iter 200, loss 8.02351, smoothed loss 8.55101, grad norm 2.57283, param norm 58.62252
INFO:root:epoch 1, iter 205, loss 8.37966, smoothed loss 8.52702, grad norm 3.04915, param norm 58.68938
INFO:root:epoch 1, iter 210, loss 7.70452, smoothed loss 8.49428, grad norm 2.41955, param norm 58.74534
INFO:root:epoch 1, iter 215, loss 8.31577, smoothed loss 8.47978, grad norm 2.43050, param norm 58.80726
INFO:root:epoch 1, iter 220, loss 7.94786, smoothed loss 8.45945, grad norm 2.46920, param norm 58.87122
INFO:root:epoch 1, iter 225, loss 8.23276, smoothed loss 8.43550, grad norm 2.29083, param norm 58.94215
INFO:root:epoch 1, iter 230, loss 7.61552, smoothed loss 8.40913, grad norm 2.60472, param norm 59.00299
INFO:root:epoch 1, iter 235, loss 7.82697, smoothed loss 8.39214, grad norm 1.94953, param norm 59.05326
INFO:root:epoch 1, iter 240, loss 8.33906, smoothed loss 8.37312, grad norm 2.45734, param norm 59.10694
INFO:root:epoch 1, iter 245, loss 7.92164, smoothed loss 8.35479, grad norm 2.15737, param norm 59.16449
INFO:root:epoch 1, iter 250, loss 8.02862, smoothed loss 8.32993, grad norm 1.98600, param norm 59.22437
INFO:root:epoch 1, iter 255, loss 7.91742, smoothed loss 8.31118, grad norm 2.06972, param norm 59.28415
INFO:root:epoch 1, iter 260, loss 7.98030, smoothed loss 8.28893, grad norm 2.38517, param norm 59.34285
INFO:root:epoch 1, iter 265, loss 7.84839, smoothed loss 8.26663, grad norm 2.33658, param norm 59.40656
INFO:root:epoch 1, iter 270, loss 7.62733, smoothed loss 8.25328, grad norm 2.60888, param norm 59.47159
INFO:root:epoch 1, iter 275, loss 8.10636, smoothed loss 8.24022, grad norm 2.44718, param norm 59.53545
INFO:root:epoch 1, iter 280, loss 8.35141, smoothed loss 8.22940, grad norm 2.57464, param norm 59.59929
INFO:root:epoch 1, iter 285, loss 8.12801, smoothed loss 8.22128, grad norm 2.13789, param norm 59.66306
INFO:root:epoch 1, iter 290, loss 7.75137, smoothed loss 8.20037, grad norm 2.18313, param norm 59.73170
INFO:root:epoch 1, iter 295, loss 7.97944, smoothed loss 8.19234, grad norm 2.44102, param norm 59.80106
INFO:root:epoch 1, iter 300, loss 8.04664, smoothed loss 8.18453, grad norm 2.02578, param norm 59.86717
INFO:root:epoch 1, iter 305, loss 8.13730, smoothed loss 8.18087, grad norm 2.13090, param norm 59.92401
INFO:root:epoch 1, iter 310, loss 8.10182, smoothed loss 8.16770, grad norm 2.90568, param norm 59.97805
INFO:root:epoch 1, iter 315, loss 7.68024, smoothed loss 8.15452, grad norm 2.06230, param norm 60.03210
INFO:root:epoch 1, iter 320, loss 8.08859, smoothed loss 8.14277, grad norm 2.47372, param norm 60.09018
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 325, loss 8.11007, smoothed loss 8.12325, grad norm 2.50838, param norm 60.14577
INFO:root:epoch 1, iter 330, loss 7.66893, smoothed loss 8.10627, grad norm 2.39170, param norm 60.20607
INFO:root:epoch 1, iter 335, loss 7.35758, smoothed loss 8.08519, grad norm 2.55769, param norm 60.26479
INFO:root:epoch 1, iter 340, loss 7.94086, smoothed loss 8.08455, grad norm 2.60478, param norm 60.32107
INFO:root:epoch 1, iter 345, loss 8.52383, smoothed loss 8.09305, grad norm 3.02792, param norm 60.37396
INFO:root:epoch 1, iter 350, loss 7.37839, smoothed loss 8.07567, grad norm 2.36198, param norm 60.43316
INFO:root:epoch 1, iter 355, loss 8.06385, smoothed loss 8.07251, grad norm 2.66942, param norm 60.49796
INFO:root:epoch 1, iter 360, loss 8.09350, smoothed loss 8.06725, grad norm 2.02054, param norm 60.56205
INFO:root:epoch 1, iter 365, loss 7.93073, smoothed loss 8.05525, grad norm 2.54332, param norm 60.62147
INFO:root:epoch 1, iter 370, loss 8.02127, smoothed loss 8.03898, grad norm 2.61997, param norm 60.67310
INFO:root:epoch 1, iter 375, loss 7.83170, smoothed loss 8.02677, grad norm 2.26232, param norm 60.72324
INFO:root:epoch 1, iter 380, loss 7.58508, smoothed loss 8.00916, grad norm 2.52389, param norm 60.77381
INFO:root:epoch 1, iter 385, loss 7.88332, smoothed loss 8.01249, grad norm 2.20650, param norm 60.82392
INFO:root:epoch 1, iter 390, loss 7.84061, smoothed loss 8.00684, grad norm 2.85514, param norm 60.88128
INFO:root:epoch 1, iter 395, loss 8.52026, smoothed loss 8.00396, grad norm 3.22816, param norm 60.94651
INFO:root:epoch 1, iter 400, loss 8.36625, smoothed loss 8.00247, grad norm 2.47279, param norm 61.00603
INFO:root:epoch 1, iter 405, loss 8.40476, smoothed loss 8.00025, grad norm 2.33445, param norm 61.06630
INFO:root:epoch 1, iter 410, loss 8.13001, smoothed loss 8.00061, grad norm 2.01972, param norm 61.12591
INFO:root:epoch 1, iter 415, loss 7.35444, smoothed loss 7.98751, grad norm 2.12562, param norm 61.18292
INFO:root:epoch 1, iter 420, loss 7.59688, smoothed loss 7.97141, grad norm 2.75087, param norm 61.24558
INFO:root:epoch 1, iter 425, loss 7.84116, smoothed loss 7.96829, grad norm 2.17588, param norm 61.30373
INFO:root:epoch 1, iter 430, loss 7.81132, smoothed loss 7.95656, grad norm 2.51454, param norm 61.35610
INFO:root:epoch 1, iter 435, loss 7.75745, smoothed loss 7.95161, grad norm 1.96312, param norm 61.41222
INFO:root:epoch 1, iter 440, loss 8.27043, smoothed loss 7.94246, grad norm 2.79103, param norm 61.46513
INFO:root:epoch 1, iter 445, loss 7.67344, smoothed loss 7.93221, grad norm 2.46864, param norm 61.51876
INFO:root:epoch 1, iter 450, loss 8.04491, smoothed loss 7.92629, grad norm 2.47597, param norm 61.57492
INFO:root:epoch 1, iter 455, loss 7.96235, smoothed loss 7.92311, grad norm 2.30065, param norm 61.63238
INFO:root:epoch 1, iter 460, loss 7.83840, smoothed loss 7.91915, grad norm 2.34366, param norm 61.69142
INFO:root:epoch 1, iter 465, loss 7.86800, smoothed loss 7.90774, grad norm 2.17460, param norm 61.74738
INFO:root:epoch 1, iter 470, loss 7.77347, smoothed loss 7.90032, grad norm 2.30288, param norm 61.79827
INFO:root:epoch 1, iter 475, loss 7.91746, smoothed loss 7.88573, grad norm 2.28402, param norm 61.85014
INFO:root:epoch 1, iter 480, loss 7.72965, smoothed loss 7.88407, grad norm 2.49600, param norm 61.89594
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 485, loss 7.83625, smoothed loss 7.87490, grad norm 2.42079, param norm 61.93902
INFO:root:epoch 1, iter 490, loss 7.99473, smoothed loss 7.87082, grad norm 2.56963, param norm 61.99152
INFO:root:epoch 1, iter 495, loss 7.57185, smoothed loss 7.86104, grad norm 2.25923, param norm 62.05043
INFO:root:epoch 1, iter 500, loss 7.85631, smoothed loss 7.86122, grad norm 2.61345, param norm 62.10765
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 1, Iter 500, dev loss: 7.732302
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.08049
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.03200
INFO:root:Epoch 1, Iter 500, Train F1 score: 0.080487, Train EM score: 0.032000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.07646
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.03077
INFO:root:Epoch 1, Iter 500, Dev F1 score: 0.076461, Dev EM score: 0.030767
INFO:root:End of epoch 1
INFO:root:epoch 1, iter 505, loss 7.63830, smoothed loss 7.85780, grad norm 2.72371, param norm 62.15961
INFO:root:epoch 1, iter 510, loss 7.88065, smoothed loss 7.85971, grad norm 1.91487, param norm 62.21675
INFO:root:epoch 1, iter 515, loss 7.65375, smoothed loss 7.85018, grad norm 2.24300, param norm 62.27810
INFO:root:epoch 1, iter 520, loss 7.85336, smoothed loss 7.85238, grad norm 2.75902, param norm 62.33823
INFO:root:epoch 1, iter 525, loss 7.93400, smoothed loss 7.84082, grad norm 3.32120, param norm 62.39307
INFO:root:epoch 1, iter 530, loss 7.53731, smoothed loss 7.83557, grad norm 2.37061, param norm 62.45215
INFO:root:epoch 1, iter 535, loss 7.83063, smoothed loss 7.84044, grad norm 2.37600, param norm 62.50943
INFO:root:epoch 1, iter 540, loss 7.52798, smoothed loss 7.83182, grad norm 2.67833, param norm 62.56372
INFO:root:epoch 1, iter 545, loss 7.46833, smoothed loss 7.82613, grad norm 2.61153, param norm 62.61630
INFO:root:epoch 1, iter 550, loss 7.67633, smoothed loss 7.81876, grad norm 2.01407, param norm 62.67726
INFO:root:epoch 1, iter 555, loss 8.21800, smoothed loss 7.82403, grad norm 2.63458, param norm 62.73748
INFO:root:epoch 1, iter 560, loss 7.84493, smoothed loss 7.82660, grad norm 2.69769, param norm 62.79374
INFO:root:epoch 1, iter 565, loss 7.49545, smoothed loss 7.82109, grad norm 2.20883, param norm 62.84230
INFO:root:epoch 1, iter 570, loss 8.09346, smoothed loss 7.81954, grad norm 2.44050, param norm 62.89091
INFO:root:epoch 1, iter 575, loss 7.71854, smoothed loss 7.81710, grad norm 2.69333, param norm 62.94905
INFO:root:epoch 1, iter 580, loss 7.84859, smoothed loss 7.81945, grad norm 2.23128, param norm 63.01016
INFO:root:epoch 1, iter 585, loss 7.87700, smoothed loss 7.80943, grad norm 2.26133, param norm 63.06504
INFO:root:epoch 1, iter 590, loss 7.72958, smoothed loss 7.79596, grad norm 2.48813, param norm 63.12051
INFO:root:epoch 1, iter 595, loss 7.86669, smoothed loss 7.79547, grad norm 2.25786, param norm 63.17103
INFO:root:epoch 1, iter 600, loss 7.42829, smoothed loss 7.78219, grad norm 2.44611, param norm 63.22236
INFO:root:epoch 1, iter 605, loss 7.90395, smoothed loss 7.78242, grad norm 3.68324, param norm 63.27495
INFO:root:epoch 1, iter 610, loss 7.77192, smoothed loss 7.77418, grad norm 2.44547, param norm 63.32137
INFO:root:epoch 1, iter 615, loss 7.83847, smoothed loss 7.76705, grad norm 2.53945, param norm 63.36821
INFO:root:epoch 1, iter 620, loss 7.74023, smoothed loss 7.76956, grad norm 2.27958, param norm 63.41801
INFO:root:epoch 1, iter 625, loss 7.53028, smoothed loss 7.77549, grad norm 2.24118, param norm 63.46899
INFO:root:epoch 1, iter 630, loss 7.75741, smoothed loss 7.77566, grad norm 2.77230, param norm 63.51746
INFO:root:epoch 1, iter 635, loss 7.45878, smoothed loss 7.76815, grad norm 2.24750, param norm 63.56235
INFO:root:epoch 1, iter 640, loss 7.33425, smoothed loss 7.75954, grad norm 2.16030, param norm 63.61286
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 645, loss 7.94254, smoothed loss 7.76747, grad norm 2.57368, param norm 63.67002
INFO:root:epoch 1, iter 650, loss 7.46167, smoothed loss 7.76671, grad norm 2.45711, param norm 63.72206
INFO:root:epoch 1, iter 655, loss 8.08547, smoothed loss 7.76368, grad norm 3.47794, param norm 63.77243
INFO:root:epoch 1, iter 660, loss 7.27716, smoothed loss 7.76148, grad norm 2.23416, param norm 63.82317
INFO:root:epoch 1, iter 665, loss 7.69770, smoothed loss 7.76521, grad norm 2.13018, param norm 63.87964
INFO:root:epoch 1, iter 670, loss 7.42001, smoothed loss 7.75639, grad norm 2.22254, param norm 63.93816
INFO:root:epoch 1, iter 675, loss 7.49219, smoothed loss 7.75363, grad norm 2.59994, param norm 63.98744
INFO:root:epoch 1, iter 680, loss 7.94684, smoothed loss 7.75421, grad norm 2.04395, param norm 64.03596
INFO:root:epoch 1, iter 685, loss 7.92557, smoothed loss 7.75594, grad norm 2.49862, param norm 64.08738
INFO:root:epoch 1, iter 690, loss 7.65177, smoothed loss 7.74148, grad norm 2.34830, param norm 64.14477
INFO:root:epoch 1, iter 695, loss 7.53951, smoothed loss 7.73464, grad norm 2.52898, param norm 64.19898
INFO:root:epoch 1, iter 700, loss 7.85285, smoothed loss 7.73029, grad norm 2.08839, param norm 64.24746
INFO:root:epoch 1, iter 705, loss 7.67298, smoothed loss 7.72690, grad norm 2.16856, param norm 64.29707
INFO:root:epoch 1, iter 710, loss 8.08357, smoothed loss 7.72670, grad norm 2.82527, param norm 64.35168
INFO:root:epoch 1, iter 715, loss 7.37276, smoothed loss 7.72324, grad norm 2.66290, param norm 64.41206
INFO:root:epoch 1, iter 720, loss 7.44047, smoothed loss 7.72155, grad norm 2.36445, param norm 64.47163
INFO:root:epoch 1, iter 725, loss 8.02118, smoothed loss 7.71124, grad norm 2.26308, param norm 64.53893
INFO:root:epoch 1, iter 730, loss 7.42966, smoothed loss 7.70345, grad norm 2.33641, param norm 64.60196
INFO:root:epoch 1, iter 735, loss 7.66437, smoothed loss 7.70174, grad norm 2.13028, param norm 64.66584
INFO:root:epoch 1, iter 740, loss 8.25666, smoothed loss 7.70144, grad norm 3.33718, param norm 64.73156
INFO:root:epoch 1, iter 745, loss 7.32515, smoothed loss 7.68998, grad norm 2.84075, param norm 64.78845
INFO:root:epoch 1, iter 750, loss 8.04923, smoothed loss 7.68614, grad norm 4.08386, param norm 64.84989
INFO:root:epoch 1, iter 755, loss 8.06889, smoothed loss 7.69618, grad norm 2.74150, param norm 64.91232
INFO:root:epoch 1, iter 760, loss 7.80744, smoothed loss 7.70129, grad norm 2.67307, param norm 64.97319
INFO:root:epoch 1, iter 765, loss 7.55056, smoothed loss 7.69454, grad norm 2.35977, param norm 65.02958
INFO:root:epoch 1, iter 770, loss 7.45440, smoothed loss 7.69642, grad norm 2.77384, param norm 65.08495
INFO:root:epoch 1, iter 775, loss 7.00365, smoothed loss 7.68643, grad norm 2.51712, param norm 65.13517
INFO:root:epoch 1, iter 780, loss 7.36346, smoothed loss 7.68091, grad norm 2.49521, param norm 65.18967
INFO:root:epoch 1, iter 785, loss 7.57841, smoothed loss 7.68121, grad norm 2.69447, param norm 65.24049
INFO:root:epoch 1, iter 790, loss 7.81239, smoothed loss 7.67277, grad norm 2.91260, param norm 65.29537
INFO:root:epoch 1, iter 795, loss 7.88948, smoothed loss 7.67300, grad norm 2.55827, param norm 65.34972
INFO:root:epoch 1, iter 800, loss 7.17297, smoothed loss 7.67185, grad norm 2.43183, param norm 65.40251
Adding batches start...
Added  144  batches
INFO:root:epoch 1, iter 805, loss 7.54564, smoothed loss 7.66962, grad norm 2.62288, param norm 65.45438
INFO:root:epoch 1, iter 810, loss 7.54964, smoothed loss 7.66035, grad norm 2.59187, param norm 65.50770
INFO:root:epoch 1, iter 815, loss 8.06542, smoothed loss 7.66341, grad norm 3.00610, param norm 65.56261
INFO:root:epoch 1, iter 820, loss 7.48765, smoothed loss 7.66021, grad norm 2.29096, param norm 65.61477
INFO:root:epoch 1, iter 825, loss 7.61808, smoothed loss 7.66037, grad norm 2.26141, param norm 65.66422
INFO:root:epoch 1, iter 830, loss 7.63510, smoothed loss 7.65921, grad norm 2.04172, param norm 65.71897
INFO:root:epoch 1, iter 835, loss 7.70543, smoothed loss 7.66092, grad norm 2.33690, param norm 65.77270
INFO:root:epoch 1, iter 840, loss 7.97288, smoothed loss 7.66006, grad norm 2.86112, param norm 65.82514
INFO:root:epoch 1, iter 845, loss 7.21404, smoothed loss 7.65696, grad norm 2.48382, param norm 65.87547
INFO:root:epoch 1, iter 850, loss 7.24270, smoothed loss 7.64094, grad norm 2.67076, param norm 65.92671
INFO:root:epoch 1, iter 855, loss 7.89931, smoothed loss 7.64032, grad norm 2.73862, param norm 65.98191
INFO:root:epoch 1, iter 860, loss 7.70861, smoothed loss 7.62599, grad norm 2.68730, param norm 66.03175
INFO:root:epoch 1, iter 865, loss 7.56418, smoothed loss 7.61919, grad norm 2.57365, param norm 66.08740
INFO:root:epoch 1, iter 870, loss 8.12944, smoothed loss 7.63110, grad norm 2.33550, param norm 66.13853
INFO:root:epoch 1, iter 875, loss 7.96702, smoothed loss 7.64190, grad norm 2.33876, param norm 66.18600
INFO:root:epoch 1, iter 880, loss 7.57395, smoothed loss 7.63121, grad norm 2.45162, param norm 66.23478
INFO:root:epoch 1, iter 885, loss 7.55683, smoothed loss 7.63519, grad norm 2.40081, param norm 66.29146
INFO:root:epoch 1, iter 890, loss 7.33743, smoothed loss 7.62822, grad norm 2.53781, param norm 66.34585
INFO:root:epoch 1, iter 895, loss 7.31055, smoothed loss 7.61857, grad norm 2.68233, param norm 66.40350
INFO:root:epoch 1, iter 900, loss 7.68569, smoothed loss 7.61661, grad norm 2.80825, param norm 66.45808
INFO:root:epoch 1, iter 905, loss 7.88157, smoothed loss 7.61770, grad norm 2.66484, param norm 66.50660
INFO:root:epoch 1, iter 910, loss 8.10101, smoothed loss 7.62556, grad norm 2.98389, param norm 66.55444
INFO:root:epoch 1, iter 915, loss 7.34170, smoothed loss 7.62067, grad norm 2.38549, param norm 66.60644
INFO:root:epoch 1, iter 920, loss 7.95973, smoothed loss 7.62785, grad norm 2.70594, param norm 66.65618
INFO:root:epoch 1, iter 925, loss 7.72710, smoothed loss 7.62518, grad norm 2.38226, param norm 66.70552
INFO:root:epoch 1, iter 930, loss 7.81519, smoothed loss 7.62337, grad norm 2.33438, param norm 66.75034
INFO:root:epoch 1, iter 935, loss 7.96980, smoothed loss 7.62796, grad norm 3.43784, param norm 66.79951
INFO:root:epoch 1, iter 940, loss 7.51037, smoothed loss 7.62199, grad norm 2.84694, param norm 66.85095
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 945, loss 7.37843, smoothed loss 7.61115, grad norm 2.74205, param norm 66.90353
INFO:root:epoch 2, iter 950, loss 7.52708, smoothed loss 7.60051, grad norm 3.19062, param norm 66.95780
INFO:root:epoch 2, iter 955, loss 7.66219, smoothed loss 7.60042, grad norm 2.83089, param norm 67.00923
INFO:root:epoch 2, iter 960, loss 7.81393, smoothed loss 7.60699, grad norm 2.49676, param norm 67.06163
INFO:root:epoch 2, iter 965, loss 7.83702, smoothed loss 7.60144, grad norm 2.51442, param norm 67.11221
INFO:root:epoch 2, iter 970, loss 7.58523, smoothed loss 7.60076, grad norm 2.47017, param norm 67.16663
INFO:root:epoch 2, iter 975, loss 7.61953, smoothed loss 7.59416, grad norm 2.23882, param norm 67.22321
INFO:root:epoch 2, iter 980, loss 7.73272, smoothed loss 7.58843, grad norm 3.03904, param norm 67.27290
INFO:root:epoch 2, iter 985, loss 7.61722, smoothed loss 7.58315, grad norm 2.56372, param norm 67.31770
INFO:root:epoch 2, iter 990, loss 7.27814, smoothed loss 7.57839, grad norm 2.40639, param norm 67.36374
INFO:root:epoch 2, iter 995, loss 7.59208, smoothed loss 7.57392, grad norm 2.45030, param norm 67.41718
INFO:root:epoch 2, iter 1000, loss 7.44949, smoothed loss 7.56704, grad norm 2.29558, param norm 67.47497
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 2, Iter 1000, dev loss: 7.526993
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.10385
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.04400
INFO:root:Epoch 2, Iter 1000, Train F1 score: 0.103849, Train EM score: 0.044000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.08946
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.05001
INFO:root:Epoch 2, Iter 1000, Dev F1 score: 0.089457, Dev EM score: 0.050014
INFO:root:End of epoch 2
INFO:root:epoch 2, iter 1005, loss 7.00610, smoothed loss 7.55440, grad norm 2.39020, param norm 67.53989
INFO:root:epoch 2, iter 1010, loss 7.22727, smoothed loss 7.54859, grad norm 2.56559, param norm 67.60201
INFO:root:epoch 2, iter 1015, loss 7.66703, smoothed loss 7.55138, grad norm 2.63580, param norm 67.65743
INFO:root:epoch 2, iter 1020, loss 7.20633, smoothed loss 7.54600, grad norm 2.89171, param norm 67.71426
INFO:root:epoch 2, iter 1025, loss 7.82796, smoothed loss 7.54212, grad norm 2.64826, param norm 67.77032
INFO:root:epoch 2, iter 1030, loss 7.50128, smoothed loss 7.53574, grad norm 2.66683, param norm 67.82386
INFO:root:epoch 2, iter 1035, loss 7.37264, smoothed loss 7.53928, grad norm 2.38463, param norm 67.87744
INFO:root:epoch 2, iter 1040, loss 7.43056, smoothed loss 7.53575, grad norm 2.35489, param norm 67.92693
INFO:root:epoch 2, iter 1045, loss 7.28035, smoothed loss 7.53371, grad norm 2.41996, param norm 67.97725
INFO:root:epoch 2, iter 1050, loss 7.10388, smoothed loss 7.52708, grad norm 2.04754, param norm 68.02995
INFO:root:epoch 2, iter 1055, loss 7.24816, smoothed loss 7.52172, grad norm 2.35853, param norm 68.08318
INFO:root:epoch 2, iter 1060, loss 7.24031, smoothed loss 7.51107, grad norm 2.54646, param norm 68.13723
INFO:root:epoch 2, iter 1065, loss 7.43298, smoothed loss 7.50726, grad norm 2.66676, param norm 68.19167
INFO:root:epoch 2, iter 1070, loss 7.11450, smoothed loss 7.50106, grad norm 2.31452, param norm 68.24074
INFO:root:epoch 2, iter 1075, loss 7.23900, smoothed loss 7.49737, grad norm 2.56990, param norm 68.28858
INFO:root:epoch 2, iter 1080, loss 7.29052, smoothed loss 7.49166, grad norm 2.29132, param norm 68.33322
INFO:root:epoch 2, iter 1085, loss 7.33711, smoothed loss 7.48604, grad norm 2.75576, param norm 68.38127
INFO:root:epoch 2, iter 1090, loss 7.48776, smoothed loss 7.49857, grad norm 2.66572, param norm 68.42702
INFO:root:epoch 2, iter 1095, loss 7.52438, smoothed loss 7.50543, grad norm 2.44743, param norm 68.46786
INFO:root:epoch 2, iter 1100, loss 7.49701, smoothed loss 7.51995, grad norm 2.48793, param norm 68.51408
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1105, loss 8.30813, smoothed loss 7.52699, grad norm 2.92054, param norm 68.56375
INFO:root:epoch 2, iter 1110, loss 7.81818, smoothed loss 7.53938, grad norm 2.33764, param norm 68.60903
INFO:root:epoch 2, iter 1115, loss 7.44678, smoothed loss 7.53237, grad norm 2.18442, param norm 68.65892
INFO:root:epoch 2, iter 1120, loss 7.34618, smoothed loss 7.53570, grad norm 2.56196, param norm 68.71394
INFO:root:epoch 2, iter 1125, loss 7.19142, smoothed loss 7.53206, grad norm 2.40784, param norm 68.76842
INFO:root:epoch 2, iter 1130, loss 7.55915, smoothed loss 7.52911, grad norm 2.23881, param norm 68.82151
INFO:root:epoch 2, iter 1135, loss 7.28857, smoothed loss 7.52780, grad norm 2.32696, param norm 68.87234
INFO:root:epoch 2, iter 1140, loss 7.35118, smoothed loss 7.52212, grad norm 2.53418, param norm 68.92951
INFO:root:epoch 2, iter 1145, loss 7.47742, smoothed loss 7.51635, grad norm 2.81222, param norm 68.98792
INFO:root:epoch 2, iter 1150, loss 7.25764, smoothed loss 7.51091, grad norm 2.98495, param norm 69.03932
INFO:root:epoch 2, iter 1155, loss 6.96766, smoothed loss 7.49683, grad norm 2.65280, param norm 69.09083
INFO:root:epoch 2, iter 1160, loss 7.64631, smoothed loss 7.48988, grad norm 3.16015, param norm 69.14403
INFO:root:epoch 2, iter 1165, loss 7.38105, smoothed loss 7.48815, grad norm 2.07369, param norm 69.19823
INFO:root:epoch 2, iter 1170, loss 7.12484, smoothed loss 7.48279, grad norm 2.54463, param norm 69.25075
INFO:root:epoch 2, iter 1175, loss 7.87457, smoothed loss 7.48412, grad norm 2.53592, param norm 69.30233
INFO:root:epoch 2, iter 1180, loss 7.72610, smoothed loss 7.48539, grad norm 2.12439, param norm 69.35130
INFO:root:epoch 2, iter 1185, loss 7.49642, smoothed loss 7.48559, grad norm 2.41154, param norm 69.40518
INFO:root:epoch 2, iter 1190, loss 7.61689, smoothed loss 7.49281, grad norm 3.10711, param norm 69.46045
INFO:root:epoch 2, iter 1195, loss 7.70532, smoothed loss 7.49051, grad norm 2.43773, param norm 69.50912
INFO:root:epoch 2, iter 1200, loss 7.43645, smoothed loss 7.49192, grad norm 2.84950, param norm 69.56129
INFO:root:epoch 2, iter 1205, loss 7.55883, smoothed loss 7.49436, grad norm 2.36613, param norm 69.61883
INFO:root:epoch 2, iter 1210, loss 7.76655, smoothed loss 7.49835, grad norm 2.12481, param norm 69.67464
INFO:root:epoch 2, iter 1215, loss 7.47700, smoothed loss 7.49244, grad norm 2.10483, param norm 69.73071
INFO:root:epoch 2, iter 1220, loss 7.45904, smoothed loss 7.48438, grad norm 2.88719, param norm 69.78691
INFO:root:epoch 2, iter 1225, loss 7.61428, smoothed loss 7.48515, grad norm 2.45636, param norm 69.84139
INFO:root:epoch 2, iter 1230, loss 7.55904, smoothed loss 7.49453, grad norm 2.34494, param norm 69.88945
INFO:root:epoch 2, iter 1235, loss 7.59914, smoothed loss 7.49526, grad norm 2.48443, param norm 69.92977
INFO:root:epoch 2, iter 1240, loss 7.42290, smoothed loss 7.49122, grad norm 3.07514, param norm 69.97682
INFO:root:epoch 2, iter 1245, loss 7.06545, smoothed loss 7.47879, grad norm 2.58011, param norm 70.02872
INFO:root:epoch 2, iter 1250, loss 8.02921, smoothed loss 7.48915, grad norm 2.47702, param norm 70.08035
INFO:root:epoch 2, iter 1255, loss 7.69594, smoothed loss 7.48310, grad norm 3.39514, param norm 70.13448
INFO:root:epoch 2, iter 1260, loss 7.83807, smoothed loss 7.48548, grad norm 2.43994, param norm 70.18026
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1265, loss 7.45144, smoothed loss 7.49085, grad norm 2.21401, param norm 70.22352
INFO:root:epoch 2, iter 1270, loss 7.61402, smoothed loss 7.48785, grad norm 2.23632, param norm 70.27375
INFO:root:epoch 2, iter 1275, loss 7.38722, smoothed loss 7.47945, grad norm 2.50874, param norm 70.31940
INFO:root:epoch 2, iter 1280, loss 7.74059, smoothed loss 7.48464, grad norm 2.81760, param norm 70.36833
INFO:root:epoch 2, iter 1285, loss 6.97708, smoothed loss 7.47624, grad norm 2.44326, param norm 70.41275
INFO:root:epoch 2, iter 1290, loss 7.39980, smoothed loss 7.47871, grad norm 2.49967, param norm 70.46240
INFO:root:epoch 2, iter 1295, loss 7.43621, smoothed loss 7.47686, grad norm 2.55589, param norm 70.51383
INFO:root:epoch 2, iter 1300, loss 7.23262, smoothed loss 7.47215, grad norm 2.62232, param norm 70.57095
INFO:root:epoch 2, iter 1305, loss 7.40217, smoothed loss 7.47949, grad norm 2.36068, param norm 70.62563
INFO:root:epoch 2, iter 1310, loss 7.09646, smoothed loss 7.47395, grad norm 2.59245, param norm 70.66947
INFO:root:epoch 2, iter 1315, loss 6.95501, smoothed loss 7.46514, grad norm 2.05068, param norm 70.71789
INFO:root:epoch 2, iter 1320, loss 7.41532, smoothed loss 7.47256, grad norm 2.32965, param norm 70.76889
INFO:root:epoch 2, iter 1325, loss 7.23577, smoothed loss 7.47098, grad norm 2.47191, param norm 70.81905
INFO:root:epoch 2, iter 1330, loss 7.20149, smoothed loss 7.47319, grad norm 2.83297, param norm 70.86825
INFO:root:epoch 2, iter 1335, loss 7.08994, smoothed loss 7.46625, grad norm 2.42489, param norm 70.92147
INFO:root:epoch 2, iter 1340, loss 7.91857, smoothed loss 7.47080, grad norm 2.96637, param norm 70.98019
INFO:root:epoch 2, iter 1345, loss 6.94410, smoothed loss 7.46724, grad norm 2.68665, param norm 71.04381
INFO:root:epoch 2, iter 1350, loss 7.51164, smoothed loss 7.46107, grad norm 2.76213, param norm 71.09734
INFO:root:epoch 2, iter 1355, loss 7.62654, smoothed loss 7.46015, grad norm 2.83527, param norm 71.14774
INFO:root:epoch 2, iter 1360, loss 7.65857, smoothed loss 7.45739, grad norm 2.62008, param norm 71.19841
INFO:root:epoch 2, iter 1365, loss 7.70620, smoothed loss 7.45926, grad norm 2.57917, param norm 71.25687
INFO:root:epoch 2, iter 1370, loss 7.40650, smoothed loss 7.46541, grad norm 2.40779, param norm 71.31484
INFO:root:epoch 2, iter 1375, loss 7.34825, smoothed loss 7.45723, grad norm 2.35803, param norm 71.36469
INFO:root:epoch 2, iter 1380, loss 8.02865, smoothed loss 7.45946, grad norm 2.56475, param norm 71.40887
INFO:root:epoch 2, iter 1385, loss 7.84966, smoothed loss 7.45785, grad norm 2.38438, param norm 71.45409
INFO:root:epoch 2, iter 1390, loss 7.01976, smoothed loss 7.45179, grad norm 2.38332, param norm 71.49843
INFO:root:epoch 2, iter 1395, loss 7.56452, smoothed loss 7.44849, grad norm 2.41495, param norm 71.55327
INFO:root:epoch 2, iter 1400, loss 7.81019, smoothed loss 7.45219, grad norm 2.61816, param norm 71.61690
INFO:root:epoch 2, iter 1405, loss 7.40476, smoothed loss 7.45022, grad norm 2.68587, param norm 71.67889
INFO:root:epoch 2, iter 1410, loss 7.49697, smoothed loss 7.45132, grad norm 2.80234, param norm 71.73534
INFO:root:epoch 2, iter 1415, loss 7.11925, smoothed loss 7.43869, grad norm 2.31853, param norm 71.79643
INFO:root:epoch 2, iter 1420, loss 6.69696, smoothed loss 7.43305, grad norm 2.66098, param norm 71.85087
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1425, loss 7.59699, smoothed loss 7.43074, grad norm 2.12234, param norm 71.90340
INFO:root:epoch 2, iter 1430, loss 7.24854, smoothed loss 7.43152, grad norm 2.33841, param norm 71.95062
INFO:root:epoch 2, iter 1435, loss 7.05872, smoothed loss 7.42384, grad norm 3.08053, param norm 71.99551
INFO:root:epoch 2, iter 1440, loss 7.80334, smoothed loss 7.42251, grad norm 2.76795, param norm 72.04050
INFO:root:epoch 2, iter 1445, loss 7.32590, smoothed loss 7.41639, grad norm 2.98332, param norm 72.08591
INFO:root:epoch 2, iter 1450, loss 7.72884, smoothed loss 7.42797, grad norm 2.65110, param norm 72.13464
INFO:root:epoch 2, iter 1455, loss 7.88494, smoothed loss 7.42651, grad norm 2.72541, param norm 72.19031
INFO:root:epoch 2, iter 1460, loss 7.49608, smoothed loss 7.42230, grad norm 2.55200, param norm 72.24387
INFO:root:epoch 2, iter 1465, loss 7.36987, smoothed loss 7.42393, grad norm 2.63426, param norm 72.29938
INFO:root:epoch 2, iter 1470, loss 7.26486, smoothed loss 7.42764, grad norm 2.55583, param norm 72.36041
INFO:root:epoch 2, iter 1475, loss 7.16710, smoothed loss 7.42278, grad norm 2.44394, param norm 72.42348
INFO:root:epoch 2, iter 1480, loss 7.46328, smoothed loss 7.42227, grad norm 2.88738, param norm 72.48819
INFO:root:epoch 2, iter 1485, loss 7.40688, smoothed loss 7.41804, grad norm 2.46703, param norm 72.55255
INFO:root:epoch 2, iter 1490, loss 6.88224, smoothed loss 7.41421, grad norm 2.34024, param norm 72.61206
INFO:root:epoch 2, iter 1495, loss 7.64896, smoothed loss 7.41237, grad norm 2.34632, param norm 72.66425
INFO:root:epoch 2, iter 1500, loss 7.54350, smoothed loss 7.42459, grad norm 2.43910, param norm 72.71027
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 2, Iter 1500, dev loss: 7.365399
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.10132
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.05000
INFO:root:Epoch 2, Iter 1500, Train F1 score: 0.101324, Train EM score: 0.050000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.09491
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.05676
INFO:root:Epoch 2, Iter 1500, Dev F1 score: 0.094908, Dev EM score: 0.056758
INFO:root:End of epoch 2
INFO:root:epoch 2, iter 1505, loss 8.07679, smoothed loss 7.42537, grad norm 3.25091, param norm 72.75488
INFO:root:epoch 2, iter 1510, loss 7.59855, smoothed loss 7.43128, grad norm 2.52167, param norm 72.81001
INFO:root:epoch 2, iter 1515, loss 6.89249, smoothed loss 7.42409, grad norm 2.32608, param norm 72.87068
INFO:root:epoch 2, iter 1520, loss 7.62283, smoothed loss 7.42200, grad norm 2.87630, param norm 72.93545
INFO:root:epoch 2, iter 1525, loss 7.46758, smoothed loss 7.42410, grad norm 2.37764, param norm 72.99639
INFO:root:epoch 2, iter 1530, loss 7.29793, smoothed loss 7.42735, grad norm 2.15024, param norm 73.05463
INFO:root:epoch 2, iter 1535, loss 8.26838, smoothed loss 7.43169, grad norm 2.96253, param norm 73.11178
INFO:root:epoch 2, iter 1540, loss 7.92892, smoothed loss 7.43337, grad norm 2.75175, param norm 73.16377
INFO:root:epoch 2, iter 1545, loss 6.97067, smoothed loss 7.41950, grad norm 2.45721, param norm 73.20959
INFO:root:epoch 2, iter 1550, loss 7.45822, smoothed loss 7.42155, grad norm 2.45750, param norm 73.25448
INFO:root:epoch 2, iter 1555, loss 7.74084, smoothed loss 7.41392, grad norm 2.56690, param norm 73.30804
INFO:root:epoch 2, iter 1560, loss 7.54488, smoothed loss 7.41201, grad norm 2.57263, param norm 73.36852
INFO:root:epoch 2, iter 1565, loss 7.70415, smoothed loss 7.41320, grad norm 2.62671, param norm 73.42306
INFO:root:epoch 2, iter 1570, loss 7.40306, smoothed loss 7.40698, grad norm 2.26082, param norm 73.47913
INFO:root:epoch 2, iter 1575, loss 7.28945, smoothed loss 7.39862, grad norm 2.81497, param norm 73.53911
INFO:root:epoch 2, iter 1580, loss 7.19165, smoothed loss 7.40016, grad norm 2.43194, param norm 73.59954
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1585, loss 7.78835, smoothed loss 7.41168, grad norm 2.53692, param norm 73.65625
INFO:root:epoch 2, iter 1590, loss 6.98623, smoothed loss 7.41031, grad norm 2.27675, param norm 73.71432
INFO:root:epoch 2, iter 1595, loss 7.25606, smoothed loss 7.40528, grad norm 2.70580, param norm 73.76864
INFO:root:epoch 2, iter 1600, loss 7.51176, smoothed loss 7.40515, grad norm 2.62232, param norm 73.81753
INFO:root:epoch 2, iter 1605, loss 7.41853, smoothed loss 7.39804, grad norm 2.52284, param norm 73.87321
INFO:root:epoch 2, iter 1610, loss 7.60305, smoothed loss 7.40561, grad norm 2.73322, param norm 73.93118
INFO:root:epoch 2, iter 1615, loss 7.66931, smoothed loss 7.40302, grad norm 2.43334, param norm 73.99516
INFO:root:epoch 2, iter 1620, loss 7.38899, smoothed loss 7.39713, grad norm 2.65484, param norm 74.06374
INFO:root:epoch 2, iter 1625, loss 7.13916, smoothed loss 7.39148, grad norm 2.56878, param norm 74.13226
INFO:root:epoch 2, iter 1630, loss 7.69657, smoothed loss 7.38873, grad norm 2.96613, param norm 74.19862
INFO:root:epoch 2, iter 1635, loss 7.44473, smoothed loss 7.39356, grad norm 2.74962, param norm 74.25847
INFO:root:epoch 2, iter 1640, loss 8.07019, smoothed loss 7.40047, grad norm 3.10419, param norm 74.31574
INFO:root:epoch 2, iter 1645, loss 7.34662, smoothed loss 7.39882, grad norm 2.24224, param norm 74.37592
INFO:root:epoch 2, iter 1650, loss 7.17851, smoothed loss 7.40367, grad norm 2.60897, param norm 74.43204
INFO:root:epoch 2, iter 1655, loss 8.23829, smoothed loss 7.40627, grad norm 3.55204, param norm 74.48415
INFO:root:epoch 2, iter 1660, loss 7.07176, smoothed loss 7.40709, grad norm 2.54305, param norm 74.53812
INFO:root:epoch 2, iter 1665, loss 7.08079, smoothed loss 7.40088, grad norm 2.47164, param norm 74.60206
INFO:root:epoch 2, iter 1670, loss 7.33728, smoothed loss 7.40550, grad norm 2.23531, param norm 74.66972
INFO:root:epoch 2, iter 1675, loss 7.24915, smoothed loss 7.41317, grad norm 2.86238, param norm 74.73544
INFO:root:epoch 2, iter 1680, loss 7.32394, smoothed loss 7.41326, grad norm 2.79548, param norm 74.79066
INFO:root:epoch 2, iter 1685, loss 7.75108, smoothed loss 7.40982, grad norm 2.70163, param norm 74.84880
INFO:root:epoch 2, iter 1690, loss 6.94455, smoothed loss 7.39562, grad norm 2.31401, param norm 74.90910
INFO:root:epoch 2, iter 1695, loss 7.68831, smoothed loss 7.39657, grad norm 2.43532, param norm 74.96635
INFO:root:epoch 2, iter 1700, loss 7.36869, smoothed loss 7.38743, grad norm 2.53908, param norm 75.02841
INFO:root:epoch 2, iter 1705, loss 7.74102, smoothed loss 7.38271, grad norm 3.03932, param norm 75.09396
INFO:root:epoch 2, iter 1710, loss 6.86496, smoothed loss 7.36692, grad norm 2.36380, param norm 75.15453
INFO:root:epoch 2, iter 1715, loss 7.64853, smoothed loss 7.36281, grad norm 3.24547, param norm 75.22009
INFO:root:epoch 2, iter 1720, loss 7.41560, smoothed loss 7.36058, grad norm 2.22944, param norm 75.28690
INFO:root:epoch 2, iter 1725, loss 7.43554, smoothed loss 7.35838, grad norm 2.84681, param norm 75.35216
INFO:root:epoch 2, iter 1730, loss 7.44837, smoothed loss 7.35615, grad norm 2.36152, param norm 75.41340
INFO:root:epoch 2, iter 1735, loss 7.73350, smoothed loss 7.35166, grad norm 3.80408, param norm 75.48429
INFO:root:epoch 2, iter 1740, loss 7.36464, smoothed loss 7.35757, grad norm 2.42371, param norm 75.55534
Adding batches start...
Added  144  batches
INFO:root:epoch 2, iter 1745, loss 7.47732, smoothed loss 7.35610, grad norm 2.74979, param norm 75.63236
INFO:root:epoch 2, iter 1750, loss 7.77974, smoothed loss 7.35300, grad norm 3.56079, param norm 75.70328
INFO:root:epoch 2, iter 1755, loss 7.33836, smoothed loss 7.35524, grad norm 3.01849, param norm 75.76941
INFO:root:epoch 2, iter 1760, loss 7.26931, smoothed loss 7.34957, grad norm 3.08172, param norm 75.84045
INFO:root:epoch 2, iter 1765, loss 7.21199, smoothed loss 7.33897, grad norm 3.02095, param norm 75.91699
INFO:root:epoch 2, iter 1770, loss 7.34294, smoothed loss 7.33393, grad norm 3.69448, param norm 75.99249
INFO:root:epoch 2, iter 1775, loss 7.31161, smoothed loss 7.34015, grad norm 2.33954, param norm 76.05895
INFO:root:epoch 2, iter 1780, loss 7.37441, smoothed loss 7.33812, grad norm 2.75674, param norm 76.12825
INFO:root:epoch 2, iter 1785, loss 7.48188, smoothed loss 7.34091, grad norm 2.24794, param norm 76.19492
INFO:root:epoch 2, iter 1790, loss 7.43653, smoothed loss 7.34087, grad norm 2.91010, param norm 76.25794
INFO:root:epoch 2, iter 1795, loss 7.54018, smoothed loss 7.34764, grad norm 2.68636, param norm 76.32134
INFO:root:epoch 2, iter 1800, loss 6.79112, smoothed loss 7.33202, grad norm 3.05253, param norm 76.39138
INFO:root:epoch 2, iter 1805, loss 7.15470, smoothed loss 7.33433, grad norm 2.89621, param norm 76.45631
INFO:root:epoch 2, iter 1810, loss 7.43669, smoothed loss 7.33122, grad norm 2.80612, param norm 76.51861
INFO:root:epoch 2, iter 1815, loss 7.04441, smoothed loss 7.32958, grad norm 2.59765, param norm 76.58678
INFO:root:epoch 2, iter 1820, loss 7.37346, smoothed loss 7.32797, grad norm 2.82895, param norm 76.65843
INFO:root:epoch 2, iter 1825, loss 6.99995, smoothed loss 7.32161, grad norm 2.84772, param norm 76.72729
INFO:root:epoch 2, iter 1830, loss 7.28361, smoothed loss 7.32736, grad norm 2.21006, param norm 76.80021
INFO:root:epoch 2, iter 1835, loss 7.50621, smoothed loss 7.32476, grad norm 4.94854, param norm 76.87885
INFO:root:epoch 2, iter 1840, loss 7.10959, smoothed loss 7.31849, grad norm 3.32165, param norm 76.96467
INFO:root:epoch 2, iter 1845, loss 7.63546, smoothed loss 7.32555, grad norm 3.51393, param norm 77.05111
INFO:root:epoch 2, iter 1850, loss 7.51887, smoothed loss 7.33037, grad norm 2.61061, param norm 77.12896
INFO:root:epoch 2, iter 1855, loss 7.29114, smoothed loss 7.32012, grad norm 2.63010, param norm 77.20630
INFO:root:epoch 2, iter 1860, loss 6.76654, smoothed loss 7.30821, grad norm 4.04852, param norm 77.28966
INFO:root:epoch 2, iter 1865, loss 7.20604, smoothed loss 7.31746, grad norm 3.92698, param norm 77.37526
INFO:root:epoch 2, iter 1870, loss 7.38728, smoothed loss 7.30714, grad norm 3.15099, param norm 77.45607
INFO:root:epoch 2, iter 1875, loss 6.93185, smoothed loss 7.30263, grad norm 3.18197, param norm 77.53833
INFO:root:epoch 2, iter 1880, loss 7.06117, smoothed loss 7.29307, grad norm 3.26785, param norm 77.62745
INFO:root:epoch 2, iter 1885, loss 7.34278, smoothed loss 7.28280, grad norm 3.47419, param norm 77.71864
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 1890, loss 7.57626, smoothed loss 7.27995, grad norm 3.77169, param norm 77.81638
INFO:root:epoch 3, iter 1895, loss 7.26411, smoothed loss 7.27224, grad norm 2.79921, param norm 77.91461
INFO:root:epoch 3, iter 1900, loss 6.47949, smoothed loss 7.27165, grad norm 4.62513, param norm 78.00626
INFO:root:epoch 3, iter 1905, loss 6.73200, smoothed loss 7.25389, grad norm 3.85678, param norm 78.10311
INFO:root:epoch 3, iter 1910, loss 6.60226, smoothed loss 7.23252, grad norm 4.39250, param norm 78.19087
INFO:root:epoch 3, iter 1915, loss 7.03255, smoothed loss 7.21745, grad norm 5.87183, param norm 78.28132
INFO:root:epoch 3, iter 1920, loss 6.96792, smoothed loss 7.21016, grad norm 3.95362, param norm 78.37558
INFO:root:epoch 3, iter 1925, loss 7.22288, smoothed loss 7.19761, grad norm 3.93314, param norm 78.47088
INFO:root:epoch 3, iter 1930, loss 6.89798, smoothed loss 7.19272, grad norm 2.84039, param norm 78.56717
INFO:root:epoch 3, iter 1935, loss 7.04607, smoothed loss 7.19033, grad norm 3.33502, param norm 78.66093
INFO:root:epoch 3, iter 1940, loss 7.01174, smoothed loss 7.18348, grad norm 4.32634, param norm 78.76418
INFO:root:epoch 3, iter 1945, loss 6.85833, smoothed loss 7.16292, grad norm 4.81620, param norm 78.86703
INFO:root:epoch 3, iter 1950, loss 6.63836, smoothed loss 7.15842, grad norm 5.36328, param norm 78.97172
INFO:root:epoch 3, iter 1955, loss 6.74092, smoothed loss 7.14938, grad norm 4.15626, param norm 79.06902
INFO:root:epoch 3, iter 1960, loss 6.28920, smoothed loss 7.12308, grad norm 2.67612, param norm 79.16570
INFO:root:epoch 3, iter 1965, loss 7.18388, smoothed loss 7.11244, grad norm 3.38377, param norm 79.26497
INFO:root:epoch 3, iter 1970, loss 6.43312, smoothed loss 7.09913, grad norm 3.79515, param norm 79.35818
INFO:root:epoch 3, iter 1975, loss 7.12219, smoothed loss 7.08734, grad norm 3.38155, param norm 79.46514
INFO:root:epoch 3, iter 1980, loss 6.67465, smoothed loss 7.08013, grad norm 2.99278, param norm 79.56874
INFO:root:epoch 3, iter 1985, loss 7.45201, smoothed loss 7.07110, grad norm 4.11374, param norm 79.67593
INFO:root:epoch 3, iter 1990, loss 6.62709, smoothed loss 7.06299, grad norm 3.12953, param norm 79.78379
INFO:root:epoch 3, iter 1995, loss 6.78878, smoothed loss 7.05415, grad norm 3.89269, param norm 79.89001
INFO:root:epoch 3, iter 2000, loss 6.96005, smoothed loss 7.04548, grad norm 4.59579, param norm 79.99881
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 3, Iter 2000, dev loss: 6.836963
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.15178
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.09200
INFO:root:Epoch 3, Iter 2000, Train F1 score: 0.151775, Train EM score: 0.092000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.14886
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.09357
INFO:root:Epoch 3, Iter 2000, Dev F1 score: 0.148861, Dev EM score: 0.093566
INFO:root:End of epoch 3
INFO:root:epoch 3, iter 2005, loss 7.22786, smoothed loss 7.04561, grad norm 4.23985, param norm 80.10477
INFO:root:epoch 3, iter 2010, loss 7.40757, smoothed loss 7.05168, grad norm 3.74021, param norm 80.20151
INFO:root:epoch 3, iter 2015, loss 7.03848, smoothed loss 7.03833, grad norm 4.19899, param norm 80.30477
INFO:root:epoch 3, iter 2020, loss 6.70918, smoothed loss 7.02331, grad norm 3.72774, param norm 80.41608
INFO:root:epoch 3, iter 2025, loss 6.79340, smoothed loss 7.01048, grad norm 3.45552, param norm 80.52163
INFO:root:epoch 3, iter 2030, loss 6.90989, smoothed loss 7.00596, grad norm 4.34657, param norm 80.62019
INFO:root:epoch 3, iter 2035, loss 6.64849, smoothed loss 6.99467, grad norm 3.31836, param norm 80.72131
INFO:root:epoch 3, iter 2040, loss 6.14913, smoothed loss 6.98184, grad norm 3.84364, param norm 80.82216
INFO:root:epoch 3, iter 2045, loss 6.87243, smoothed loss 6.97000, grad norm 4.53012, param norm 80.92329
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2050, loss 6.23494, smoothed loss 6.94797, grad norm 3.91896, param norm 81.03288
INFO:root:epoch 3, iter 2055, loss 6.88942, smoothed loss 6.94533, grad norm 5.18714, param norm 81.14313
INFO:root:epoch 3, iter 2060, loss 6.92180, smoothed loss 6.93509, grad norm 3.70246, param norm 81.24874
INFO:root:epoch 3, iter 2065, loss 6.85344, smoothed loss 6.93440, grad norm 3.20489, param norm 81.35703
INFO:root:epoch 3, iter 2070, loss 7.02866, smoothed loss 6.92121, grad norm 3.72130, param norm 81.45734
INFO:root:epoch 3, iter 2075, loss 6.72178, smoothed loss 6.91227, grad norm 3.70713, param norm 81.55649
INFO:root:epoch 3, iter 2080, loss 6.53679, smoothed loss 6.91102, grad norm 3.25103, param norm 81.65413
INFO:root:epoch 3, iter 2085, loss 6.92686, smoothed loss 6.91587, grad norm 3.35213, param norm 81.75506
INFO:root:epoch 3, iter 2090, loss 6.46088, smoothed loss 6.91091, grad norm 4.29344, param norm 81.85918
INFO:root:epoch 3, iter 2095, loss 5.98580, smoothed loss 6.90548, grad norm 3.56012, param norm 81.96063
INFO:root:epoch 3, iter 2100, loss 6.52343, smoothed loss 6.88922, grad norm 3.07609, param norm 82.05472
INFO:root:epoch 3, iter 2105, loss 6.55819, smoothed loss 6.88971, grad norm 3.76761, param norm 82.14813
INFO:root:epoch 3, iter 2110, loss 6.67599, smoothed loss 6.88787, grad norm 4.34766, param norm 82.24610
INFO:root:epoch 3, iter 2115, loss 6.04750, smoothed loss 6.86586, grad norm 4.34690, param norm 82.34646
INFO:root:epoch 3, iter 2120, loss 6.87189, smoothed loss 6.86003, grad norm 4.11638, param norm 82.44468
INFO:root:epoch 3, iter 2125, loss 7.19177, smoothed loss 6.85324, grad norm 3.38888, param norm 82.54249
INFO:root:epoch 3, iter 2130, loss 6.55923, smoothed loss 6.84237, grad norm 3.02769, param norm 82.64629
INFO:root:epoch 3, iter 2135, loss 6.77750, smoothed loss 6.83176, grad norm 4.10183, param norm 82.75991
INFO:root:epoch 3, iter 2140, loss 7.28082, smoothed loss 6.83719, grad norm 4.66041, param norm 82.87067
INFO:root:epoch 3, iter 2145, loss 6.77963, smoothed loss 6.82864, grad norm 4.74692, param norm 82.97301
INFO:root:epoch 3, iter 2150, loss 6.68500, smoothed loss 6.81257, grad norm 3.25523, param norm 83.07507
INFO:root:epoch 3, iter 2155, loss 6.09794, smoothed loss 6.80197, grad norm 3.69690, param norm 83.17337
INFO:root:epoch 3, iter 2160, loss 6.77182, smoothed loss 6.79548, grad norm 4.29895, param norm 83.26893
INFO:root:epoch 3, iter 2165, loss 7.00276, smoothed loss 6.79842, grad norm 4.07714, param norm 83.36056
INFO:root:epoch 3, iter 2170, loss 6.70173, smoothed loss 6.78840, grad norm 4.21164, param norm 83.45311
INFO:root:epoch 3, iter 2175, loss 6.23389, smoothed loss 6.76511, grad norm 3.64235, param norm 83.54614
INFO:root:epoch 3, iter 2180, loss 6.49748, smoothed loss 6.76460, grad norm 3.65162, param norm 83.64368
INFO:root:epoch 3, iter 2185, loss 6.75118, smoothed loss 6.75610, grad norm 4.44737, param norm 83.74088
INFO:root:epoch 3, iter 2190, loss 6.15240, smoothed loss 6.74695, grad norm 3.86910, param norm 83.84325
INFO:root:epoch 3, iter 2195, loss 6.18726, smoothed loss 6.74480, grad norm 4.02730, param norm 83.94741
INFO:root:epoch 3, iter 2200, loss 7.23996, smoothed loss 6.74044, grad norm 3.96262, param norm 84.05194
INFO:root:epoch 3, iter 2205, loss 6.63973, smoothed loss 6.75019, grad norm 3.44173, param norm 84.14646
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2210, loss 6.26231, smoothed loss 6.74701, grad norm 4.31745, param norm 84.22724
INFO:root:epoch 3, iter 2215, loss 6.15394, smoothed loss 6.72460, grad norm 4.53103, param norm 84.31940
INFO:root:epoch 3, iter 2220, loss 6.54275, smoothed loss 6.70756, grad norm 5.48209, param norm 84.42339
INFO:root:epoch 3, iter 2225, loss 6.78824, smoothed loss 6.70458, grad norm 3.61358, param norm 84.53173
INFO:root:epoch 3, iter 2230, loss 6.47229, smoothed loss 6.69930, grad norm 4.15358, param norm 84.64025
INFO:root:epoch 3, iter 2235, loss 6.25598, smoothed loss 6.69246, grad norm 3.43869, param norm 84.74575
INFO:root:epoch 3, iter 2240, loss 6.59858, smoothed loss 6.69291, grad norm 3.27261, param norm 84.84341
INFO:root:epoch 3, iter 2245, loss 7.34528, smoothed loss 6.68771, grad norm 4.45085, param norm 84.93691
INFO:root:epoch 3, iter 2250, loss 6.55364, smoothed loss 6.68517, grad norm 4.41642, param norm 85.02731
INFO:root:epoch 3, iter 2255, loss 6.75220, smoothed loss 6.68006, grad norm 3.77257, param norm 85.11758
INFO:root:epoch 3, iter 2260, loss 6.51548, smoothed loss 6.67492, grad norm 3.28767, param norm 85.20859
INFO:root:epoch 3, iter 2265, loss 6.50301, smoothed loss 6.67045, grad norm 3.65902, param norm 85.30104
INFO:root:epoch 3, iter 2270, loss 6.50117, smoothed loss 6.67121, grad norm 5.14481, param norm 85.38976
INFO:root:epoch 3, iter 2275, loss 6.36533, smoothed loss 6.67288, grad norm 3.90710, param norm 85.47747
INFO:root:epoch 3, iter 2280, loss 6.58329, smoothed loss 6.65655, grad norm 4.43392, param norm 85.56678
INFO:root:epoch 3, iter 2285, loss 5.79880, smoothed loss 6.64219, grad norm 3.20564, param norm 85.65842
INFO:root:epoch 3, iter 2290, loss 6.61217, smoothed loss 6.63698, grad norm 4.46214, param norm 85.75568
INFO:root:epoch 3, iter 2295, loss 7.18019, smoothed loss 6.63438, grad norm 3.82435, param norm 85.84699
INFO:root:epoch 3, iter 2300, loss 6.27682, smoothed loss 6.63002, grad norm 3.83718, param norm 85.93862
INFO:root:epoch 3, iter 2305, loss 5.72013, smoothed loss 6.61698, grad norm 3.94729, param norm 86.02908
INFO:root:epoch 3, iter 2310, loss 6.18337, smoothed loss 6.60664, grad norm 4.50375, param norm 86.12743
INFO:root:epoch 3, iter 2315, loss 6.16011, smoothed loss 6.60014, grad norm 4.38572, param norm 86.22150
INFO:root:epoch 3, iter 2320, loss 6.20570, smoothed loss 6.59754, grad norm 4.09637, param norm 86.31769
INFO:root:epoch 3, iter 2325, loss 6.06386, smoothed loss 6.57827, grad norm 3.85327, param norm 86.41492
INFO:root:epoch 3, iter 2330, loss 6.36859, smoothed loss 6.56523, grad norm 3.55631, param norm 86.51700
INFO:root:epoch 3, iter 2335, loss 6.27287, smoothed loss 6.55318, grad norm 3.67819, param norm 86.60596
INFO:root:epoch 3, iter 2340, loss 6.47973, smoothed loss 6.53988, grad norm 3.75359, param norm 86.69379
INFO:root:epoch 3, iter 2345, loss 6.76020, smoothed loss 6.52661, grad norm 3.44990, param norm 86.78419
INFO:root:epoch 3, iter 2350, loss 6.93758, smoothed loss 6.53125, grad norm 3.98090, param norm 86.87420
INFO:root:epoch 3, iter 2355, loss 6.28491, smoothed loss 6.53095, grad norm 3.85813, param norm 86.95535
INFO:root:epoch 3, iter 2360, loss 6.41460, smoothed loss 6.52615, grad norm 3.47051, param norm 87.03745
INFO:root:epoch 3, iter 2365, loss 6.64783, smoothed loss 6.52934, grad norm 3.48462, param norm 87.11808
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2370, loss 5.84073, smoothed loss 6.51773, grad norm 4.56629, param norm 87.19446
INFO:root:epoch 3, iter 2375, loss 6.56420, smoothed loss 6.51111, grad norm 3.63230, param norm 87.27338
INFO:root:epoch 3, iter 2380, loss 5.81003, smoothed loss 6.51248, grad norm 4.27816, param norm 87.34973
INFO:root:epoch 3, iter 2385, loss 6.21050, smoothed loss 6.50753, grad norm 4.34127, param norm 87.42947
INFO:root:epoch 3, iter 2390, loss 6.96227, smoothed loss 6.50755, grad norm 4.85995, param norm 87.51096
INFO:root:epoch 3, iter 2395, loss 6.70715, smoothed loss 6.49871, grad norm 4.27347, param norm 87.58768
INFO:root:epoch 3, iter 2400, loss 6.76797, smoothed loss 6.50646, grad norm 3.93227, param norm 87.66372
INFO:root:epoch 3, iter 2405, loss 6.69182, smoothed loss 6.49592, grad norm 3.90703, param norm 87.74032
INFO:root:epoch 3, iter 2410, loss 6.49743, smoothed loss 6.48986, grad norm 3.66155, param norm 87.82259
INFO:root:epoch 3, iter 2415, loss 6.91909, smoothed loss 6.48811, grad norm 4.98014, param norm 87.90737
INFO:root:epoch 3, iter 2420, loss 6.17102, smoothed loss 6.48796, grad norm 3.62268, param norm 87.98546
INFO:root:epoch 3, iter 2425, loss 6.22985, smoothed loss 6.46967, grad norm 4.09187, param norm 88.06622
INFO:root:epoch 3, iter 2430, loss 5.96954, smoothed loss 6.46777, grad norm 3.76699, param norm 88.14291
INFO:root:epoch 3, iter 2435, loss 6.25472, smoothed loss 6.46726, grad norm 4.15840, param norm 88.21981
INFO:root:epoch 3, iter 2440, loss 6.55199, smoothed loss 6.47001, grad norm 4.23978, param norm 88.30323
INFO:root:epoch 3, iter 2445, loss 6.69159, smoothed loss 6.46138, grad norm 3.76327, param norm 88.37694
INFO:root:epoch 3, iter 2450, loss 6.33412, smoothed loss 6.44527, grad norm 3.96747, param norm 88.44872
INFO:root:epoch 3, iter 2455, loss 6.35456, smoothed loss 6.45907, grad norm 5.31115, param norm 88.52114
INFO:root:epoch 3, iter 2460, loss 6.63675, smoothed loss 6.45736, grad norm 3.93402, param norm 88.59759
INFO:root:epoch 3, iter 2465, loss 6.40857, smoothed loss 6.45746, grad norm 3.61427, param norm 88.67862
INFO:root:epoch 3, iter 2470, loss 6.44006, smoothed loss 6.43988, grad norm 3.99497, param norm 88.76408
INFO:root:epoch 3, iter 2475, loss 6.28219, smoothed loss 6.44659, grad norm 3.83806, param norm 88.84702
INFO:root:epoch 3, iter 2480, loss 6.49735, smoothed loss 6.44211, grad norm 3.47152, param norm 88.93072
INFO:root:epoch 3, iter 2485, loss 7.27302, smoothed loss 6.45534, grad norm 6.03531, param norm 89.01899
INFO:root:epoch 3, iter 2490, loss 6.52695, smoothed loss 6.45759, grad norm 3.43952, param norm 89.10115
INFO:root:epoch 3, iter 2495, loss 5.83946, smoothed loss 6.44303, grad norm 3.62016, param norm 89.18452
INFO:root:epoch 3, iter 2500, loss 6.45135, smoothed loss 6.43619, grad norm 4.54338, param norm 89.27115
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 3, Iter 2500, dev loss: 6.229040
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.22701
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.14700
INFO:root:Epoch 3, Iter 2500, Train F1 score: 0.227014, Train EM score: 0.147000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.20524
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.13796
INFO:root:Epoch 3, Iter 2500, Dev F1 score: 0.205238, Dev EM score: 0.137960
INFO:root:End of epoch 3
INFO:root:epoch 3, iter 2505, loss 6.25335, smoothed loss 6.42448, grad norm 4.86655, param norm 89.35344
INFO:root:epoch 3, iter 2510, loss 6.51007, smoothed loss 6.42022, grad norm 4.22886, param norm 89.43268
INFO:root:epoch 3, iter 2515, loss 7.38855, smoothed loss 6.42480, grad norm 4.84173, param norm 89.50732
INFO:root:epoch 3, iter 2520, loss 6.48512, smoothed loss 6.42316, grad norm 3.28712, param norm 89.58021
INFO:root:epoch 3, iter 2525, loss 7.02941, smoothed loss 6.42837, grad norm 3.57537, param norm 89.65055
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2530, loss 6.34270, smoothed loss 6.42666, grad norm 3.44569, param norm 89.71609
INFO:root:epoch 3, iter 2535, loss 6.66034, smoothed loss 6.41284, grad norm 3.75840, param norm 89.78699
INFO:root:epoch 3, iter 2540, loss 5.87163, smoothed loss 6.40482, grad norm 3.80108, param norm 89.86048
INFO:root:epoch 3, iter 2545, loss 6.53881, smoothed loss 6.41506, grad norm 4.43029, param norm 89.93855
INFO:root:epoch 3, iter 2550, loss 6.16658, smoothed loss 6.40186, grad norm 3.62928, param norm 90.01692
INFO:root:epoch 3, iter 2555, loss 5.51465, smoothed loss 6.38873, grad norm 3.17322, param norm 90.09406
INFO:root:epoch 3, iter 2560, loss 6.76480, smoothed loss 6.37442, grad norm 3.98461, param norm 90.17296
INFO:root:epoch 3, iter 2565, loss 6.33142, smoothed loss 6.38245, grad norm 5.24642, param norm 90.24698
INFO:root:epoch 3, iter 2570, loss 6.35198, smoothed loss 6.38036, grad norm 4.02324, param norm 90.32106
INFO:root:epoch 3, iter 2575, loss 7.31800, smoothed loss 6.39011, grad norm 4.27371, param norm 90.38793
INFO:root:epoch 3, iter 2580, loss 7.11486, smoothed loss 6.39378, grad norm 3.51182, param norm 90.45771
INFO:root:epoch 3, iter 2585, loss 5.55463, smoothed loss 6.38950, grad norm 3.54407, param norm 90.53436
INFO:root:epoch 3, iter 2590, loss 6.55513, smoothed loss 6.39093, grad norm 4.06214, param norm 90.61031
INFO:root:epoch 3, iter 2595, loss 5.99806, smoothed loss 6.37243, grad norm 4.39821, param norm 90.68719
INFO:root:epoch 3, iter 2600, loss 5.66701, smoothed loss 6.36011, grad norm 4.21667, param norm 90.76337
INFO:root:epoch 3, iter 2605, loss 6.91834, smoothed loss 6.35388, grad norm 4.49723, param norm 90.84873
INFO:root:epoch 3, iter 2610, loss 6.07904, smoothed loss 6.34803, grad norm 4.02816, param norm 90.93311
INFO:root:epoch 3, iter 2615, loss 5.68334, smoothed loss 6.34359, grad norm 4.54002, param norm 91.01149
INFO:root:epoch 3, iter 2620, loss 6.62633, smoothed loss 6.35074, grad norm 4.18823, param norm 91.08357
INFO:root:epoch 3, iter 2625, loss 6.71029, smoothed loss 6.35740, grad norm 4.43341, param norm 91.15407
INFO:root:epoch 3, iter 2630, loss 5.82794, smoothed loss 6.35848, grad norm 3.76395, param norm 91.22488
INFO:root:epoch 3, iter 2635, loss 6.33679, smoothed loss 6.36263, grad norm 3.64297, param norm 91.29356
INFO:root:epoch 3, iter 2640, loss 6.69578, smoothed loss 6.35554, grad norm 3.41562, param norm 91.36242
INFO:root:epoch 3, iter 2645, loss 6.48712, smoothed loss 6.35928, grad norm 3.85121, param norm 91.43420
INFO:root:epoch 3, iter 2650, loss 5.94751, smoothed loss 6.34918, grad norm 3.98033, param norm 91.50900
INFO:root:epoch 3, iter 2655, loss 5.76422, smoothed loss 6.33870, grad norm 3.49472, param norm 91.58328
INFO:root:epoch 3, iter 2660, loss 5.89868, smoothed loss 6.32198, grad norm 3.90820, param norm 91.65794
INFO:root:epoch 3, iter 2665, loss 6.29614, smoothed loss 6.31901, grad norm 3.81952, param norm 91.73112
INFO:root:epoch 3, iter 2670, loss 6.53878, smoothed loss 6.32340, grad norm 5.33915, param norm 91.79337
INFO:root:epoch 3, iter 2675, loss 6.34109, smoothed loss 6.32736, grad norm 3.82882, param norm 91.85971
INFO:root:epoch 3, iter 2680, loss 6.06753, smoothed loss 6.32075, grad norm 4.14933, param norm 91.92867
INFO:root:epoch 3, iter 2685, loss 5.83354, smoothed loss 6.30837, grad norm 3.50934, param norm 91.99834
Adding batches start...
Added  144  batches
INFO:root:epoch 3, iter 2690, loss 6.42892, smoothed loss 6.31424, grad norm 3.95529, param norm 92.06801
INFO:root:epoch 3, iter 2695, loss 6.06425, smoothed loss 6.31815, grad norm 4.10551, param norm 92.13794
INFO:root:epoch 3, iter 2700, loss 6.31539, smoothed loss 6.32465, grad norm 5.28015, param norm 92.21043
INFO:root:epoch 3, iter 2705, loss 5.64320, smoothed loss 6.31522, grad norm 3.63518, param norm 92.28797
INFO:root:epoch 3, iter 2710, loss 6.44356, smoothed loss 6.31932, grad norm 4.65835, param norm 92.36561
INFO:root:epoch 3, iter 2715, loss 6.08970, smoothed loss 6.30336, grad norm 4.07768, param norm 92.43681
INFO:root:epoch 3, iter 2720, loss 5.92569, smoothed loss 6.29224, grad norm 3.81103, param norm 92.51101
INFO:root:epoch 3, iter 2725, loss 6.03997, smoothed loss 6.28169, grad norm 3.80694, param norm 92.58752
INFO:root:epoch 3, iter 2730, loss 6.16860, smoothed loss 6.28522, grad norm 3.53035, param norm 92.66379
INFO:root:epoch 3, iter 2735, loss 5.92150, smoothed loss 6.28621, grad norm 3.69459, param norm 92.73897
INFO:root:epoch 3, iter 2740, loss 6.63823, smoothed loss 6.29109, grad norm 4.06155, param norm 92.80675
INFO:root:epoch 3, iter 2745, loss 6.48073, smoothed loss 6.30664, grad norm 4.08593, param norm 92.87170
INFO:root:epoch 3, iter 2750, loss 6.23312, smoothed loss 6.30535, grad norm 3.48328, param norm 92.93616
INFO:root:epoch 3, iter 2755, loss 5.50210, smoothed loss 6.27887, grad norm 3.73793, param norm 93.00200
INFO:root:epoch 3, iter 2760, loss 6.29020, smoothed loss 6.27246, grad norm 4.28980, param norm 93.06998
INFO:root:epoch 3, iter 2765, loss 6.47715, smoothed loss 6.26880, grad norm 3.69849, param norm 93.13853
INFO:root:epoch 3, iter 2770, loss 6.59809, smoothed loss 6.28204, grad norm 3.97929, param norm 93.20449
INFO:root:epoch 3, iter 2775, loss 6.03752, smoothed loss 6.26521, grad norm 3.89786, param norm 93.26601
INFO:root:epoch 3, iter 2780, loss 6.48730, smoothed loss 6.27000, grad norm 4.38444, param norm 93.32985
INFO:root:epoch 3, iter 2785, loss 5.46485, smoothed loss 6.26081, grad norm 3.80025, param norm 93.39516
INFO:root:epoch 3, iter 2790, loss 5.72060, smoothed loss 6.24530, grad norm 4.68003, param norm 93.46560
INFO:root:epoch 3, iter 2795, loss 6.78101, smoothed loss 6.25438, grad norm 4.76545, param norm 93.53897
INFO:root:epoch 3, iter 2800, loss 6.07506, smoothed loss 6.25125, grad norm 4.06833, param norm 93.60859
INFO:root:epoch 3, iter 2805, loss 5.91229, smoothed loss 6.24305, grad norm 3.51226, param norm 93.67831
INFO:root:epoch 3, iter 2810, loss 5.41895, smoothed loss 6.22965, grad norm 3.85635, param norm 93.74838
INFO:root:epoch 3, iter 2815, loss 6.01240, smoothed loss 6.21134, grad norm 4.28923, param norm 93.81996
INFO:root:epoch 3, iter 2820, loss 5.83650, smoothed loss 6.21840, grad norm 4.65160, param norm 93.88843
INFO:root:epoch 3, iter 2825, loss 6.63958, smoothed loss 6.21486, grad norm 4.14969, param norm 93.95789
INFO:root:epoch 3, iter 2830, loss 5.86609, smoothed loss 6.20536, grad norm 4.53568, param norm 94.02621
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 2835, loss 6.44079, smoothed loss 6.20141, grad norm 4.26870, param norm 94.09663
INFO:root:epoch 4, iter 2840, loss 5.96826, smoothed loss 6.19999, grad norm 3.92847, param norm 94.16382
INFO:root:epoch 4, iter 2845, loss 5.87921, smoothed loss 6.18619, grad norm 3.64512, param norm 94.23418
INFO:root:epoch 4, iter 2850, loss 6.01928, smoothed loss 6.18715, grad norm 3.98187, param norm 94.29783
INFO:root:epoch 4, iter 2855, loss 6.25960, smoothed loss 6.18216, grad norm 4.31515, param norm 94.36018
INFO:root:epoch 4, iter 2860, loss 6.51851, smoothed loss 6.18026, grad norm 4.28213, param norm 94.42908
INFO:root:epoch 4, iter 2865, loss 6.10587, smoothed loss 6.17768, grad norm 4.89152, param norm 94.49773
INFO:root:epoch 4, iter 2870, loss 5.56674, smoothed loss 6.16664, grad norm 4.20993, param norm 94.56662
INFO:root:epoch 4, iter 2875, loss 5.70757, smoothed loss 6.16450, grad norm 3.52392, param norm 94.63158
INFO:root:epoch 4, iter 2880, loss 6.18868, smoothed loss 6.15737, grad norm 4.14442, param norm 94.69862
INFO:root:epoch 4, iter 2885, loss 6.23026, smoothed loss 6.14781, grad norm 4.26880, param norm 94.76548
INFO:root:epoch 4, iter 2890, loss 6.46974, smoothed loss 6.14380, grad norm 3.96745, param norm 94.82880
INFO:root:epoch 4, iter 2895, loss 6.53322, smoothed loss 6.13141, grad norm 3.86837, param norm 94.89319
INFO:root:epoch 4, iter 2900, loss 6.04453, smoothed loss 6.13428, grad norm 4.16572, param norm 94.95782
INFO:root:epoch 4, iter 2905, loss 5.65119, smoothed loss 6.13070, grad norm 3.51095, param norm 95.02841
INFO:root:epoch 4, iter 2910, loss 6.12791, smoothed loss 6.12173, grad norm 4.33209, param norm 95.10316
INFO:root:epoch 4, iter 2915, loss 6.45256, smoothed loss 6.11844, grad norm 4.38330, param norm 95.17176
INFO:root:epoch 4, iter 2920, loss 5.89138, smoothed loss 6.13138, grad norm 4.08073, param norm 95.24239
INFO:root:epoch 4, iter 2925, loss 5.99293, smoothed loss 6.13652, grad norm 4.75861, param norm 95.31056
INFO:root:epoch 4, iter 2930, loss 5.99731, smoothed loss 6.12704, grad norm 3.97386, param norm 95.37920
INFO:root:epoch 4, iter 2935, loss 6.04716, smoothed loss 6.11820, grad norm 3.85945, param norm 95.44579
INFO:root:epoch 4, iter 2940, loss 5.96893, smoothed loss 6.10244, grad norm 3.89984, param norm 95.51661
INFO:root:epoch 4, iter 2945, loss 6.20056, smoothed loss 6.10631, grad norm 4.41304, param norm 95.59023
INFO:root:epoch 4, iter 2950, loss 6.27070, smoothed loss 6.10925, grad norm 4.70373, param norm 95.65730
INFO:root:epoch 4, iter 2955, loss 5.71381, smoothed loss 6.11445, grad norm 3.98674, param norm 95.72190
INFO:root:epoch 4, iter 2960, loss 6.24907, smoothed loss 6.13027, grad norm 4.74689, param norm 95.79038
INFO:root:epoch 4, iter 2965, loss 5.49417, smoothed loss 6.12037, grad norm 3.96017, param norm 95.85902
INFO:root:epoch 4, iter 2970, loss 5.43259, smoothed loss 6.10143, grad norm 3.88489, param norm 95.92496
INFO:root:epoch 4, iter 2975, loss 6.24098, smoothed loss 6.10853, grad norm 3.94471, param norm 95.98749
INFO:root:epoch 4, iter 2980, loss 6.18341, smoothed loss 6.11903, grad norm 3.80754, param norm 96.05099
INFO:root:epoch 4, iter 2985, loss 5.21515, smoothed loss 6.10395, grad norm 4.11480, param norm 96.12299
INFO:root:epoch 4, iter 2990, loss 6.38494, smoothed loss 6.10163, grad norm 3.84538, param norm 96.19630
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 2995, loss 6.73794, smoothed loss 6.10162, grad norm 4.05811, param norm 96.26304
INFO:root:epoch 4, iter 3000, loss 5.97920, smoothed loss 6.10538, grad norm 3.56719, param norm 96.32626
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 4, Iter 3000, dev loss: 5.921249
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.28676
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.18600
INFO:root:Epoch 4, Iter 3000, Train F1 score: 0.286765, Train EM score: 0.186000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.24322
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.16704
INFO:root:Epoch 4, Iter 3000, Dev F1 score: 0.243221, Dev EM score: 0.167041
INFO:root:End of epoch 4
INFO:root:epoch 4, iter 3005, loss 6.14653, smoothed loss 6.10695, grad norm 3.80178, param norm 96.38710
INFO:root:epoch 4, iter 3010, loss 5.61872, smoothed loss 6.10106, grad norm 3.72850, param norm 96.45176
INFO:root:epoch 4, iter 3015, loss 5.89626, smoothed loss 6.09104, grad norm 4.67364, param norm 96.52223
INFO:root:epoch 4, iter 3020, loss 6.14308, smoothed loss 6.10669, grad norm 4.09116, param norm 96.58990
INFO:root:epoch 4, iter 3025, loss 5.87937, smoothed loss 6.09656, grad norm 3.84847, param norm 96.65107
INFO:root:epoch 4, iter 3030, loss 5.29412, smoothed loss 6.07747, grad norm 4.27746, param norm 96.71756
INFO:root:epoch 4, iter 3035, loss 6.14440, smoothed loss 6.07129, grad norm 4.46067, param norm 96.78733
INFO:root:epoch 4, iter 3040, loss 5.75234, smoothed loss 6.06279, grad norm 4.14358, param norm 96.85201
INFO:root:epoch 4, iter 3045, loss 6.32762, smoothed loss 6.06414, grad norm 4.33624, param norm 96.91929
INFO:root:epoch 4, iter 3050, loss 6.05729, smoothed loss 6.05718, grad norm 4.18110, param norm 96.98785
INFO:root:epoch 4, iter 3055, loss 6.53938, smoothed loss 6.04580, grad norm 4.86114, param norm 97.05820
INFO:root:epoch 4, iter 3060, loss 6.40133, smoothed loss 6.05238, grad norm 4.30272, param norm 97.12643
INFO:root:epoch 4, iter 3065, loss 5.82495, smoothed loss 6.04776, grad norm 3.92871, param norm 97.19248
INFO:root:epoch 4, iter 3070, loss 5.46461, smoothed loss 6.03490, grad norm 3.58166, param norm 97.26264
INFO:root:epoch 4, iter 3075, loss 6.11766, smoothed loss 6.03868, grad norm 5.04489, param norm 97.32957
INFO:root:epoch 4, iter 3080, loss 5.34964, smoothed loss 6.03288, grad norm 3.49784, param norm 97.38949
INFO:root:epoch 4, iter 3085, loss 6.04009, smoothed loss 6.03070, grad norm 4.08840, param norm 97.44427
INFO:root:epoch 4, iter 3090, loss 6.27995, smoothed loss 6.03663, grad norm 4.63583, param norm 97.49651
INFO:root:epoch 4, iter 3095, loss 5.83250, smoothed loss 6.03769, grad norm 5.15922, param norm 97.55285
INFO:root:epoch 4, iter 3100, loss 6.02343, smoothed loss 6.03973, grad norm 3.39573, param norm 97.61741
INFO:root:epoch 4, iter 3105, loss 5.43276, smoothed loss 6.02802, grad norm 3.28654, param norm 97.68587
INFO:root:epoch 4, iter 3110, loss 5.91014, smoothed loss 6.01782, grad norm 3.28531, param norm 97.75524
INFO:root:epoch 4, iter 3115, loss 5.54729, smoothed loss 6.00970, grad norm 4.51967, param norm 97.82274
INFO:root:epoch 4, iter 3120, loss 5.68777, smoothed loss 6.01269, grad norm 5.06748, param norm 97.88252
INFO:root:epoch 4, iter 3125, loss 5.88439, smoothed loss 6.00471, grad norm 4.31840, param norm 97.94319
INFO:root:epoch 4, iter 3130, loss 6.17443, smoothed loss 6.01622, grad norm 3.99391, param norm 98.00748
INFO:root:epoch 4, iter 3135, loss 5.77504, smoothed loss 6.01025, grad norm 4.72605, param norm 98.07133
INFO:root:epoch 4, iter 3140, loss 6.10991, smoothed loss 5.99707, grad norm 4.58409, param norm 98.13905
INFO:root:epoch 4, iter 3145, loss 5.82590, smoothed loss 5.98688, grad norm 4.53808, param norm 98.20670
INFO:root:epoch 4, iter 3150, loss 6.02143, smoothed loss 5.98590, grad norm 4.42109, param norm 98.26630
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3155, loss 6.69033, smoothed loss 6.00210, grad norm 5.17789, param norm 98.32682
INFO:root:epoch 4, iter 3160, loss 6.00627, smoothed loss 5.98507, grad norm 3.63139, param norm 98.39053
INFO:root:epoch 4, iter 3165, loss 6.41908, smoothed loss 5.98766, grad norm 5.23493, param norm 98.45112
INFO:root:epoch 4, iter 3170, loss 5.85625, smoothed loss 5.97603, grad norm 4.22379, param norm 98.50867
INFO:root:epoch 4, iter 3175, loss 5.98727, smoothed loss 5.97964, grad norm 4.09160, param norm 98.56915
INFO:root:epoch 4, iter 3180, loss 5.84694, smoothed loss 5.97458, grad norm 4.23857, param norm 98.63316
INFO:root:epoch 4, iter 3185, loss 5.71357, smoothed loss 5.96820, grad norm 4.66068, param norm 98.69438
INFO:root:epoch 4, iter 3190, loss 5.89991, smoothed loss 5.97579, grad norm 4.22105, param norm 98.76022
INFO:root:epoch 4, iter 3195, loss 5.91226, smoothed loss 5.96277, grad norm 5.37358, param norm 98.83027
INFO:root:epoch 4, iter 3200, loss 5.79662, smoothed loss 5.95873, grad norm 4.85592, param norm 98.89307
INFO:root:epoch 4, iter 3205, loss 6.18186, smoothed loss 5.95496, grad norm 4.83568, param norm 98.95626
INFO:root:epoch 4, iter 3210, loss 6.51782, smoothed loss 5.95345, grad norm 4.36944, param norm 99.01506
INFO:root:epoch 4, iter 3215, loss 6.59584, smoothed loss 5.95900, grad norm 5.60680, param norm 99.07309
INFO:root:epoch 4, iter 3220, loss 6.87184, smoothed loss 5.95994, grad norm 5.22765, param norm 99.13104
INFO:root:epoch 4, iter 3225, loss 5.58408, smoothed loss 5.95703, grad norm 4.47007, param norm 99.19051
INFO:root:epoch 4, iter 3230, loss 6.20017, smoothed loss 5.95386, grad norm 4.10659, param norm 99.25262
INFO:root:epoch 4, iter 3235, loss 5.79874, smoothed loss 5.95708, grad norm 4.60894, param norm 99.31598
INFO:root:epoch 4, iter 3240, loss 5.93128, smoothed loss 5.94938, grad norm 4.11856, param norm 99.37802
INFO:root:epoch 4, iter 3245, loss 5.80652, smoothed loss 5.94911, grad norm 4.03223, param norm 99.43705
INFO:root:epoch 4, iter 3250, loss 5.72542, smoothed loss 5.94381, grad norm 4.66066, param norm 99.49215
INFO:root:epoch 4, iter 3255, loss 5.50541, smoothed loss 5.92772, grad norm 3.92709, param norm 99.54684
INFO:root:epoch 4, iter 3260, loss 5.44389, smoothed loss 5.91494, grad norm 4.55417, param norm 99.60559
INFO:root:epoch 4, iter 3265, loss 5.32123, smoothed loss 5.89964, grad norm 4.50519, param norm 99.66696
INFO:root:epoch 4, iter 3270, loss 5.86505, smoothed loss 5.90210, grad norm 4.67944, param norm 99.72786
INFO:root:epoch 4, iter 3275, loss 5.23016, smoothed loss 5.88745, grad norm 3.54236, param norm 99.78849
INFO:root:epoch 4, iter 3280, loss 6.13954, smoothed loss 5.87630, grad norm 3.78778, param norm 99.85165
INFO:root:epoch 4, iter 3285, loss 6.42083, smoothed loss 5.89602, grad norm 3.92137, param norm 99.91609
INFO:root:epoch 4, iter 3290, loss 6.19250, smoothed loss 5.88608, grad norm 3.86928, param norm 99.97617
INFO:root:epoch 4, iter 3295, loss 5.17777, smoothed loss 5.88581, grad norm 4.45040, param norm 100.03636
INFO:root:epoch 4, iter 3300, loss 5.87567, smoothed loss 5.87852, grad norm 4.24160, param norm 100.09551
INFO:root:epoch 4, iter 3305, loss 6.57010, smoothed loss 5.87802, grad norm 6.04668, param norm 100.14799
INFO:root:epoch 4, iter 3310, loss 6.11077, smoothed loss 5.86762, grad norm 5.14331, param norm 100.20495
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3315, loss 6.02455, smoothed loss 5.86413, grad norm 4.56778, param norm 100.26353
INFO:root:epoch 4, iter 3320, loss 5.87581, smoothed loss 5.86324, grad norm 5.42786, param norm 100.31773
INFO:root:epoch 4, iter 3325, loss 6.06510, smoothed loss 5.86753, grad norm 3.99207, param norm 100.36964
INFO:root:epoch 4, iter 3330, loss 6.05681, smoothed loss 5.86604, grad norm 7.29900, param norm 100.42314
INFO:root:epoch 4, iter 3335, loss 6.49218, smoothed loss 5.86857, grad norm 5.11736, param norm 100.48390
INFO:root:epoch 4, iter 3340, loss 5.91208, smoothed loss 5.88330, grad norm 3.65837, param norm 100.54179
INFO:root:epoch 4, iter 3345, loss 6.27854, smoothed loss 5.89067, grad norm 3.84199, param norm 100.60174
INFO:root:epoch 4, iter 3350, loss 6.25845, smoothed loss 5.88921, grad norm 4.86930, param norm 100.66251
INFO:root:epoch 4, iter 3355, loss 5.51214, smoothed loss 5.87590, grad norm 4.26361, param norm 100.71812
INFO:root:epoch 4, iter 3360, loss 6.77545, smoothed loss 5.88270, grad norm 5.77447, param norm 100.77563
INFO:root:epoch 4, iter 3365, loss 4.97199, smoothed loss 5.87102, grad norm 3.94708, param norm 100.83973
INFO:root:epoch 4, iter 3370, loss 5.88196, smoothed loss 5.87518, grad norm 4.10341, param norm 100.90436
INFO:root:epoch 4, iter 3375, loss 5.57356, smoothed loss 5.87122, grad norm 4.67101, param norm 100.96951
INFO:root:epoch 4, iter 3380, loss 6.04850, smoothed loss 5.87663, grad norm 4.15803, param norm 101.03459
INFO:root:epoch 4, iter 3385, loss 4.96273, smoothed loss 5.87350, grad norm 3.61185, param norm 101.09577
INFO:root:epoch 4, iter 3390, loss 5.91893, smoothed loss 5.87202, grad norm 4.26900, param norm 101.16106
INFO:root:epoch 4, iter 3395, loss 5.50859, smoothed loss 5.87765, grad norm 4.86216, param norm 101.22279
INFO:root:epoch 4, iter 3400, loss 5.95252, smoothed loss 5.87505, grad norm 4.27230, param norm 101.28188
INFO:root:epoch 4, iter 3405, loss 6.16946, smoothed loss 5.86774, grad norm 4.86152, param norm 101.33932
INFO:root:epoch 4, iter 3410, loss 5.66614, smoothed loss 5.86053, grad norm 4.66515, param norm 101.39947
INFO:root:epoch 4, iter 3415, loss 6.45322, smoothed loss 5.85220, grad norm 4.06814, param norm 101.46294
INFO:root:epoch 4, iter 3420, loss 6.04529, smoothed loss 5.84820, grad norm 5.09190, param norm 101.52509
INFO:root:epoch 4, iter 3425, loss 5.30598, smoothed loss 5.84188, grad norm 3.54762, param norm 101.58973
INFO:root:epoch 4, iter 3430, loss 6.27130, smoothed loss 5.84949, grad norm 4.45117, param norm 101.64993
INFO:root:epoch 4, iter 3435, loss 6.31226, smoothed loss 5.84530, grad norm 5.17393, param norm 101.71078
INFO:root:epoch 4, iter 3440, loss 5.94450, smoothed loss 5.84332, grad norm 4.74006, param norm 101.76606
INFO:root:epoch 4, iter 3445, loss 5.52538, smoothed loss 5.84002, grad norm 4.56703, param norm 101.82139
INFO:root:epoch 4, iter 3450, loss 6.55627, smoothed loss 5.83031, grad norm 5.00320, param norm 101.87899
INFO:root:epoch 4, iter 3455, loss 5.19317, smoothed loss 5.81615, grad norm 4.23762, param norm 101.93812
INFO:root:epoch 4, iter 3460, loss 5.95550, smoothed loss 5.81248, grad norm 4.52801, param norm 101.99701
INFO:root:epoch 4, iter 3465, loss 6.74066, smoothed loss 5.80947, grad norm 5.22832, param norm 102.04969
INFO:root:epoch 4, iter 3470, loss 6.28799, smoothed loss 5.80758, grad norm 5.22613, param norm 102.10287
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3475, loss 5.84270, smoothed loss 5.80567, grad norm 4.84150, param norm 102.15907
INFO:root:epoch 4, iter 3480, loss 6.07156, smoothed loss 5.81888, grad norm 3.53648, param norm 102.21633
INFO:root:epoch 4, iter 3485, loss 5.41804, smoothed loss 5.81174, grad norm 3.84587, param norm 102.26884
INFO:root:epoch 4, iter 3490, loss 4.86190, smoothed loss 5.80058, grad norm 4.06274, param norm 102.32561
INFO:root:epoch 4, iter 3495, loss 5.21743, smoothed loss 5.79541, grad norm 4.11487, param norm 102.38194
INFO:root:epoch 4, iter 3500, loss 6.19507, smoothed loss 5.79250, grad norm 4.74290, param norm 102.43875
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 4, Iter 3500, dev loss: 5.715061
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.31375
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.20300
INFO:root:Epoch 4, Iter 3500, Train F1 score: 0.313749, Train EM score: 0.203000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.26812
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.18671
INFO:root:Epoch 4, Iter 3500, Dev F1 score: 0.268123, Dev EM score: 0.186710
INFO:root:End of epoch 4
INFO:root:epoch 4, iter 3505, loss 5.95899, smoothed loss 5.77634, grad norm 7.17690, param norm 102.49377
INFO:root:epoch 4, iter 3510, loss 6.22364, smoothed loss 5.75772, grad norm 5.33949, param norm 102.55695
INFO:root:epoch 4, iter 3515, loss 5.68684, smoothed loss 5.75000, grad norm 5.71335, param norm 102.61974
INFO:root:epoch 4, iter 3520, loss 6.00107, smoothed loss 5.73699, grad norm 4.84249, param norm 102.68050
INFO:root:epoch 4, iter 3525, loss 5.74190, smoothed loss 5.73814, grad norm 4.86776, param norm 102.73904
INFO:root:epoch 4, iter 3530, loss 5.84894, smoothed loss 5.74320, grad norm 5.20055, param norm 102.80051
INFO:root:epoch 4, iter 3535, loss 6.77476, smoothed loss 5.75680, grad norm 5.15081, param norm 102.86620
INFO:root:epoch 4, iter 3540, loss 5.53607, smoothed loss 5.75220, grad norm 3.96150, param norm 102.93377
INFO:root:epoch 4, iter 3545, loss 5.62039, smoothed loss 5.73501, grad norm 4.27566, param norm 103.00134
INFO:root:epoch 4, iter 3550, loss 5.98624, smoothed loss 5.75748, grad norm 3.87810, param norm 103.06295
INFO:root:epoch 4, iter 3555, loss 5.53971, smoothed loss 5.76016, grad norm 4.40343, param norm 103.11889
INFO:root:epoch 4, iter 3560, loss 6.57391, smoothed loss 5.78829, grad norm 4.02546, param norm 103.17507
INFO:root:epoch 4, iter 3565, loss 5.78738, smoothed loss 5.79258, grad norm 4.49742, param norm 103.23196
INFO:root:epoch 4, iter 3570, loss 5.48064, smoothed loss 5.79372, grad norm 4.43080, param norm 103.28951
INFO:root:epoch 4, iter 3575, loss 5.48692, smoothed loss 5.78882, grad norm 4.74616, param norm 103.34225
INFO:root:epoch 4, iter 3580, loss 6.34356, smoothed loss 5.78791, grad norm 4.85932, param norm 103.39760
INFO:root:epoch 4, iter 3585, loss 5.37941, smoothed loss 5.78929, grad norm 4.35055, param norm 103.45769
INFO:root:epoch 4, iter 3590, loss 5.54179, smoothed loss 5.77686, grad norm 4.65198, param norm 103.51582
INFO:root:epoch 4, iter 3595, loss 6.41197, smoothed loss 5.78923, grad norm 4.55715, param norm 103.57065
INFO:root:epoch 4, iter 3600, loss 5.54907, smoothed loss 5.79448, grad norm 4.06115, param norm 103.61835
INFO:root:epoch 4, iter 3605, loss 6.45737, smoothed loss 5.81069, grad norm 5.13451, param norm 103.67073
INFO:root:epoch 4, iter 3610, loss 5.33218, smoothed loss 5.79919, grad norm 4.06072, param norm 103.72324
INFO:root:epoch 4, iter 3615, loss 5.80492, smoothed loss 5.79620, grad norm 4.45851, param norm 103.77557
INFO:root:epoch 4, iter 3620, loss 6.24044, smoothed loss 5.79797, grad norm 4.27131, param norm 103.82558
INFO:root:epoch 4, iter 3625, loss 5.86122, smoothed loss 5.79205, grad norm 5.60065, param norm 103.87695
INFO:root:epoch 4, iter 3630, loss 5.21095, smoothed loss 5.79462, grad norm 4.18156, param norm 103.92944
Adding batches start...
Added  144  batches
INFO:root:epoch 4, iter 3635, loss 5.10976, smoothed loss 5.79395, grad norm 3.90163, param norm 103.98471
INFO:root:epoch 4, iter 3640, loss 6.08557, smoothed loss 5.79470, grad norm 4.87695, param norm 104.04018
INFO:root:epoch 4, iter 3645, loss 6.03899, smoothed loss 5.78444, grad norm 4.98726, param norm 104.09523
INFO:root:epoch 4, iter 3650, loss 4.99416, smoothed loss 5.76369, grad norm 3.91414, param norm 104.14812
INFO:root:epoch 4, iter 3655, loss 5.84118, smoothed loss 5.76228, grad norm 4.11425, param norm 104.20073
INFO:root:epoch 4, iter 3660, loss 5.90701, smoothed loss 5.76526, grad norm 4.81362, param norm 104.25401
INFO:root:epoch 4, iter 3665, loss 6.31413, smoothed loss 5.76249, grad norm 4.86902, param norm 104.30759
INFO:root:epoch 4, iter 3670, loss 6.33533, smoothed loss 5.77268, grad norm 4.00737, param norm 104.36366
INFO:root:epoch 4, iter 3675, loss 5.72360, smoothed loss 5.78915, grad norm 3.90645, param norm 104.42056
INFO:root:epoch 4, iter 3680, loss 5.45826, smoothed loss 5.78401, grad norm 3.56189, param norm 104.47818
INFO:root:epoch 4, iter 3685, loss 5.85831, smoothed loss 5.78316, grad norm 4.77249, param norm 104.53143
INFO:root:epoch 4, iter 3690, loss 5.59002, smoothed loss 5.78271, grad norm 4.32579, param norm 104.58214
INFO:root:epoch 4, iter 3695, loss 5.40956, smoothed loss 5.77336, grad norm 4.68457, param norm 104.63651
INFO:root:epoch 4, iter 3700, loss 5.70771, smoothed loss 5.77407, grad norm 5.67642, param norm 104.68756
INFO:root:epoch 4, iter 3705, loss 5.84227, smoothed loss 5.76927, grad norm 4.55803, param norm 104.74430
INFO:root:epoch 4, iter 3710, loss 5.93192, smoothed loss 5.77239, grad norm 4.62579, param norm 104.80473
INFO:root:epoch 4, iter 3715, loss 5.73552, smoothed loss 5.76629, grad norm 5.30865, param norm 104.86365
INFO:root:epoch 4, iter 3720, loss 6.57399, smoothed loss 5.77393, grad norm 4.43514, param norm 104.92248
INFO:root:epoch 4, iter 3725, loss 5.89267, smoothed loss 5.77713, grad norm 4.50608, param norm 104.97642
INFO:root:epoch 4, iter 3730, loss 6.20128, smoothed loss 5.77176, grad norm 5.99325, param norm 105.03307
INFO:root:epoch 4, iter 3735, loss 5.31994, smoothed loss 5.78043, grad norm 3.81652, param norm 105.09084
INFO:root:epoch 4, iter 3740, loss 5.85835, smoothed loss 5.78146, grad norm 4.02996, param norm 105.14842
INFO:root:epoch 4, iter 3745, loss 5.84039, smoothed loss 5.78846, grad norm 4.26230, param norm 105.20228
INFO:root:epoch 4, iter 3750, loss 6.13618, smoothed loss 5.79193, grad norm 5.23734, param norm 105.25513
INFO:root:epoch 4, iter 3755, loss 5.39252, smoothed loss 5.78425, grad norm 4.32913, param norm 105.31039
INFO:root:epoch 4, iter 3760, loss 5.19343, smoothed loss 5.77229, grad norm 4.08985, param norm 105.36709
INFO:root:epoch 4, iter 3765, loss 5.02509, smoothed loss 5.76213, grad norm 3.49517, param norm 105.42995
INFO:root:epoch 4, iter 3770, loss 5.29505, smoothed loss 5.74865, grad norm 4.10633, param norm 105.49348
INFO:root:epoch 4, iter 3775, loss 5.75972, smoothed loss 5.73731, grad norm 5.02314, param norm 105.55309
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 3780, loss 5.68926, smoothed loss 5.73062, grad norm 6.29456, param norm 105.61687
INFO:root:epoch 5, iter 3785, loss 5.89431, smoothed loss 5.71541, grad norm 5.21271, param norm 105.68203
INFO:root:epoch 5, iter 3790, loss 5.94608, smoothed loss 5.71558, grad norm 5.92227, param norm 105.73932
INFO:root:epoch 5, iter 3795, loss 5.46578, smoothed loss 5.72280, grad norm 5.48018, param norm 105.78790
INFO:root:epoch 5, iter 3800, loss 5.16954, smoothed loss 5.72098, grad norm 4.71733, param norm 105.83896
INFO:root:epoch 5, iter 3805, loss 5.78097, smoothed loss 5.72347, grad norm 4.98266, param norm 105.89912
INFO:root:epoch 5, iter 3810, loss 5.48309, smoothed loss 5.71872, grad norm 4.14780, param norm 105.95967
INFO:root:epoch 5, iter 3815, loss 5.65130, smoothed loss 5.71538, grad norm 5.22395, param norm 106.01555
INFO:root:epoch 5, iter 3820, loss 5.77890, smoothed loss 5.70466, grad norm 4.43478, param norm 106.07275
INFO:root:epoch 5, iter 3825, loss 6.50743, smoothed loss 5.71334, grad norm 4.69285, param norm 106.13068
INFO:root:epoch 5, iter 3830, loss 5.99166, smoothed loss 5.72228, grad norm 3.89671, param norm 106.18901
INFO:root:epoch 5, iter 3835, loss 5.92725, smoothed loss 5.71940, grad norm 4.00643, param norm 106.24660
INFO:root:epoch 5, iter 3840, loss 5.63453, smoothed loss 5.71014, grad norm 4.58314, param norm 106.30738
INFO:root:epoch 5, iter 3845, loss 5.23366, smoothed loss 5.69775, grad norm 4.35552, param norm 106.36388
INFO:root:epoch 5, iter 3850, loss 5.69085, smoothed loss 5.70628, grad norm 4.60589, param norm 106.41626
INFO:root:epoch 5, iter 3855, loss 4.93573, smoothed loss 5.69510, grad norm 4.77284, param norm 106.46717
INFO:root:epoch 5, iter 3860, loss 5.22231, smoothed loss 5.67832, grad norm 4.23472, param norm 106.52164
INFO:root:epoch 5, iter 3865, loss 5.79658, smoothed loss 5.66445, grad norm 4.37016, param norm 106.57938
INFO:root:epoch 5, iter 3870, loss 5.70018, smoothed loss 5.66036, grad norm 4.92148, param norm 106.63561
INFO:root:epoch 5, iter 3875, loss 6.26253, smoothed loss 5.66859, grad norm 4.99125, param norm 106.68836
INFO:root:epoch 5, iter 3880, loss 5.83172, smoothed loss 5.66356, grad norm 4.71188, param norm 106.74186
INFO:root:epoch 5, iter 3885, loss 5.80950, smoothed loss 5.65813, grad norm 4.68697, param norm 106.79755
INFO:root:epoch 5, iter 3890, loss 5.69980, smoothed loss 5.65807, grad norm 4.48252, param norm 106.85590
INFO:root:epoch 5, iter 3895, loss 4.70828, smoothed loss 5.64040, grad norm 4.38490, param norm 106.91205
INFO:root:epoch 5, iter 3900, loss 5.98383, smoothed loss 5.64364, grad norm 4.58730, param norm 106.96499
INFO:root:epoch 5, iter 3905, loss 5.17171, smoothed loss 5.62643, grad norm 5.32321, param norm 107.01732
INFO:root:epoch 5, iter 3910, loss 5.14658, smoothed loss 5.60932, grad norm 4.93548, param norm 107.07306
INFO:root:epoch 5, iter 3915, loss 5.66174, smoothed loss 5.59443, grad norm 5.05596, param norm 107.13004
INFO:root:epoch 5, iter 3920, loss 6.30863, smoothed loss 5.59484, grad norm 4.68744, param norm 107.18778
INFO:root:epoch 5, iter 3925, loss 5.27768, smoothed loss 5.59129, grad norm 4.26846, param norm 107.24475
INFO:root:epoch 5, iter 3930, loss 5.89483, smoothed loss 5.59057, grad norm 4.43235, param norm 107.30100
INFO:root:epoch 5, iter 3935, loss 5.38864, smoothed loss 5.59692, grad norm 4.68530, param norm 107.35593
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 3940, loss 5.04386, smoothed loss 5.59583, grad norm 4.63612, param norm 107.40649
INFO:root:epoch 5, iter 3945, loss 5.38353, smoothed loss 5.61270, grad norm 4.17576, param norm 107.45692
INFO:root:epoch 5, iter 3950, loss 5.58011, smoothed loss 5.59968, grad norm 4.70560, param norm 107.50989
INFO:root:epoch 5, iter 3955, loss 5.53342, smoothed loss 5.60708, grad norm 4.70650, param norm 107.56544
INFO:root:epoch 5, iter 3960, loss 5.44114, smoothed loss 5.60471, grad norm 4.29174, param norm 107.62177
INFO:root:epoch 5, iter 3965, loss 5.86607, smoothed loss 5.59798, grad norm 4.86155, param norm 107.67633
INFO:root:epoch 5, iter 3970, loss 5.58956, smoothed loss 5.59795, grad norm 4.20317, param norm 107.73181
INFO:root:epoch 5, iter 3975, loss 5.22805, smoothed loss 5.59246, grad norm 4.41424, param norm 107.78841
INFO:root:epoch 5, iter 3980, loss 5.60058, smoothed loss 5.59865, grad norm 4.29237, param norm 107.84158
INFO:root:epoch 5, iter 3985, loss 6.05327, smoothed loss 5.59581, grad norm 4.18559, param norm 107.89758
INFO:root:epoch 5, iter 3990, loss 5.66409, smoothed loss 5.60447, grad norm 3.97071, param norm 107.94997
INFO:root:epoch 5, iter 3995, loss 5.12985, smoothed loss 5.59678, grad norm 4.48427, param norm 107.99823
INFO:root:epoch 5, iter 4000, loss 6.07556, smoothed loss 5.61467, grad norm 4.14834, param norm 108.04705
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 5, Iter 4000, dev loss: 5.417529
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.37913
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.25700
INFO:root:Epoch 5, Iter 4000, Train F1 score: 0.379130, Train EM score: 0.257000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.31318
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.22310
INFO:root:Epoch 5, Iter 4000, Dev F1 score: 0.313176, Dev EM score: 0.223096
INFO:root:End of epoch 5
INFO:root:epoch 5, iter 4005, loss 4.91429, smoothed loss 5.62221, grad norm 4.73628, param norm 108.09205
INFO:root:epoch 5, iter 4010, loss 5.67303, smoothed loss 5.62611, grad norm 4.18282, param norm 108.14149
INFO:root:epoch 5, iter 4015, loss 4.99532, smoothed loss 5.61877, grad norm 5.01167, param norm 108.19003
INFO:root:epoch 5, iter 4020, loss 5.36510, smoothed loss 5.62663, grad norm 4.31153, param norm 108.23943
INFO:root:epoch 5, iter 4025, loss 4.92148, smoothed loss 5.60565, grad norm 4.56842, param norm 108.28873
INFO:root:epoch 5, iter 4030, loss 4.97489, smoothed loss 5.58686, grad norm 4.76094, param norm 108.34316
INFO:root:epoch 5, iter 4035, loss 5.69803, smoothed loss 5.57548, grad norm 4.87368, param norm 108.40388
INFO:root:epoch 5, iter 4040, loss 5.18025, smoothed loss 5.56673, grad norm 4.80686, param norm 108.46263
INFO:root:epoch 5, iter 4045, loss 5.22407, smoothed loss 5.57190, grad norm 4.18524, param norm 108.51554
INFO:root:epoch 5, iter 4050, loss 6.16716, smoothed loss 5.57583, grad norm 5.41053, param norm 108.56669
INFO:root:epoch 5, iter 4055, loss 5.32106, smoothed loss 5.56341, grad norm 3.79898, param norm 108.62431
INFO:root:epoch 5, iter 4060, loss 5.11261, smoothed loss 5.57102, grad norm 4.36297, param norm 108.68210
INFO:root:epoch 5, iter 4065, loss 5.45217, smoothed loss 5.57078, grad norm 4.65367, param norm 108.73466
INFO:root:epoch 5, iter 4070, loss 5.67536, smoothed loss 5.56288, grad norm 4.93508, param norm 108.78203
INFO:root:epoch 5, iter 4075, loss 5.95819, smoothed loss 5.56104, grad norm 4.87368, param norm 108.83183
INFO:root:epoch 5, iter 4080, loss 5.04864, smoothed loss 5.55442, grad norm 4.39463, param norm 108.88105
INFO:root:epoch 5, iter 4085, loss 5.81615, smoothed loss 5.54682, grad norm 5.72806, param norm 108.92921
INFO:root:epoch 5, iter 4090, loss 4.92782, smoothed loss 5.53711, grad norm 4.17320, param norm 108.97749
INFO:root:epoch 5, iter 4095, loss 5.22574, smoothed loss 5.52972, grad norm 4.55194, param norm 109.03019
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4100, loss 5.88477, smoothed loss 5.52481, grad norm 5.30747, param norm 109.08523
INFO:root:epoch 5, iter 4105, loss 5.33432, smoothed loss 5.51749, grad norm 6.33761, param norm 109.14175
INFO:root:epoch 5, iter 4110, loss 6.10559, smoothed loss 5.53463, grad norm 5.55836, param norm 109.19785
INFO:root:epoch 5, iter 4115, loss 5.22122, smoothed loss 5.51087, grad norm 4.63283, param norm 109.25365
INFO:root:epoch 5, iter 4120, loss 5.45790, smoothed loss 5.51449, grad norm 4.96994, param norm 109.30915
INFO:root:epoch 5, iter 4125, loss 5.29933, smoothed loss 5.50861, grad norm 3.83767, param norm 109.35963
INFO:root:epoch 5, iter 4130, loss 5.79200, smoothed loss 5.50732, grad norm 6.05857, param norm 109.40961
INFO:root:epoch 5, iter 4135, loss 6.17358, smoothed loss 5.52593, grad norm 6.39333, param norm 109.46484
INFO:root:epoch 5, iter 4140, loss 5.34253, smoothed loss 5.51441, grad norm 4.23145, param norm 109.52119
INFO:root:epoch 5, iter 4145, loss 5.30426, smoothed loss 5.51280, grad norm 4.25910, param norm 109.57620
INFO:root:epoch 5, iter 4150, loss 5.25567, smoothed loss 5.49559, grad norm 4.14755, param norm 109.62946
INFO:root:epoch 5, iter 4155, loss 5.37786, smoothed loss 5.48688, grad norm 5.34868, param norm 109.68317
INFO:root:epoch 5, iter 4160, loss 5.41657, smoothed loss 5.48623, grad norm 4.51749, param norm 109.73763
INFO:root:epoch 5, iter 4165, loss 5.88665, smoothed loss 5.47745, grad norm 4.65816, param norm 109.78886
INFO:root:epoch 5, iter 4170, loss 5.30351, smoothed loss 5.46716, grad norm 4.18167, param norm 109.84148
INFO:root:epoch 5, iter 4175, loss 5.28572, smoothed loss 5.46925, grad norm 4.30127, param norm 109.89333
INFO:root:epoch 5, iter 4180, loss 5.11021, smoothed loss 5.47583, grad norm 4.32705, param norm 109.94331
INFO:root:epoch 5, iter 4185, loss 4.90717, smoothed loss 5.46735, grad norm 4.17145, param norm 109.99599
INFO:root:epoch 5, iter 4190, loss 4.91272, smoothed loss 5.46944, grad norm 5.22734, param norm 110.04627
INFO:root:epoch 5, iter 4195, loss 6.19438, smoothed loss 5.46880, grad norm 4.42374, param norm 110.09522
INFO:root:epoch 5, iter 4200, loss 5.60028, smoothed loss 5.45633, grad norm 4.91290, param norm 110.14576
INFO:root:epoch 5, iter 4205, loss 5.46250, smoothed loss 5.45122, grad norm 5.84050, param norm 110.19575
INFO:root:epoch 5, iter 4210, loss 5.72543, smoothed loss 5.45937, grad norm 5.57781, param norm 110.24757
INFO:root:epoch 5, iter 4215, loss 5.29954, smoothed loss 5.46331, grad norm 4.51836, param norm 110.29932
INFO:root:epoch 5, iter 4220, loss 4.29481, smoothed loss 5.45493, grad norm 4.83382, param norm 110.35077
INFO:root:epoch 5, iter 4225, loss 5.43557, smoothed loss 5.45135, grad norm 5.51951, param norm 110.40421
INFO:root:epoch 5, iter 4230, loss 5.08234, smoothed loss 5.44264, grad norm 6.34769, param norm 110.45743
INFO:root:epoch 5, iter 4235, loss 5.27622, smoothed loss 5.43211, grad norm 5.16500, param norm 110.51421
INFO:root:epoch 5, iter 4240, loss 5.47674, smoothed loss 5.44066, grad norm 5.80311, param norm 110.56709
INFO:root:epoch 5, iter 4245, loss 5.34562, smoothed loss 5.42823, grad norm 4.35871, param norm 110.61693
INFO:root:epoch 5, iter 4250, loss 4.94261, smoothed loss 5.41494, grad norm 3.90806, param norm 110.66463
INFO:root:epoch 5, iter 4255, loss 5.42234, smoothed loss 5.41386, grad norm 4.49442, param norm 110.71637
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4260, loss 5.06835, smoothed loss 5.38998, grad norm 4.80905, param norm 110.76662
INFO:root:epoch 5, iter 4265, loss 5.14966, smoothed loss 5.38432, grad norm 6.43206, param norm 110.81962
INFO:root:epoch 5, iter 4270, loss 5.59811, smoothed loss 5.37273, grad norm 4.94186, param norm 110.87240
INFO:root:epoch 5, iter 4275, loss 5.48013, smoothed loss 5.38192, grad norm 5.89245, param norm 110.92278
INFO:root:epoch 5, iter 4280, loss 5.28013, smoothed loss 5.38116, grad norm 6.09091, param norm 110.97375
INFO:root:epoch 5, iter 4285, loss 6.06262, smoothed loss 5.39755, grad norm 5.45684, param norm 111.02027
INFO:root:epoch 5, iter 4290, loss 5.72059, smoothed loss 5.40736, grad norm 5.11913, param norm 111.06698
INFO:root:epoch 5, iter 4295, loss 5.51989, smoothed loss 5.42144, grad norm 4.33860, param norm 111.11468
INFO:root:epoch 5, iter 4300, loss 5.13617, smoothed loss 5.42632, grad norm 4.62225, param norm 111.16034
INFO:root:epoch 5, iter 4305, loss 5.27558, smoothed loss 5.43421, grad norm 4.20013, param norm 111.20300
INFO:root:epoch 5, iter 4310, loss 6.19193, smoothed loss 5.42846, grad norm 5.07983, param norm 111.24842
INFO:root:epoch 5, iter 4315, loss 4.88923, smoothed loss 5.41294, grad norm 4.27308, param norm 111.29598
INFO:root:epoch 5, iter 4320, loss 5.42126, smoothed loss 5.40658, grad norm 5.14791, param norm 111.34566
INFO:root:epoch 5, iter 4325, loss 5.11922, smoothed loss 5.40655, grad norm 4.98321, param norm 111.39556
INFO:root:epoch 5, iter 4330, loss 5.35074, smoothed loss 5.40987, grad norm 5.83379, param norm 111.44608
INFO:root:epoch 5, iter 4335, loss 5.76796, smoothed loss 5.41341, grad norm 5.56792, param norm 111.50034
INFO:root:epoch 5, iter 4340, loss 5.34292, smoothed loss 5.42263, grad norm 4.34880, param norm 111.55442
INFO:root:epoch 5, iter 4345, loss 6.26618, smoothed loss 5.42405, grad norm 4.57233, param norm 111.61146
INFO:root:epoch 5, iter 4350, loss 5.67344, smoothed loss 5.41743, grad norm 5.36475, param norm 111.66293
INFO:root:epoch 5, iter 4355, loss 5.21714, smoothed loss 5.41981, grad norm 5.51040, param norm 111.71445
INFO:root:epoch 5, iter 4360, loss 4.27136, smoothed loss 5.40344, grad norm 4.23540, param norm 111.76926
INFO:root:epoch 5, iter 4365, loss 5.57397, smoothed loss 5.40532, grad norm 5.90130, param norm 111.82751
INFO:root:epoch 5, iter 4370, loss 5.61495, smoothed loss 5.41119, grad norm 4.76993, param norm 111.88396
INFO:root:epoch 5, iter 4375, loss 5.24409, smoothed loss 5.41146, grad norm 4.91592, param norm 111.94011
INFO:root:epoch 5, iter 4380, loss 5.44492, smoothed loss 5.40821, grad norm 4.85707, param norm 112.00209
INFO:root:epoch 5, iter 4385, loss 5.69209, smoothed loss 5.40940, grad norm 6.55495, param norm 112.05683
INFO:root:epoch 5, iter 4390, loss 5.37637, smoothed loss 5.39268, grad norm 5.37422, param norm 112.11211
INFO:root:epoch 5, iter 4395, loss 5.67319, smoothed loss 5.39753, grad norm 5.42012, param norm 112.16496
INFO:root:epoch 5, iter 4400, loss 5.61263, smoothed loss 5.39669, grad norm 4.89714, param norm 112.21226
INFO:root:epoch 5, iter 4405, loss 4.37103, smoothed loss 5.39213, grad norm 4.56171, param norm 112.25922
INFO:root:epoch 5, iter 4410, loss 5.50860, smoothed loss 5.38239, grad norm 5.17586, param norm 112.31018
INFO:root:epoch 5, iter 4415, loss 4.55558, smoothed loss 5.36768, grad norm 5.06740, param norm 112.36134
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4420, loss 5.28151, smoothed loss 5.37088, grad norm 5.76955, param norm 112.41331
INFO:root:epoch 5, iter 4425, loss 5.09463, smoothed loss 5.37306, grad norm 4.63013, param norm 112.46395
INFO:root:epoch 5, iter 4430, loss 5.12865, smoothed loss 5.36096, grad norm 5.39233, param norm 112.51281
INFO:root:epoch 5, iter 4435, loss 4.90222, smoothed loss 5.36160, grad norm 4.68851, param norm 112.56507
INFO:root:epoch 5, iter 4440, loss 5.85221, smoothed loss 5.37949, grad norm 5.49711, param norm 112.61444
INFO:root:epoch 5, iter 4445, loss 6.05943, smoothed loss 5.38675, grad norm 5.05933, param norm 112.66454
INFO:root:epoch 5, iter 4450, loss 4.57952, smoothed loss 5.38378, grad norm 4.40282, param norm 112.71560
INFO:root:epoch 5, iter 4455, loss 5.50947, smoothed loss 5.39948, grad norm 5.15524, param norm 112.76584
INFO:root:epoch 5, iter 4460, loss 4.97829, smoothed loss 5.38850, grad norm 5.12962, param norm 112.81519
INFO:root:epoch 5, iter 4465, loss 4.95884, smoothed loss 5.37475, grad norm 5.59481, param norm 112.86707
INFO:root:epoch 5, iter 4470, loss 5.11175, smoothed loss 5.37379, grad norm 6.14896, param norm 112.92066
INFO:root:epoch 5, iter 4475, loss 5.72543, smoothed loss 5.39120, grad norm 5.08234, param norm 112.97114
INFO:root:epoch 5, iter 4480, loss 4.74742, smoothed loss 5.36872, grad norm 4.07279, param norm 113.02055
INFO:root:epoch 5, iter 4485, loss 5.96116, smoothed loss 5.37789, grad norm 5.38108, param norm 113.07355
INFO:root:epoch 5, iter 4490, loss 5.14783, smoothed loss 5.37728, grad norm 4.97769, param norm 113.12341
INFO:root:epoch 5, iter 4495, loss 5.46723, smoothed loss 5.38557, grad norm 5.64159, param norm 113.17245
INFO:root:epoch 5, iter 4500, loss 5.43595, smoothed loss 5.39043, grad norm 5.57484, param norm 113.22117
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 5, Iter 4500, dev loss: 5.019359
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.39944
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.31000
INFO:root:Epoch 5, Iter 4500, Train F1 score: 0.399440, Train EM score: 0.310000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.36315
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.26033
INFO:root:Epoch 5, Iter 4500, Dev F1 score: 0.363152, Dev EM score: 0.260326
INFO:root:End of epoch 5
INFO:root:epoch 5, iter 4505, loss 5.05828, smoothed loss 5.39014, grad norm 5.64461, param norm 113.27068
INFO:root:epoch 5, iter 4510, loss 5.89611, smoothed loss 5.38545, grad norm 5.34398, param norm 113.32296
INFO:root:epoch 5, iter 4515, loss 5.43514, smoothed loss 5.37828, grad norm 4.76497, param norm 113.37605
INFO:root:epoch 5, iter 4520, loss 4.41410, smoothed loss 5.36360, grad norm 4.87919, param norm 113.42838
INFO:root:epoch 5, iter 4525, loss 5.51103, smoothed loss 5.35043, grad norm 5.12548, param norm 113.48316
INFO:root:epoch 5, iter 4530, loss 5.50285, smoothed loss 5.35118, grad norm 5.04413, param norm 113.53796
INFO:root:epoch 5, iter 4535, loss 4.98644, smoothed loss 5.34795, grad norm 4.00738, param norm 113.59814
INFO:root:epoch 5, iter 4540, loss 5.11213, smoothed loss 5.33970, grad norm 4.72332, param norm 113.66054
INFO:root:epoch 5, iter 4545, loss 5.23261, smoothed loss 5.33267, grad norm 5.10944, param norm 113.71801
INFO:root:epoch 5, iter 4550, loss 5.62981, smoothed loss 5.32610, grad norm 4.51776, param norm 113.77057
INFO:root:epoch 5, iter 4555, loss 4.90716, smoothed loss 5.32139, grad norm 5.70689, param norm 113.82281
INFO:root:epoch 5, iter 4560, loss 5.15182, smoothed loss 5.30629, grad norm 6.32496, param norm 113.87710
INFO:root:epoch 5, iter 4565, loss 4.83713, smoothed loss 5.29053, grad norm 4.81283, param norm 113.92986
INFO:root:epoch 5, iter 4570, loss 5.26017, smoothed loss 5.29247, grad norm 4.80849, param norm 113.98186
INFO:root:epoch 5, iter 4575, loss 4.43707, smoothed loss 5.28582, grad norm 4.58478, param norm 114.03429
Adding batches start...
Added  144  batches
INFO:root:epoch 5, iter 4580, loss 4.54246, smoothed loss 5.28211, grad norm 5.37531, param norm 114.08432
INFO:root:epoch 5, iter 4585, loss 5.30513, smoothed loss 5.28460, grad norm 4.55043, param norm 114.14014
INFO:root:epoch 5, iter 4590, loss 5.70516, smoothed loss 5.28608, grad norm 4.61967, param norm 114.19855
INFO:root:epoch 5, iter 4595, loss 5.11239, smoothed loss 5.27421, grad norm 4.67485, param norm 114.25256
INFO:root:epoch 5, iter 4600, loss 5.40120, smoothed loss 5.27038, grad norm 5.49458, param norm 114.30132
INFO:root:epoch 5, iter 4605, loss 5.21945, smoothed loss 5.28580, grad norm 6.15456, param norm 114.34676
INFO:root:epoch 5, iter 4610, loss 5.34488, smoothed loss 5.28354, grad norm 4.35405, param norm 114.38975
INFO:root:epoch 5, iter 4615, loss 5.68468, smoothed loss 5.28065, grad norm 5.01311, param norm 114.43920
INFO:root:epoch 5, iter 4620, loss 5.56931, smoothed loss 5.28676, grad norm 5.12239, param norm 114.48933
INFO:root:epoch 5, iter 4625, loss 5.00946, smoothed loss 5.27061, grad norm 6.38351, param norm 114.53877
INFO:root:epoch 5, iter 4630, loss 4.95818, smoothed loss 5.26827, grad norm 4.79556, param norm 114.59052
INFO:root:epoch 5, iter 4635, loss 5.08591, smoothed loss 5.26695, grad norm 6.10911, param norm 114.64310
INFO:root:epoch 5, iter 4640, loss 4.85061, smoothed loss 5.26434, grad norm 5.08315, param norm 114.69128
INFO:root:epoch 5, iter 4645, loss 5.73887, smoothed loss 5.26470, grad norm 5.45458, param norm 114.73637
INFO:root:epoch 5, iter 4650, loss 5.76117, smoothed loss 5.28125, grad norm 5.39045, param norm 114.78550
INFO:root:epoch 5, iter 4655, loss 4.79906, smoothed loss 5.25997, grad norm 4.89140, param norm 114.84096
INFO:root:epoch 5, iter 4660, loss 5.77787, smoothed loss 5.26408, grad norm 5.94799, param norm 114.89640
INFO:root:epoch 5, iter 4665, loss 4.95556, smoothed loss 5.26049, grad norm 5.46582, param norm 114.95200
INFO:root:epoch 5, iter 4670, loss 5.40993, smoothed loss 5.24712, grad norm 6.07323, param norm 115.00719
INFO:root:epoch 5, iter 4675, loss 4.86079, smoothed loss 5.24622, grad norm 5.42155, param norm 115.06369
INFO:root:epoch 5, iter 4680, loss 5.44815, smoothed loss 5.24606, grad norm 5.17019, param norm 115.11843
INFO:root:epoch 5, iter 4685, loss 5.11806, smoothed loss 5.24662, grad norm 5.17994, param norm 115.17181
INFO:root:epoch 5, iter 4690, loss 5.18550, smoothed loss 5.24106, grad norm 5.27322, param norm 115.22551
INFO:root:epoch 5, iter 4695, loss 5.64357, smoothed loss 5.24044, grad norm 5.65895, param norm 115.27711
INFO:root:epoch 5, iter 4700, loss 5.10623, smoothed loss 5.23884, grad norm 5.32754, param norm 115.32845
INFO:root:epoch 5, iter 4705, loss 5.78946, smoothed loss 5.24203, grad norm 4.84787, param norm 115.37900
INFO:root:epoch 5, iter 4710, loss 4.93675, smoothed loss 5.23866, grad norm 5.34478, param norm 115.43037
INFO:root:epoch 5, iter 4715, loss 5.74955, smoothed loss 5.24063, grad norm 5.51422, param norm 115.48640
INFO:root:epoch 5, iter 4720, loss 4.82228, smoothed loss 5.21318, grad norm 5.41788, param norm 115.54131
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 4725, loss 4.69936, smoothed loss 5.19976, grad norm 5.55709, param norm 115.59555
INFO:root:epoch 6, iter 4730, loss 4.49871, smoothed loss 5.18141, grad norm 5.90008, param norm 115.64490
INFO:root:epoch 6, iter 4735, loss 5.05555, smoothed loss 5.17091, grad norm 4.99556, param norm 115.69688
INFO:root:epoch 6, iter 4740, loss 5.27913, smoothed loss 5.17254, grad norm 6.38990, param norm 115.74734
INFO:root:epoch 6, iter 4745, loss 5.18657, smoothed loss 5.16779, grad norm 6.07763, param norm 115.79461
INFO:root:epoch 6, iter 4750, loss 5.30909, smoothed loss 5.16767, grad norm 6.08828, param norm 115.84679
INFO:root:epoch 6, iter 4755, loss 4.77414, smoothed loss 5.15954, grad norm 4.39299, param norm 115.89313
INFO:root:epoch 6, iter 4760, loss 4.75888, smoothed loss 5.15327, grad norm 4.75834, param norm 115.93938
INFO:root:epoch 6, iter 4765, loss 4.68446, smoothed loss 5.15619, grad norm 4.84591, param norm 115.98582
INFO:root:epoch 6, iter 4770, loss 5.03264, smoothed loss 5.15184, grad norm 4.90014, param norm 116.03689
INFO:root:epoch 6, iter 4775, loss 4.46845, smoothed loss 5.12984, grad norm 4.82925, param norm 116.09533
INFO:root:epoch 6, iter 4780, loss 5.39322, smoothed loss 5.12335, grad norm 5.77595, param norm 116.15475
INFO:root:epoch 6, iter 4785, loss 4.95750, smoothed loss 5.13316, grad norm 6.01661, param norm 116.20961
INFO:root:epoch 6, iter 4790, loss 4.35966, smoothed loss 5.14045, grad norm 5.21228, param norm 116.26026
INFO:root:epoch 6, iter 4795, loss 4.85088, smoothed loss 5.14069, grad norm 5.79116, param norm 116.31272
INFO:root:epoch 6, iter 4800, loss 5.43014, smoothed loss 5.12164, grad norm 5.44979, param norm 116.36557
INFO:root:epoch 6, iter 4805, loss 4.86980, smoothed loss 5.11549, grad norm 5.01350, param norm 116.41151
INFO:root:epoch 6, iter 4810, loss 5.28540, smoothed loss 5.12352, grad norm 5.64819, param norm 116.45552
INFO:root:epoch 6, iter 4815, loss 5.18154, smoothed loss 5.12119, grad norm 5.99776, param norm 116.49929
INFO:root:epoch 6, iter 4820, loss 4.78830, smoothed loss 5.12015, grad norm 4.58490, param norm 116.54408
INFO:root:epoch 6, iter 4825, loss 5.00676, smoothed loss 5.10497, grad norm 5.12992, param norm 116.58966
INFO:root:epoch 6, iter 4830, loss 5.01179, smoothed loss 5.08729, grad norm 5.28593, param norm 116.63792
INFO:root:epoch 6, iter 4835, loss 4.97858, smoothed loss 5.09358, grad norm 5.17363, param norm 116.68295
INFO:root:epoch 6, iter 4840, loss 5.54603, smoothed loss 5.09707, grad norm 5.46128, param norm 116.72999
INFO:root:epoch 6, iter 4845, loss 4.85386, smoothed loss 5.08878, grad norm 6.79550, param norm 116.77860
INFO:root:epoch 6, iter 4850, loss 5.19619, smoothed loss 5.07928, grad norm 5.90140, param norm 116.82922
INFO:root:epoch 6, iter 4855, loss 5.37782, smoothed loss 5.09137, grad norm 6.65912, param norm 116.88004
INFO:root:epoch 6, iter 4860, loss 4.57023, smoothed loss 5.08009, grad norm 5.92692, param norm 116.93285
INFO:root:epoch 6, iter 4865, loss 5.17358, smoothed loss 5.07221, grad norm 4.76837, param norm 116.98516
INFO:root:epoch 6, iter 4870, loss 4.57605, smoothed loss 5.05886, grad norm 4.98711, param norm 117.03229
INFO:root:epoch 6, iter 4875, loss 5.32463, smoothed loss 5.05391, grad norm 5.14433, param norm 117.07754
INFO:root:epoch 6, iter 4880, loss 5.71449, smoothed loss 5.06605, grad norm 6.84322, param norm 117.12647
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 4885, loss 4.38983, smoothed loss 5.04525, grad norm 5.07817, param norm 117.17725
INFO:root:epoch 6, iter 4890, loss 5.58683, smoothed loss 5.05275, grad norm 6.66619, param norm 117.23230
INFO:root:epoch 6, iter 4895, loss 5.08896, smoothed loss 5.05438, grad norm 4.99766, param norm 117.28344
INFO:root:epoch 6, iter 4900, loss 5.44064, smoothed loss 5.04991, grad norm 6.75624, param norm 117.33273
INFO:root:epoch 6, iter 4905, loss 4.58158, smoothed loss 5.03975, grad norm 5.46764, param norm 117.38771
INFO:root:epoch 6, iter 4910, loss 4.65633, smoothed loss 5.02375, grad norm 6.34343, param norm 117.44552
INFO:root:epoch 6, iter 4915, loss 4.77282, smoothed loss 5.01509, grad norm 6.19264, param norm 117.50198
INFO:root:epoch 6, iter 4920, loss 4.90352, smoothed loss 5.00107, grad norm 6.35562, param norm 117.55559
INFO:root:epoch 6, iter 4925, loss 5.35656, smoothed loss 4.99719, grad norm 5.37795, param norm 117.61076
INFO:root:epoch 6, iter 4930, loss 4.33805, smoothed loss 4.98245, grad norm 6.14775, param norm 117.66209
INFO:root:epoch 6, iter 4935, loss 5.06833, smoothed loss 4.98643, grad norm 5.99830, param norm 117.71278
INFO:root:epoch 6, iter 4940, loss 4.96109, smoothed loss 4.99078, grad norm 5.11083, param norm 117.75909
INFO:root:epoch 6, iter 4945, loss 4.40409, smoothed loss 4.97495, grad norm 4.62111, param norm 117.80640
INFO:root:epoch 6, iter 4950, loss 5.41657, smoothed loss 4.97954, grad norm 6.36459, param norm 117.85314
INFO:root:epoch 6, iter 4955, loss 5.29227, smoothed loss 4.98664, grad norm 6.07817, param norm 117.90432
INFO:root:epoch 6, iter 4960, loss 5.20281, smoothed loss 4.96936, grad norm 5.31374, param norm 117.95622
INFO:root:epoch 6, iter 4965, loss 4.08706, smoothed loss 4.95842, grad norm 6.39828, param norm 118.00973
INFO:root:epoch 6, iter 4970, loss 5.03580, smoothed loss 4.95114, grad norm 5.69990, param norm 118.06396
INFO:root:epoch 6, iter 4975, loss 4.61653, smoothed loss 4.92337, grad norm 6.19145, param norm 118.11381
INFO:root:epoch 6, iter 4980, loss 4.74531, smoothed loss 4.91327, grad norm 6.02324, param norm 118.16212
INFO:root:epoch 6, iter 4985, loss 5.38525, smoothed loss 4.89985, grad norm 5.94841, param norm 118.20953
INFO:root:epoch 6, iter 4990, loss 4.51275, smoothed loss 4.90195, grad norm 4.65418, param norm 118.25719
INFO:root:epoch 6, iter 4995, loss 4.84939, smoothed loss 4.89714, grad norm 6.12882, param norm 118.30730
INFO:root:epoch 6, iter 5000, loss 5.11085, smoothed loss 4.90289, grad norm 5.29221, param norm 118.35386
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 6, Iter 5000, dev loss: 4.317488
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.52370
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.39700
INFO:root:Epoch 6, Iter 5000, Train F1 score: 0.523698, Train EM score: 0.397000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.46619
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.33479
INFO:root:Epoch 6, Iter 5000, Dev F1 score: 0.466185, Dev EM score: 0.334785
INFO:root:End of epoch 6
INFO:root:epoch 6, iter 5005, loss 5.18990, smoothed loss 4.90388, grad norm 5.07658, param norm 118.39715
INFO:root:epoch 6, iter 5010, loss 4.59879, smoothed loss 4.89737, grad norm 5.41765, param norm 118.44265
INFO:root:epoch 6, iter 5015, loss 5.54903, smoothed loss 4.90118, grad norm 5.60705, param norm 118.48788
INFO:root:epoch 6, iter 5020, loss 5.56132, smoothed loss 4.92245, grad norm 5.93700, param norm 118.53020
INFO:root:epoch 6, iter 5025, loss 4.76508, smoothed loss 4.92350, grad norm 5.30454, param norm 118.57225
INFO:root:epoch 6, iter 5030, loss 4.86248, smoothed loss 4.92139, grad norm 5.03798, param norm 118.61888
INFO:root:epoch 6, iter 5035, loss 4.15143, smoothed loss 4.91125, grad norm 5.34486, param norm 118.67020
INFO:root:epoch 6, iter 5040, loss 4.13812, smoothed loss 4.90529, grad norm 5.34819, param norm 118.72015
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5045, loss 4.31665, smoothed loss 4.88706, grad norm 5.49459, param norm 118.77095
INFO:root:epoch 6, iter 5050, loss 4.35643, smoothed loss 4.88032, grad norm 5.32301, param norm 118.82355
INFO:root:epoch 6, iter 5055, loss 4.96761, smoothed loss 4.87081, grad norm 6.19940, param norm 118.87458
INFO:root:epoch 6, iter 5060, loss 5.91418, smoothed loss 4.87129, grad norm 5.59087, param norm 118.92044
INFO:root:epoch 6, iter 5065, loss 4.85711, smoothed loss 4.86576, grad norm 6.44161, param norm 118.96586
INFO:root:epoch 6, iter 5070, loss 4.39690, smoothed loss 4.86011, grad norm 5.20811, param norm 119.01591
INFO:root:epoch 6, iter 5075, loss 4.97364, smoothed loss 4.85549, grad norm 5.07880, param norm 119.06203
INFO:root:epoch 6, iter 5080, loss 5.31758, smoothed loss 4.85999, grad norm 5.18156, param norm 119.10921
INFO:root:epoch 6, iter 5085, loss 4.59897, smoothed loss 4.84976, grad norm 5.39839, param norm 119.15928
INFO:root:epoch 6, iter 5090, loss 5.05072, smoothed loss 4.83256, grad norm 5.59956, param norm 119.21194
INFO:root:epoch 6, iter 5095, loss 4.32043, smoothed loss 4.82182, grad norm 5.45807, param norm 119.26472
INFO:root:epoch 6, iter 5100, loss 3.33218, smoothed loss 4.80739, grad norm 4.87178, param norm 119.32207
INFO:root:epoch 6, iter 5105, loss 4.34203, smoothed loss 4.79048, grad norm 6.01081, param norm 119.37844
INFO:root:epoch 6, iter 5110, loss 4.28146, smoothed loss 4.77471, grad norm 6.31547, param norm 119.42934
INFO:root:epoch 6, iter 5115, loss 3.91971, smoothed loss 4.77081, grad norm 5.15918, param norm 119.47675
INFO:root:epoch 6, iter 5120, loss 4.36876, smoothed loss 4.77230, grad norm 5.28446, param norm 119.52058
INFO:root:epoch 6, iter 5125, loss 4.82435, smoothed loss 4.76132, grad norm 5.32172, param norm 119.56444
INFO:root:epoch 6, iter 5130, loss 4.68406, smoothed loss 4.76018, grad norm 4.69822, param norm 119.61216
INFO:root:epoch 6, iter 5135, loss 4.87487, smoothed loss 4.75638, grad norm 5.55868, param norm 119.66345
INFO:root:epoch 6, iter 5140, loss 3.97148, smoothed loss 4.74155, grad norm 5.20060, param norm 119.71521
INFO:root:epoch 6, iter 5145, loss 5.57801, smoothed loss 4.74020, grad norm 5.33674, param norm 119.76431
INFO:root:epoch 6, iter 5150, loss 4.71856, smoothed loss 4.74288, grad norm 4.72564, param norm 119.80827
INFO:root:epoch 6, iter 5155, loss 4.55555, smoothed loss 4.72603, grad norm 5.94325, param norm 119.85520
INFO:root:epoch 6, iter 5160, loss 3.99120, smoothed loss 4.70640, grad norm 5.07977, param norm 119.90351
INFO:root:epoch 6, iter 5165, loss 4.38402, smoothed loss 4.70565, grad norm 6.24231, param norm 119.95427
INFO:root:epoch 6, iter 5170, loss 4.60134, smoothed loss 4.69369, grad norm 5.16597, param norm 120.00374
INFO:root:epoch 6, iter 5175, loss 4.74803, smoothed loss 4.68157, grad norm 5.33943, param norm 120.04925
INFO:root:epoch 6, iter 5180, loss 4.78387, smoothed loss 4.67566, grad norm 5.67597, param norm 120.09741
INFO:root:epoch 6, iter 5185, loss 5.02891, smoothed loss 4.68324, grad norm 5.97874, param norm 120.14104
INFO:root:epoch 6, iter 5190, loss 5.13774, smoothed loss 4.69523, grad norm 7.62345, param norm 120.18347
INFO:root:epoch 6, iter 5195, loss 4.77639, smoothed loss 4.70581, grad norm 5.53385, param norm 120.22729
INFO:root:epoch 6, iter 5200, loss 4.26752, smoothed loss 4.69318, grad norm 5.48655, param norm 120.27420
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5205, loss 4.30174, smoothed loss 4.67176, grad norm 5.05595, param norm 120.32391
INFO:root:epoch 6, iter 5210, loss 3.99634, smoothed loss 4.65423, grad norm 5.41506, param norm 120.37585
INFO:root:epoch 6, iter 5215, loss 4.83146, smoothed loss 4.65618, grad norm 5.65523, param norm 120.42219
INFO:root:epoch 6, iter 5220, loss 4.98452, smoothed loss 4.65076, grad norm 6.09765, param norm 120.46590
INFO:root:epoch 6, iter 5225, loss 4.52714, smoothed loss 4.64383, grad norm 6.30791, param norm 120.51363
INFO:root:epoch 6, iter 5230, loss 4.86640, smoothed loss 4.63511, grad norm 5.41973, param norm 120.55827
INFO:root:epoch 6, iter 5235, loss 4.47310, smoothed loss 4.64619, grad norm 5.28903, param norm 120.60556
INFO:root:epoch 6, iter 5240, loss 4.84932, smoothed loss 4.64563, grad norm 5.93971, param norm 120.65408
INFO:root:epoch 6, iter 5245, loss 4.62187, smoothed loss 4.64032, grad norm 5.41534, param norm 120.69620
INFO:root:epoch 6, iter 5250, loss 4.44441, smoothed loss 4.63231, grad norm 6.27788, param norm 120.73579
INFO:root:epoch 6, iter 5255, loss 4.61216, smoothed loss 4.62807, grad norm 5.94290, param norm 120.77911
INFO:root:epoch 6, iter 5260, loss 5.21640, smoothed loss 4.62430, grad norm 6.06677, param norm 120.82594
INFO:root:epoch 6, iter 5265, loss 4.34671, smoothed loss 4.61754, grad norm 5.06807, param norm 120.87019
INFO:root:epoch 6, iter 5270, loss 4.56859, smoothed loss 4.62057, grad norm 5.03988, param norm 120.91216
INFO:root:epoch 6, iter 5275, loss 4.67389, smoothed loss 4.63125, grad norm 5.23244, param norm 120.95382
INFO:root:epoch 6, iter 5280, loss 5.17171, smoothed loss 4.63062, grad norm 5.76768, param norm 120.99964
INFO:root:epoch 6, iter 5285, loss 5.10633, smoothed loss 4.63694, grad norm 5.39345, param norm 121.05067
INFO:root:epoch 6, iter 5290, loss 4.39165, smoothed loss 4.63934, grad norm 5.71573, param norm 121.09864
INFO:root:epoch 6, iter 5295, loss 4.78954, smoothed loss 4.62783, grad norm 5.23940, param norm 121.14399
INFO:root:epoch 6, iter 5300, loss 5.10213, smoothed loss 4.62855, grad norm 6.20266, param norm 121.18692
INFO:root:epoch 6, iter 5305, loss 4.73455, smoothed loss 4.63921, grad norm 5.54965, param norm 121.23037
INFO:root:epoch 6, iter 5310, loss 4.19887, smoothed loss 4.60892, grad norm 5.61855, param norm 121.27551
INFO:root:epoch 6, iter 5315, loss 4.59174, smoothed loss 4.59932, grad norm 6.59995, param norm 121.32482
INFO:root:epoch 6, iter 5320, loss 3.87911, smoothed loss 4.59286, grad norm 5.47796, param norm 121.37153
INFO:root:epoch 6, iter 5325, loss 4.61752, smoothed loss 4.58863, grad norm 5.56880, param norm 121.41791
INFO:root:epoch 6, iter 5330, loss 3.84789, smoothed loss 4.58557, grad norm 4.89133, param norm 121.46152
INFO:root:epoch 6, iter 5335, loss 4.06647, smoothed loss 4.57601, grad norm 4.96289, param norm 121.50388
INFO:root:epoch 6, iter 5340, loss 5.17300, smoothed loss 4.58856, grad norm 5.66810, param norm 121.54420
INFO:root:epoch 6, iter 5345, loss 3.99447, smoothed loss 4.59056, grad norm 5.22549, param norm 121.58906
INFO:root:epoch 6, iter 5350, loss 3.92336, smoothed loss 4.58978, grad norm 5.12104, param norm 121.63156
INFO:root:epoch 6, iter 5355, loss 4.39975, smoothed loss 4.58897, grad norm 6.56015, param norm 121.68391
INFO:root:epoch 6, iter 5360, loss 5.29596, smoothed loss 4.59014, grad norm 6.69531, param norm 121.74104
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5365, loss 4.10180, smoothed loss 4.59418, grad norm 5.55859, param norm 121.78609
INFO:root:epoch 6, iter 5370, loss 4.99498, smoothed loss 4.58673, grad norm 5.19957, param norm 121.82627
INFO:root:epoch 6, iter 5375, loss 4.37353, smoothed loss 4.58070, grad norm 5.63604, param norm 121.86869
INFO:root:epoch 6, iter 5380, loss 4.73863, smoothed loss 4.58075, grad norm 4.86749, param norm 121.91737
INFO:root:epoch 6, iter 5385, loss 5.05830, smoothed loss 4.59439, grad norm 6.52406, param norm 121.96234
INFO:root:epoch 6, iter 5390, loss 4.78615, smoothed loss 4.58609, grad norm 5.98265, param norm 122.01002
INFO:root:epoch 6, iter 5395, loss 4.88075, smoothed loss 4.59004, grad norm 6.19090, param norm 122.05676
INFO:root:epoch 6, iter 5400, loss 4.29013, smoothed loss 4.57709, grad norm 5.79846, param norm 122.10212
INFO:root:epoch 6, iter 5405, loss 4.62389, smoothed loss 4.57205, grad norm 5.67908, param norm 122.14672
INFO:root:epoch 6, iter 5410, loss 4.60564, smoothed loss 4.57309, grad norm 5.55176, param norm 122.19293
INFO:root:epoch 6, iter 5415, loss 4.40553, smoothed loss 4.57045, grad norm 6.02355, param norm 122.23650
INFO:root:epoch 6, iter 5420, loss 4.58393, smoothed loss 4.57115, grad norm 5.57952, param norm 122.27896
INFO:root:epoch 6, iter 5425, loss 4.27480, smoothed loss 4.55772, grad norm 5.12423, param norm 122.32388
INFO:root:epoch 6, iter 5430, loss 5.05051, smoothed loss 4.55481, grad norm 5.46748, param norm 122.36797
INFO:root:epoch 6, iter 5435, loss 4.38918, smoothed loss 4.55739, grad norm 7.14103, param norm 122.40573
INFO:root:epoch 6, iter 5440, loss 4.78510, smoothed loss 4.55429, grad norm 5.43182, param norm 122.44347
INFO:root:epoch 6, iter 5445, loss 4.44517, smoothed loss 4.54207, grad norm 5.56683, param norm 122.48750
INFO:root:epoch 6, iter 5450, loss 4.74272, smoothed loss 4.54698, grad norm 5.81629, param norm 122.53533
INFO:root:epoch 6, iter 5455, loss 4.97689, smoothed loss 4.56124, grad norm 6.01646, param norm 122.58056
INFO:root:epoch 6, iter 5460, loss 3.92607, smoothed loss 4.53830, grad norm 7.94104, param norm 122.62479
INFO:root:epoch 6, iter 5465, loss 4.65698, smoothed loss 4.54903, grad norm 6.10864, param norm 122.67191
INFO:root:epoch 6, iter 5470, loss 4.75971, smoothed loss 4.53784, grad norm 5.00113, param norm 122.71449
INFO:root:epoch 6, iter 5475, loss 4.79993, smoothed loss 4.53299, grad norm 5.99334, param norm 122.75523
INFO:root:epoch 6, iter 5480, loss 5.59863, smoothed loss 4.55347, grad norm 6.18618, param norm 122.79581
INFO:root:epoch 6, iter 5485, loss 5.31002, smoothed loss 4.55684, grad norm 5.09907, param norm 122.83725
INFO:root:epoch 6, iter 5490, loss 5.47179, smoothed loss 4.55281, grad norm 5.38877, param norm 122.87909
INFO:root:epoch 6, iter 5495, loss 4.11279, smoothed loss 4.53880, grad norm 5.51779, param norm 122.92607
INFO:root:epoch 6, iter 5500, loss 4.57016, smoothed loss 4.53040, grad norm 5.84690, param norm 122.97385
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 6, Iter 5500, dev loss: 3.889327
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.55480
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.42600
INFO:root:Epoch 6, Iter 5500, Train F1 score: 0.554799, Train EM score: 0.426000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.52386
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.38213
INFO:root:Epoch 6, Iter 5500, Dev F1 score: 0.523862, Dev EM score: 0.382130
INFO:root:End of epoch 6
INFO:root:epoch 6, iter 5505, loss 4.65340, smoothed loss 4.53802, grad norm 5.36447, param norm 123.02088
INFO:root:epoch 6, iter 5510, loss 3.90469, smoothed loss 4.54947, grad norm 5.07464, param norm 123.05944
INFO:root:epoch 6, iter 5515, loss 4.75257, smoothed loss 4.55328, grad norm 6.17142, param norm 123.10060
INFO:root:epoch 6, iter 5520, loss 4.32379, smoothed loss 4.54466, grad norm 5.38187, param norm 123.14535
Adding batches start...
Added  144  batches
INFO:root:epoch 6, iter 5525, loss 3.92182, smoothed loss 4.53301, grad norm 6.06064, param norm 123.18939
INFO:root:epoch 6, iter 5530, loss 4.84640, smoothed loss 4.53359, grad norm 5.13100, param norm 123.23150
INFO:root:epoch 6, iter 5535, loss 5.01341, smoothed loss 4.54156, grad norm 6.52763, param norm 123.27071
INFO:root:epoch 6, iter 5540, loss 4.09534, smoothed loss 4.53409, grad norm 5.57859, param norm 123.30996
INFO:root:epoch 6, iter 5545, loss 3.65209, smoothed loss 4.52980, grad norm 5.64490, param norm 123.35300
INFO:root:epoch 6, iter 5550, loss 4.03493, smoothed loss 4.51409, grad norm 5.57514, param norm 123.39860
INFO:root:epoch 6, iter 5555, loss 4.12705, smoothed loss 4.51762, grad norm 5.63470, param norm 123.44370
INFO:root:epoch 6, iter 5560, loss 3.95384, smoothed loss 4.51301, grad norm 6.05424, param norm 123.48528
INFO:root:epoch 6, iter 5565, loss 5.05100, smoothed loss 4.51468, grad norm 5.78057, param norm 123.52512
INFO:root:epoch 6, iter 5570, loss 3.72474, smoothed loss 4.50578, grad norm 5.52068, param norm 123.56414
INFO:root:epoch 6, iter 5575, loss 4.86470, smoothed loss 4.50403, grad norm 6.60952, param norm 123.60293
INFO:root:epoch 6, iter 5580, loss 4.24414, smoothed loss 4.49823, grad norm 6.23214, param norm 123.64058
INFO:root:epoch 6, iter 5585, loss 4.71157, smoothed loss 4.48466, grad norm 6.39549, param norm 123.67725
INFO:root:epoch 6, iter 5590, loss 3.99711, smoothed loss 4.47194, grad norm 5.16728, param norm 123.71421
INFO:root:epoch 6, iter 5595, loss 4.17078, smoothed loss 4.46781, grad norm 5.20151, param norm 123.75475
INFO:root:epoch 6, iter 5600, loss 4.92217, smoothed loss 4.46590, grad norm 5.76758, param norm 123.80021
INFO:root:epoch 6, iter 5605, loss 4.40257, smoothed loss 4.47721, grad norm 5.65382, param norm 123.84916
INFO:root:epoch 6, iter 5610, loss 3.96385, smoothed loss 4.47132, grad norm 5.50477, param norm 123.89778
INFO:root:epoch 6, iter 5615, loss 4.22480, smoothed loss 4.47322, grad norm 4.52545, param norm 123.94484
INFO:root:epoch 6, iter 5620, loss 3.93894, smoothed loss 4.47414, grad norm 4.49611, param norm 123.98858
INFO:root:epoch 6, iter 5625, loss 4.90481, smoothed loss 4.46862, grad norm 6.17626, param norm 124.03037
INFO:root:epoch 6, iter 5630, loss 4.23873, smoothed loss 4.45337, grad norm 5.83392, param norm 124.07782
INFO:root:epoch 6, iter 5635, loss 3.86638, smoothed loss 4.44601, grad norm 5.71919, param norm 124.12357
INFO:root:epoch 6, iter 5640, loss 4.28334, smoothed loss 4.43346, grad norm 5.70809, param norm 124.16836
INFO:root:epoch 6, iter 5645, loss 3.99013, smoothed loss 4.40864, grad norm 6.11071, param norm 124.21097
INFO:root:epoch 6, iter 5650, loss 3.51572, smoothed loss 4.39767, grad norm 4.54217, param norm 124.25713
INFO:root:epoch 6, iter 5655, loss 5.07553, smoothed loss 4.40062, grad norm 6.41832, param norm 124.30086
INFO:root:epoch 6, iter 5660, loss 3.89110, smoothed loss 4.39831, grad norm 5.23608, param norm 124.34170
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5665, loss 4.18271, smoothed loss 4.41170, grad norm 5.80927, param norm 124.38167
INFO:root:epoch 7, iter 5670, loss 4.15421, smoothed loss 4.40529, grad norm 4.48228, param norm 124.41998
INFO:root:epoch 7, iter 5675, loss 4.36402, smoothed loss 4.40578, grad norm 6.62453, param norm 124.45847
INFO:root:epoch 7, iter 5680, loss 4.03721, smoothed loss 4.39394, grad norm 5.63836, param norm 124.50075
INFO:root:epoch 7, iter 5685, loss 4.52957, smoothed loss 4.40635, grad norm 5.74012, param norm 124.53985
INFO:root:epoch 7, iter 5690, loss 4.67675, smoothed loss 4.40169, grad norm 5.95952, param norm 124.57407
INFO:root:epoch 7, iter 5695, loss 4.19592, smoothed loss 4.40424, grad norm 5.85777, param norm 124.60724
INFO:root:epoch 7, iter 5700, loss 4.58851, smoothed loss 4.40516, grad norm 4.73342, param norm 124.64081
INFO:root:epoch 7, iter 5705, loss 3.73941, smoothed loss 4.40583, grad norm 4.95158, param norm 124.67550
INFO:root:epoch 7, iter 5710, loss 4.16134, smoothed loss 4.39655, grad norm 5.68288, param norm 124.71594
INFO:root:epoch 7, iter 5715, loss 4.20525, smoothed loss 4.38568, grad norm 5.78312, param norm 124.75693
INFO:root:epoch 7, iter 5720, loss 4.24790, smoothed loss 4.38509, grad norm 5.53428, param norm 124.79805
INFO:root:epoch 7, iter 5725, loss 3.93102, smoothed loss 4.37885, grad norm 5.45273, param norm 124.83809
INFO:root:epoch 7, iter 5730, loss 3.76122, smoothed loss 4.36680, grad norm 4.86056, param norm 124.87791
INFO:root:epoch 7, iter 5735, loss 4.51766, smoothed loss 4.35910, grad norm 5.98371, param norm 124.92003
INFO:root:epoch 7, iter 5740, loss 4.48539, smoothed loss 4.34816, grad norm 5.61222, param norm 124.95990
INFO:root:epoch 7, iter 5745, loss 3.67365, smoothed loss 4.32488, grad norm 6.60993, param norm 124.99793
INFO:root:epoch 7, iter 5750, loss 4.38539, smoothed loss 4.31444, grad norm 6.31612, param norm 125.03770
INFO:root:epoch 7, iter 5755, loss 4.62571, smoothed loss 4.31028, grad norm 5.98892, param norm 125.07744
INFO:root:epoch 7, iter 5760, loss 3.84190, smoothed loss 4.29950, grad norm 6.22523, param norm 125.11778
INFO:root:epoch 7, iter 5765, loss 5.01694, smoothed loss 4.29334, grad norm 5.71264, param norm 125.15279
INFO:root:epoch 7, iter 5770, loss 4.08520, smoothed loss 4.30191, grad norm 5.42291, param norm 125.18593
INFO:root:epoch 7, iter 5775, loss 5.14256, smoothed loss 4.31728, grad norm 6.18120, param norm 125.22316
INFO:root:epoch 7, iter 5780, loss 4.18781, smoothed loss 4.30129, grad norm 5.33865, param norm 125.26266
INFO:root:epoch 7, iter 5785, loss 4.68263, smoothed loss 4.29085, grad norm 5.22008, param norm 125.30429
INFO:root:epoch 7, iter 5790, loss 4.85249, smoothed loss 4.28507, grad norm 6.18064, param norm 125.34364
INFO:root:epoch 7, iter 5795, loss 3.72295, smoothed loss 4.26988, grad norm 5.31762, param norm 125.38625
INFO:root:epoch 7, iter 5800, loss 4.51281, smoothed loss 4.25970, grad norm 5.18003, param norm 125.43340
INFO:root:epoch 7, iter 5805, loss 4.10862, smoothed loss 4.26816, grad norm 4.66674, param norm 125.47723
INFO:root:epoch 7, iter 5810, loss 4.39452, smoothed loss 4.27139, grad norm 4.84437, param norm 125.51471
INFO:root:epoch 7, iter 5815, loss 4.73507, smoothed loss 4.28240, grad norm 6.48688, param norm 125.55212
INFO:root:epoch 7, iter 5820, loss 4.71276, smoothed loss 4.28124, grad norm 5.88011, param norm 125.59126
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5825, loss 3.71859, smoothed loss 4.26083, grad norm 5.56876, param norm 125.63499
INFO:root:epoch 7, iter 5830, loss 4.20198, smoothed loss 4.26379, grad norm 5.61025, param norm 125.68102
INFO:root:epoch 7, iter 5835, loss 4.15205, smoothed loss 4.26357, grad norm 6.34176, param norm 125.71996
INFO:root:epoch 7, iter 5840, loss 4.57191, smoothed loss 4.26142, grad norm 6.28641, param norm 125.75610
INFO:root:epoch 7, iter 5845, loss 4.72563, smoothed loss 4.25553, grad norm 5.75323, param norm 125.79380
INFO:root:epoch 7, iter 5850, loss 4.69840, smoothed loss 4.26114, grad norm 6.33422, param norm 125.83450
INFO:root:epoch 7, iter 5855, loss 4.07882, smoothed loss 4.27413, grad norm 5.02702, param norm 125.87515
INFO:root:epoch 7, iter 5860, loss 4.81185, smoothed loss 4.26809, grad norm 5.74547, param norm 125.91703
INFO:root:epoch 7, iter 5865, loss 3.91246, smoothed loss 4.25736, grad norm 5.31427, param norm 125.95532
INFO:root:epoch 7, iter 5870, loss 4.51381, smoothed loss 4.26802, grad norm 6.56875, param norm 125.99281
INFO:root:epoch 7, iter 5875, loss 4.04669, smoothed loss 4.24470, grad norm 5.35842, param norm 126.03269
INFO:root:epoch 7, iter 5880, loss 4.09631, smoothed loss 4.24702, grad norm 5.82600, param norm 126.07436
INFO:root:epoch 7, iter 5885, loss 4.46730, smoothed loss 4.23681, grad norm 6.69347, param norm 126.10898
INFO:root:epoch 7, iter 5890, loss 4.29615, smoothed loss 4.23356, grad norm 7.24081, param norm 126.14580
INFO:root:epoch 7, iter 5895, loss 4.17718, smoothed loss 4.22938, grad norm 5.24073, param norm 126.18431
INFO:root:epoch 7, iter 5900, loss 3.46792, smoothed loss 4.22526, grad norm 6.54221, param norm 126.22616
INFO:root:epoch 7, iter 5905, loss 4.28763, smoothed loss 4.23349, grad norm 6.64668, param norm 126.26816
INFO:root:epoch 7, iter 5910, loss 3.87474, smoothed loss 4.24169, grad norm 5.84331, param norm 126.30619
INFO:root:epoch 7, iter 5915, loss 4.59111, smoothed loss 4.24808, grad norm 5.44703, param norm 126.34339
INFO:root:epoch 7, iter 5920, loss 4.23729, smoothed loss 4.25121, grad norm 5.00301, param norm 126.38143
INFO:root:epoch 7, iter 5925, loss 4.02988, smoothed loss 4.23140, grad norm 4.84375, param norm 126.42406
INFO:root:epoch 7, iter 5930, loss 4.48987, smoothed loss 4.24335, grad norm 5.44274, param norm 126.47021
INFO:root:epoch 7, iter 5935, loss 4.62120, smoothed loss 4.24774, grad norm 6.30810, param norm 126.51180
INFO:root:epoch 7, iter 5940, loss 4.20598, smoothed loss 4.24594, grad norm 6.21863, param norm 126.55168
INFO:root:epoch 7, iter 5945, loss 3.91604, smoothed loss 4.23433, grad norm 5.81856, param norm 126.59200
INFO:root:epoch 7, iter 5950, loss 3.85853, smoothed loss 4.22916, grad norm 5.98124, param norm 126.62933
INFO:root:epoch 7, iter 5955, loss 5.10048, smoothed loss 4.24486, grad norm 6.35824, param norm 126.66593
INFO:root:epoch 7, iter 5960, loss 3.75304, smoothed loss 4.24881, grad norm 4.72202, param norm 126.69818
INFO:root:epoch 7, iter 5965, loss 3.53988, smoothed loss 4.23312, grad norm 5.64066, param norm 126.73195
INFO:root:epoch 7, iter 5970, loss 4.50459, smoothed loss 4.23282, grad norm 5.82161, param norm 126.76996
INFO:root:epoch 7, iter 5975, loss 4.38964, smoothed loss 4.23008, grad norm 5.53457, param norm 126.81663
INFO:root:epoch 7, iter 5980, loss 4.23139, smoothed loss 4.23077, grad norm 5.05964, param norm 126.86251
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5985, loss 3.29774, smoothed loss 4.22227, grad norm 4.23964, param norm 126.90704
INFO:root:epoch 7, iter 5990, loss 4.21061, smoothed loss 4.21796, grad norm 5.52414, param norm 126.94993
INFO:root:epoch 7, iter 5995, loss 4.18171, smoothed loss 4.21037, grad norm 5.49874, param norm 126.99320
INFO:root:epoch 7, iter 6000, loss 3.94791, smoothed loss 4.20878, grad norm 5.32444, param norm 127.03033
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 7, Iter 6000, dev loss: 3.646046
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.60243
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.47200
INFO:root:Epoch 7, Iter 6000, Train F1 score: 0.602428, Train EM score: 0.472000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.55270
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.41233
INFO:root:Epoch 7, Iter 6000, Dev F1 score: 0.552702, Dev EM score: 0.412335
INFO:root:End of epoch 7
INFO:root:epoch 7, iter 6005, loss 3.45966, smoothed loss 4.19404, grad norm 5.13263, param norm 127.06671
INFO:root:epoch 7, iter 6010, loss 2.56060, smoothed loss 4.17823, grad norm 4.47157, param norm 127.10528
INFO:root:epoch 7, iter 6015, loss 4.46656, smoothed loss 4.17895, grad norm 5.47714, param norm 127.14650
INFO:root:epoch 7, iter 6020, loss 4.50610, smoothed loss 4.18125, grad norm 5.77008, param norm 127.19021
INFO:root:epoch 7, iter 6025, loss 3.85353, smoothed loss 4.19451, grad norm 5.80516, param norm 127.22919
INFO:root:epoch 7, iter 6030, loss 4.03937, smoothed loss 4.19697, grad norm 5.63396, param norm 127.26654
INFO:root:epoch 7, iter 6035, loss 3.88692, smoothed loss 4.19933, grad norm 5.49119, param norm 127.30325
INFO:root:epoch 7, iter 6040, loss 4.48612, smoothed loss 4.19434, grad norm 5.15015, param norm 127.34006
INFO:root:epoch 7, iter 6045, loss 3.91900, smoothed loss 4.18363, grad norm 5.97632, param norm 127.37669
INFO:root:epoch 7, iter 6050, loss 4.28659, smoothed loss 4.18142, grad norm 5.80275, param norm 127.41186
INFO:root:epoch 7, iter 6055, loss 4.23083, smoothed loss 4.17701, grad norm 5.74154, param norm 127.45234
INFO:root:epoch 7, iter 6060, loss 3.76065, smoothed loss 4.17109, grad norm 4.93123, param norm 127.49576
INFO:root:epoch 7, iter 6065, loss 3.10626, smoothed loss 4.16860, grad norm 4.77037, param norm 127.53667
INFO:root:epoch 7, iter 6070, loss 4.39941, smoothed loss 4.16278, grad norm 5.02508, param norm 127.57404
INFO:root:epoch 7, iter 6075, loss 3.83827, smoothed loss 4.15907, grad norm 4.89765, param norm 127.61256
INFO:root:epoch 7, iter 6080, loss 3.97950, smoothed loss 4.15142, grad norm 5.24024, param norm 127.65285
INFO:root:epoch 7, iter 6085, loss 3.30473, smoothed loss 4.13723, grad norm 5.20781, param norm 127.69370
INFO:root:epoch 7, iter 6090, loss 3.45340, smoothed loss 4.14066, grad norm 4.82933, param norm 127.73508
INFO:root:epoch 7, iter 6095, loss 3.63799, smoothed loss 4.14847, grad norm 5.21493, param norm 127.76984
INFO:root:epoch 7, iter 6100, loss 2.65143, smoothed loss 4.14873, grad norm 4.44816, param norm 127.80209
INFO:root:epoch 7, iter 6105, loss 4.07188, smoothed loss 4.13805, grad norm 4.96117, param norm 127.83558
INFO:root:epoch 7, iter 6110, loss 4.33959, smoothed loss 4.14231, grad norm 5.95099, param norm 127.87108
INFO:root:epoch 7, iter 6115, loss 4.12861, smoothed loss 4.15053, grad norm 5.36087, param norm 127.90739
INFO:root:epoch 7, iter 6120, loss 3.97762, smoothed loss 4.15029, grad norm 5.35236, param norm 127.94104
INFO:root:epoch 7, iter 6125, loss 4.01543, smoothed loss 4.13938, grad norm 4.59814, param norm 127.97754
INFO:root:epoch 7, iter 6130, loss 3.32285, smoothed loss 4.13019, grad norm 4.83870, param norm 128.01605
INFO:root:epoch 7, iter 6135, loss 4.82337, smoothed loss 4.13531, grad norm 6.33884, param norm 128.05487
INFO:root:epoch 7, iter 6140, loss 3.79460, smoothed loss 4.12962, grad norm 7.13142, param norm 128.09442
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 6145, loss 3.72454, smoothed loss 4.09831, grad norm 5.90441, param norm 128.13474
INFO:root:epoch 7, iter 6150, loss 3.98407, smoothed loss 4.09215, grad norm 5.29973, param norm 128.17439
INFO:root:epoch 7, iter 6155, loss 3.58290, smoothed loss 4.06904, grad norm 5.05906, param norm 128.21057
INFO:root:epoch 7, iter 6160, loss 4.60343, smoothed loss 4.06906, grad norm 5.72931, param norm 128.24696
INFO:root:epoch 7, iter 6165, loss 4.30766, smoothed loss 4.06894, grad norm 4.96831, param norm 128.28555
INFO:root:epoch 7, iter 6170, loss 3.75467, smoothed loss 4.06840, grad norm 4.89760, param norm 128.32352
INFO:root:epoch 7, iter 6175, loss 3.37973, smoothed loss 4.07516, grad norm 5.19403, param norm 128.36429
INFO:root:epoch 7, iter 6180, loss 4.36895, smoothed loss 4.09024, grad norm 4.83335, param norm 128.40172
INFO:root:epoch 7, iter 6185, loss 4.20712, smoothed loss 4.08696, grad norm 5.75483, param norm 128.44060
INFO:root:epoch 7, iter 6190, loss 4.41531, smoothed loss 4.08504, grad norm 5.31541, param norm 128.48056
INFO:root:epoch 7, iter 6195, loss 4.31638, smoothed loss 4.07325, grad norm 4.99088, param norm 128.52171
INFO:root:epoch 7, iter 6200, loss 4.63000, smoothed loss 4.08619, grad norm 5.19359, param norm 128.56601
INFO:root:epoch 7, iter 6205, loss 4.16302, smoothed loss 4.09123, grad norm 4.64093, param norm 128.60655
INFO:root:epoch 7, iter 6210, loss 3.13104, smoothed loss 4.09274, grad norm 4.52524, param norm 128.64958
INFO:root:epoch 7, iter 6215, loss 4.28217, smoothed loss 4.09807, grad norm 5.52794, param norm 128.69627
INFO:root:epoch 7, iter 6220, loss 4.52410, smoothed loss 4.09655, grad norm 5.47296, param norm 128.74249
INFO:root:epoch 7, iter 6225, loss 4.23298, smoothed loss 4.09077, grad norm 5.69680, param norm 128.78566
INFO:root:epoch 7, iter 6230, loss 4.20195, smoothed loss 4.09755, grad norm 4.95424, param norm 128.82236
INFO:root:epoch 7, iter 6235, loss 3.35224, smoothed loss 4.09476, grad norm 5.16811, param norm 128.86124
INFO:root:epoch 7, iter 6240, loss 3.12838, smoothed loss 4.07743, grad norm 4.74002, param norm 128.90363
INFO:root:epoch 7, iter 6245, loss 4.39872, smoothed loss 4.08791, grad norm 5.26486, param norm 128.94948
INFO:root:epoch 7, iter 6250, loss 3.83379, smoothed loss 4.07150, grad norm 5.80471, param norm 128.99464
INFO:root:epoch 7, iter 6255, loss 4.32779, smoothed loss 4.07739, grad norm 5.69064, param norm 129.03842
INFO:root:epoch 7, iter 6260, loss 4.34084, smoothed loss 4.07927, grad norm 5.20397, param norm 129.07870
INFO:root:epoch 7, iter 6265, loss 3.70154, smoothed loss 4.08208, grad norm 5.18827, param norm 129.11803
INFO:root:epoch 7, iter 6270, loss 4.50085, smoothed loss 4.07247, grad norm 5.92876, param norm 129.15569
INFO:root:epoch 7, iter 6275, loss 3.88442, smoothed loss 4.06636, grad norm 5.14018, param norm 129.19325
INFO:root:epoch 7, iter 6280, loss 4.23081, smoothed loss 4.06503, grad norm 5.21532, param norm 129.23131
INFO:root:epoch 7, iter 6285, loss 3.91249, smoothed loss 4.07531, grad norm 5.42979, param norm 129.27034
INFO:root:epoch 7, iter 6290, loss 4.06022, smoothed loss 4.06652, grad norm 5.32032, param norm 129.30972
INFO:root:epoch 7, iter 6295, loss 3.43029, smoothed loss 4.05340, grad norm 5.36294, param norm 129.35106
INFO:root:epoch 7, iter 6300, loss 5.11379, smoothed loss 4.06453, grad norm 5.34929, param norm 129.39392
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 6305, loss 4.56126, smoothed loss 4.05311, grad norm 6.14712, param norm 129.43423
INFO:root:epoch 7, iter 6310, loss 4.36531, smoothed loss 4.05192, grad norm 5.14124, param norm 129.47763
INFO:root:epoch 7, iter 6315, loss 4.35826, smoothed loss 4.06398, grad norm 5.67608, param norm 129.51749
INFO:root:epoch 7, iter 6320, loss 3.49924, smoothed loss 4.05790, grad norm 5.96857, param norm 129.55408
INFO:root:epoch 7, iter 6325, loss 4.39865, smoothed loss 4.07668, grad norm 5.58099, param norm 129.58928
INFO:root:epoch 7, iter 6330, loss 3.91098, smoothed loss 4.06683, grad norm 5.57263, param norm 129.62529
INFO:root:epoch 7, iter 6335, loss 4.46978, smoothed loss 4.07919, grad norm 5.10648, param norm 129.66072
INFO:root:epoch 7, iter 6340, loss 4.25851, smoothed loss 4.07992, grad norm 5.41161, param norm 129.70016
INFO:root:epoch 7, iter 6345, loss 3.33192, smoothed loss 4.07748, grad norm 4.67010, param norm 129.74016
INFO:root:epoch 7, iter 6350, loss 3.69382, smoothed loss 4.07390, grad norm 4.56708, param norm 129.78099
INFO:root:epoch 7, iter 6355, loss 3.67089, smoothed loss 4.07641, grad norm 5.85608, param norm 129.81898
INFO:root:epoch 7, iter 6360, loss 3.88258, smoothed loss 4.06207, grad norm 5.12303, param norm 129.85736
INFO:root:epoch 7, iter 6365, loss 4.19704, smoothed loss 4.06533, grad norm 6.34797, param norm 129.89429
INFO:root:epoch 7, iter 6370, loss 5.10212, smoothed loss 4.08685, grad norm 5.99984, param norm 129.92851
INFO:root:epoch 7, iter 6375, loss 4.27610, smoothed loss 4.08063, grad norm 5.44804, param norm 129.95891
INFO:root:epoch 7, iter 6380, loss 4.81323, smoothed loss 4.10152, grad norm 5.33590, param norm 129.99211
INFO:root:epoch 7, iter 6385, loss 3.63764, smoothed loss 4.08715, grad norm 4.39704, param norm 130.02322
INFO:root:epoch 7, iter 6390, loss 3.53504, smoothed loss 4.08229, grad norm 4.84860, param norm 130.05603
INFO:root:epoch 7, iter 6395, loss 4.61067, smoothed loss 4.09059, grad norm 4.95282, param norm 130.09367
INFO:root:epoch 7, iter 6400, loss 4.39759, smoothed loss 4.08750, grad norm 5.08666, param norm 130.13597
INFO:root:epoch 7, iter 6405, loss 4.41321, smoothed loss 4.07909, grad norm 5.42558, param norm 130.17950
INFO:root:epoch 7, iter 6410, loss 4.12274, smoothed loss 4.06927, grad norm 4.92794, param norm 130.22398
INFO:root:epoch 7, iter 6415, loss 3.94979, smoothed loss 4.07185, grad norm 5.35065, param norm 130.26471
INFO:root:epoch 7, iter 6420, loss 3.44589, smoothed loss 4.06322, grad norm 5.08775, param norm 130.30275
INFO:root:epoch 7, iter 6425, loss 3.33741, smoothed loss 4.05117, grad norm 4.88667, param norm 130.34073
INFO:root:epoch 7, iter 6430, loss 4.81724, smoothed loss 4.05333, grad norm 6.53569, param norm 130.38338
INFO:root:epoch 7, iter 6435, loss 4.15216, smoothed loss 4.05218, grad norm 5.39213, param norm 130.42747
INFO:root:epoch 7, iter 6440, loss 4.52667, smoothed loss 4.04524, grad norm 7.25538, param norm 130.46669
INFO:root:epoch 7, iter 6445, loss 4.69658, smoothed loss 4.05659, grad norm 5.54532, param norm 130.50252
INFO:root:epoch 7, iter 6450, loss 3.79156, smoothed loss 4.05803, grad norm 5.69658, param norm 130.53886
INFO:root:epoch 7, iter 6455, loss 4.31358, smoothed loss 4.05946, grad norm 4.81337, param norm 130.57800
INFO:root:epoch 7, iter 6460, loss 4.23107, smoothed loss 4.05311, grad norm 5.19964, param norm 130.62059
Adding batches start...
Added  144  batches
INFO:root:epoch 7, iter 6465, loss 4.36517, smoothed loss 4.05006, grad norm 5.58982, param norm 130.66222
INFO:root:epoch 7, iter 6470, loss 4.65389, smoothed loss 4.06523, grad norm 4.74592, param norm 130.69922
INFO:root:epoch 7, iter 6475, loss 4.00260, smoothed loss 4.08331, grad norm 6.12613, param norm 130.73012
INFO:root:epoch 7, iter 6480, loss 4.72613, smoothed loss 4.09838, grad norm 5.27981, param norm 130.76433
INFO:root:epoch 7, iter 6485, loss 3.71905, smoothed loss 4.09465, grad norm 4.86498, param norm 130.80212
INFO:root:epoch 7, iter 6490, loss 4.65802, smoothed loss 4.10255, grad norm 6.04945, param norm 130.83870
INFO:root:epoch 7, iter 6495, loss 3.91244, smoothed loss 4.08876, grad norm 5.77516, param norm 130.87834
INFO:root:epoch 7, iter 6500, loss 3.61941, smoothed loss 4.06778, grad norm 5.31785, param norm 130.91563
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 7, Iter 6500, dev loss: 3.533124
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.61514
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.43100
INFO:root:Epoch 7, Iter 6500, Train F1 score: 0.615139, Train EM score: 0.431000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.57271
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.42343
INFO:root:Epoch 7, Iter 6500, Dev F1 score: 0.572712, Dev EM score: 0.423434
INFO:root:End of epoch 7
INFO:root:epoch 7, iter 6505, loss 4.60229, smoothed loss 4.07628, grad norm 5.36369, param norm 130.95168
INFO:root:epoch 7, iter 6510, loss 3.47255, smoothed loss 4.06440, grad norm 4.60804, param norm 130.98688
INFO:root:epoch 7, iter 6515, loss 3.27764, smoothed loss 4.04272, grad norm 5.18575, param norm 131.02950
INFO:root:epoch 7, iter 6520, loss 3.81060, smoothed loss 4.03268, grad norm 4.78081, param norm 131.06772
INFO:root:epoch 7, iter 6525, loss 4.30511, smoothed loss 4.02546, grad norm 5.37964, param norm 131.10342
INFO:root:epoch 7, iter 6530, loss 4.07408, smoothed loss 4.03308, grad norm 4.70897, param norm 131.14288
INFO:root:epoch 7, iter 6535, loss 3.30622, smoothed loss 4.04040, grad norm 3.91285, param norm 131.18109
INFO:root:epoch 7, iter 6540, loss 4.21718, smoothed loss 4.04193, grad norm 5.24535, param norm 131.22340
INFO:root:epoch 7, iter 6545, loss 4.26294, smoothed loss 4.03719, grad norm 5.73234, param norm 131.26535
INFO:root:epoch 7, iter 6550, loss 4.53597, smoothed loss 4.03681, grad norm 5.81480, param norm 131.30524
INFO:root:epoch 7, iter 6555, loss 3.30325, smoothed loss 4.00753, grad norm 5.59713, param norm 131.34309
INFO:root:epoch 7, iter 6560, loss 3.16235, smoothed loss 3.99431, grad norm 5.90075, param norm 131.38173
INFO:root:epoch 7, iter 6565, loss 3.79383, smoothed loss 3.98845, grad norm 5.56398, param norm 131.42104
INFO:root:epoch 7, iter 6570, loss 4.09738, smoothed loss 3.98808, grad norm 6.20884, param norm 131.45963
INFO:root:epoch 7, iter 6575, loss 3.32470, smoothed loss 3.98443, grad norm 4.29657, param norm 131.49634
INFO:root:epoch 7, iter 6580, loss 3.72182, smoothed loss 3.99109, grad norm 4.84620, param norm 131.53397
INFO:root:epoch 7, iter 6585, loss 4.45139, smoothed loss 4.01239, grad norm 5.48748, param norm 131.57014
INFO:root:epoch 7, iter 6590, loss 3.65741, smoothed loss 4.00490, grad norm 5.91191, param norm 131.60631
INFO:root:epoch 7, iter 6595, loss 3.69306, smoothed loss 4.00303, grad norm 4.51237, param norm 131.64522
INFO:root:epoch 7, iter 6600, loss 3.91580, smoothed loss 3.99258, grad norm 4.58610, param norm 131.68430
INFO:root:epoch 7, iter 6605, loss 4.56383, smoothed loss 3.98340, grad norm 5.53237, param norm 131.72476
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6610, loss 3.78531, smoothed loss 3.98006, grad norm 5.27044, param norm 131.76334
INFO:root:epoch 8, iter 6615, loss 3.89639, smoothed loss 3.98327, grad norm 4.73053, param norm 131.79607
INFO:root:epoch 8, iter 6620, loss 3.70427, smoothed loss 3.97366, grad norm 5.11394, param norm 131.82861
INFO:root:epoch 8, iter 6625, loss 3.63025, smoothed loss 3.97529, grad norm 5.15853, param norm 131.85954
INFO:root:epoch 8, iter 6630, loss 3.74856, smoothed loss 3.96673, grad norm 5.68795, param norm 131.89542
INFO:root:epoch 8, iter 6635, loss 3.41893, smoothed loss 3.97232, grad norm 5.23735, param norm 131.93135
INFO:root:epoch 8, iter 6640, loss 4.22772, smoothed loss 3.98370, grad norm 5.35389, param norm 131.96393
INFO:root:epoch 8, iter 6645, loss 4.38986, smoothed loss 3.99287, grad norm 5.23665, param norm 131.99374
INFO:root:epoch 8, iter 6650, loss 4.08904, smoothed loss 3.98811, grad norm 5.59206, param norm 132.02693
INFO:root:epoch 8, iter 6655, loss 3.88138, smoothed loss 3.98415, grad norm 5.17429, param norm 132.06128
INFO:root:epoch 8, iter 6660, loss 3.70013, smoothed loss 3.98661, grad norm 6.76817, param norm 132.10011
INFO:root:epoch 8, iter 6665, loss 3.50073, smoothed loss 3.97940, grad norm 4.67055, param norm 132.13884
INFO:root:epoch 8, iter 6670, loss 3.31584, smoothed loss 3.97126, grad norm 5.65473, param norm 132.17723
INFO:root:epoch 8, iter 6675, loss 3.65143, smoothed loss 3.95922, grad norm 6.15990, param norm 132.21635
INFO:root:epoch 8, iter 6680, loss 4.28897, smoothed loss 3.94278, grad norm 5.69293, param norm 132.25305
INFO:root:epoch 8, iter 6685, loss 3.55745, smoothed loss 3.93833, grad norm 5.51270, param norm 132.28989
INFO:root:epoch 8, iter 6690, loss 4.02066, smoothed loss 3.93348, grad norm 6.66949, param norm 132.32838
INFO:root:epoch 8, iter 6695, loss 5.03028, smoothed loss 3.94478, grad norm 6.18936, param norm 132.36327
INFO:root:epoch 8, iter 6700, loss 3.87555, smoothed loss 3.95207, grad norm 5.02030, param norm 132.39499
INFO:root:epoch 8, iter 6705, loss 4.84210, smoothed loss 3.95069, grad norm 5.05877, param norm 132.42632
INFO:root:epoch 8, iter 6710, loss 3.57336, smoothed loss 3.93802, grad norm 4.75321, param norm 132.45816
INFO:root:epoch 8, iter 6715, loss 4.35343, smoothed loss 3.93668, grad norm 7.49094, param norm 132.49045
INFO:root:epoch 8, iter 6720, loss 4.11245, smoothed loss 3.93060, grad norm 5.99975, param norm 132.52481
INFO:root:epoch 8, iter 6725, loss 3.57804, smoothed loss 3.91716, grad norm 5.58706, param norm 132.56065
INFO:root:epoch 8, iter 6730, loss 3.50688, smoothed loss 3.90755, grad norm 6.04244, param norm 132.59523
INFO:root:epoch 8, iter 6735, loss 4.41487, smoothed loss 3.91981, grad norm 5.32804, param norm 132.62706
INFO:root:epoch 8, iter 6740, loss 3.63368, smoothed loss 3.91123, grad norm 5.82690, param norm 132.66022
INFO:root:epoch 8, iter 6745, loss 3.41718, smoothed loss 3.91183, grad norm 4.89988, param norm 132.69658
INFO:root:epoch 8, iter 6750, loss 3.88526, smoothed loss 3.92379, grad norm 5.13024, param norm 132.73370
INFO:root:epoch 8, iter 6755, loss 3.47006, smoothed loss 3.92177, grad norm 5.29327, param norm 132.76968
INFO:root:epoch 8, iter 6760, loss 3.44750, smoothed loss 3.91184, grad norm 5.41385, param norm 132.80492
INFO:root:epoch 8, iter 6765, loss 3.38094, smoothed loss 3.90906, grad norm 4.38704, param norm 132.84113
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6770, loss 3.59610, smoothed loss 3.90205, grad norm 5.56140, param norm 132.87759
INFO:root:epoch 8, iter 6775, loss 3.78054, smoothed loss 3.89507, grad norm 5.34891, param norm 132.91307
INFO:root:epoch 8, iter 6780, loss 3.63836, smoothed loss 3.88885, grad norm 4.92276, param norm 132.94618
INFO:root:epoch 8, iter 6785, loss 4.20072, smoothed loss 3.88345, grad norm 4.96527, param norm 132.98186
INFO:root:epoch 8, iter 6790, loss 2.96392, smoothed loss 3.87312, grad norm 5.03776, param norm 133.02246
INFO:root:epoch 8, iter 6795, loss 3.85780, smoothed loss 3.86868, grad norm 5.52759, param norm 133.06119
INFO:root:epoch 8, iter 6800, loss 3.73809, smoothed loss 3.87576, grad norm 5.72722, param norm 133.09677
INFO:root:epoch 8, iter 6805, loss 4.06484, smoothed loss 3.87358, grad norm 5.84533, param norm 133.13150
INFO:root:epoch 8, iter 6810, loss 3.79973, smoothed loss 3.87872, grad norm 4.74450, param norm 133.16379
INFO:root:epoch 8, iter 6815, loss 3.59464, smoothed loss 3.86497, grad norm 5.72291, param norm 133.19449
INFO:root:epoch 8, iter 6820, loss 3.41139, smoothed loss 3.85737, grad norm 4.43310, param norm 133.22887
INFO:root:epoch 8, iter 6825, loss 3.54659, smoothed loss 3.85948, grad norm 4.79717, param norm 133.26657
INFO:root:epoch 8, iter 6830, loss 5.09748, smoothed loss 3.86629, grad norm 7.03066, param norm 133.30307
INFO:root:epoch 8, iter 6835, loss 3.87878, smoothed loss 3.89549, grad norm 5.10915, param norm 133.33524
INFO:root:epoch 8, iter 6840, loss 4.08188, smoothed loss 3.89938, grad norm 5.04708, param norm 133.36972
INFO:root:epoch 8, iter 6845, loss 3.39183, smoothed loss 3.88364, grad norm 5.10222, param norm 133.40779
INFO:root:epoch 8, iter 6850, loss 3.69882, smoothed loss 3.88941, grad norm 4.49125, param norm 133.44597
INFO:root:epoch 8, iter 6855, loss 3.81171, smoothed loss 3.89759, grad norm 5.13644, param norm 133.48085
INFO:root:epoch 8, iter 6860, loss 3.44193, smoothed loss 3.89678, grad norm 5.17779, param norm 133.51343
INFO:root:epoch 8, iter 6865, loss 3.72386, smoothed loss 3.90312, grad norm 4.39015, param norm 133.54750
INFO:root:epoch 8, iter 6870, loss 4.00293, smoothed loss 3.88587, grad norm 4.89291, param norm 133.58400
INFO:root:epoch 8, iter 6875, loss 2.99007, smoothed loss 3.86276, grad norm 4.89285, param norm 133.61859
INFO:root:epoch 8, iter 6880, loss 3.62590, smoothed loss 3.86997, grad norm 7.54170, param norm 133.65550
INFO:root:epoch 8, iter 6885, loss 3.35957, smoothed loss 3.85567, grad norm 5.55050, param norm 133.69263
INFO:root:epoch 8, iter 6890, loss 3.75640, smoothed loss 3.86203, grad norm 4.49918, param norm 133.72852
INFO:root:epoch 8, iter 6895, loss 4.11266, smoothed loss 3.86216, grad norm 5.90407, param norm 133.76331
INFO:root:epoch 8, iter 6900, loss 4.01848, smoothed loss 3.87238, grad norm 5.07072, param norm 133.79744
INFO:root:epoch 8, iter 6905, loss 4.13538, smoothed loss 3.87399, grad norm 5.91476, param norm 133.83501
INFO:root:epoch 8, iter 6910, loss 4.49602, smoothed loss 3.88157, grad norm 4.76521, param norm 133.87100
INFO:root:epoch 8, iter 6915, loss 4.06586, smoothed loss 3.87983, grad norm 5.40994, param norm 133.90634
INFO:root:epoch 8, iter 6920, loss 3.40059, smoothed loss 3.87266, grad norm 5.11649, param norm 133.94214
INFO:root:epoch 8, iter 6925, loss 3.81636, smoothed loss 3.88247, grad norm 6.07025, param norm 133.98061
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6930, loss 3.85449, smoothed loss 3.88193, grad norm 4.81635, param norm 134.02229
INFO:root:epoch 8, iter 6935, loss 3.27827, smoothed loss 3.86537, grad norm 5.28150, param norm 134.06270
INFO:root:epoch 8, iter 6940, loss 3.19802, smoothed loss 3.86474, grad norm 5.37134, param norm 134.09953
INFO:root:epoch 8, iter 6945, loss 3.72428, smoothed loss 3.83984, grad norm 5.29639, param norm 134.13818
INFO:root:epoch 8, iter 6950, loss 3.70938, smoothed loss 3.82606, grad norm 6.31075, param norm 134.17453
INFO:root:epoch 8, iter 6955, loss 3.35490, smoothed loss 3.83771, grad norm 5.80253, param norm 134.20433
INFO:root:epoch 8, iter 6960, loss 3.51909, smoothed loss 3.84219, grad norm 4.97972, param norm 134.23451
INFO:root:epoch 8, iter 6965, loss 3.89327, smoothed loss 3.85773, grad norm 5.77419, param norm 134.26859
INFO:root:epoch 8, iter 6970, loss 3.82489, smoothed loss 3.84852, grad norm 4.75359, param norm 134.30379
INFO:root:epoch 8, iter 6975, loss 4.10111, smoothed loss 3.85911, grad norm 5.13008, param norm 134.33551
INFO:root:epoch 8, iter 6980, loss 4.42069, smoothed loss 3.85836, grad norm 7.37040, param norm 134.36630
INFO:root:epoch 8, iter 6985, loss 4.24017, smoothed loss 3.86361, grad norm 6.36909, param norm 134.39854
INFO:root:epoch 8, iter 6990, loss 3.94081, smoothed loss 3.86083, grad norm 5.95441, param norm 134.42914
INFO:root:epoch 8, iter 6995, loss 3.85800, smoothed loss 3.85591, grad norm 5.16257, param norm 134.46350
INFO:root:epoch 8, iter 7000, loss 3.27945, smoothed loss 3.85596, grad norm 4.69538, param norm 134.49883
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 8, Iter 7000, dev loss: 3.440657
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.62848
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.49900
INFO:root:Epoch 8, Iter 7000, Train F1 score: 0.628478, Train EM score: 0.499000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.58118
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.43706
INFO:root:Epoch 8, Iter 7000, Dev F1 score: 0.581184, Dev EM score: 0.437061
INFO:root:End of epoch 8
INFO:root:epoch 8, iter 7005, loss 2.83036, smoothed loss 3.81710, grad norm 4.80503, param norm 134.53372
INFO:root:epoch 8, iter 7010, loss 3.50633, smoothed loss 3.81680, grad norm 5.47448, param norm 134.56752
INFO:root:epoch 8, iter 7015, loss 3.42466, smoothed loss 3.82203, grad norm 5.21584, param norm 134.59497
INFO:root:epoch 8, iter 7020, loss 4.54906, smoothed loss 3.82550, grad norm 5.84918, param norm 134.62454
INFO:root:epoch 8, iter 7025, loss 4.08848, smoothed loss 3.83532, grad norm 5.55558, param norm 134.65446
INFO:root:epoch 8, iter 7030, loss 4.25503, smoothed loss 3.83555, grad norm 5.62145, param norm 134.68271
INFO:root:epoch 8, iter 7035, loss 3.88672, smoothed loss 3.83434, grad norm 4.63100, param norm 134.71352
INFO:root:epoch 8, iter 7040, loss 3.83464, smoothed loss 3.84691, grad norm 5.08630, param norm 134.74451
INFO:root:epoch 8, iter 7045, loss 3.60102, smoothed loss 3.84981, grad norm 5.10181, param norm 134.77777
INFO:root:epoch 8, iter 7050, loss 4.15673, smoothed loss 3.84217, grad norm 5.12559, param norm 134.81630
INFO:root:epoch 8, iter 7055, loss 3.29529, smoothed loss 3.83311, grad norm 4.90753, param norm 134.85126
INFO:root:epoch 8, iter 7060, loss 3.93192, smoothed loss 3.83355, grad norm 5.36610, param norm 134.88643
INFO:root:epoch 8, iter 7065, loss 3.89150, smoothed loss 3.82801, grad norm 4.78614, param norm 134.92357
INFO:root:epoch 8, iter 7070, loss 3.72794, smoothed loss 3.82103, grad norm 5.26342, param norm 134.95892
INFO:root:epoch 8, iter 7075, loss 3.15357, smoothed loss 3.80672, grad norm 4.55619, param norm 134.99347
INFO:root:epoch 8, iter 7080, loss 3.74097, smoothed loss 3.81338, grad norm 6.35405, param norm 135.02847
INFO:root:epoch 8, iter 7085, loss 3.92314, smoothed loss 3.79576, grad norm 5.40277, param norm 135.06613
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 7090, loss 3.78530, smoothed loss 3.79323, grad norm 4.83329, param norm 135.10342
INFO:root:epoch 8, iter 7095, loss 3.09194, smoothed loss 3.78569, grad norm 4.31335, param norm 135.13953
INFO:root:epoch 8, iter 7100, loss 3.44862, smoothed loss 3.79710, grad norm 4.99201, param norm 135.17320
INFO:root:epoch 8, iter 7105, loss 3.82728, smoothed loss 3.79116, grad norm 5.59616, param norm 135.20517
INFO:root:epoch 8, iter 7110, loss 4.21761, smoothed loss 3.78768, grad norm 4.84013, param norm 135.23978
INFO:root:epoch 8, iter 7115, loss 4.47784, smoothed loss 3.78877, grad norm 5.41819, param norm 135.27576
INFO:root:epoch 8, iter 7120, loss 4.17243, smoothed loss 3.80131, grad norm 4.89040, param norm 135.31122
INFO:root:epoch 8, iter 7125, loss 3.84554, smoothed loss 3.79052, grad norm 5.04468, param norm 135.34435
INFO:root:epoch 8, iter 7130, loss 3.65722, smoothed loss 3.78347, grad norm 4.37354, param norm 135.38075
INFO:root:epoch 8, iter 7135, loss 3.53646, smoothed loss 3.79154, grad norm 5.31980, param norm 135.41470
INFO:root:epoch 8, iter 7140, loss 3.83469, smoothed loss 3.78857, grad norm 4.94011, param norm 135.45219
INFO:root:epoch 8, iter 7145, loss 4.71218, smoothed loss 3.79397, grad norm 6.33390, param norm 135.49156
INFO:root:epoch 8, iter 7150, loss 3.48495, smoothed loss 3.79539, grad norm 4.52777, param norm 135.52434
INFO:root:epoch 8, iter 7155, loss 3.75855, smoothed loss 3.79703, grad norm 5.77750, param norm 135.55624
INFO:root:epoch 8, iter 7160, loss 3.00980, smoothed loss 3.77493, grad norm 5.45980, param norm 135.59154
INFO:root:epoch 8, iter 7165, loss 3.95345, smoothed loss 3.76929, grad norm 5.31790, param norm 135.63087
INFO:root:epoch 8, iter 7170, loss 4.12577, smoothed loss 3.78755, grad norm 5.39205, param norm 135.66647
INFO:root:epoch 8, iter 7175, loss 3.99026, smoothed loss 3.78764, grad norm 5.02619, param norm 135.70117
INFO:root:epoch 8, iter 7180, loss 3.93415, smoothed loss 3.76921, grad norm 5.30993, param norm 135.73807
INFO:root:epoch 8, iter 7185, loss 4.12912, smoothed loss 3.76172, grad norm 5.50708, param norm 135.77628
INFO:root:epoch 8, iter 7190, loss 4.07503, smoothed loss 3.76904, grad norm 5.93355, param norm 135.80931
INFO:root:epoch 8, iter 7195, loss 4.08187, smoothed loss 3.77851, grad norm 5.42727, param norm 135.84073
INFO:root:epoch 8, iter 7200, loss 3.41718, smoothed loss 3.77283, grad norm 4.72773, param norm 135.87091
INFO:root:epoch 8, iter 7205, loss 3.89094, smoothed loss 3.76818, grad norm 5.73478, param norm 135.90570
INFO:root:epoch 8, iter 7210, loss 3.22778, smoothed loss 3.76866, grad norm 5.24141, param norm 135.93835
INFO:root:epoch 8, iter 7215, loss 4.12675, smoothed loss 3.77618, grad norm 4.78032, param norm 135.97169
INFO:root:epoch 8, iter 7220, loss 3.98658, smoothed loss 3.78588, grad norm 5.41909, param norm 135.99994
INFO:root:epoch 8, iter 7225, loss 4.58336, smoothed loss 3.78642, grad norm 4.97090, param norm 136.03110
INFO:root:epoch 8, iter 7230, loss 3.42997, smoothed loss 3.77790, grad norm 5.63925, param norm 136.06340
INFO:root:epoch 8, iter 7235, loss 3.45511, smoothed loss 3.76824, grad norm 5.40668, param norm 136.09937
INFO:root:epoch 8, iter 7240, loss 4.39142, smoothed loss 3.78190, grad norm 5.97686, param norm 136.13564
INFO:root:epoch 8, iter 7245, loss 3.62468, smoothed loss 3.77132, grad norm 5.62293, param norm 136.17099
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 7250, loss 4.11458, smoothed loss 3.76895, grad norm 4.95382, param norm 136.20448
INFO:root:epoch 8, iter 7255, loss 4.26523, smoothed loss 3.76964, grad norm 4.88319, param norm 136.23830
INFO:root:epoch 8, iter 7260, loss 3.52539, smoothed loss 3.76941, grad norm 4.68148, param norm 136.26997
INFO:root:epoch 8, iter 7265, loss 4.00466, smoothed loss 3.77969, grad norm 4.97927, param norm 136.30318
INFO:root:epoch 8, iter 7270, loss 3.85374, smoothed loss 3.78949, grad norm 5.65125, param norm 136.33603
INFO:root:epoch 8, iter 7275, loss 3.83248, smoothed loss 3.80295, grad norm 5.98431, param norm 136.36929
INFO:root:epoch 8, iter 7280, loss 3.86852, smoothed loss 3.80167, grad norm 5.16501, param norm 136.40187
INFO:root:epoch 8, iter 7285, loss 3.94105, smoothed loss 3.79323, grad norm 5.40164, param norm 136.43614
INFO:root:epoch 8, iter 7290, loss 4.14912, smoothed loss 3.78869, grad norm 5.06755, param norm 136.46909
INFO:root:epoch 8, iter 7295, loss 3.65949, smoothed loss 3.80049, grad norm 4.78675, param norm 136.50333
INFO:root:epoch 8, iter 7300, loss 4.24290, smoothed loss 3.81063, grad norm 4.87286, param norm 136.53966
INFO:root:epoch 8, iter 7305, loss 3.39372, smoothed loss 3.80721, grad norm 4.64445, param norm 136.57034
INFO:root:epoch 8, iter 7310, loss 4.03324, smoothed loss 3.81075, grad norm 4.97415, param norm 136.60167
INFO:root:epoch 8, iter 7315, loss 3.65799, smoothed loss 3.79221, grad norm 5.02472, param norm 136.63434
INFO:root:epoch 8, iter 7320, loss 3.20345, smoothed loss 3.79391, grad norm 5.08814, param norm 136.67413
INFO:root:epoch 8, iter 7325, loss 3.65671, smoothed loss 3.79626, grad norm 5.01193, param norm 136.71225
INFO:root:epoch 8, iter 7330, loss 4.08094, smoothed loss 3.78578, grad norm 5.14309, param norm 136.74698
INFO:root:epoch 8, iter 7335, loss 4.06546, smoothed loss 3.79063, grad norm 4.78705, param norm 136.78064
INFO:root:epoch 8, iter 7340, loss 3.36873, smoothed loss 3.78412, grad norm 5.33904, param norm 136.81342
INFO:root:epoch 8, iter 7345, loss 4.02573, smoothed loss 3.80829, grad norm 5.37302, param norm 136.84238
INFO:root:epoch 8, iter 7350, loss 3.06491, smoothed loss 3.78956, grad norm 4.25531, param norm 136.87297
INFO:root:epoch 8, iter 7355, loss 3.98438, smoothed loss 3.80583, grad norm 4.86954, param norm 136.90948
INFO:root:epoch 8, iter 7360, loss 3.55031, smoothed loss 3.79424, grad norm 4.91256, param norm 136.94678
INFO:root:epoch 8, iter 7365, loss 4.08739, smoothed loss 3.81624, grad norm 4.84869, param norm 136.98567
INFO:root:epoch 8, iter 7370, loss 3.70795, smoothed loss 3.81824, grad norm 5.09977, param norm 137.02126
INFO:root:epoch 8, iter 7375, loss 3.67096, smoothed loss 3.81536, grad norm 6.92835, param norm 137.05824
INFO:root:epoch 8, iter 7380, loss 3.43065, smoothed loss 3.80775, grad norm 5.79077, param norm 137.09256
INFO:root:epoch 8, iter 7385, loss 3.57094, smoothed loss 3.79355, grad norm 5.21048, param norm 137.12814
INFO:root:epoch 8, iter 7390, loss 4.01449, smoothed loss 3.78485, grad norm 5.13230, param norm 137.16266
INFO:root:epoch 8, iter 7395, loss 4.60773, smoothed loss 3.80374, grad norm 6.62693, param norm 137.19518
INFO:root:epoch 8, iter 7400, loss 4.35394, smoothed loss 3.81659, grad norm 4.78593, param norm 137.22655
INFO:root:epoch 8, iter 7405, loss 3.27651, smoothed loss 3.81737, grad norm 4.41642, param norm 137.25545
Adding batches start...
Added  144  batches
INFO:root:epoch 8, iter 7410, loss 3.77877, smoothed loss 3.80494, grad norm 4.93885, param norm 137.28859
INFO:root:epoch 8, iter 7415, loss 3.55938, smoothed loss 3.79389, grad norm 4.37073, param norm 137.32434
INFO:root:epoch 8, iter 7420, loss 4.12128, smoothed loss 3.78604, grad norm 5.40604, param norm 137.36182
INFO:root:epoch 8, iter 7425, loss 3.76763, smoothed loss 3.77920, grad norm 5.50029, param norm 137.39796
INFO:root:epoch 8, iter 7430, loss 4.29309, smoothed loss 3.78742, grad norm 6.52819, param norm 137.43417
INFO:root:epoch 8, iter 7435, loss 3.40557, smoothed loss 3.77395, grad norm 4.50417, param norm 137.46524
INFO:root:epoch 8, iter 7440, loss 3.87050, smoothed loss 3.78504, grad norm 5.37618, param norm 137.49113
INFO:root:epoch 8, iter 7445, loss 3.90156, smoothed loss 3.78743, grad norm 5.21942, param norm 137.52130
INFO:root:epoch 8, iter 7450, loss 4.15318, smoothed loss 3.77238, grad norm 5.29765, param norm 137.55290
INFO:root:epoch 8, iter 7455, loss 3.37446, smoothed loss 3.76798, grad norm 5.63900, param norm 137.58699
INFO:root:epoch 8, iter 7460, loss 3.70703, smoothed loss 3.76815, grad norm 5.47103, param norm 137.62350
INFO:root:epoch 8, iter 7465, loss 4.61234, smoothed loss 3.76790, grad norm 6.41800, param norm 137.65553
INFO:root:epoch 8, iter 7470, loss 4.02371, smoothed loss 3.77466, grad norm 4.81270, param norm 137.68291
INFO:root:epoch 8, iter 7475, loss 3.26427, smoothed loss 3.77711, grad norm 5.06809, param norm 137.71208
INFO:root:epoch 8, iter 7480, loss 3.17724, smoothed loss 3.76310, grad norm 4.20142, param norm 137.74246
INFO:root:epoch 8, iter 7485, loss 4.16813, smoothed loss 3.76577, grad norm 5.39832, param norm 137.77455
INFO:root:epoch 8, iter 7490, loss 3.51989, smoothed loss 3.76913, grad norm 5.17850, param norm 137.80461
INFO:root:epoch 8, iter 7495, loss 3.26637, smoothed loss 3.76108, grad norm 4.80591, param norm 137.83646
INFO:root:epoch 8, iter 7500, loss 4.66093, smoothed loss 3.76263, grad norm 5.85649, param norm 137.87122
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 8, Iter 7500, dev loss: 3.330329
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.63261
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.48400
INFO:root:Epoch 8, Iter 7500, Train F1 score: 0.632615, Train EM score: 0.484000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.60169
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.45434
INFO:root:Epoch 8, Iter 7500, Dev F1 score: 0.601686, Dev EM score: 0.454341
INFO:root:End of epoch 8
INFO:root:epoch 8, iter 7505, loss 4.35315, smoothed loss 3.77103, grad norm 5.31997, param norm 137.90364
INFO:root:epoch 8, iter 7510, loss 4.05818, smoothed loss 3.76026, grad norm 5.23705, param norm 137.93536
INFO:root:epoch 8, iter 7515, loss 3.56670, smoothed loss 3.76209, grad norm 4.55468, param norm 137.96329
INFO:root:epoch 8, iter 7520, loss 3.79816, smoothed loss 3.77080, grad norm 4.65034, param norm 137.99289
INFO:root:epoch 8, iter 7525, loss 3.68312, smoothed loss 3.76919, grad norm 4.87984, param norm 138.02789
INFO:root:epoch 8, iter 7530, loss 3.79668, smoothed loss 3.76877, grad norm 5.27017, param norm 138.06258
INFO:root:epoch 8, iter 7535, loss 3.90926, smoothed loss 3.78217, grad norm 4.60617, param norm 138.09749
INFO:root:epoch 8, iter 7540, loss 4.03629, smoothed loss 3.78658, grad norm 6.41999, param norm 138.13025
INFO:root:epoch 8, iter 7545, loss 3.84971, smoothed loss 3.78067, grad norm 5.03177, param norm 138.16052
INFO:root:epoch 8, iter 7550, loss 3.76278, smoothed loss 3.78313, grad norm 5.16675, param norm 138.19002
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7555, loss 3.43123, smoothed loss 3.78020, grad norm 6.07467, param norm 138.22192
INFO:root:epoch 9, iter 7560, loss 3.81572, smoothed loss 3.76575, grad norm 4.92164, param norm 138.25429
INFO:root:epoch 9, iter 7565, loss 3.40871, smoothed loss 3.75312, grad norm 4.33370, param norm 138.28716
INFO:root:epoch 9, iter 7570, loss 4.02155, smoothed loss 3.75725, grad norm 5.45945, param norm 138.31786
INFO:root:epoch 9, iter 7575, loss 3.89888, smoothed loss 3.75703, grad norm 5.93674, param norm 138.35191
INFO:root:epoch 9, iter 7580, loss 3.69600, smoothed loss 3.76221, grad norm 4.66365, param norm 138.38692
INFO:root:epoch 9, iter 7585, loss 3.78730, smoothed loss 3.74799, grad norm 5.01852, param norm 138.42256
INFO:root:epoch 9, iter 7590, loss 3.95206, smoothed loss 3.75792, grad norm 4.90895, param norm 138.45685
INFO:root:epoch 9, iter 7595, loss 3.57103, smoothed loss 3.74429, grad norm 5.17721, param norm 138.48895
INFO:root:epoch 9, iter 7600, loss 3.84982, smoothed loss 3.72677, grad norm 5.26817, param norm 138.52173
INFO:root:epoch 9, iter 7605, loss 3.49597, smoothed loss 3.72149, grad norm 6.12920, param norm 138.55226
INFO:root:epoch 9, iter 7610, loss 3.66104, smoothed loss 3.72856, grad norm 5.28200, param norm 138.58263
INFO:root:epoch 9, iter 7615, loss 3.07705, smoothed loss 3.71918, grad norm 5.56554, param norm 138.61249
INFO:root:epoch 9, iter 7620, loss 3.15287, smoothed loss 3.70092, grad norm 4.47605, param norm 138.64590
INFO:root:epoch 9, iter 7625, loss 4.51596, smoothed loss 3.71659, grad norm 5.34062, param norm 138.68030
INFO:root:epoch 9, iter 7630, loss 3.51881, smoothed loss 3.72643, grad norm 5.40366, param norm 138.71211
INFO:root:epoch 9, iter 7635, loss 3.11139, smoothed loss 3.71179, grad norm 4.21622, param norm 138.75050
INFO:root:epoch 9, iter 7640, loss 3.73388, smoothed loss 3.71159, grad norm 5.61520, param norm 138.78900
INFO:root:epoch 9, iter 7645, loss 2.73478, smoothed loss 3.70219, grad norm 4.86172, param norm 138.82257
INFO:root:epoch 9, iter 7650, loss 3.32597, smoothed loss 3.69372, grad norm 4.64180, param norm 138.85390
INFO:root:epoch 9, iter 7655, loss 3.92662, smoothed loss 3.69547, grad norm 5.83221, param norm 138.88562
INFO:root:epoch 9, iter 7660, loss 3.50846, smoothed loss 3.68606, grad norm 4.98267, param norm 138.91904
INFO:root:epoch 9, iter 7665, loss 4.03962, smoothed loss 3.68038, grad norm 4.98101, param norm 138.95256
INFO:root:epoch 9, iter 7670, loss 3.99170, smoothed loss 3.68885, grad norm 4.62749, param norm 138.98494
INFO:root:epoch 9, iter 7675, loss 4.36202, smoothed loss 3.70753, grad norm 5.20235, param norm 139.01898
INFO:root:epoch 9, iter 7680, loss 3.91661, smoothed loss 3.70938, grad norm 4.84153, param norm 139.05182
INFO:root:epoch 9, iter 7685, loss 3.64265, smoothed loss 3.72051, grad norm 5.57938, param norm 139.08163
INFO:root:epoch 9, iter 7690, loss 4.18865, smoothed loss 3.72862, grad norm 5.62207, param norm 139.11397
INFO:root:epoch 9, iter 7695, loss 3.67914, smoothed loss 3.74028, grad norm 5.11311, param norm 139.14764
INFO:root:epoch 9, iter 7700, loss 3.60558, smoothed loss 3.74020, grad norm 4.08401, param norm 139.18040
INFO:root:epoch 9, iter 7705, loss 3.70096, smoothed loss 3.74168, grad norm 5.11424, param norm 139.20879
INFO:root:epoch 9, iter 7710, loss 3.84019, smoothed loss 3.74230, grad norm 5.14345, param norm 139.24048
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7715, loss 4.65378, smoothed loss 3.75554, grad norm 5.04907, param norm 139.26985
INFO:root:epoch 9, iter 7720, loss 3.95845, smoothed loss 3.74549, grad norm 4.57482, param norm 139.29956
INFO:root:epoch 9, iter 7725, loss 4.02961, smoothed loss 3.75496, grad norm 5.31579, param norm 139.32872
INFO:root:epoch 9, iter 7730, loss 3.31053, smoothed loss 3.74384, grad norm 4.52152, param norm 139.35895
INFO:root:epoch 9, iter 7735, loss 3.51452, smoothed loss 3.74903, grad norm 4.80701, param norm 139.39044
INFO:root:epoch 9, iter 7740, loss 3.86724, smoothed loss 3.74060, grad norm 5.45047, param norm 139.42047
INFO:root:epoch 9, iter 7745, loss 3.83215, smoothed loss 3.72715, grad norm 5.28183, param norm 139.45171
INFO:root:epoch 9, iter 7750, loss 3.96621, smoothed loss 3.72139, grad norm 5.50046, param norm 139.48489
INFO:root:epoch 9, iter 7755, loss 3.90269, smoothed loss 3.72275, grad norm 4.54093, param norm 139.51869
INFO:root:epoch 9, iter 7760, loss 3.26707, smoothed loss 3.71121, grad norm 3.92036, param norm 139.54993
INFO:root:epoch 9, iter 7765, loss 4.39228, smoothed loss 3.70444, grad norm 5.77869, param norm 139.58214
INFO:root:epoch 9, iter 7770, loss 3.34150, smoothed loss 3.69879, grad norm 4.60591, param norm 139.61525
INFO:root:epoch 9, iter 7775, loss 3.57201, smoothed loss 3.68927, grad norm 5.49178, param norm 139.64651
INFO:root:epoch 9, iter 7780, loss 3.50653, smoothed loss 3.69096, grad norm 4.94129, param norm 139.67986
INFO:root:epoch 9, iter 7785, loss 3.67134, smoothed loss 3.68536, grad norm 5.59490, param norm 139.71019
INFO:root:epoch 9, iter 7790, loss 3.49301, smoothed loss 3.67340, grad norm 4.55320, param norm 139.74257
INFO:root:epoch 9, iter 7795, loss 3.95519, smoothed loss 3.67572, grad norm 5.83797, param norm 139.77458
INFO:root:epoch 9, iter 7800, loss 3.73267, smoothed loss 3.68266, grad norm 5.45860, param norm 139.80698
INFO:root:epoch 9, iter 7805, loss 4.59203, smoothed loss 3.68466, grad norm 5.28262, param norm 139.84109
INFO:root:epoch 9, iter 7810, loss 3.27265, smoothed loss 3.68093, grad norm 4.64977, param norm 139.87788
INFO:root:epoch 9, iter 7815, loss 3.61548, smoothed loss 3.67816, grad norm 5.34063, param norm 139.91333
INFO:root:epoch 9, iter 7820, loss 3.66016, smoothed loss 3.68143, grad norm 6.27179, param norm 139.95016
INFO:root:epoch 9, iter 7825, loss 3.70632, smoothed loss 3.68432, grad norm 4.81471, param norm 139.98958
INFO:root:epoch 9, iter 7830, loss 3.36496, smoothed loss 3.68082, grad norm 4.35262, param norm 140.02359
INFO:root:epoch 9, iter 7835, loss 3.33154, smoothed loss 3.68013, grad norm 4.07364, param norm 140.05763
INFO:root:epoch 9, iter 7840, loss 3.21219, smoothed loss 3.67093, grad norm 4.69993, param norm 140.09081
INFO:root:epoch 9, iter 7845, loss 3.75258, smoothed loss 3.67647, grad norm 4.64996, param norm 140.12338
INFO:root:epoch 9, iter 7850, loss 3.24920, smoothed loss 3.67937, grad norm 4.98901, param norm 140.15181
INFO:root:epoch 9, iter 7855, loss 4.39776, smoothed loss 3.69929, grad norm 6.02289, param norm 140.18134
INFO:root:epoch 9, iter 7860, loss 3.54004, smoothed loss 3.70060, grad norm 5.30813, param norm 140.21582
INFO:root:epoch 9, iter 7865, loss 3.34256, smoothed loss 3.69038, grad norm 5.64082, param norm 140.25227
INFO:root:epoch 9, iter 7870, loss 2.89039, smoothed loss 3.68673, grad norm 4.48324, param norm 140.28937
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7875, loss 4.20616, smoothed loss 3.70394, grad norm 5.45785, param norm 140.32161
INFO:root:epoch 9, iter 7880, loss 4.06644, smoothed loss 3.70985, grad norm 5.80174, param norm 140.34972
INFO:root:epoch 9, iter 7885, loss 3.24941, smoothed loss 3.69437, grad norm 4.16679, param norm 140.38306
INFO:root:epoch 9, iter 7890, loss 3.72528, smoothed loss 3.69596, grad norm 5.22013, param norm 140.41446
INFO:root:epoch 9, iter 7895, loss 3.61018, smoothed loss 3.69439, grad norm 5.13182, param norm 140.44496
INFO:root:epoch 9, iter 7900, loss 4.04658, smoothed loss 3.67633, grad norm 4.79516, param norm 140.47792
INFO:root:epoch 9, iter 7905, loss 3.50095, smoothed loss 3.67558, grad norm 4.74345, param norm 140.51039
INFO:root:epoch 9, iter 7910, loss 3.52480, smoothed loss 3.66540, grad norm 4.88196, param norm 140.54332
INFO:root:epoch 9, iter 7915, loss 3.22052, smoothed loss 3.66780, grad norm 4.31536, param norm 140.57677
INFO:root:epoch 9, iter 7920, loss 4.04520, smoothed loss 3.65344, grad norm 5.72846, param norm 140.60936
INFO:root:epoch 9, iter 7925, loss 4.61704, smoothed loss 3.66797, grad norm 5.25258, param norm 140.64095
INFO:root:epoch 9, iter 7930, loss 3.52989, smoothed loss 3.66144, grad norm 4.80410, param norm 140.67134
INFO:root:epoch 9, iter 7935, loss 3.81123, smoothed loss 3.65698, grad norm 5.28716, param norm 140.70274
INFO:root:epoch 9, iter 7940, loss 3.72983, smoothed loss 3.65485, grad norm 5.04742, param norm 140.73605
INFO:root:epoch 9, iter 7945, loss 4.32058, smoothed loss 3.65105, grad norm 4.71418, param norm 140.76588
INFO:root:epoch 9, iter 7950, loss 3.46978, smoothed loss 3.64951, grad norm 6.37179, param norm 140.79625
INFO:root:epoch 9, iter 7955, loss 3.32740, smoothed loss 3.63946, grad norm 4.71677, param norm 140.83035
INFO:root:epoch 9, iter 7960, loss 3.53440, smoothed loss 3.62951, grad norm 6.03531, param norm 140.86389
INFO:root:epoch 9, iter 7965, loss 3.13260, smoothed loss 3.62981, grad norm 4.90254, param norm 140.89749
INFO:root:epoch 9, iter 7970, loss 3.56462, smoothed loss 3.62673, grad norm 4.82423, param norm 140.92744
INFO:root:epoch 9, iter 7975, loss 3.33341, smoothed loss 3.61531, grad norm 4.85094, param norm 140.95831
INFO:root:epoch 9, iter 7980, loss 3.89099, smoothed loss 3.60905, grad norm 4.96856, param norm 140.98955
INFO:root:epoch 9, iter 7985, loss 3.58483, smoothed loss 3.61271, grad norm 5.76806, param norm 141.01881
INFO:root:epoch 9, iter 7990, loss 4.16995, smoothed loss 3.60905, grad norm 5.60712, param norm 141.04555
INFO:root:epoch 9, iter 7995, loss 3.54608, smoothed loss 3.59295, grad norm 4.44222, param norm 141.07463
INFO:root:epoch 9, iter 8000, loss 3.62843, smoothed loss 3.58770, grad norm 5.37250, param norm 141.10506
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 9, Iter 8000, dev loss: 3.268823
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.65097
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.52900
INFO:root:Epoch 9, Iter 8000, Train F1 score: 0.650969, Train EM score: 0.529000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.60953
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.46108
INFO:root:Epoch 9, Iter 8000, Dev F1 score: 0.609529, Dev EM score: 0.461085
INFO:root:End of epoch 9
INFO:root:epoch 9, iter 8005, loss 3.05761, smoothed loss 3.58905, grad norm 4.30313, param norm 141.13730
INFO:root:epoch 9, iter 8010, loss 3.26965, smoothed loss 3.58557, grad norm 4.70942, param norm 141.16800
INFO:root:epoch 9, iter 8015, loss 3.72934, smoothed loss 3.57195, grad norm 5.35768, param norm 141.20175
INFO:root:epoch 9, iter 8020, loss 3.44380, smoothed loss 3.58115, grad norm 4.83976, param norm 141.23286
INFO:root:epoch 9, iter 8025, loss 2.87822, smoothed loss 3.58416, grad norm 5.50555, param norm 141.26137
INFO:root:epoch 9, iter 8030, loss 3.57905, smoothed loss 3.57947, grad norm 5.16963, param norm 141.29047
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 8035, loss 3.98918, smoothed loss 3.59008, grad norm 6.31217, param norm 141.31868
INFO:root:epoch 9, iter 8040, loss 3.92451, smoothed loss 3.58554, grad norm 6.03567, param norm 141.34723
INFO:root:epoch 9, iter 8045, loss 3.51718, smoothed loss 3.59067, grad norm 3.98951, param norm 141.37738
INFO:root:epoch 9, iter 8050, loss 3.40509, smoothed loss 3.59018, grad norm 4.34758, param norm 141.40976
INFO:root:epoch 9, iter 8055, loss 3.74534, smoothed loss 3.57556, grad norm 5.98482, param norm 141.44008
INFO:root:epoch 9, iter 8060, loss 3.15867, smoothed loss 3.57898, grad norm 5.35362, param norm 141.46846
INFO:root:epoch 9, iter 8065, loss 3.76102, smoothed loss 3.58207, grad norm 5.19606, param norm 141.50084
INFO:root:epoch 9, iter 8070, loss 3.60321, smoothed loss 3.59173, grad norm 4.93566, param norm 141.53282
INFO:root:epoch 9, iter 8075, loss 3.23459, smoothed loss 3.59233, grad norm 4.91459, param norm 141.56833
INFO:root:epoch 9, iter 8080, loss 4.36357, smoothed loss 3.60224, grad norm 6.25545, param norm 141.60042
INFO:root:epoch 9, iter 8085, loss 3.86420, smoothed loss 3.61303, grad norm 5.63400, param norm 141.63040
INFO:root:epoch 9, iter 8090, loss 3.50986, smoothed loss 3.60036, grad norm 4.70133, param norm 141.66339
INFO:root:epoch 9, iter 8095, loss 3.22400, smoothed loss 3.60811, grad norm 5.05516, param norm 141.69701
INFO:root:epoch 9, iter 8100, loss 3.63114, smoothed loss 3.61033, grad norm 5.49915, param norm 141.72839
INFO:root:epoch 9, iter 8105, loss 4.03651, smoothed loss 3.61527, grad norm 5.24040, param norm 141.76315
INFO:root:epoch 9, iter 8110, loss 3.83290, smoothed loss 3.61699, grad norm 5.75871, param norm 141.79575
INFO:root:epoch 9, iter 8115, loss 3.91066, smoothed loss 3.60531, grad norm 4.82956, param norm 141.82745
INFO:root:epoch 9, iter 8120, loss 3.72386, smoothed loss 3.60234, grad norm 5.82337, param norm 141.86145
INFO:root:epoch 9, iter 8125, loss 3.28861, smoothed loss 3.59591, grad norm 4.65636, param norm 141.89557
INFO:root:epoch 9, iter 8130, loss 3.26915, smoothed loss 3.59450, grad norm 4.71221, param norm 141.92871
INFO:root:epoch 9, iter 8135, loss 4.14937, smoothed loss 3.59354, grad norm 6.38071, param norm 141.96225
INFO:root:epoch 9, iter 8140, loss 3.65923, smoothed loss 3.59361, grad norm 4.52833, param norm 141.99763
INFO:root:epoch 9, iter 8145, loss 3.12814, smoothed loss 3.58315, grad norm 4.84515, param norm 142.03491
INFO:root:epoch 9, iter 8150, loss 3.10045, smoothed loss 3.57473, grad norm 4.49872, param norm 142.06700
INFO:root:epoch 9, iter 8155, loss 3.28204, smoothed loss 3.56144, grad norm 5.52733, param norm 142.09801
INFO:root:epoch 9, iter 8160, loss 3.50192, smoothed loss 3.57846, grad norm 4.51741, param norm 142.12706
INFO:root:epoch 9, iter 8165, loss 3.57923, smoothed loss 3.55853, grad norm 5.10021, param norm 142.15907
INFO:root:epoch 9, iter 8170, loss 3.38603, smoothed loss 3.55101, grad norm 5.38517, param norm 142.19351
INFO:root:epoch 9, iter 8175, loss 3.72017, smoothed loss 3.57326, grad norm 5.68507, param norm 142.22737
INFO:root:epoch 9, iter 8180, loss 3.77317, smoothed loss 3.58592, grad norm 4.51771, param norm 142.26115
INFO:root:epoch 9, iter 8185, loss 3.52316, smoothed loss 3.57165, grad norm 4.85228, param norm 142.29648
INFO:root:epoch 9, iter 8190, loss 3.49072, smoothed loss 3.57031, grad norm 4.68376, param norm 142.33275
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 8195, loss 4.66053, smoothed loss 3.57049, grad norm 5.23133, param norm 142.36787
INFO:root:epoch 9, iter 8200, loss 3.90655, smoothed loss 3.56917, grad norm 5.94971, param norm 142.39850
INFO:root:epoch 9, iter 8205, loss 3.53284, smoothed loss 3.58019, grad norm 5.48632, param norm 142.42360
INFO:root:epoch 9, iter 8210, loss 3.97049, smoothed loss 3.58535, grad norm 4.92780, param norm 142.45314
INFO:root:epoch 9, iter 8215, loss 3.94370, smoothed loss 3.58949, grad norm 4.57221, param norm 142.48453
INFO:root:epoch 9, iter 8220, loss 3.71896, smoothed loss 3.59439, grad norm 4.59209, param norm 142.51254
INFO:root:epoch 9, iter 8225, loss 3.79628, smoothed loss 3.60368, grad norm 5.37795, param norm 142.54344
INFO:root:epoch 9, iter 8230, loss 3.12764, smoothed loss 3.60014, grad norm 4.31361, param norm 142.57501
INFO:root:epoch 9, iter 8235, loss 3.59390, smoothed loss 3.59403, grad norm 5.37346, param norm 142.60481
INFO:root:epoch 9, iter 8240, loss 4.05931, smoothed loss 3.61185, grad norm 5.38359, param norm 142.63457
INFO:root:epoch 9, iter 8245, loss 3.33668, smoothed loss 3.60980, grad norm 4.96011, param norm 142.66190
INFO:root:epoch 9, iter 8250, loss 3.66905, smoothed loss 3.62006, grad norm 4.82611, param norm 142.69008
INFO:root:epoch 9, iter 8255, loss 3.83626, smoothed loss 3.63033, grad norm 4.47741, param norm 142.72034
INFO:root:epoch 9, iter 8260, loss 3.73122, smoothed loss 3.62676, grad norm 5.16551, param norm 142.75233
INFO:root:epoch 9, iter 8265, loss 4.08568, smoothed loss 3.62394, grad norm 5.26399, param norm 142.78781
INFO:root:epoch 9, iter 8270, loss 3.69544, smoothed loss 3.61876, grad norm 5.47479, param norm 142.82346
INFO:root:epoch 9, iter 8275, loss 2.72520, smoothed loss 3.61242, grad norm 4.02494, param norm 142.85474
INFO:root:epoch 9, iter 8280, loss 3.65738, smoothed loss 3.60710, grad norm 4.90452, param norm 142.88426
INFO:root:epoch 9, iter 8285, loss 3.27302, smoothed loss 3.60660, grad norm 4.93686, param norm 142.91286
INFO:root:epoch 9, iter 8290, loss 3.62351, smoothed loss 3.61417, grad norm 4.81680, param norm 142.94174
INFO:root:epoch 9, iter 8295, loss 4.27082, smoothed loss 3.62353, grad norm 5.22834, param norm 142.97545
INFO:root:epoch 9, iter 8300, loss 3.19477, smoothed loss 3.61426, grad norm 4.24027, param norm 143.00813
INFO:root:epoch 9, iter 8305, loss 3.16438, smoothed loss 3.60771, grad norm 4.28165, param norm 143.04259
INFO:root:epoch 9, iter 8310, loss 3.47882, smoothed loss 3.60569, grad norm 4.70525, param norm 143.07945
INFO:root:epoch 9, iter 8315, loss 3.22467, smoothed loss 3.61041, grad norm 5.55629, param norm 143.11652
INFO:root:epoch 9, iter 8320, loss 3.96585, smoothed loss 3.61399, grad norm 5.21513, param norm 143.15326
INFO:root:epoch 9, iter 8325, loss 4.77386, smoothed loss 3.64524, grad norm 5.40465, param norm 143.18527
INFO:root:epoch 9, iter 8330, loss 3.61636, smoothed loss 3.65710, grad norm 4.85917, param norm 143.21312
INFO:root:epoch 9, iter 8335, loss 3.69364, smoothed loss 3.65279, grad norm 4.79987, param norm 143.24232
INFO:root:epoch 9, iter 8340, loss 3.80105, smoothed loss 3.65304, grad norm 5.39440, param norm 143.27119
INFO:root:epoch 9, iter 8345, loss 3.05915, smoothed loss 3.64106, grad norm 5.09536, param norm 143.30295
INFO:root:epoch 9, iter 8350, loss 3.40598, smoothed loss 3.64233, grad norm 4.75334, param norm 143.33334
Adding batches start...
Added  144  batches
INFO:root:epoch 9, iter 8355, loss 3.85563, smoothed loss 3.65106, grad norm 5.31648, param norm 143.36656
INFO:root:epoch 9, iter 8360, loss 3.91551, smoothed loss 3.65746, grad norm 5.86844, param norm 143.39798
INFO:root:epoch 9, iter 8365, loss 4.30966, smoothed loss 3.67656, grad norm 5.56172, param norm 143.42915
INFO:root:epoch 9, iter 8370, loss 3.80068, smoothed loss 3.66969, grad norm 5.10276, param norm 143.46323
INFO:root:epoch 9, iter 8375, loss 3.99157, smoothed loss 3.66328, grad norm 5.10377, param norm 143.49686
INFO:root:epoch 9, iter 8380, loss 3.34429, smoothed loss 3.65230, grad norm 5.00887, param norm 143.52907
INFO:root:epoch 9, iter 8385, loss 3.16964, smoothed loss 3.65841, grad norm 4.65683, param norm 143.56255
INFO:root:epoch 9, iter 8390, loss 4.07673, smoothed loss 3.66681, grad norm 4.91045, param norm 143.59541
INFO:root:epoch 9, iter 8395, loss 2.70392, smoothed loss 3.65592, grad norm 3.85666, param norm 143.62585
INFO:root:epoch 9, iter 8400, loss 3.97765, smoothed loss 3.66045, grad norm 4.73974, param norm 143.65753
INFO:root:epoch 9, iter 8405, loss 3.14524, smoothed loss 3.66106, grad norm 4.37671, param norm 143.68842
INFO:root:epoch 9, iter 8410, loss 3.56825, smoothed loss 3.64202, grad norm 5.18747, param norm 143.72006
INFO:root:epoch 9, iter 8415, loss 3.09306, smoothed loss 3.63185, grad norm 4.70919, param norm 143.75354
INFO:root:epoch 9, iter 8420, loss 3.48832, smoothed loss 3.61258, grad norm 5.09464, param norm 143.78694
INFO:root:epoch 9, iter 8425, loss 3.54291, smoothed loss 3.60110, grad norm 5.06731, param norm 143.81940
INFO:root:epoch 9, iter 8430, loss 2.57127, smoothed loss 3.58364, grad norm 4.57462, param norm 143.84883
INFO:root:epoch 9, iter 8435, loss 4.18743, smoothed loss 3.58462, grad norm 6.70468, param norm 143.87833
INFO:root:epoch 9, iter 8440, loss 3.05109, smoothed loss 3.56531, grad norm 4.25747, param norm 143.90952
INFO:root:epoch 9, iter 8445, loss 3.27046, smoothed loss 3.56140, grad norm 4.83001, param norm 143.93976
INFO:root:epoch 9, iter 8450, loss 3.38103, smoothed loss 3.55709, grad norm 4.34064, param norm 143.96840
INFO:root:epoch 9, iter 8455, loss 3.84179, smoothed loss 3.56483, grad norm 5.04122, param norm 143.99597
INFO:root:epoch 9, iter 8460, loss 3.45903, smoothed loss 3.55853, grad norm 4.72657, param norm 144.02629
INFO:root:epoch 9, iter 8465, loss 3.68200, smoothed loss 3.56433, grad norm 5.20257, param norm 144.06044
INFO:root:epoch 9, iter 8470, loss 3.74877, smoothed loss 3.56803, grad norm 4.78582, param norm 144.08922
INFO:root:epoch 9, iter 8475, loss 3.71777, smoothed loss 3.57207, grad norm 4.21085, param norm 144.11809
INFO:root:epoch 9, iter 8480, loss 4.06274, smoothed loss 3.56661, grad norm 5.67793, param norm 144.15060
INFO:root:epoch 9, iter 8485, loss 3.75226, smoothed loss 3.57270, grad norm 5.29997, param norm 144.18466
INFO:root:epoch 9, iter 8490, loss 3.93774, smoothed loss 3.57357, grad norm 5.36334, param norm 144.21910
INFO:root:epoch 9, iter 8495, loss 3.40507, smoothed loss 3.57962, grad norm 5.33819, param norm 144.24852
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8500, loss 3.42949, smoothed loss 3.56663, grad norm 4.71006, param norm 144.27649
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 10, Iter 8500, dev loss: 3.245214
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.66364
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.50200
INFO:root:Epoch 10, Iter 8500, Train F1 score: 0.663641, Train EM score: 0.502000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.60811
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.46235
INFO:root:Epoch 10, Iter 8500, Dev F1 score: 0.608111, Dev EM score: 0.462349
INFO:root:End of epoch 10
INFO:root:epoch 10, iter 8505, loss 3.46039, smoothed loss 3.56754, grad norm 4.86672, param norm 144.30495
INFO:root:epoch 10, iter 8510, loss 3.06544, smoothed loss 3.55313, grad norm 4.89885, param norm 144.33572
INFO:root:epoch 10, iter 8515, loss 3.06104, smoothed loss 3.53093, grad norm 4.38801, param norm 144.36678
INFO:root:epoch 10, iter 8520, loss 4.02277, smoothed loss 3.55263, grad norm 4.98231, param norm 144.39429
INFO:root:epoch 10, iter 8525, loss 3.13238, smoothed loss 3.55017, grad norm 4.68947, param norm 144.42555
INFO:root:epoch 10, iter 8530, loss 3.00370, smoothed loss 3.53526, grad norm 5.72194, param norm 144.45731
INFO:root:epoch 10, iter 8535, loss 3.62225, smoothed loss 3.54415, grad norm 4.75946, param norm 144.48613
INFO:root:epoch 10, iter 8540, loss 2.78363, smoothed loss 3.53662, grad norm 4.12027, param norm 144.51340
INFO:root:epoch 10, iter 8545, loss 3.47171, smoothed loss 3.55039, grad norm 4.59155, param norm 144.53949
INFO:root:epoch 10, iter 8550, loss 4.13979, smoothed loss 3.54932, grad norm 5.26278, param norm 144.57008
INFO:root:epoch 10, iter 8555, loss 3.81845, smoothed loss 3.55730, grad norm 4.73915, param norm 144.60312
INFO:root:epoch 10, iter 8560, loss 3.61786, smoothed loss 3.54232, grad norm 4.86984, param norm 144.63606
INFO:root:epoch 10, iter 8565, loss 3.32016, smoothed loss 3.52939, grad norm 4.42070, param norm 144.66899
INFO:root:epoch 10, iter 8570, loss 3.06128, smoothed loss 3.51882, grad norm 4.59934, param norm 144.70265
INFO:root:epoch 10, iter 8575, loss 3.39999, smoothed loss 3.51620, grad norm 4.86812, param norm 144.73402
INFO:root:epoch 10, iter 8580, loss 3.80557, smoothed loss 3.52468, grad norm 4.99755, param norm 144.76482
INFO:root:epoch 10, iter 8585, loss 4.01115, smoothed loss 3.53035, grad norm 5.32485, param norm 144.79585
INFO:root:epoch 10, iter 8590, loss 3.71341, smoothed loss 3.52893, grad norm 4.98234, param norm 144.82579
INFO:root:epoch 10, iter 8595, loss 3.93451, smoothed loss 3.53017, grad norm 5.38677, param norm 144.85567
INFO:root:epoch 10, iter 8600, loss 3.81262, smoothed loss 3.53107, grad norm 4.73622, param norm 144.88097
INFO:root:epoch 10, iter 8605, loss 3.02337, smoothed loss 3.52222, grad norm 4.66443, param norm 144.90912
INFO:root:epoch 10, iter 8610, loss 3.91221, smoothed loss 3.53801, grad norm 5.06902, param norm 144.94267
INFO:root:epoch 10, iter 8615, loss 2.69706, smoothed loss 3.51679, grad norm 4.63965, param norm 144.97821
INFO:root:epoch 10, iter 8620, loss 3.37219, smoothed loss 3.51154, grad norm 4.74516, param norm 145.01201
INFO:root:epoch 10, iter 8625, loss 3.50543, smoothed loss 3.51727, grad norm 5.05062, param norm 145.04466
INFO:root:epoch 10, iter 8630, loss 3.72664, smoothed loss 3.51844, grad norm 5.57617, param norm 145.07184
INFO:root:epoch 10, iter 8635, loss 3.30200, smoothed loss 3.52713, grad norm 4.49735, param norm 145.09839
INFO:root:epoch 10, iter 8640, loss 3.10511, smoothed loss 3.50823, grad norm 5.11173, param norm 145.12933
INFO:root:epoch 10, iter 8645, loss 3.45227, smoothed loss 3.52315, grad norm 4.93317, param norm 145.16013
INFO:root:epoch 10, iter 8650, loss 3.91795, smoothed loss 3.53601, grad norm 4.83868, param norm 145.18846
INFO:root:epoch 10, iter 8655, loss 3.37389, smoothed loss 3.52731, grad norm 5.24834, param norm 145.22202
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8660, loss 3.94046, smoothed loss 3.52829, grad norm 5.75710, param norm 145.25430
INFO:root:epoch 10, iter 8665, loss 3.22092, smoothed loss 3.52083, grad norm 5.27107, param norm 145.28223
INFO:root:epoch 10, iter 8670, loss 4.03692, smoothed loss 3.51238, grad norm 5.10612, param norm 145.30952
INFO:root:epoch 10, iter 8675, loss 3.78458, smoothed loss 3.50827, grad norm 5.61908, param norm 145.33424
INFO:root:epoch 10, iter 8680, loss 3.26385, smoothed loss 3.51109, grad norm 5.30123, param norm 145.36137
INFO:root:epoch 10, iter 8685, loss 3.64715, smoothed loss 3.50375, grad norm 5.22059, param norm 145.38893
INFO:root:epoch 10, iter 8690, loss 3.80904, smoothed loss 3.51243, grad norm 4.94378, param norm 145.41489
INFO:root:epoch 10, iter 8695, loss 3.86698, smoothed loss 3.50310, grad norm 4.51778, param norm 145.44212
INFO:root:epoch 10, iter 8700, loss 3.86150, smoothed loss 3.50182, grad norm 4.94987, param norm 145.46739
INFO:root:epoch 10, iter 8705, loss 3.84652, smoothed loss 3.49195, grad norm 5.21183, param norm 145.49141
INFO:root:epoch 10, iter 8710, loss 4.02302, smoothed loss 3.50144, grad norm 4.85449, param norm 145.51675
INFO:root:epoch 10, iter 8715, loss 3.47302, smoothed loss 3.49758, grad norm 4.69361, param norm 145.54370
INFO:root:epoch 10, iter 8720, loss 3.24501, smoothed loss 3.48440, grad norm 4.25937, param norm 145.57193
INFO:root:epoch 10, iter 8725, loss 4.33017, smoothed loss 3.48789, grad norm 5.32027, param norm 145.60260
INFO:root:epoch 10, iter 8730, loss 2.54838, smoothed loss 3.48959, grad norm 5.05254, param norm 145.63014
INFO:root:epoch 10, iter 8735, loss 3.94139, smoothed loss 3.48804, grad norm 5.09011, param norm 145.66205
INFO:root:epoch 10, iter 8740, loss 3.67447, smoothed loss 3.49765, grad norm 4.60245, param norm 145.69472
INFO:root:epoch 10, iter 8745, loss 3.97540, smoothed loss 3.50513, grad norm 4.86780, param norm 145.72430
INFO:root:epoch 10, iter 8750, loss 3.69780, smoothed loss 3.50406, grad norm 5.67471, param norm 145.75481
INFO:root:epoch 10, iter 8755, loss 3.74154, smoothed loss 3.49526, grad norm 4.70599, param norm 145.78482
INFO:root:epoch 10, iter 8760, loss 2.87067, smoothed loss 3.48387, grad norm 5.00896, param norm 145.81419
INFO:root:epoch 10, iter 8765, loss 3.50627, smoothed loss 3.48581, grad norm 5.68055, param norm 145.84258
INFO:root:epoch 10, iter 8770, loss 3.94611, smoothed loss 3.49056, grad norm 5.06148, param norm 145.86938
INFO:root:epoch 10, iter 8775, loss 4.60951, smoothed loss 3.49637, grad norm 5.85856, param norm 145.89548
INFO:root:epoch 10, iter 8780, loss 3.77145, smoothed loss 3.50997, grad norm 4.71736, param norm 145.92329
INFO:root:epoch 10, iter 8785, loss 3.99811, smoothed loss 3.51048, grad norm 4.66989, param norm 145.95274
INFO:root:epoch 10, iter 8790, loss 3.36022, smoothed loss 3.51223, grad norm 4.58748, param norm 145.98488
INFO:root:epoch 10, iter 8795, loss 3.27149, smoothed loss 3.50070, grad norm 5.03166, param norm 146.02083
INFO:root:epoch 10, iter 8800, loss 4.07271, smoothed loss 3.50711, grad norm 5.70935, param norm 146.05487
INFO:root:epoch 10, iter 8805, loss 3.86801, smoothed loss 3.51289, grad norm 4.76118, param norm 146.08833
INFO:root:epoch 10, iter 8810, loss 3.06835, smoothed loss 3.51910, grad norm 4.92757, param norm 146.12267
INFO:root:epoch 10, iter 8815, loss 3.23635, smoothed loss 3.52066, grad norm 5.23281, param norm 146.15640
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8820, loss 3.83730, smoothed loss 3.53491, grad norm 6.08677, param norm 146.18704
INFO:root:epoch 10, iter 8825, loss 3.98791, smoothed loss 3.53944, grad norm 5.59974, param norm 146.21417
INFO:root:epoch 10, iter 8830, loss 3.21264, smoothed loss 3.53069, grad norm 5.38087, param norm 146.24249
INFO:root:epoch 10, iter 8835, loss 3.03708, smoothed loss 3.51999, grad norm 5.27536, param norm 146.27260
INFO:root:epoch 10, iter 8840, loss 3.61371, smoothed loss 3.51475, grad norm 6.12475, param norm 146.30194
INFO:root:epoch 10, iter 8845, loss 3.38713, smoothed loss 3.51094, grad norm 4.67123, param norm 146.33124
INFO:root:epoch 10, iter 8850, loss 4.29755, smoothed loss 3.52038, grad norm 5.39782, param norm 146.35855
INFO:root:epoch 10, iter 8855, loss 3.39490, smoothed loss 3.52620, grad norm 4.51042, param norm 146.38673
INFO:root:epoch 10, iter 8860, loss 2.72590, smoothed loss 3.51119, grad norm 4.84280, param norm 146.41902
INFO:root:epoch 10, iter 8865, loss 3.82261, smoothed loss 3.50943, grad norm 4.57762, param norm 146.45349
INFO:root:epoch 10, iter 8870, loss 3.50651, smoothed loss 3.50759, grad norm 5.03775, param norm 146.48756
INFO:root:epoch 10, iter 8875, loss 4.19759, smoothed loss 3.50799, grad norm 5.16214, param norm 146.51929
INFO:root:epoch 10, iter 8880, loss 3.34985, smoothed loss 3.49707, grad norm 4.77135, param norm 146.54976
INFO:root:epoch 10, iter 8885, loss 3.62512, smoothed loss 3.50668, grad norm 5.61246, param norm 146.57610
INFO:root:epoch 10, iter 8890, loss 3.20638, smoothed loss 3.51176, grad norm 4.28458, param norm 146.60019
INFO:root:epoch 10, iter 8895, loss 2.95105, smoothed loss 3.49017, grad norm 4.10233, param norm 146.62857
INFO:root:epoch 10, iter 8900, loss 3.41274, smoothed loss 3.47792, grad norm 5.36481, param norm 146.65889
INFO:root:epoch 10, iter 8905, loss 3.92286, smoothed loss 3.48059, grad norm 5.03918, param norm 146.69264
INFO:root:epoch 10, iter 8910, loss 3.75078, smoothed loss 3.48356, grad norm 4.72823, param norm 146.72417
INFO:root:epoch 10, iter 8915, loss 3.42297, smoothed loss 3.46194, grad norm 5.14915, param norm 146.75482
INFO:root:epoch 10, iter 8920, loss 3.21338, smoothed loss 3.46198, grad norm 4.67961, param norm 146.78455
INFO:root:epoch 10, iter 8925, loss 3.76770, smoothed loss 3.46234, grad norm 4.68878, param norm 146.81384
INFO:root:epoch 10, iter 8930, loss 3.75826, smoothed loss 3.47837, grad norm 5.28912, param norm 146.84166
INFO:root:epoch 10, iter 8935, loss 3.27064, smoothed loss 3.48071, grad norm 5.22806, param norm 146.86998
INFO:root:epoch 10, iter 8940, loss 2.84137, smoothed loss 3.47705, grad norm 4.20205, param norm 146.89931
INFO:root:epoch 10, iter 8945, loss 3.53376, smoothed loss 3.48035, grad norm 4.59195, param norm 146.92949
INFO:root:epoch 10, iter 8950, loss 2.72360, smoothed loss 3.45732, grad norm 3.86080, param norm 146.95889
INFO:root:epoch 10, iter 8955, loss 4.03911, smoothed loss 3.46497, grad norm 5.97734, param norm 146.98959
INFO:root:epoch 10, iter 8960, loss 2.71338, smoothed loss 3.46620, grad norm 4.18824, param norm 147.02283
INFO:root:epoch 10, iter 8965, loss 2.23070, smoothed loss 3.45192, grad norm 3.38009, param norm 147.05496
INFO:root:epoch 10, iter 8970, loss 3.13065, smoothed loss 3.44080, grad norm 4.81665, param norm 147.08749
INFO:root:epoch 10, iter 8975, loss 3.52620, smoothed loss 3.43232, grad norm 5.20783, param norm 147.12308
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8980, loss 2.81801, smoothed loss 3.44198, grad norm 4.60815, param norm 147.15894
INFO:root:epoch 10, iter 8985, loss 3.86612, smoothed loss 3.43221, grad norm 5.46194, param norm 147.19630
INFO:root:epoch 10, iter 8990, loss 3.81770, smoothed loss 3.43464, grad norm 4.55901, param norm 147.22781
INFO:root:epoch 10, iter 8995, loss 3.15461, smoothed loss 3.43607, grad norm 5.26565, param norm 147.25620
INFO:root:epoch 10, iter 9000, loss 3.73405, smoothed loss 3.43439, grad norm 4.92252, param norm 147.28934
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 10, Iter 9000, dev loss: 3.232198
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.65486
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.53300
INFO:root:Epoch 10, Iter 9000, Train F1 score: 0.654856, Train EM score: 0.533000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.60908
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.46516
INFO:root:Epoch 10, Iter 9000, Dev F1 score: 0.609078, Dev EM score: 0.465159
INFO:root:End of epoch 10
INFO:root:epoch 10, iter 9005, loss 3.36970, smoothed loss 3.42348, grad norm 5.15516, param norm 147.32002
INFO:root:epoch 10, iter 9010, loss 2.77501, smoothed loss 3.41828, grad norm 4.40199, param norm 147.34714
INFO:root:epoch 10, iter 9015, loss 3.24909, smoothed loss 3.41098, grad norm 5.05357, param norm 147.37465
INFO:root:epoch 10, iter 9020, loss 2.65914, smoothed loss 3.41709, grad norm 4.54812, param norm 147.40309
INFO:root:epoch 10, iter 9025, loss 4.22330, smoothed loss 3.44264, grad norm 4.63350, param norm 147.43231
INFO:root:epoch 10, iter 9030, loss 3.69905, smoothed loss 3.45580, grad norm 5.30153, param norm 147.46068
INFO:root:epoch 10, iter 9035, loss 3.92133, smoothed loss 3.47310, grad norm 4.51879, param norm 147.49106
INFO:root:epoch 10, iter 9040, loss 3.36650, smoothed loss 3.48216, grad norm 4.75997, param norm 147.52075
INFO:root:epoch 10, iter 9045, loss 4.10451, smoothed loss 3.48775, grad norm 5.13387, param norm 147.55145
INFO:root:epoch 10, iter 9050, loss 3.27897, smoothed loss 3.48545, grad norm 4.83040, param norm 147.57887
INFO:root:epoch 10, iter 9055, loss 3.92319, smoothed loss 3.47583, grad norm 5.28764, param norm 147.60738
INFO:root:epoch 10, iter 9060, loss 4.02830, smoothed loss 3.48393, grad norm 4.73186, param norm 147.63745
INFO:root:epoch 10, iter 9065, loss 3.88329, smoothed loss 3.46831, grad norm 5.33762, param norm 147.66542
INFO:root:epoch 10, iter 9070, loss 3.71108, smoothed loss 3.46209, grad norm 5.41040, param norm 147.69241
INFO:root:epoch 10, iter 9075, loss 4.12282, smoothed loss 3.47320, grad norm 5.34609, param norm 147.71962
INFO:root:epoch 10, iter 9080, loss 2.24081, smoothed loss 3.45859, grad norm 3.85447, param norm 147.74594
INFO:root:epoch 10, iter 9085, loss 3.40206, smoothed loss 3.44486, grad norm 4.72009, param norm 147.77672
INFO:root:epoch 10, iter 9090, loss 2.71256, smoothed loss 3.44793, grad norm 4.15540, param norm 147.80798
INFO:root:epoch 10, iter 9095, loss 3.65444, smoothed loss 3.44414, grad norm 5.42499, param norm 147.83847
INFO:root:epoch 10, iter 9100, loss 3.74321, smoothed loss 3.43995, grad norm 5.15729, param norm 147.86591
INFO:root:epoch 10, iter 9105, loss 3.94787, smoothed loss 3.44913, grad norm 4.98003, param norm 147.89351
INFO:root:epoch 10, iter 9110, loss 3.15543, smoothed loss 3.44220, grad norm 5.05999, param norm 147.92323
INFO:root:epoch 10, iter 9115, loss 3.71559, smoothed loss 3.45996, grad norm 4.22597, param norm 147.95276
INFO:root:epoch 10, iter 9120, loss 3.40488, smoothed loss 3.45257, grad norm 4.04976, param norm 147.98334
INFO:root:epoch 10, iter 9125, loss 3.62865, smoothed loss 3.45219, grad norm 4.94697, param norm 148.01259
INFO:root:epoch 10, iter 9130, loss 4.07995, smoothed loss 3.46106, grad norm 5.61204, param norm 148.04405
INFO:root:epoch 10, iter 9135, loss 3.23142, smoothed loss 3.45041, grad norm 4.84677, param norm 148.08040
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 9140, loss 3.00679, smoothed loss 3.44461, grad norm 4.18930, param norm 148.11597
INFO:root:epoch 10, iter 9145, loss 3.35390, smoothed loss 3.45230, grad norm 4.47509, param norm 148.14734
INFO:root:epoch 10, iter 9150, loss 3.70886, smoothed loss 3.47424, grad norm 5.36073, param norm 148.17697
INFO:root:epoch 10, iter 9155, loss 3.29608, smoothed loss 3.47541, grad norm 5.74592, param norm 148.20462
INFO:root:epoch 10, iter 9160, loss 3.47261, smoothed loss 3.47702, grad norm 4.63344, param norm 148.23369
INFO:root:epoch 10, iter 9165, loss 3.03003, smoothed loss 3.46246, grad norm 4.59081, param norm 148.26439
INFO:root:epoch 10, iter 9170, loss 3.09094, smoothed loss 3.45572, grad norm 4.02358, param norm 148.29373
INFO:root:epoch 10, iter 9175, loss 3.76227, smoothed loss 3.46482, grad norm 5.05799, param norm 148.32211
INFO:root:epoch 10, iter 9180, loss 3.53151, smoothed loss 3.47174, grad norm 4.57232, param norm 148.34802
INFO:root:epoch 10, iter 9185, loss 3.01525, smoothed loss 3.46667, grad norm 4.46865, param norm 148.37500
INFO:root:epoch 10, iter 9190, loss 3.42579, smoothed loss 3.47290, grad norm 4.90354, param norm 148.40361
INFO:root:epoch 10, iter 9195, loss 2.88262, smoothed loss 3.45799, grad norm 4.21987, param norm 148.43266
INFO:root:epoch 10, iter 9200, loss 3.49907, smoothed loss 3.46437, grad norm 5.72214, param norm 148.46167
INFO:root:epoch 10, iter 9205, loss 3.51029, smoothed loss 3.48488, grad norm 5.37348, param norm 148.48990
INFO:root:epoch 10, iter 9210, loss 3.16918, smoothed loss 3.47584, grad norm 4.17099, param norm 148.51953
INFO:root:epoch 10, iter 9215, loss 3.47643, smoothed loss 3.46929, grad norm 4.91070, param norm 148.54565
INFO:root:epoch 10, iter 9220, loss 3.09882, smoothed loss 3.46602, grad norm 4.71726, param norm 148.57324
INFO:root:epoch 10, iter 9225, loss 3.19356, smoothed loss 3.45513, grad norm 4.70759, param norm 148.60359
INFO:root:epoch 10, iter 9230, loss 3.53812, smoothed loss 3.44738, grad norm 4.59981, param norm 148.63490
INFO:root:epoch 10, iter 9235, loss 4.14374, smoothed loss 3.45924, grad norm 4.89280, param norm 148.66579
INFO:root:epoch 10, iter 9240, loss 4.32885, smoothed loss 3.46870, grad norm 5.12141, param norm 148.69429
INFO:root:epoch 10, iter 9245, loss 3.65918, smoothed loss 3.47865, grad norm 4.50710, param norm 148.72313
INFO:root:epoch 10, iter 9250, loss 3.21577, smoothed loss 3.47151, grad norm 4.16101, param norm 148.75284
INFO:root:epoch 10, iter 9255, loss 2.75976, smoothed loss 3.44746, grad norm 4.43878, param norm 148.78212
INFO:root:epoch 10, iter 9260, loss 3.87815, smoothed loss 3.46196, grad norm 5.28143, param norm 148.81122
INFO:root:epoch 10, iter 9265, loss 4.24741, smoothed loss 3.47697, grad norm 5.02697, param norm 148.83934
INFO:root:epoch 10, iter 9270, loss 3.64692, smoothed loss 3.47732, grad norm 5.42889, param norm 148.86617
INFO:root:epoch 10, iter 9275, loss 3.78395, smoothed loss 3.48341, grad norm 5.10344, param norm 148.89323
INFO:root:epoch 10, iter 9280, loss 2.98159, smoothed loss 3.49197, grad norm 4.35562, param norm 148.91777
INFO:root:epoch 10, iter 9285, loss 3.83121, smoothed loss 3.50016, grad norm 5.37688, param norm 148.94278
INFO:root:epoch 10, iter 9290, loss 3.18291, smoothed loss 3.50765, grad norm 5.37490, param norm 148.97089
INFO:root:epoch 10, iter 9295, loss 3.69341, smoothed loss 3.53264, grad norm 4.46341, param norm 149.00316
Adding batches start...
Added  144  batches
INFO:root:epoch 10, iter 9300, loss 3.13100, smoothed loss 3.52755, grad norm 4.48055, param norm 149.03389
INFO:root:epoch 10, iter 9305, loss 2.99872, smoothed loss 3.53202, grad norm 4.58624, param norm 149.06694
INFO:root:epoch 10, iter 9310, loss 4.11611, smoothed loss 3.53102, grad norm 5.56241, param norm 149.09943
INFO:root:epoch 10, iter 9315, loss 3.12783, smoothed loss 3.50427, grad norm 4.62677, param norm 149.12898
INFO:root:epoch 10, iter 9320, loss 2.80767, smoothed loss 3.49593, grad norm 4.31421, param norm 149.15558
INFO:root:epoch 10, iter 9325, loss 2.49147, smoothed loss 3.49504, grad norm 4.41094, param norm 149.18346
INFO:root:epoch 10, iter 9330, loss 3.29595, smoothed loss 3.49101, grad norm 4.32657, param norm 149.21237
INFO:root:epoch 10, iter 9335, loss 3.19193, smoothed loss 3.48141, grad norm 5.18227, param norm 149.24083
INFO:root:epoch 10, iter 9340, loss 2.95451, smoothed loss 3.46882, grad norm 4.86395, param norm 149.27347
INFO:root:epoch 10, iter 9345, loss 3.34780, smoothed loss 3.46719, grad norm 5.62735, param norm 149.30278
INFO:root:epoch 10, iter 9350, loss 3.48488, smoothed loss 3.46757, grad norm 4.16532, param norm 149.33232
INFO:root:epoch 10, iter 9355, loss 3.56915, smoothed loss 3.48021, grad norm 4.83461, param norm 149.36012
INFO:root:epoch 10, iter 9360, loss 3.43167, smoothed loss 3.50233, grad norm 4.47915, param norm 149.38423
INFO:root:epoch 10, iter 9365, loss 3.14724, smoothed loss 3.48950, grad norm 4.56387, param norm 149.41106
INFO:root:epoch 10, iter 9370, loss 2.90742, smoothed loss 3.49235, grad norm 4.23301, param norm 149.44070
INFO:root:epoch 10, iter 9375, loss 3.42463, smoothed loss 3.48735, grad norm 4.84746, param norm 149.47235
INFO:root:epoch 10, iter 9380, loss 3.28533, smoothed loss 3.48500, grad norm 4.40691, param norm 149.50203
INFO:root:epoch 10, iter 9385, loss 3.12413, smoothed loss 3.47121, grad norm 3.94558, param norm 149.53113
INFO:root:epoch 10, iter 9390, loss 3.25301, smoothed loss 3.46435, grad norm 4.44553, param norm 149.56049
INFO:root:epoch 10, iter 9395, loss 3.18017, smoothed loss 3.45241, grad norm 4.69654, param norm 149.59122
INFO:root:epoch 10, iter 9400, loss 3.58827, smoothed loss 3.45422, grad norm 4.78282, param norm 149.62401
INFO:root:epoch 10, iter 9405, loss 3.09003, smoothed loss 3.45210, grad norm 4.04089, param norm 149.65689
INFO:root:epoch 10, iter 9410, loss 3.33858, smoothed loss 3.45682, grad norm 5.31405, param norm 149.68794
INFO:root:epoch 10, iter 9415, loss 3.25290, smoothed loss 3.45599, grad norm 4.22526, param norm 149.71873
INFO:root:epoch 10, iter 9420, loss 2.85159, smoothed loss 3.44647, grad norm 4.08455, param norm 149.74815
INFO:root:epoch 10, iter 9425, loss 3.54941, smoothed loss 3.45225, grad norm 4.13740, param norm 149.77740
INFO:root:epoch 10, iter 9430, loss 3.76518, smoothed loss 3.46077, grad norm 4.34816, param norm 149.80464
INFO:root:epoch 10, iter 9435, loss 3.30060, smoothed loss 3.45929, grad norm 4.58759, param norm 149.83215
INFO:root:epoch 10, iter 9440, loss 3.71679, smoothed loss 3.47995, grad norm 5.03578, param norm 149.85632
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9445, loss 3.10121, smoothed loss 3.46867, grad norm 4.61427, param norm 149.88023
INFO:root:epoch 11, iter 9450, loss 3.88028, smoothed loss 3.47461, grad norm 5.35036, param norm 149.90997
INFO:root:epoch 11, iter 9455, loss 3.26074, smoothed loss 3.47021, grad norm 4.20125, param norm 149.94044
INFO:root:epoch 11, iter 9460, loss 2.93419, smoothed loss 3.46833, grad norm 4.04138, param norm 149.96992
INFO:root:epoch 11, iter 9465, loss 2.57193, smoothed loss 3.45337, grad norm 4.17030, param norm 149.99873
INFO:root:epoch 11, iter 9470, loss 3.30902, smoothed loss 3.44879, grad norm 4.45685, param norm 150.02800
INFO:root:epoch 11, iter 9475, loss 2.63911, smoothed loss 3.43971, grad norm 4.29850, param norm 150.05618
INFO:root:epoch 11, iter 9480, loss 3.08629, smoothed loss 3.43387, grad norm 5.00753, param norm 150.08102
INFO:root:epoch 11, iter 9485, loss 3.81514, smoothed loss 3.43115, grad norm 4.48721, param norm 150.10757
INFO:root:epoch 11, iter 9490, loss 3.66431, smoothed loss 3.41883, grad norm 5.62393, param norm 150.13763
INFO:root:epoch 11, iter 9495, loss 3.82442, smoothed loss 3.42792, grad norm 4.39087, param norm 150.16499
INFO:root:epoch 11, iter 9500, loss 3.51392, smoothed loss 3.43133, grad norm 4.19413, param norm 150.19080
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 11, Iter 9500, dev loss: 3.159832
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples got a score of 0.69070
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples got a score: 0.55500
INFO:root:Epoch 11, Iter 9500, Train F1 score: 0.690700, Train EM score: 0.555000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples got a score of 0.61949
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples got a score: 0.46895
INFO:root:Epoch 11, Iter 9500, Dev F1 score: 0.619486, Dev EM score: 0.468952
INFO:root:End of epoch 11
INFO:root:epoch 11, iter 9505, loss 3.20247, smoothed loss 3.42875, grad norm 4.68309, param norm 150.22058
INFO:root:epoch 11, iter 9510, loss 3.98888, smoothed loss 3.44213, grad norm 4.41144, param norm 150.24855
INFO:root:epoch 11, iter 9515, loss 3.10807, smoothed loss 3.44582, grad norm 4.20141, param norm 150.27721
INFO:root:epoch 11, iter 9520, loss 3.67572, smoothed loss 3.44696, grad norm 5.38099, param norm 150.30989
INFO:root:epoch 11, iter 9525, loss 3.62055, smoothed loss 3.43862, grad norm 5.55675, param norm 150.34375
INFO:root:epoch 11, iter 9530, loss 3.40586, smoothed loss 3.44287, grad norm 5.31172, param norm 150.37346
INFO:root:epoch 11, iter 9535, loss 3.29397, smoothed loss 3.43900, grad norm 4.03059, param norm 150.40160
INFO:root:epoch 11, iter 9540, loss 3.22548, smoothed loss 3.43761, grad norm 5.39197, param norm 150.42789
INFO:root:epoch 11, iter 9545, loss 3.72332, smoothed loss 3.45215, grad norm 4.82057, param norm 150.45360
INFO:root:epoch 11, iter 9550, loss 3.06718, smoothed loss 3.45880, grad norm 3.81684, param norm 150.48346
INFO:root:epoch 11, iter 9555, loss 3.23211, smoothed loss 3.45383, grad norm 5.08206, param norm 150.51306
INFO:root:epoch 11, iter 9560, loss 3.38765, smoothed loss 3.44309, grad norm 5.02429, param norm 150.54321
INFO:root:epoch 11, iter 9565, loss 2.55331, smoothed loss 3.42264, grad norm 4.54877, param norm 150.57202
INFO:root:epoch 11, iter 9570, loss 3.19992, smoothed loss 3.42132, grad norm 4.65107, param norm 150.60010
INFO:root:epoch 11, iter 9575, loss 3.62588, smoothed loss 3.42058, grad norm 5.30821, param norm 150.62720
INFO:root:epoch 11, iter 9580, loss 3.19300, smoothed loss 3.40822, grad norm 4.58897, param norm 150.65627
INFO:root:epoch 11, iter 9585, loss 4.17493, smoothed loss 3.42404, grad norm 5.60098, param norm 150.68819
INFO:root:epoch 11, iter 9590, loss 3.00590, smoothed loss 3.41782, grad norm 4.52866, param norm 150.71735
INFO:root:epoch 11, iter 9595, loss 3.59533, smoothed loss 3.42053, grad norm 4.80506, param norm 150.74472
INFO:root:epoch 11, iter 9600, loss 3.58684, smoothed loss 3.42643, grad norm 5.20731, param norm 150.76805
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9605, loss 3.78557, smoothed loss 3.41575, grad norm 5.79782, param norm 150.79559
INFO:root:epoch 11, iter 9610, loss 3.97343, smoothed loss 3.42294, grad norm 4.94037, param norm 150.82462
INFO:root:epoch 11, iter 9615, loss 2.89604, smoothed loss 3.41841, grad norm 3.94990, param norm 150.85474
INFO:root:epoch 11, iter 9620, loss 3.94581, smoothed loss 3.42150, grad norm 4.52419, param norm 150.88753
INFO:root:epoch 11, iter 9625, loss 3.49485, smoothed loss 3.40970, grad norm 4.53046, param norm 150.91774
INFO:root:epoch 11, iter 9630, loss 3.69271, smoothed loss 3.41927, grad norm 4.93496, param norm 150.94521
INFO:root:epoch 11, iter 9635, loss 3.23742, smoothed loss 3.41539, grad norm 4.28621, param norm 150.97307
INFO:root:epoch 11, iter 9640, loss 3.40000, smoothed loss 3.42056, grad norm 4.31300, param norm 151.00194
INFO:root:epoch 11, iter 9645, loss 3.45292, smoothed loss 3.40993, grad norm 4.65443, param norm 151.02907
INFO:root:epoch 11, iter 9650, loss 3.42894, smoothed loss 3.40690, grad norm 4.40582, param norm 151.05557
INFO:root:epoch 11, iter 9655, loss 3.65130, smoothed loss 3.40771, grad norm 4.07072, param norm 151.08087
INFO:root:epoch 11, iter 9660, loss 3.21536, smoothed loss 3.41200, grad norm 5.04557, param norm 151.10231
INFO:root:epoch 11, iter 9665, loss 4.23728, smoothed loss 3.43327, grad norm 4.88818, param norm 151.12497
INFO:root:epoch 11, iter 9670, loss 2.64468, smoothed loss 3.43334, grad norm 4.28156, param norm 151.14835
INFO:root:epoch 11, iter 9675, loss 3.31132, smoothed loss 3.42722, grad norm 5.34099, param norm 151.17671
INFO:root:epoch 11, iter 9680, loss 3.68523, smoothed loss 3.42119, grad norm 4.81749, param norm 151.21030
INFO:root:epoch 11, iter 9685, loss 3.61034, smoothed loss 3.41182, grad norm 4.86567, param norm 151.24422
INFO:root:epoch 11, iter 9690, loss 3.18811, smoothed loss 3.40204, grad norm 4.52962, param norm 151.27690
INFO:root:epoch 11, iter 9695, loss 3.47534, smoothed loss 3.39366, grad norm 5.26613, param norm 151.30365
INFO:root:epoch 11, iter 9700, loss 4.36119, smoothed loss 3.40756, grad norm 4.69378, param norm 151.32964
INFO:root:epoch 11, iter 9705, loss 3.35396, smoothed loss 3.40058, grad norm 5.00377, param norm 151.35620
INFO:root:epoch 11, iter 9710, loss 4.25455, smoothed loss 3.41022, grad norm 4.90579, param norm 151.38800
INFO:root:epoch 11, iter 9715, loss 3.59748, smoothed loss 3.40458, grad norm 4.47176, param norm 151.42026
INFO:root:epoch 11, iter 9720, loss 3.19336, smoothed loss 3.39956, grad norm 4.32645, param norm 151.45129
INFO:root:epoch 11, iter 9725, loss 3.78965, smoothed loss 3.40656, grad norm 4.54977, param norm 151.48041
INFO:root:epoch 11, iter 9730, loss 2.94724, smoothed loss 3.40691, grad norm 4.28637, param norm 151.50685
INFO:root:epoch 11, iter 9735, loss 3.43385, smoothed loss 3.40198, grad norm 5.02059, param norm 151.52835
INFO:root:epoch 11, iter 9740, loss 3.38164, smoothed loss 3.39405, grad norm 5.07605, param norm 151.55289
INFO:root:epoch 11, iter 9745, loss 3.19580, smoothed loss 3.39016, grad norm 4.29832, param norm 151.57779
INFO:root:epoch 11, iter 9750, loss 2.85669, smoothed loss 3.40512, grad norm 5.02684, param norm 151.60146
INFO:root:epoch 11, iter 9755, loss 3.85414, smoothed loss 3.41423, grad norm 4.78448, param norm 151.63026
INFO:root:epoch 11, iter 9760, loss 3.48521, smoothed loss 3.41731, grad norm 4.71247, param norm 151.65935
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9765, loss 4.25877, smoothed loss 3.41707, grad norm 5.35797, param norm 151.68956
INFO:root:epoch 11, iter 9770, loss 3.39843, smoothed loss 3.41134, grad norm 4.25353, param norm 151.71660
INFO:root:epoch 11, iter 9775, loss 3.23002, smoothed loss 3.40085, grad norm 3.79440, param norm 151.74405
INFO:root:epoch 11, iter 9780, loss 3.85533, smoothed loss 3.40451, grad norm 5.88033, param norm 151.77029

