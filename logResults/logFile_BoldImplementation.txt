epoch 1, iter 5, loss 9.45566, smoothed loss 9.71140, grad norm 1.10107, param norm 65.35284
epoch 1, iter 10, loss 9.56957, smoothed loss 9.69848, grad norm 1.21430, param norm 65.40553
epoch 1, iter 15, loss 9.27452, smoothed loss 9.68259, grad norm 1.34372, param norm 65.46325
epoch 1, iter 20, loss 9.40349, smoothed loss 9.66322, grad norm 1.49779, param norm 65.52398
epoch 1, iter 25, loss 8.91353, smoothed loss 9.63829, grad norm 1.79089, param norm 65.58734
epoch 1, iter 30, loss 9.29021, smoothed loss 9.62081, grad norm 1.91613, param norm 65.66106
epoch 1, iter 35, loss 8.88476, smoothed loss 9.58748, grad norm 2.05679, param norm 65.74459
epoch 1, iter 40, loss 8.64855, smoothed loss 9.55019, grad norm 2.45006, param norm 65.85540
epoch 1, iter 45, loss 8.87145, smoothed loss 9.50706, grad norm 2.66444, param norm 65.97755
epoch 1, iter 50, loss 8.65276, smoothed loss 9.46490, grad norm 2.73695, param norm 66.09547
epoch 1, iter 55, loss 8.40231, smoothed loss 9.41960, grad norm 2.41068, param norm 66.20636
epoch 1, iter 60, loss 8.34508, smoothed loss 9.37101, grad norm 2.24224, param norm 66.32340
epoch 1, iter 65, loss 8.58697, smoothed loss 9.33072, grad norm 2.45520, param norm 66.43995
epoch 1, iter 70, loss 8.34145, smoothed loss 9.27831, grad norm 2.32048, param norm 66.55161
epoch 1, iter 75, loss 8.21639, smoothed loss 9.22648, grad norm 2.78725, param norm 66.66359
epoch 1, iter 80, loss 8.06456, smoothed loss 9.17843, grad norm 2.45502, param norm 66.73990
epoch 1, iter 85, loss 8.35941, smoothed loss 9.13218, grad norm 2.28655, param norm 66.82968
epoch 1, iter 90, loss 7.86395, smoothed loss 9.07907, grad norm 2.45300, param norm 66.91184
epoch 1, iter 95, loss 8.56132, smoothed loss 9.03476, grad norm 2.36311, param norm 66.99023
epoch 1, iter 100, loss 7.96547, smoothed loss 8.98974, grad norm 2.08775, param norm 67.07036
epoch 1, iter 105, loss 7.92163, smoothed loss 8.94456, grad norm 1.91945, param norm 67.14127
epoch 1, iter 110, loss 7.78379, smoothed loss 8.88993, grad norm 2.05489, param norm 67.22319
epoch 1, iter 115, loss 8.05844, smoothed loss 8.84190, grad norm 2.26784, param norm 67.30453
epoch 1, iter 120, loss 7.99864, smoothed loss 8.79723, grad norm 2.06595, param norm 67.37075
epoch 1, iter 125, loss 7.88539, smoothed loss 8.75303, grad norm 2.03580, param norm 67.44616
epoch 1, iter 130, loss 8.12652, smoothed loss 8.70581, grad norm 2.34655, param norm 67.52355
epoch 1, iter 135, loss 8.11201, smoothed loss 8.65782, grad norm 2.12976, param norm 67.58742
epoch 1, iter 140, loss 7.81333, smoothed loss 8.61950, grad norm 2.18587, param norm 67.66193
epoch 1, iter 145, loss 7.49342, smoothed loss 8.58067, grad norm 2.13582, param norm 67.74779
epoch 1, iter 150, loss 8.10863, smoothed loss 8.54866, grad norm 2.31972, param norm 67.82154
epoch 1, iter 155, loss 7.61350, smoothed loss 8.50243, grad norm 2.26071, param norm 67.89475
epoch 1, iter 160, loss 8.03885, smoothed loss 8.47278, grad norm 2.37014, param norm 67.96445
epoch 1, iter 165, loss 7.49118, smoothed loss 8.43140, grad norm 2.18746, param norm 68.03852
epoch 1, iter 170, loss 7.62506, smoothed loss 8.39626, grad norm 2.31705, param norm 68.12071
epoch 1, iter 175, loss 7.71064, smoothed loss 8.36200, grad norm 2.02423, param norm 68.18364
epoch 1, iter 180, loss 7.74576, smoothed loss 8.32594, grad norm 2.19758, param norm 68.25385
epoch 1, iter 185, loss 7.68124, smoothed loss 8.29555, grad norm 2.08430, param norm 68.34357
epoch 1, iter 190, loss 7.85089, smoothed loss 8.26957, grad norm 2.26101, param norm 68.43049
epoch 1, iter 195, loss 7.46747, smoothed loss 8.24069, grad norm 2.14925, param norm 68.50927
epoch 1, iter 200, loss 7.90793, smoothed loss 8.21388, grad norm 2.30540, param norm 68.59709
epoch 1, iter 205, loss 7.31354, smoothed loss 8.17760, grad norm 2.24733, param norm 68.68145
epoch 1, iter 210, loss 7.74382, smoothed loss 8.15306, grad norm 2.30041, param norm 68.76340
epoch 1, iter 215, loss 7.51421, smoothed loss 8.12782, grad norm 1.92225, param norm 68.83577
epoch 1, iter 220, loss 6.94541, smoothed loss 8.08569, grad norm 2.31402, param norm 68.92570
epoch 1, iter 225, loss 7.37519, smoothed loss 8.06048, grad norm 2.04285, param norm 69.02032
epoch 1, iter 230, loss 7.30347, smoothed loss 8.03609, grad norm 2.39892, param norm 69.10008
epoch 1, iter 235, loss 8.09210, smoothed loss 8.02115, grad norm 2.64730, param norm 69.17529
epoch 1, iter 240, loss 7.72070, smoothed loss 8.01045, grad norm 2.54416, param norm 69.26089
epoch 1, iter 245, loss 7.38652, smoothed loss 7.98640, grad norm 2.28319, param norm 69.36523
epoch 1, iter 250, loss 7.72281, smoothed loss 7.96278, grad norm 2.42657, param norm 69.47105
epoch 1, iter 255, loss 7.46656, smoothed loss 7.94692, grad norm 2.73166, param norm 69.57509
epoch 1, iter 260, loss 7.51858, smoothed loss 7.92795, grad norm 2.74522, param norm 69.66937
epoch 1, iter 265, loss 7.35181, smoothed loss 7.89931, grad norm 2.29559, param norm 69.77530
epoch 1, iter 270, loss 7.45482, smoothed loss 7.87289, grad norm 2.28318, param norm 69.87739
epoch 1, iter 275, loss 7.79694, smoothed loss 7.84526, grad norm 2.81016, param norm 69.96693
epoch 1, iter 280, loss 7.52064, smoothed loss 7.82983, grad norm 2.17961, param norm 70.06973
epoch 1, iter 285, loss 6.96814, smoothed loss 7.79212, grad norm 2.37925, param norm 70.18976
epoch 1, iter 290, loss 6.92198, smoothed loss 7.76702, grad norm 2.67213, param norm 70.29524
epoch 1, iter 295, loss 7.09720, smoothed loss 7.73524, grad norm 2.34470, param norm 70.38226
epoch 1, iter 300, loss 6.99073, smoothed loss 7.70439, grad norm 2.41298, param norm 70.49162
epoch 1, iter 305, loss 7.58828, smoothed loss 7.68112, grad norm 3.09681, param norm 70.59699
epoch 1, iter 310, loss 6.94770, smoothed loss 7.65774, grad norm 2.45589, param norm 70.69472
epoch 1, iter 315, loss 7.33781, smoothed loss 7.63921, grad norm 2.59240, param norm 70.79021
epoch 1, iter 320, loss 7.04692, smoothed loss 7.61977, grad norm 2.36114, param norm 70.87149
epoch 1, iter 325, loss 6.68240, smoothed loss 7.58606, grad norm 2.41100, param norm 70.95815
epoch 1, iter 330, loss 6.91663, smoothed loss 7.56552, grad norm 2.70572, param norm 71.07133
epoch 1, iter 335, loss 6.85153, smoothed loss 7.52706, grad norm 2.92667, param norm 71.20117
epoch 1, iter 340, loss 6.84141, smoothed loss 7.49562, grad norm 2.78651, param norm 71.31892
epoch 1, iter 345, loss 7.08988, smoothed loss 7.47515, grad norm 2.63664, param norm 71.41360
epoch 1, iter 350, loss 7.17677, smoothed loss 7.44733, grad norm 2.60306, param norm 71.51725
epoch 1, iter 355, loss 7.18240, smoothed loss 7.41722, grad norm 2.78572, param norm 71.63285
epoch 1, iter 360, loss 6.90747, smoothed loss 7.38216, grad norm 2.93719, param norm 71.74310
epoch 1, iter 365, loss 7.43007, smoothed loss 7.35997, grad norm 2.74547, param norm 71.83571
epoch 1, iter 370, loss 7.02919, smoothed loss 7.34246, grad norm 2.99180, param norm 71.92739
epoch 1, iter 375, loss 7.01750, smoothed loss 7.31617, grad norm 2.68311, param norm 72.02911
epoch 1, iter 380, loss 6.93138, smoothed loss 7.28664, grad norm 3.28299, param norm 72.13198
epoch 1, iter 385, loss 6.57981, smoothed loss 7.25974, grad norm 2.51205, param norm 72.24197
epoch 1, iter 390, loss 6.86672, smoothed loss 7.22685, grad norm 3.14334, param norm 72.35019
epoch 1, iter 395, loss 6.86094, smoothed loss 7.21979, grad norm 2.79884, param norm 72.45088
epoch 1, iter 400, loss 6.73542, smoothed loss 7.19385, grad norm 3.03295, param norm 72.55012
epoch 1, iter 405, loss 6.28695, smoothed loss 7.17246, grad norm 2.93765, param norm 72.65044
epoch 1, iter 410, loss 6.75327, smoothed loss 7.14729, grad norm 2.66633, param norm 72.73862
epoch 1, iter 415, loss 6.51405, smoothed loss 7.11651, grad norm 3.01571, param norm 72.83681
epoch 1, iter 420, loss 6.48666, smoothed loss 7.10149, grad norm 2.56441, param norm 72.93963
epoch 1, iter 425, loss 6.93849, smoothed loss 7.07752, grad norm 2.81347, param norm 73.04266
epoch 1, iter 430, loss 6.89550, smoothed loss 7.05970, grad norm 3.14910, param norm 73.13556
epoch 1, iter 435, loss 6.29538, smoothed loss 7.03104, grad norm 3.12427, param norm 73.22808
epoch 1, iter 440, loss 6.92907, smoothed loss 7.00406, grad norm 2.83569, param norm 73.33884
epoch 1, iter 445, loss 6.66999, smoothed loss 6.98665, grad norm 3.43696, param norm 73.44600
epoch 1, iter 450, loss 6.06773, smoothed loss 6.96195, grad norm 3.01456, param norm 73.54095
epoch 1, iter 455, loss 6.47532, smoothed loss 6.94410, grad norm 2.43720, param norm 73.63480
epoch 1, iter 460, loss 6.49829, smoothed loss 6.92024, grad norm 2.68746, param norm 73.72618
epoch 1, iter 465, loss 5.94534, smoothed loss 6.89602, grad norm 2.78620, param norm 73.81658
epoch 1, iter 470, loss 6.93721, smoothed loss 6.87489, grad norm 3.43811, param norm 73.91641
epoch 1, iter 475, loss 6.86912, smoothed loss 6.87032, grad norm 2.84775, param norm 74.00826
epoch 1, iter 480, loss 6.52737, smoothed loss 6.85656, grad norm 2.80943, param norm 74.09225
epoch 1, iter 485, loss 6.66832, smoothed loss 6.84776, grad norm 3.10710, param norm 74.18408
epoch 1, iter 490, loss 6.83521, smoothed loss 6.83256, grad norm 3.16069, param norm 74.28355
epoch 1, iter 495, loss 6.16618, smoothed loss 6.82300, grad norm 2.91452, param norm 74.36930
epoch 1, iter 500, loss 6.54571, smoothed loss 6.81051, grad norm 3.03912, param norm 74.45103
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 1, Iter 500, dev loss: 6.336831
Calculating Train F1/EM...
F1 train: 1000 examples got a score of 0.27821
Exact Match train: 1000 examples got a score: 0.18100
Epoch 1, Iter 500, Train F1 score: 0.278212, Train EM score: 0.181000
Calculating Dev F1/EM...
F1 dev: 7118 examples got a score of 0.25910
Exact Match dev: 7118 examples got a score: 0.16269
Epoch 1, Iter 500, Dev F1 score: 0.259105, Dev EM score: 0.162686
End of epoch 1
epoch 1, iter 505, loss 6.58622, smoothed loss 6.78231, grad norm 2.93823, param norm 74.54647
epoch 1, iter 510, loss 5.67231, smoothed loss 6.74656, grad norm 2.81108, param norm 74.63779
epoch 1, iter 515, loss 6.74256, smoothed loss 6.73439, grad norm 3.36442, param norm 74.71334
epoch 1, iter 520, loss 5.87505, smoothed loss 6.71695, grad norm 3.11593, param norm 74.78169
epoch 1, iter 525, loss 5.87453, smoothed loss 6.69726, grad norm 3.00997, param norm 74.87228
epoch 1, iter 530, loss 6.10047, smoothed loss 6.68656, grad norm 2.95914, param norm 74.96466
epoch 1, iter 535, loss 6.26289, smoothed loss 6.67595, grad norm 2.94536, param norm 75.05338
epoch 1, iter 540, loss 6.73165, smoothed loss 6.65396, grad norm 3.28165, param norm 75.14200
epoch 1, iter 545, loss 7.45185, smoothed loss 6.65389, grad norm 3.13560, param norm 75.22934
epoch 1, iter 550, loss 6.23128, smoothed loss 6.64488, grad norm 2.80741, param norm 75.30138
epoch 1, iter 555, loss 6.22987, smoothed loss 6.63046, grad norm 2.52137, param norm 75.37255
epoch 1, iter 560, loss 6.68484, smoothed loss 6.61689, grad norm 3.45959, param norm 75.45660
epoch 1, iter 565, loss 6.24284, smoothed loss 6.59398, grad norm 2.75523, param norm 75.54164
epoch 1, iter 570, loss 6.65615, smoothed loss 6.58933, grad norm 3.01305, param norm 75.62052
epoch 1, iter 575, loss 6.21307, smoothed loss 6.56557, grad norm 2.92869, param norm 75.69919
epoch 1, iter 580, loss 5.94262, smoothed loss 6.54988, grad norm 2.64652, param norm 75.77528
epoch 1, iter 585, loss 6.53071, smoothed loss 6.53508, grad norm 3.03437, param norm 75.86491
epoch 1, iter 590, loss 5.95359, smoothed loss 6.51728, grad norm 3.26735, param norm 75.94762
epoch 1, iter 595, loss 6.15226, smoothed loss 6.49664, grad norm 3.19740, param norm 76.02937
epoch 1, iter 600, loss 6.61203, smoothed loss 6.50099, grad norm 2.87782, param norm 76.10733
epoch 1, iter 605, loss 6.18462, smoothed loss 6.49475, grad norm 3.28760, param norm 76.18200
epoch 1, iter 610, loss 6.23513, smoothed loss 6.48160, grad norm 2.97364, param norm 76.26016
epoch 1, iter 615, loss 6.45984, smoothed loss 6.48091, grad norm 3.03536, param norm 76.32956
epoch 1, iter 620, loss 6.69384, smoothed loss 6.47485, grad norm 3.19362, param norm 76.41312
epoch 1, iter 625, loss 6.07329, smoothed loss 6.45786, grad norm 3.28747, param norm 76.50416
epoch 1, iter 630, loss 6.40293, smoothed loss 6.43977, grad norm 2.95434, param norm 76.59637
epoch 1, iter 635, loss 6.49281, smoothed loss 6.42804, grad norm 3.29621, param norm 76.67520
epoch 1, iter 640, loss 6.11533, smoothed loss 6.41152, grad norm 2.86582, param norm 76.75509
epoch 1, iter 645, loss 5.89878, smoothed loss 6.38516, grad norm 3.21579, param norm 76.83102
epoch 1, iter 650, loss 5.88751, smoothed loss 6.37057, grad norm 3.40439, param norm 76.90788
epoch 1, iter 655, loss 5.51765, smoothed loss 6.35675, grad norm 3.47961, param norm 76.97931
epoch 1, iter 660, loss 5.90543, smoothed loss 6.34683, grad norm 2.95264, param norm 77.05142
epoch 1, iter 665, loss 5.71676, smoothed loss 6.32640, grad norm 2.52417, param norm 77.12545
epoch 1, iter 670, loss 5.70973, smoothed loss 6.31547, grad norm 2.82669, param norm 77.19969
epoch 1, iter 675, loss 5.82237, smoothed loss 6.28712, grad norm 3.21451, param norm 77.28348
epoch 1, iter 680, loss 6.21259, smoothed loss 6.28058, grad norm 3.70184, param norm 77.36882
epoch 1, iter 685, loss 5.92017, smoothed loss 6.26743, grad norm 3.34653, param norm 77.44382
epoch 1, iter 690, loss 6.60299, smoothed loss 6.25317, grad norm 3.23110, param norm 77.52054
epoch 1, iter 695, loss 6.26547, smoothed loss 6.24574, grad norm 3.09568, param norm 77.59583
epoch 1, iter 700, loss 6.19032, smoothed loss 6.23756, grad norm 3.21442, param norm 77.67692
epoch 1, iter 705, loss 5.71874, smoothed loss 6.22833, grad norm 3.15361, param norm 77.75819
epoch 1, iter 710, loss 6.12199, smoothed loss 6.22565, grad norm 3.45167, param norm 77.83067
epoch 1, iter 715, loss 5.78395, smoothed loss 6.21502, grad norm 3.55014, param norm 77.89608
epoch 1, iter 720, loss 6.39313, smoothed loss 6.19700, grad norm 2.82583, param norm 77.97340
epoch 1, iter 725, loss 5.58610, smoothed loss 6.19232, grad norm 2.90877, param norm 78.05062
epoch 1, iter 730, loss 6.04629, smoothed loss 6.20453, grad norm 3.55754, param norm 78.12720
epoch 1, iter 735, loss 7.25543, smoothed loss 6.20562, grad norm 3.58064, param norm 78.20846
epoch 1, iter 740, loss 5.40338, smoothed loss 6.19278, grad norm 3.10797, param norm 78.28980
epoch 1, iter 745, loss 6.49828, smoothed loss 6.17288, grad norm 3.46805, param norm 78.37514
epoch 1, iter 750, loss 6.42229, smoothed loss 6.16631, grad norm 3.73798, param norm 78.46421
epoch 1, iter 755, loss 6.14214, smoothed loss 6.15692, grad norm 3.29702, param norm 78.54078
epoch 1, iter 760, loss 6.13950, smoothed loss 6.15211, grad norm 3.35197, param norm 78.61552
epoch 1, iter 765, loss 5.39544, smoothed loss 6.13979, grad norm 2.85429, param norm 78.69767
epoch 1, iter 770, loss 5.21331, smoothed loss 6.13196, grad norm 2.92426, param norm 78.77768
epoch 1, iter 775, loss 6.41137, smoothed loss 6.12349, grad norm 3.31703, param norm 78.85367
epoch 1, iter 780, loss 5.26407, smoothed loss 6.10669, grad norm 2.85027, param norm 78.91524
epoch 1, iter 785, loss 6.33798, smoothed loss 6.10464, grad norm 3.34579, param norm 78.98287
epoch 1, iter 790, loss 5.85748, smoothed loss 6.09577, grad norm 3.13331, param norm 79.06055
epoch 1, iter 795, loss 5.75613, smoothed loss 6.06644, grad norm 3.36715, param norm 79.13500
epoch 1, iter 800, loss 6.15624, smoothed loss 6.05733, grad norm 3.48426, param norm 79.20807
epoch 1, iter 805, loss 6.08109, smoothed loss 6.05728, grad norm 3.35854, param norm 79.27614
epoch 1, iter 810, loss 5.21535, smoothed loss 6.04728, grad norm 3.02633, param norm 79.34166
epoch 1, iter 815, loss 6.23623, smoothed loss 6.03447, grad norm 3.18034, param norm 79.41365
epoch 1, iter 820, loss 5.52578, smoothed loss 6.02056, grad norm 3.01924, param norm 79.48178
epoch 1, iter 825, loss 5.55090, smoothed loss 5.99385, grad norm 2.85694, param norm 79.54892
epoch 1, iter 830, loss 5.69516, smoothed loss 5.99555, grad norm 2.96437, param norm 79.62861
