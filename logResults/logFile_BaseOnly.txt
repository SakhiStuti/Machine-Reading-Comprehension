epoch 1, iter 5, loss 9.24137, smoothed loss 9.53752, grad norm 1.32077, param norm 42.50354
epoch 1, iter 10, loss 8.77661, smoothed loss 9.51573, grad norm 1.55459, param norm 42.54225
epoch 1, iter 15, loss 8.48160, smoothed loss 9.48139, grad norm 1.70469, param norm 42.58749
epoch 1, iter 20, loss 8.63003, smoothed loss 9.44030, grad norm 1.85966, param norm 42.63778
epoch 1, iter 25, loss 8.51634, smoothed loss 9.38909, grad norm 1.95161, param norm 42.68135
epoch 1, iter 30, loss 8.35362, smoothed loss 9.34713, grad norm 1.86998, param norm 42.72292
epoch 1, iter 35, loss 8.15949, smoothed loss 9.29911, grad norm 1.93315, param norm 42.76201
epoch 1, iter 40, loss 8.04653, smoothed loss 9.24513, grad norm 1.98393, param norm 42.80935
epoch 1, iter 45, loss 8.45448, smoothed loss 9.19970, grad norm 1.94912, param norm 42.85498
epoch 1, iter 50, loss 8.00222, smoothed loss 9.15207, grad norm 1.89834, param norm 42.89613
epoch 1, iter 55, loss 7.85812, smoothed loss 9.10390, grad norm 1.89441, param norm 42.93232
epoch 1, iter 60, loss 8.16093, smoothed loss 9.05803, grad norm 1.90686, param norm 42.97311
epoch 1, iter 65, loss 7.84112, smoothed loss 9.01018, grad norm 1.89970, param norm 43.01130
epoch 1, iter 70, loss 8.05375, smoothed loss 8.95303, grad norm 2.09369, param norm 43.05382
epoch 1, iter 75, loss 8.08285, smoothed loss 8.90942, grad norm 2.11719, param norm 43.09621
epoch 1, iter 80, loss 8.17378, smoothed loss 8.86251, grad norm 1.94737, param norm 43.13486
epoch 1, iter 85, loss 7.79471, smoothed loss 8.81345, grad norm 2.36180, param norm 43.17392
epoch 1, iter 90, loss 7.56748, smoothed loss 8.77954, grad norm 2.07100, param norm 43.21198
epoch 1, iter 95, loss 8.19885, smoothed loss 8.73687, grad norm 2.29352, param norm 43.25213
epoch 1, iter 100, loss 7.89267, smoothed loss 8.69918, grad norm 2.40673, param norm 43.30029
epoch 1, iter 105, loss 7.50568, smoothed loss 8.65262, grad norm 2.40073, param norm 43.35173
epoch 1, iter 110, loss 7.84396, smoothed loss 8.61311, grad norm 2.14924, param norm 43.39438
epoch 1, iter 115, loss 7.68937, smoothed loss 8.56769, grad norm 1.96785, param norm 43.43024
epoch 1, iter 120, loss 7.69343, smoothed loss 8.53276, grad norm 1.97803, param norm 43.47656
epoch 1, iter 125, loss 7.44272, smoothed loss 8.49449, grad norm 2.40252, param norm 43.52795
epoch 1, iter 130, loss 8.00569, smoothed loss 8.45442, grad norm 2.68336, param norm 43.57847
epoch 1, iter 135, loss 7.11026, smoothed loss 8.40882, grad norm 2.32693, param norm 43.61812
epoch 1, iter 140, loss 7.39283, smoothed loss 8.36572, grad norm 2.77376, param norm 43.66053
epoch 1, iter 145, loss 7.63610, smoothed loss 8.31597, grad norm 2.94835, param norm 43.71346
epoch 1, iter 150, loss 7.49949, smoothed loss 8.27613, grad norm 2.59933, param norm 43.76639
epoch 1, iter 155, loss 7.56893, smoothed loss 8.24175, grad norm 3.25119, param norm 43.81350
epoch 1, iter 160, loss 7.37732, smoothed loss 8.20673, grad norm 2.33271, param norm 43.85911
epoch 1, iter 165, loss 7.48371, smoothed loss 8.16459, grad norm 3.11877, param norm 43.90692
epoch 1, iter 170, loss 7.05821, smoothed loss 8.12415, grad norm 2.65048, param norm 43.95915
epoch 1, iter 175, loss 7.58025, smoothed loss 8.08874, grad norm 3.19201, param norm 44.00560
epoch 1, iter 180, loss 7.28841, smoothed loss 8.05377, grad norm 2.66837, param norm 44.06035
epoch 1, iter 185, loss 7.40923, smoothed loss 8.02373, grad norm 2.90519, param norm 44.11377
epoch 1, iter 190, loss 7.74269, smoothed loss 7.99820, grad norm 2.73857, param norm 44.16196
epoch 1, iter 195, loss 7.94291, smoothed loss 7.97728, grad norm 2.40866, param norm 44.19582
epoch 1, iter 200, loss 7.34073, smoothed loss 7.95039, grad norm 2.41465, param norm 44.22980
epoch 1, iter 205, loss 7.49146, smoothed loss 7.91727, grad norm 2.41992, param norm 44.27743
epoch 1, iter 210, loss 7.59501, smoothed loss 7.88080, grad norm 2.63020, param norm 44.33095
epoch 1, iter 215, loss 7.31230, smoothed loss 7.85018, grad norm 3.31935, param norm 44.37477
epoch 1, iter 220, loss 6.98999, smoothed loss 7.82257, grad norm 2.59021, param norm 44.40353
epoch 1, iter 225, loss 7.13581, smoothed loss 7.78597, grad norm 2.38387, param norm 44.44460
epoch 1, iter 230, loss 7.63993, smoothed loss 7.76343, grad norm 2.95025, param norm 44.49422
epoch 1, iter 235, loss 7.40630, smoothed loss 7.73996, grad norm 2.61414, param norm 44.54079
epoch 1, iter 240, loss 7.17436, smoothed loss 7.70552, grad norm 2.41191, param norm 44.58623
epoch 1, iter 245, loss 7.16639, smoothed loss 7.67641, grad norm 3.81024, param norm 44.63795
epoch 1, iter 250, loss 7.09163, smoothed loss 7.64807, grad norm 3.09751, param norm 44.68992
epoch 1, iter 255, loss 6.72978, smoothed loss 7.61139, grad norm 3.07726, param norm 44.72966
epoch 1, iter 260, loss 7.27704, smoothed loss 7.58762, grad norm 2.76905, param norm 44.77359
epoch 1, iter 265, loss 6.73274, smoothed loss 7.56678, grad norm 2.78108, param norm 44.81602
epoch 1, iter 270, loss 6.98753, smoothed loss 7.52530, grad norm 2.49213, param norm 44.85519
epoch 1, iter 275, loss 7.71937, smoothed loss 7.51824, grad norm 2.63523, param norm 44.88674
epoch 1, iter 280, loss 6.77688, smoothed loss 7.49968, grad norm 2.71036, param norm 44.91907
epoch 1, iter 285, loss 6.66474, smoothed loss 7.47891, grad norm 2.83222, param norm 44.95472
epoch 1, iter 290, loss 7.00051, smoothed loss 7.44809, grad norm 3.04150, param norm 44.99779
epoch 1, iter 295, loss 7.10263, smoothed loss 7.42643, grad norm 3.29385, param norm 45.04153
epoch 1, iter 300, loss 6.75167, smoothed loss 7.39274, grad norm 2.86818, param norm 45.08732
epoch 1, iter 305, loss 7.08402, smoothed loss 7.39368, grad norm 2.89116, param norm 45.12768
epoch 1, iter 310, loss 6.88774, smoothed loss 7.36795, grad norm 2.85351, param norm 45.16767
epoch 1, iter 315, loss 6.07813, smoothed loss 7.33844, grad norm 3.27004, param norm 45.20954
epoch 1, iter 320, loss 6.84877, smoothed loss 7.31703, grad norm 3.37460, param norm 45.25168
epoch 1, iter 325, loss 6.90717, smoothed loss 7.28728, grad norm 3.38845, param norm 45.28246
epoch 1, iter 330, loss 7.32904, smoothed loss 7.27840, grad norm 3.03852, param norm 45.32289
epoch 1, iter 335, loss 6.61687, smoothed loss 7.26136, grad norm 3.62570, param norm 45.37211
epoch 1, iter 340, loss 6.32707, smoothed loss 7.23433, grad norm 3.13066, param norm 45.42215
epoch 1, iter 345, loss 7.05330, smoothed loss 7.21546, grad norm 2.48778, param norm 45.46343
epoch 1, iter 350, loss 6.34048, smoothed loss 7.18950, grad norm 2.83960, param norm 45.49824
epoch 1, iter 355, loss 6.40191, smoothed loss 7.16543, grad norm 2.63442, param norm 45.54006
epoch 1, iter 360, loss 7.03618, smoothed loss 7.16106, grad norm 2.88736, param norm 45.58004
epoch 1, iter 365, loss 6.51436, smoothed loss 7.13547, grad norm 2.55448, param norm 45.62083
epoch 1, iter 370, loss 6.94563, smoothed loss 7.12889, grad norm 3.22242, param norm 45.66550
epoch 1, iter 375, loss 7.04917, smoothed loss 7.11815, grad norm 3.77390, param norm 45.70111
epoch 1, iter 380, loss 6.92792, smoothed loss 7.10702, grad norm 2.69379, param norm 45.74086
epoch 1, iter 385, loss 6.87363, smoothed loss 7.10847, grad norm 2.76165, param norm 45.78101
epoch 1, iter 390, loss 6.35233, smoothed loss 7.09123, grad norm 3.12298, param norm 45.82464
epoch 1, iter 395, loss 6.32519, smoothed loss 7.07699, grad norm 2.68565, param norm 45.87315
epoch 1, iter 400, loss 6.95402, smoothed loss 7.05981, grad norm 3.09584, param norm 45.91677
epoch 1, iter 405, loss 6.54375, smoothed loss 7.04888, grad norm 2.63882, param norm 45.95371
epoch 1, iter 410, loss 6.24710, smoothed loss 7.02344, grad norm 2.81784, param norm 45.99194
epoch 1, iter 415, loss 7.06220, smoothed loss 7.02152, grad norm 2.45402, param norm 46.03640
epoch 1, iter 420, loss 7.46656, smoothed loss 7.01166, grad norm 3.07408, param norm 46.08337
epoch 1, iter 425, loss 7.07911, smoothed loss 6.99188, grad norm 2.98079, param norm 46.12279
epoch 1, iter 430, loss 6.92274, smoothed loss 6.97952, grad norm 3.33735, param norm 46.15744
epoch 1, iter 435, loss 6.61872, smoothed loss 6.95759, grad norm 3.17427, param norm 46.19777
epoch 1, iter 440, loss 7.50095, smoothed loss 6.94041, grad norm 2.88773, param norm 46.23705
epoch 1, iter 445, loss 6.32935, smoothed loss 6.91453, grad norm 3.41785, param norm 46.27726
epoch 1, iter 450, loss 7.67774, smoothed loss 6.91673, grad norm 4.41518, param norm 46.31213
epoch 1, iter 455, loss 7.13721, smoothed loss 6.90546, grad norm 3.41417, param norm 46.35532
epoch 1, iter 460, loss 7.06641, smoothed loss 6.90774, grad norm 3.02361, param norm 46.39450
epoch 1, iter 465, loss 6.58049, smoothed loss 6.90594, grad norm 2.81447, param norm 46.42990
epoch 1, iter 470, loss 6.33011, smoothed loss 6.87929, grad norm 2.64062, param norm 46.47756
epoch 1, iter 475, loss 7.17229, smoothed loss 6.86675, grad norm 3.69966, param norm 46.53165
epoch 1, iter 480, loss 6.44913, smoothed loss 6.85749, grad norm 3.59586, param norm 46.57526
epoch 1, iter 485, loss 6.85354, smoothed loss 6.84894, grad norm 2.95952, param norm 46.61563
epoch 1, iter 490, loss 6.33356, smoothed loss 6.82419, grad norm 2.68953, param norm 46.65475
epoch 1, iter 495, loss 7.04413, smoothed loss 6.81564, grad norm 3.09575, param norm 46.70210
epoch 1, iter 500, loss 6.23020, smoothed loss 6.80317, grad norm 3.24277, param norm 46.73925
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 1, Iter 500, dev loss: 6.496128
Calculating Train F1/EM...
F1 train: 1000 examples took 10.65971 seconds [Score: 0.19161]
Exact Match train: 1000 examples took 8.66824 seconds [Score: 0.12900]
Epoch 1, Iter 500, Train F1 score: 0.191610, Train EM score: 0.129000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.99142 seconds [Score: 0.18522]
Exact Match dev: 7118 examples took 57.59524 seconds [Score: 0.11394]
Epoch 1, Iter 500, Dev F1 score: 0.185224, Dev EM score: 0.113936
End of epoch 1
epoch 1, iter 505, loss 6.80736, smoothed loss 6.79825, grad norm 3.74646, param norm 46.77380
epoch 1, iter 510, loss 6.96394, smoothed loss 6.79220, grad norm 2.84595, param norm 46.81194
epoch 1, iter 515, loss 6.65066, smoothed loss 6.78555, grad norm 3.07109, param norm 46.85313
epoch 1, iter 520, loss 6.60120, smoothed loss 6.76384, grad norm 3.03784, param norm 46.89804
epoch 1, iter 525, loss 6.85870, smoothed loss 6.76066, grad norm 3.02187, param norm 46.94300
epoch 1, iter 530, loss 6.39183, smoothed loss 6.74220, grad norm 3.41701, param norm 46.98238
epoch 1, iter 535, loss 6.52838, smoothed loss 6.73282, grad norm 3.24766, param norm 47.02438
epoch 1, iter 540, loss 6.58302, smoothed loss 6.72173, grad norm 3.12281, param norm 47.06415
epoch 1, iter 545, loss 5.81784, smoothed loss 6.69629, grad norm 2.76664, param norm 47.10205
epoch 1, iter 550, loss 5.82084, smoothed loss 6.69560, grad norm 3.04029, param norm 47.13826
epoch 1, iter 555, loss 6.65233, smoothed loss 6.68940, grad norm 3.43518, param norm 47.17688
epoch 1, iter 560, loss 6.74185, smoothed loss 6.68376, grad norm 2.81673, param norm 47.22084
epoch 1, iter 565, loss 6.64202, smoothed loss 6.68984, grad norm 2.90634, param norm 47.26286
epoch 1, iter 570, loss 7.05090, smoothed loss 6.68572, grad norm 3.24069, param norm 47.30896
epoch 1, iter 575, loss 6.50925, smoothed loss 6.67043, grad norm 2.82390, param norm 47.35614
epoch 1, iter 580, loss 6.64501, smoothed loss 6.65943, grad norm 3.66473, param norm 47.39573
epoch 1, iter 585, loss 6.61819, smoothed loss 6.65082, grad norm 3.14202, param norm 47.43264
epoch 1, iter 590, loss 6.16833, smoothed loss 6.63847, grad norm 3.00351, param norm 47.47260
epoch 1, iter 595, loss 7.15304, smoothed loss 6.62835, grad norm 3.23678, param norm 47.52034
epoch 1, iter 600, loss 7.51568, smoothed loss 6.62966, grad norm 3.00297, param norm 47.56490
epoch 1, iter 605, loss 6.15370, smoothed loss 6.61121, grad norm 3.07891, param norm 47.60267
epoch 1, iter 610, loss 6.36469, smoothed loss 6.60182, grad norm 2.86093, param norm 47.64290
epoch 1, iter 615, loss 6.98669, smoothed loss 6.59190, grad norm 3.85269, param norm 47.67947
epoch 1, iter 620, loss 6.59244, smoothed loss 6.57925, grad norm 2.86163, param norm 47.71704
epoch 1, iter 625, loss 5.94127, smoothed loss 6.56358, grad norm 3.56290, param norm 47.75485
epoch 1, iter 630, loss 6.46399, smoothed loss 6.55026, grad norm 3.14286, param norm 47.79743
epoch 1, iter 635, loss 6.34581, smoothed loss 6.56085, grad norm 3.37487, param norm 47.84017
epoch 1, iter 640, loss 6.53779, smoothed loss 6.55002, grad norm 3.09673, param norm 47.87776
epoch 1, iter 645, loss 6.83507, smoothed loss 6.54271, grad norm 3.08329, param norm 47.91827
epoch 1, iter 650, loss 6.27093, smoothed loss 6.52966, grad norm 2.82269, param norm 47.96309
epoch 1, iter 655, loss 6.11382, smoothed loss 6.52019, grad norm 3.40540, param norm 48.00555
epoch 1, iter 660, loss 6.13984, smoothed loss 6.52825, grad norm 2.91907, param norm 48.04180
epoch 1, iter 665, loss 6.29842, smoothed loss 6.51191, grad norm 3.29363, param norm 48.07872
epoch 1, iter 670, loss 6.50558, smoothed loss 6.51730, grad norm 3.67218, param norm 48.11315
epoch 1, iter 675, loss 6.19883, smoothed loss 6.50581, grad norm 3.08696, param norm 48.15010
epoch 1, iter 680, loss 7.56940, smoothed loss 6.50474, grad norm 3.28087, param norm 48.19059
epoch 1, iter 685, loss 6.44125, smoothed loss 6.48924, grad norm 4.07375, param norm 48.23004
epoch 1, iter 690, loss 6.46749, smoothed loss 6.49646, grad norm 3.27083, param norm 48.26679
epoch 1, iter 695, loss 6.36898, smoothed loss 6.48388, grad norm 3.46530, param norm 48.30228
epoch 1, iter 700, loss 6.44243, smoothed loss 6.47810, grad norm 3.35413, param norm 48.34001
epoch 1, iter 705, loss 6.18295, smoothed loss 6.46215, grad norm 2.92007, param norm 48.37713
epoch 1, iter 710, loss 6.52672, smoothed loss 6.45219, grad norm 3.29615, param norm 48.41772
epoch 1, iter 715, loss 6.34077, smoothed loss 6.44778, grad norm 3.65090, param norm 48.45065
epoch 1, iter 720, loss 6.81870, smoothed loss 6.44706, grad norm 3.19216, param norm 48.47848
epoch 1, iter 725, loss 6.07541, smoothed loss 6.41803, grad norm 3.23409, param norm 48.50541
epoch 1, iter 730, loss 5.84820, smoothed loss 6.41476, grad norm 4.26084, param norm 48.54212
epoch 1, iter 735, loss 6.20402, smoothed loss 6.40297, grad norm 3.00512, param norm 48.58091
epoch 1, iter 740, loss 5.65999, smoothed loss 6.39934, grad norm 2.95593, param norm 48.61424
epoch 1, iter 745, loss 5.67345, smoothed loss 6.39170, grad norm 2.52920, param norm 48.65051
epoch 1, iter 750, loss 6.54499, smoothed loss 6.39930, grad norm 3.13129, param norm 48.68628
epoch 1, iter 755, loss 6.13452, smoothed loss 6.40307, grad norm 3.07498, param norm 48.71786
epoch 1, iter 760, loss 6.48116, smoothed loss 6.39350, grad norm 2.79190, param norm 48.75404
epoch 1, iter 765, loss 6.92333, smoothed loss 6.39033, grad norm 3.53554, param norm 48.79033
epoch 1, iter 770, loss 6.20418, smoothed loss 6.39802, grad norm 2.96279, param norm 48.82445
epoch 1, iter 775, loss 5.32369, smoothed loss 6.38844, grad norm 2.72594, param norm 48.85717
epoch 1, iter 780, loss 6.04794, smoothed loss 6.38827, grad norm 2.94698, param norm 48.89260
epoch 1, iter 785, loss 6.28453, smoothed loss 6.39050, grad norm 3.15794, param norm 48.92434
epoch 1, iter 790, loss 6.41606, smoothed loss 6.39129, grad norm 2.83792, param norm 48.95337
epoch 1, iter 795, loss 6.71931, smoothed loss 6.41079, grad norm 2.95242, param norm 48.98059
epoch 1, iter 800, loss 6.39632, smoothed loss 6.40213, grad norm 3.14019, param norm 49.01562
epoch 1, iter 805, loss 6.87619, smoothed loss 6.40760, grad norm 3.45721, param norm 49.05875
epoch 1, iter 810, loss 6.32320, smoothed loss 6.39050, grad norm 3.91366, param norm 49.10304
epoch 1, iter 815, loss 6.18850, smoothed loss 6.39272, grad norm 3.37506, param norm 49.13923
epoch 1, iter 820, loss 6.89962, smoothed loss 6.39918, grad norm 3.65784, param norm 49.17410
epoch 1, iter 825, loss 6.59601, smoothed loss 6.39396, grad norm 3.34331, param norm 49.20399
epoch 1, iter 830, loss 6.07821, smoothed loss 6.38064, grad norm 2.86850, param norm 49.24293
epoch 1, iter 835, loss 6.09442, smoothed loss 6.38221, grad norm 3.10748, param norm 49.28343
epoch 1, iter 840, loss 6.29867, smoothed loss 6.38043, grad norm 3.15135, param norm 49.31668
epoch 1, iter 845, loss 5.50312, smoothed loss 6.36766, grad norm 2.99577, param norm 49.34769
epoch 1, iter 850, loss 6.25446, smoothed loss 6.36764, grad norm 3.37114, param norm 49.38357
epoch 1, iter 855, loss 5.91158, smoothed loss 6.36402, grad norm 3.37560, param norm 49.41891
epoch 1, iter 860, loss 5.73593, smoothed loss 6.34652, grad norm 4.18103, param norm 49.45773
epoch 1, iter 865, loss 6.10525, smoothed loss 6.33667, grad norm 3.37715, param norm 49.49679
epoch 1, iter 870, loss 6.24568, smoothed loss 6.33125, grad norm 3.30392, param norm 49.52892
epoch 1, iter 875, loss 6.43904, smoothed loss 6.33895, grad norm 2.72182, param norm 49.55807
epoch 1, iter 880, loss 6.02337, smoothed loss 6.33776, grad norm 4.20297, param norm 49.58951
epoch 1, iter 885, loss 6.23129, smoothed loss 6.33000, grad norm 3.17898, param norm 49.63092
epoch 1, iter 890, loss 6.46139, smoothed loss 6.32761, grad norm 3.54683, param norm 49.67369
epoch 1, iter 895, loss 6.14340, smoothed loss 6.33441, grad norm 3.46935, param norm 49.71609
epoch 1, iter 900, loss 6.11314, smoothed loss 6.33046, grad norm 3.46427, param norm 49.75441
epoch 1, iter 905, loss 6.45663, smoothed loss 6.32126, grad norm 3.51779, param norm 49.79689
epoch 1, iter 910, loss 5.85282, smoothed loss 6.31922, grad norm 2.70717, param norm 49.83245
epoch 1, iter 915, loss 6.50178, smoothed loss 6.30725, grad norm 2.93754, param norm 49.87042
epoch 1, iter 920, loss 6.17271, smoothed loss 6.29897, grad norm 3.13508, param norm 49.91432
epoch 1, iter 925, loss 6.16911, smoothed loss 6.31024, grad norm 3.11159, param norm 49.95334
epoch 1, iter 930, loss 5.35758, smoothed loss 6.30226, grad norm 3.12928, param norm 49.98574
epoch 1, iter 935, loss 6.13578, smoothed loss 6.29714, grad norm 3.34561, param norm 50.01698
epoch 1, iter 940, loss 6.21995, smoothed loss 6.29651, grad norm 3.09702, param norm 50.04985
epoch 2, iter 945, loss 5.90910, smoothed loss 6.28713, grad norm 3.35313, param norm 50.08504
epoch 2, iter 950, loss 7.06559, smoothed loss 6.29844, grad norm 3.64452, param norm 50.12094
epoch 2, iter 955, loss 5.96692, smoothed loss 6.27858, grad norm 3.15734, param norm 50.15462
epoch 2, iter 960, loss 5.43593, smoothed loss 6.27117, grad norm 3.36666, param norm 50.18747
epoch 2, iter 965, loss 6.13207, smoothed loss 6.25729, grad norm 3.92647, param norm 50.22538
epoch 2, iter 970, loss 6.23734, smoothed loss 6.25775, grad norm 3.40989, param norm 50.25525
epoch 2, iter 975, loss 5.67500, smoothed loss 6.25614, grad norm 3.24046, param norm 50.28721
epoch 2, iter 980, loss 6.11222, smoothed loss 6.24727, grad norm 3.08595, param norm 50.32894
epoch 2, iter 985, loss 5.67236, smoothed loss 6.24681, grad norm 3.60551, param norm 50.36878
epoch 2, iter 990, loss 6.83855, smoothed loss 6.24862, grad norm 3.12170, param norm 50.40716
epoch 2, iter 995, loss 6.20360, smoothed loss 6.23868, grad norm 3.39824, param norm 50.44753
epoch 2, iter 1000, loss 6.52241, smoothed loss 6.24368, grad norm 3.24343, param norm 50.48569
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 2, Iter 1000, dev loss: 6.029838
Calculating Train F1/EM...
F1 train: 1000 examples took 9.03672 seconds [Score: 0.23813]
Exact Match train: 1000 examples took 9.12424 seconds [Score: 0.16400]
Epoch 2, Iter 1000, Train F1 score: 0.238134, Train EM score: 0.164000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.61671 seconds [Score: 0.22455]
Exact Match dev: 7118 examples took 57.27936 seconds [Score: 0.14667]
Epoch 2, Iter 1000, Dev F1 score: 0.224551, Dev EM score: 0.146670
End of epoch 2
epoch 2, iter 1005, loss 5.46828, smoothed loss 6.24022, grad norm 3.76042, param norm 50.52140
epoch 2, iter 1010, loss 6.32057, smoothed loss 6.23977, grad norm 3.38028, param norm 50.55852
epoch 2, iter 1015, loss 7.18762, smoothed loss 6.23959, grad norm 3.56705, param norm 50.59696
epoch 2, iter 1020, loss 5.79223, smoothed loss 6.22587, grad norm 3.39706, param norm 50.63227
epoch 2, iter 1025, loss 6.41293, smoothed loss 6.22450, grad norm 3.23290, param norm 50.66828
epoch 2, iter 1030, loss 5.98209, smoothed loss 6.21577, grad norm 4.08054, param norm 50.70187
epoch 2, iter 1035, loss 5.88364, smoothed loss 6.19657, grad norm 3.19420, param norm 50.74096
epoch 2, iter 1040, loss 6.12124, smoothed loss 6.19575, grad norm 3.44164, param norm 50.77697
epoch 2, iter 1045, loss 5.68889, smoothed loss 6.18985, grad norm 2.95975, param norm 50.81017
epoch 2, iter 1050, loss 5.91556, smoothed loss 6.19010, grad norm 3.53515, param norm 50.84583
epoch 2, iter 1055, loss 5.76357, smoothed loss 6.19054, grad norm 3.19360, param norm 50.87878
epoch 2, iter 1060, loss 6.33876, smoothed loss 6.18801, grad norm 3.34109, param norm 50.91614
epoch 2, iter 1065, loss 6.37921, smoothed loss 6.17631, grad norm 3.22776, param norm 50.95320
epoch 2, iter 1070, loss 6.00946, smoothed loss 6.17378, grad norm 2.78653, param norm 50.98703
epoch 2, iter 1075, loss 5.39590, smoothed loss 6.16570, grad norm 3.48757, param norm 51.02250
epoch 2, iter 1080, loss 6.59626, smoothed loss 6.16719, grad norm 3.74390, param norm 51.06405
epoch 2, iter 1085, loss 5.71607, smoothed loss 6.17829, grad norm 3.25386, param norm 51.09636
epoch 2, iter 1090, loss 6.29686, smoothed loss 6.18248, grad norm 4.36736, param norm 51.12785
epoch 2, iter 1095, loss 6.61386, smoothed loss 6.17689, grad norm 3.28010, param norm 51.16206
epoch 2, iter 1100, loss 6.59458, smoothed loss 6.17883, grad norm 3.35516, param norm 51.19598
epoch 2, iter 1105, loss 5.98912, smoothed loss 6.16840, grad norm 3.26980, param norm 51.22878
epoch 2, iter 1110, loss 5.87227, smoothed loss 6.17168, grad norm 3.17158, param norm 51.26651
epoch 2, iter 1115, loss 5.62851, smoothed loss 6.16329, grad norm 3.64776, param norm 51.29698
epoch 2, iter 1120, loss 5.76877, smoothed loss 6.15492, grad norm 3.21470, param norm 51.32597
epoch 2, iter 1125, loss 5.95461, smoothed loss 6.13754, grad norm 3.35708, param norm 51.36263
epoch 2, iter 1130, loss 5.58136, smoothed loss 6.12615, grad norm 3.13812, param norm 51.40358
epoch 2, iter 1135, loss 6.42420, smoothed loss 6.11541, grad norm 3.88385, param norm 51.44094
epoch 2, iter 1140, loss 5.70667, smoothed loss 6.11152, grad norm 3.24090, param norm 51.47669
epoch 2, iter 1145, loss 6.08340, smoothed loss 6.10096, grad norm 3.38711, param norm 51.51418
epoch 2, iter 1150, loss 6.89383, smoothed loss 6.09312, grad norm 3.70124, param norm 51.55304
epoch 2, iter 1155, loss 6.23223, smoothed loss 6.10308, grad norm 3.58844, param norm 51.58686
epoch 2, iter 1160, loss 6.18205, smoothed loss 6.10186, grad norm 3.28176, param norm 51.62073
epoch 2, iter 1165, loss 6.00850, smoothed loss 6.10034, grad norm 3.44587, param norm 51.65619
epoch 2, iter 1170, loss 6.35176, smoothed loss 6.10326, grad norm 4.44905, param norm 51.69369
epoch 2, iter 1175, loss 5.29288, smoothed loss 6.10364, grad norm 3.05812, param norm 51.72779
epoch 2, iter 1180, loss 6.20704, smoothed loss 6.10518, grad norm 3.64627, param norm 51.76170
epoch 2, iter 1185, loss 6.90055, smoothed loss 6.11532, grad norm 3.70197, param norm 51.79125
epoch 2, iter 1190, loss 6.08274, smoothed loss 6.11765, grad norm 3.11716, param norm 51.82140
epoch 2, iter 1195, loss 5.78288, smoothed loss 6.11345, grad norm 2.59133, param norm 51.85371
epoch 2, iter 1200, loss 6.23138, smoothed loss 6.12604, grad norm 3.24083, param norm 51.88891
epoch 2, iter 1205, loss 6.35519, smoothed loss 6.12901, grad norm 3.26640, param norm 51.92105
epoch 2, iter 1210, loss 5.89554, smoothed loss 6.11028, grad norm 3.06995, param norm 51.95793
epoch 2, iter 1215, loss 6.00737, smoothed loss 6.10247, grad norm 3.44264, param norm 51.99220
epoch 2, iter 1220, loss 6.43022, smoothed loss 6.10080, grad norm 3.21481, param norm 52.02177
epoch 2, iter 1225, loss 6.00590, smoothed loss 6.09680, grad norm 3.17163, param norm 52.05630
epoch 2, iter 1230, loss 5.19606, smoothed loss 6.08009, grad norm 2.92132, param norm 52.09207
epoch 2, iter 1235, loss 6.18488, smoothed loss 6.09127, grad norm 3.18090, param norm 52.12373
epoch 2, iter 1240, loss 5.52701, smoothed loss 6.09066, grad norm 3.36038, param norm 52.15120
epoch 2, iter 1245, loss 6.02577, smoothed loss 6.08350, grad norm 3.56313, param norm 52.18202
epoch 2, iter 1250, loss 5.69433, smoothed loss 6.07097, grad norm 3.22579, param norm 52.21660
epoch 2, iter 1255, loss 6.26101, smoothed loss 6.06674, grad norm 3.41219, param norm 52.24867
epoch 2, iter 1260, loss 6.17962, smoothed loss 6.06591, grad norm 3.81739, param norm 52.27831
epoch 2, iter 1265, loss 6.58695, smoothed loss 6.07980, grad norm 3.49683, param norm 52.30409
epoch 2, iter 1270, loss 6.09991, smoothed loss 6.07411, grad norm 3.44198, param norm 52.33260
epoch 2, iter 1275, loss 6.33486, smoothed loss 6.08625, grad norm 3.14109, param norm 52.36140
epoch 2, iter 1280, loss 5.82322, smoothed loss 6.08226, grad norm 3.48664, param norm 52.39290
epoch 2, iter 1285, loss 6.35329, smoothed loss 6.08811, grad norm 3.91130, param norm 52.43081
epoch 2, iter 1290, loss 6.40831, smoothed loss 6.08543, grad norm 4.07180, param norm 52.47020
epoch 2, iter 1295, loss 6.82871, smoothed loss 6.09278, grad norm 3.59852, param norm 52.50094
epoch 2, iter 1300, loss 6.11226, smoothed loss 6.07719, grad norm 2.71606, param norm 52.53371
epoch 2, iter 1305, loss 5.41891, smoothed loss 6.04950, grad norm 3.26722, param norm 52.57596
epoch 2, iter 1310, loss 6.69027, smoothed loss 6.05902, grad norm 3.89993, param norm 52.62243
epoch 2, iter 1315, loss 5.36211, smoothed loss 6.05502, grad norm 3.39124, param norm 52.65701
epoch 2, iter 1320, loss 5.54557, smoothed loss 6.05204, grad norm 3.08154, param norm 52.69052
epoch 2, iter 1325, loss 6.72019, smoothed loss 6.05645, grad norm 3.42742, param norm 52.72780
epoch 2, iter 1330, loss 5.95294, smoothed loss 6.05155, grad norm 3.63745, param norm 52.76332
epoch 2, iter 1335, loss 5.83742, smoothed loss 6.04633, grad norm 3.06959, param norm 52.80116
epoch 2, iter 1340, loss 6.16317, smoothed loss 6.04484, grad norm 3.67693, param norm 52.84149
epoch 2, iter 1345, loss 6.74774, smoothed loss 6.04650, grad norm 3.60616, param norm 52.87582
epoch 2, iter 1350, loss 6.21199, smoothed loss 6.04274, grad norm 3.35005, param norm 52.90687
epoch 2, iter 1355, loss 5.68844, smoothed loss 6.03940, grad norm 2.99871, param norm 52.93876
epoch 2, iter 1360, loss 5.61456, smoothed loss 6.04857, grad norm 3.33686, param norm 52.96888
epoch 2, iter 1365, loss 5.31209, smoothed loss 6.03499, grad norm 3.22945, param norm 53.00011
epoch 2, iter 1370, loss 6.30556, smoothed loss 6.02829, grad norm 3.54963, param norm 53.03717
epoch 2, iter 1375, loss 5.31123, smoothed loss 6.01828, grad norm 3.08791, param norm 53.06960
epoch 2, iter 1380, loss 5.85550, smoothed loss 6.01616, grad norm 3.81767, param norm 53.10129
epoch 2, iter 1385, loss 5.79017, smoothed loss 6.00983, grad norm 3.51183, param norm 53.13358
epoch 2, iter 1390, loss 5.65954, smoothed loss 5.99431, grad norm 3.32961, param norm 53.16885
epoch 2, iter 1395, loss 6.57337, smoothed loss 6.00028, grad norm 3.77055, param norm 53.20640
epoch 2, iter 1400, loss 5.86863, smoothed loss 5.98054, grad norm 3.25749, param norm 53.23416
epoch 2, iter 1405, loss 5.85113, smoothed loss 5.98047, grad norm 3.37879, param norm 53.26378
epoch 2, iter 1410, loss 6.36593, smoothed loss 5.97238, grad norm 3.80246, param norm 53.29675
epoch 2, iter 1415, loss 5.46309, smoothed loss 5.95689, grad norm 3.22433, param norm 53.33080
epoch 2, iter 1420, loss 5.54014, smoothed loss 5.94381, grad norm 3.08537, param norm 53.36951
epoch 2, iter 1425, loss 6.04950, smoothed loss 5.94074, grad norm 3.50345, param norm 53.40342
epoch 2, iter 1430, loss 6.06982, smoothed loss 5.92924, grad norm 3.72738, param norm 53.44003
epoch 2, iter 1435, loss 5.75234, smoothed loss 5.93224, grad norm 3.45634, param norm 53.47692
epoch 2, iter 1440, loss 6.33329, smoothed loss 5.92428, grad norm 3.90251, param norm 53.51173
epoch 2, iter 1445, loss 5.78672, smoothed loss 5.91061, grad norm 3.66540, param norm 53.54567
epoch 2, iter 1450, loss 5.94788, smoothed loss 5.91296, grad norm 3.50038, param norm 53.57410
epoch 2, iter 1455, loss 5.97866, smoothed loss 5.91749, grad norm 3.40158, param norm 53.60023
epoch 2, iter 1460, loss 5.69457, smoothed loss 5.91794, grad norm 3.03626, param norm 53.62768
epoch 2, iter 1465, loss 5.89064, smoothed loss 5.91407, grad norm 3.32622, param norm 53.66154
epoch 2, iter 1470, loss 6.48560, smoothed loss 5.92307, grad norm 4.01803, param norm 53.69243
epoch 2, iter 1475, loss 5.81915, smoothed loss 5.91756, grad norm 3.21198, param norm 53.72551
epoch 2, iter 1480, loss 6.23920, smoothed loss 5.90258, grad norm 3.01955, param norm 53.76214
epoch 2, iter 1485, loss 5.48594, smoothed loss 5.89736, grad norm 3.30770, param norm 53.79882
epoch 2, iter 1490, loss 6.13582, smoothed loss 5.90636, grad norm 3.59696, param norm 53.83306
epoch 2, iter 1495, loss 5.52568, smoothed loss 5.89408, grad norm 3.42476, param norm 53.86708
epoch 2, iter 1500, loss 5.56270, smoothed loss 5.88942, grad norm 3.55390, param norm 53.90370
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 2, Iter 1500, dev loss: 5.772048
Calculating Train F1/EM...
F1 train: 1000 examples took 8.69309 seconds [Score: 0.28202]
Exact Match train: 1000 examples took 9.32063 seconds [Score: 0.18400]
Epoch 2, Iter 1500, Train F1 score: 0.282025, Train EM score: 0.184000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.74351 seconds [Score: 0.25013]
Exact Match dev: 7118 examples took 57.58217 seconds [Score: 0.16395]
Epoch 2, Iter 1500, Dev F1 score: 0.250127, Dev EM score: 0.163951
End of epoch 2
epoch 2, iter 1505, loss 6.06203, smoothed loss 5.88529, grad norm 3.27981, param norm 53.94077
epoch 2, iter 1510, loss 6.06469, smoothed loss 5.88068, grad norm 3.76328, param norm 53.97500
epoch 2, iter 1515, loss 5.99008, smoothed loss 5.87502, grad norm 4.20939, param norm 54.00710
epoch 2, iter 1520, loss 5.49023, smoothed loss 5.86290, grad norm 3.86830, param norm 54.03686
epoch 2, iter 1525, loss 5.32846, smoothed loss 5.85285, grad norm 3.43509, param norm 54.06726
epoch 2, iter 1530, loss 5.78651, smoothed loss 5.86058, grad norm 3.66340, param norm 54.09791
epoch 2, iter 1535, loss 5.86202, smoothed loss 5.85654, grad norm 2.90508, param norm 54.12487
epoch 2, iter 1540, loss 5.85054, smoothed loss 5.86666, grad norm 3.55917, param norm 54.15379
epoch 2, iter 1545, loss 5.63566, smoothed loss 5.85985, grad norm 3.23369, param norm 54.18550
epoch 2, iter 1550, loss 5.89144, smoothed loss 5.85831, grad norm 3.75938, param norm 54.21525
epoch 2, iter 1555, loss 5.78605, smoothed loss 5.86399, grad norm 3.28955, param norm 54.23922
epoch 2, iter 1560, loss 5.65871, smoothed loss 5.85784, grad norm 3.37424, param norm 54.26620
epoch 2, iter 1565, loss 5.87807, smoothed loss 5.85662, grad norm 3.26050, param norm 54.29176
epoch 2, iter 1570, loss 6.08236, smoothed loss 5.84866, grad norm 3.42634, param norm 54.32246
epoch 2, iter 1575, loss 5.82220, smoothed loss 5.83684, grad norm 3.05511, param norm 54.35052
epoch 2, iter 1580, loss 5.74888, smoothed loss 5.82075, grad norm 2.80429, param norm 54.38421
epoch 2, iter 1585, loss 5.50145, smoothed loss 5.81471, grad norm 3.80877, param norm 54.41887
epoch 2, iter 1590, loss 5.28120, smoothed loss 5.81026, grad norm 3.27045, param norm 54.45338
epoch 2, iter 1595, loss 6.05799, smoothed loss 5.81285, grad norm 3.61291, param norm 54.48839
epoch 2, iter 1600, loss 6.24964, smoothed loss 5.81207, grad norm 4.20014, param norm 54.51716
epoch 2, iter 1605, loss 6.32717, smoothed loss 5.81263, grad norm 2.92139, param norm 54.54652
epoch 2, iter 1610, loss 5.89263, smoothed loss 5.80641, grad norm 3.08540, param norm 54.57882
epoch 2, iter 1615, loss 5.39529, smoothed loss 5.80193, grad norm 3.42461, param norm 54.61621
epoch 2, iter 1620, loss 5.94879, smoothed loss 5.80755, grad norm 3.39932, param norm 54.65088
epoch 2, iter 1625, loss 5.79415, smoothed loss 5.82007, grad norm 3.85901, param norm 54.67816
epoch 2, iter 1630, loss 6.50984, smoothed loss 5.82570, grad norm 3.06570, param norm 54.70702
epoch 2, iter 1635, loss 5.51851, smoothed loss 5.80370, grad norm 3.20479, param norm 54.73985
epoch 2, iter 1640, loss 5.80502, smoothed loss 5.80666, grad norm 3.34036, param norm 54.77040
epoch 2, iter 1645, loss 6.22049, smoothed loss 5.79704, grad norm 3.34326, param norm 54.79871
epoch 2, iter 1650, loss 5.90206, smoothed loss 5.79770, grad norm 3.50014, param norm 54.82754
epoch 2, iter 1655, loss 5.66962, smoothed loss 5.79313, grad norm 3.29056, param norm 54.85991
epoch 2, iter 1660, loss 5.99346, smoothed loss 5.80600, grad norm 3.41074, param norm 54.89683
epoch 2, iter 1665, loss 5.71676, smoothed loss 5.80814, grad norm 3.38582, param norm 54.93069
epoch 2, iter 1670, loss 6.36757, smoothed loss 5.81675, grad norm 3.28142, param norm 54.96117
epoch 2, iter 1675, loss 5.52552, smoothed loss 5.81687, grad norm 3.28967, param norm 54.99501
epoch 2, iter 1680, loss 6.20108, smoothed loss 5.82099, grad norm 3.17275, param norm 55.02595
epoch 2, iter 1685, loss 6.13285, smoothed loss 5.83159, grad norm 3.33461, param norm 55.04983
epoch 2, iter 1690, loss 4.97827, smoothed loss 5.82462, grad norm 3.23043, param norm 55.07031
epoch 2, iter 1695, loss 5.83584, smoothed loss 5.83535, grad norm 3.42820, param norm 55.09466
epoch 2, iter 1700, loss 5.95488, smoothed loss 5.83429, grad norm 3.55887, param norm 55.11884
epoch 2, iter 1705, loss 5.43805, smoothed loss 5.83320, grad norm 3.83623, param norm 55.14977
epoch 2, iter 1710, loss 5.20391, smoothed loss 5.82488, grad norm 3.26072, param norm 55.18586
epoch 2, iter 1715, loss 5.36907, smoothed loss 5.80778, grad norm 3.57376, param norm 55.22409
epoch 2, iter 1720, loss 6.27772, smoothed loss 5.80926, grad norm 4.00396, param norm 55.25507
epoch 2, iter 1725, loss 6.03295, smoothed loss 5.80457, grad norm 3.21126, param norm 55.27847
epoch 2, iter 1730, loss 6.34369, smoothed loss 5.81638, grad norm 3.70054, param norm 55.30092
epoch 2, iter 1735, loss 6.14365, smoothed loss 5.82457, grad norm 3.12056, param norm 55.32856
epoch 2, iter 1740, loss 5.43016, smoothed loss 5.82129, grad norm 2.92970, param norm 55.35810
epoch 2, iter 1745, loss 5.72967, smoothed loss 5.80558, grad norm 3.95626, param norm 55.39281
epoch 2, iter 1750, loss 5.47574, smoothed loss 5.80803, grad norm 3.63632, param norm 55.42097
epoch 2, iter 1755, loss 6.07240, smoothed loss 5.81092, grad norm 3.96546, param norm 55.44875
epoch 2, iter 1760, loss 5.87202, smoothed loss 5.82222, grad norm 3.89746, param norm 55.47824
epoch 2, iter 1765, loss 5.94691, smoothed loss 5.81194, grad norm 3.47465, param norm 55.50744
epoch 2, iter 1770, loss 5.87535, smoothed loss 5.80906, grad norm 3.58765, param norm 55.53851
epoch 2, iter 1775, loss 5.71034, smoothed loss 5.80916, grad norm 3.51261, param norm 55.56824
epoch 2, iter 1780, loss 6.02486, smoothed loss 5.81750, grad norm 3.86496, param norm 55.60061
epoch 2, iter 1785, loss 6.10786, smoothed loss 5.81109, grad norm 3.67528, param norm 55.63360
epoch 2, iter 1790, loss 6.04629, smoothed loss 5.80826, grad norm 3.81864, param norm 55.66667
epoch 2, iter 1795, loss 5.78016, smoothed loss 5.80982, grad norm 3.84168, param norm 55.69611
epoch 2, iter 1800, loss 5.35280, smoothed loss 5.79842, grad norm 3.76447, param norm 55.72781
epoch 2, iter 1805, loss 5.67090, smoothed loss 5.79151, grad norm 3.42811, param norm 55.76235
epoch 2, iter 1810, loss 5.31986, smoothed loss 5.77341, grad norm 3.35965, param norm 55.79439
epoch 2, iter 1815, loss 6.07350, smoothed loss 5.77974, grad norm 3.66536, param norm 55.82568
epoch 2, iter 1820, loss 6.47711, smoothed loss 5.77615, grad norm 3.81221, param norm 55.85474
epoch 2, iter 1825, loss 6.17758, smoothed loss 5.77419, grad norm 2.99836, param norm 55.88448
epoch 2, iter 1830, loss 5.89632, smoothed loss 5.77034, grad norm 3.96852, param norm 55.91412
epoch 2, iter 1835, loss 5.55229, smoothed loss 5.78266, grad norm 3.52397, param norm 55.94079
epoch 2, iter 1840, loss 5.74528, smoothed loss 5.78478, grad norm 3.40989, param norm 55.96864
epoch 2, iter 1845, loss 5.22898, smoothed loss 5.78285, grad norm 3.65970, param norm 55.99282
epoch 2, iter 1850, loss 5.97964, smoothed loss 5.79102, grad norm 3.98191, param norm 56.02490
epoch 2, iter 1855, loss 6.06073, smoothed loss 5.78292, grad norm 3.33155, param norm 56.05791
epoch 2, iter 1860, loss 5.44238, smoothed loss 5.77386, grad norm 3.32772, param norm 56.08637
epoch 2, iter 1865, loss 5.78093, smoothed loss 5.77141, grad norm 3.06346, param norm 56.11106
epoch 2, iter 1870, loss 5.61034, smoothed loss 5.77498, grad norm 3.60554, param norm 56.14111
epoch 2, iter 1875, loss 5.54498, smoothed loss 5.75750, grad norm 3.43012, param norm 56.17475
epoch 2, iter 1880, loss 5.68215, smoothed loss 5.75400, grad norm 3.63009, param norm 56.20918
epoch 2, iter 1885, loss 5.75496, smoothed loss 5.74781, grad norm 3.51251, param norm 56.24226
epoch 3, iter 1890, loss 5.78631, smoothed loss 5.73789, grad norm 3.61146, param norm 56.26872
epoch 3, iter 1895, loss 5.20683, smoothed loss 5.74422, grad norm 3.30329, param norm 56.29539
epoch 3, iter 1900, loss 5.95988, smoothed loss 5.74879, grad norm 3.59508, param norm 56.31998
epoch 3, iter 1905, loss 5.85051, smoothed loss 5.74108, grad norm 3.45907, param norm 56.34763
epoch 3, iter 1910, loss 6.24421, smoothed loss 5.73573, grad norm 3.21557, param norm 56.37210
epoch 3, iter 1915, loss 5.68864, smoothed loss 5.74304, grad norm 3.65587, param norm 56.39392
epoch 3, iter 1920, loss 5.10594, smoothed loss 5.74189, grad norm 3.60982, param norm 56.42285
epoch 3, iter 1925, loss 5.67427, smoothed loss 5.73609, grad norm 3.73985, param norm 56.45924
epoch 3, iter 1930, loss 5.12465, smoothed loss 5.73093, grad norm 3.92973, param norm 56.49868
epoch 3, iter 1935, loss 5.52165, smoothed loss 5.74537, grad norm 3.42513, param norm 56.53481
epoch 3, iter 1940, loss 5.32920, smoothed loss 5.74348, grad norm 3.36781, param norm 56.56118
epoch 3, iter 1945, loss 5.68131, smoothed loss 5.73513, grad norm 4.03177, param norm 56.58715
epoch 3, iter 1950, loss 5.97978, smoothed loss 5.73236, grad norm 3.43625, param norm 56.61568
epoch 3, iter 1955, loss 5.20718, smoothed loss 5.73390, grad norm 3.16392, param norm 56.64518
epoch 3, iter 1960, loss 5.58549, smoothed loss 5.72719, grad norm 3.16146, param norm 56.67077
epoch 3, iter 1965, loss 5.25835, smoothed loss 5.71167, grad norm 2.97277, param norm 56.69377
epoch 3, iter 1970, loss 5.39620, smoothed loss 5.70311, grad norm 3.18857, param norm 56.72189
epoch 3, iter 1975, loss 6.34329, smoothed loss 5.70618, grad norm 3.43354, param norm 56.75021
epoch 3, iter 1980, loss 5.73549, smoothed loss 5.70274, grad norm 3.53732, param norm 56.77507
epoch 3, iter 1985, loss 5.63522, smoothed loss 5.70834, grad norm 3.26008, param norm 56.79819
epoch 3, iter 1990, loss 5.67789, smoothed loss 5.71744, grad norm 2.99886, param norm 56.82183
epoch 3, iter 1995, loss 5.81822, smoothed loss 5.71669, grad norm 3.31085, param norm 56.84980
epoch 3, iter 2000, loss 5.65032, smoothed loss 5.72833, grad norm 3.28178, param norm 56.87891
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 2000, dev loss: 5.635034
Calculating Train F1/EM...
F1 train: 1000 examples took 8.98089 seconds [Score: 0.35943]
Exact Match train: 1000 examples took 9.00668 seconds [Score: 0.23200]
Epoch 3, Iter 2000, Train F1 score: 0.359433, Train EM score: 0.232000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.08518 seconds [Score: 0.27439]
Exact Match dev: 7118 examples took 57.42671 seconds [Score: 0.18278]
Epoch 3, Iter 2000, Dev F1 score: 0.274388, Dev EM score: 0.182776
End of epoch 3
epoch 3, iter 2005, loss 5.95525, smoothed loss 5.73127, grad norm 3.44170, param norm 56.91401
epoch 3, iter 2010, loss 5.50762, smoothed loss 5.73234, grad norm 4.02337, param norm 56.94877
epoch 3, iter 2015, loss 6.45990, smoothed loss 5.73831, grad norm 4.07581, param norm 56.98059
epoch 3, iter 2020, loss 6.21596, smoothed loss 5.73156, grad norm 4.15291, param norm 57.00998
epoch 3, iter 2025, loss 4.90498, smoothed loss 5.70932, grad norm 3.40975, param norm 57.04044
epoch 3, iter 2030, loss 6.28531, smoothed loss 5.69755, grad norm 4.05738, param norm 57.07199
epoch 3, iter 2035, loss 5.95934, smoothed loss 5.69811, grad norm 4.03958, param norm 57.10118
epoch 3, iter 2040, loss 5.51914, smoothed loss 5.69076, grad norm 3.58700, param norm 57.13082
epoch 3, iter 2045, loss 6.41616, smoothed loss 5.70491, grad norm 4.10405, param norm 57.15733
epoch 3, iter 2050, loss 5.77949, smoothed loss 5.70151, grad norm 3.62480, param norm 57.18663
epoch 3, iter 2055, loss 5.75628, smoothed loss 5.70107, grad norm 3.67423, param norm 57.22300
epoch 3, iter 2060, loss 5.39816, smoothed loss 5.68632, grad norm 4.00791, param norm 57.26259
epoch 3, iter 2065, loss 5.72298, smoothed loss 5.67957, grad norm 3.50415, param norm 57.29737
epoch 3, iter 2070, loss 5.70192, smoothed loss 5.67234, grad norm 3.32968, param norm 57.32045
epoch 3, iter 2075, loss 5.52834, smoothed loss 5.67334, grad norm 3.89761, param norm 57.34315
epoch 3, iter 2080, loss 5.84208, smoothed loss 5.67645, grad norm 3.57884, param norm 57.36798
epoch 3, iter 2085, loss 5.27784, smoothed loss 5.67029, grad norm 3.56662, param norm 57.39875
epoch 3, iter 2090, loss 5.64071, smoothed loss 5.66274, grad norm 3.71545, param norm 57.42570
epoch 3, iter 2095, loss 6.30433, smoothed loss 5.65592, grad norm 3.70147, param norm 57.45029
epoch 3, iter 2100, loss 6.58975, smoothed loss 5.65966, grad norm 4.13316, param norm 57.47651
epoch 3, iter 2105, loss 5.67104, smoothed loss 5.65982, grad norm 3.20702, param norm 57.50094
epoch 3, iter 2110, loss 5.69793, smoothed loss 5.65450, grad norm 3.70043, param norm 57.52948
epoch 3, iter 2115, loss 6.07570, smoothed loss 5.66360, grad norm 3.73298, param norm 57.55676
epoch 3, iter 2120, loss 5.83226, smoothed loss 5.67115, grad norm 4.15446, param norm 57.57701
epoch 3, iter 2125, loss 6.39568, smoothed loss 5.68547, grad norm 3.59709, param norm 57.59795
epoch 3, iter 2130, loss 5.89984, smoothed loss 5.68168, grad norm 3.42046, param norm 57.62391
epoch 3, iter 2135, loss 6.38402, smoothed loss 5.69349, grad norm 3.58746, param norm 57.64534
epoch 3, iter 2140, loss 5.54232, smoothed loss 5.68571, grad norm 3.50215, param norm 57.67051
epoch 3, iter 2145, loss 5.88046, smoothed loss 5.67590, grad norm 3.96009, param norm 57.70561
epoch 3, iter 2150, loss 5.65163, smoothed loss 5.68015, grad norm 3.97071, param norm 57.73888
epoch 3, iter 2155, loss 6.12194, smoothed loss 5.69016, grad norm 3.50922, param norm 57.76925
epoch 3, iter 2160, loss 5.53571, smoothed loss 5.68236, grad norm 3.71347, param norm 57.79980
epoch 3, iter 2165, loss 5.96004, smoothed loss 5.68288, grad norm 3.36848, param norm 57.82956
epoch 3, iter 2170, loss 5.05154, smoothed loss 5.66978, grad norm 3.46544, param norm 57.85871
epoch 3, iter 2175, loss 6.18566, smoothed loss 5.67791, grad norm 3.39481, param norm 57.88568
epoch 3, iter 2180, loss 6.24476, smoothed loss 5.68526, grad norm 3.57650, param norm 57.91092
epoch 3, iter 2185, loss 5.30179, smoothed loss 5.68366, grad norm 3.06153, param norm 57.93747
epoch 3, iter 2190, loss 6.20136, smoothed loss 5.68327, grad norm 3.11328, param norm 57.96700
epoch 3, iter 2195, loss 5.23033, smoothed loss 5.67934, grad norm 4.07089, param norm 57.99767
epoch 3, iter 2200, loss 5.91265, smoothed loss 5.68809, grad norm 3.78948, param norm 58.02953
epoch 3, iter 2205, loss 5.50994, smoothed loss 5.67785, grad norm 3.53921, param norm 58.05592
epoch 3, iter 2210, loss 6.00127, smoothed loss 5.67269, grad norm 3.81100, param norm 58.08153
epoch 3, iter 2215, loss 4.96919, smoothed loss 5.67971, grad norm 3.21391, param norm 58.10742
epoch 3, iter 2220, loss 5.63941, smoothed loss 5.67252, grad norm 3.58689, param norm 58.13668
epoch 3, iter 2225, loss 5.94880, smoothed loss 5.66946, grad norm 3.89012, param norm 58.17186
epoch 3, iter 2230, loss 5.59997, smoothed loss 5.67364, grad norm 3.66714, param norm 58.20615
epoch 3, iter 2235, loss 5.91767, smoothed loss 5.66944, grad norm 3.99600, param norm 58.23850
epoch 3, iter 2240, loss 6.08784, smoothed loss 5.67315, grad norm 3.68662, param norm 58.26680
epoch 3, iter 2245, loss 6.00256, smoothed loss 5.67781, grad norm 3.29205, param norm 58.29280
epoch 3, iter 2250, loss 5.93945, smoothed loss 5.67975, grad norm 3.69293, param norm 58.32226
epoch 3, iter 2255, loss 5.04651, smoothed loss 5.66663, grad norm 3.30136, param norm 58.35122
epoch 3, iter 2260, loss 5.26758, smoothed loss 5.65749, grad norm 3.36634, param norm 58.37942
epoch 3, iter 2265, loss 5.98671, smoothed loss 5.67103, grad norm 3.40942, param norm 58.40385
epoch 3, iter 2270, loss 6.02301, smoothed loss 5.67252, grad norm 3.39387, param norm 58.42670
epoch 3, iter 2275, loss 5.60616, smoothed loss 5.65907, grad norm 3.46409, param norm 58.45298
epoch 3, iter 2280, loss 5.51792, smoothed loss 5.65332, grad norm 3.41787, param norm 58.48454
epoch 3, iter 2285, loss 5.16312, smoothed loss 5.64448, grad norm 3.44357, param norm 58.51373
epoch 3, iter 2290, loss 6.33709, smoothed loss 5.64493, grad norm 4.66948, param norm 58.54402
epoch 3, iter 2295, loss 5.76791, smoothed loss 5.62926, grad norm 4.09781, param norm 58.57712
epoch 3, iter 2300, loss 5.86067, smoothed loss 5.63657, grad norm 4.10542, param norm 58.60301
epoch 3, iter 2305, loss 5.78690, smoothed loss 5.64339, grad norm 3.16042, param norm 58.62768
epoch 3, iter 2310, loss 5.67673, smoothed loss 5.64219, grad norm 3.68242, param norm 58.65206
epoch 3, iter 2315, loss 5.19609, smoothed loss 5.62996, grad norm 3.20797, param norm 58.67871
epoch 3, iter 2320, loss 6.10482, smoothed loss 5.64556, grad norm 3.58592, param norm 58.70483
epoch 3, iter 2325, loss 5.65448, smoothed loss 5.63521, grad norm 3.59198, param norm 58.72985
epoch 3, iter 2330, loss 5.67159, smoothed loss 5.63038, grad norm 3.40761, param norm 58.75747
epoch 3, iter 2335, loss 5.03217, smoothed loss 5.62583, grad norm 3.53981, param norm 58.78046
epoch 3, iter 2340, loss 5.76451, smoothed loss 5.61962, grad norm 3.25925, param norm 58.80402
epoch 3, iter 2345, loss 5.21257, smoothed loss 5.61601, grad norm 3.35001, param norm 58.82579
epoch 3, iter 2350, loss 5.30246, smoothed loss 5.61296, grad norm 3.56156, param norm 58.85034
epoch 3, iter 2355, loss 5.20291, smoothed loss 5.60572, grad norm 2.99210, param norm 58.87504
epoch 3, iter 2360, loss 4.84552, smoothed loss 5.59446, grad norm 3.33982, param norm 58.89928
epoch 3, iter 2365, loss 4.99430, smoothed loss 5.57405, grad norm 3.34466, param norm 58.92586
epoch 3, iter 2370, loss 5.44476, smoothed loss 5.58675, grad norm 3.34838, param norm 58.95653
epoch 3, iter 2375, loss 5.51168, smoothed loss 5.56313, grad norm 3.71491, param norm 58.98884
epoch 3, iter 2380, loss 5.24235, smoothed loss 5.55809, grad norm 3.69017, param norm 59.01715
epoch 3, iter 2385, loss 5.37194, smoothed loss 5.54759, grad norm 3.42009, param norm 59.04255
epoch 3, iter 2390, loss 5.68503, smoothed loss 5.54683, grad norm 3.59072, param norm 59.06991
epoch 3, iter 2395, loss 5.86537, smoothed loss 5.53647, grad norm 3.48824, param norm 59.09947
epoch 3, iter 2400, loss 4.74287, smoothed loss 5.52362, grad norm 3.76933, param norm 59.12780
epoch 3, iter 2405, loss 5.32203, smoothed loss 5.52694, grad norm 3.80083, param norm 59.15654
epoch 3, iter 2410, loss 5.17635, smoothed loss 5.51487, grad norm 3.97444, param norm 59.18387
epoch 3, iter 2415, loss 5.65289, smoothed loss 5.53263, grad norm 3.76938, param norm 59.20684
epoch 3, iter 2420, loss 5.34732, smoothed loss 5.53416, grad norm 3.67946, param norm 59.23081
epoch 3, iter 2425, loss 5.70235, smoothed loss 5.54409, grad norm 3.94042, param norm 59.25468
epoch 3, iter 2430, loss 5.55382, smoothed loss 5.54583, grad norm 2.95526, param norm 59.27550
epoch 3, iter 2435, loss 6.03841, smoothed loss 5.55595, grad norm 4.26645, param norm 59.30104
epoch 3, iter 2440, loss 5.45786, smoothed loss 5.55593, grad norm 3.33258, param norm 59.32936
epoch 3, iter 2445, loss 5.74851, smoothed loss 5.56616, grad norm 3.58472, param norm 59.35701
epoch 3, iter 2450, loss 5.03750, smoothed loss 5.55300, grad norm 3.65204, param norm 59.38542
epoch 3, iter 2455, loss 5.81268, smoothed loss 5.55693, grad norm 3.45054, param norm 59.41353
epoch 3, iter 2460, loss 5.78180, smoothed loss 5.55840, grad norm 3.41256, param norm 59.43795
epoch 3, iter 2465, loss 5.11037, smoothed loss 5.55181, grad norm 3.19485, param norm 59.46241
epoch 3, iter 2470, loss 5.55892, smoothed loss 5.54701, grad norm 3.50087, param norm 59.49156
epoch 3, iter 2475, loss 4.36636, smoothed loss 5.53122, grad norm 3.28110, param norm 59.52047
epoch 3, iter 2480, loss 4.72085, smoothed loss 5.52958, grad norm 3.27517, param norm 59.55086
epoch 3, iter 2485, loss 5.45551, smoothed loss 5.52904, grad norm 3.70279, param norm 59.57859
epoch 3, iter 2490, loss 5.62189, smoothed loss 5.52749, grad norm 3.94095, param norm 59.60717
epoch 3, iter 2495, loss 5.82571, smoothed loss 5.52336, grad norm 3.56640, param norm 59.63468
epoch 3, iter 2500, loss 5.19101, smoothed loss 5.51342, grad norm 3.73263, param norm 59.66257
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 2500, dev loss: 5.489510
Calculating Train F1/EM...
F1 train: 1000 examples took 8.90083 seconds [Score: 0.31621]
Exact Match train: 1000 examples took 9.01360 seconds [Score: 0.25800]
Epoch 3, Iter 2500, Train F1 score: 0.316210, Train EM score: 0.258000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.91062 seconds [Score: 0.28237]
Exact Match dev: 7118 examples took 57.70722 seconds [Score: 0.19458]
Epoch 3, Iter 2500, Dev F1 score: 0.282367, Dev EM score: 0.194577
End of epoch 3
epoch 3, iter 2505, loss 5.70389, smoothed loss 5.49914, grad norm 3.78900, param norm 59.69324
epoch 3, iter 2510, loss 4.67055, smoothed loss 5.48251, grad norm 3.53764, param norm 59.72298
epoch 3, iter 2515, loss 5.72781, smoothed loss 5.48422, grad norm 3.61090, param norm 59.75021
epoch 3, iter 2520, loss 4.74871, smoothed loss 5.47668, grad norm 3.41524, param norm 59.77905
epoch 3, iter 2525, loss 5.29546, smoothed loss 5.47029, grad norm 3.69576, param norm 59.80968
epoch 3, iter 2530, loss 5.29659, smoothed loss 5.46576, grad norm 4.03434, param norm 59.83320
epoch 3, iter 2535, loss 5.05909, smoothed loss 5.46772, grad norm 3.77468, param norm 59.85263
epoch 3, iter 2540, loss 5.69448, smoothed loss 5.47437, grad norm 3.94664, param norm 59.87348
epoch 3, iter 2545, loss 6.04455, smoothed loss 5.48303, grad norm 3.68779, param norm 59.89592
epoch 3, iter 2550, loss 5.67120, smoothed loss 5.47831, grad norm 3.21189, param norm 59.92467
epoch 3, iter 2555, loss 5.16991, smoothed loss 5.46998, grad norm 3.21656, param norm 59.95695
epoch 3, iter 2560, loss 5.32833, smoothed loss 5.46753, grad norm 3.59225, param norm 59.98620
epoch 3, iter 2565, loss 5.59647, smoothed loss 5.47425, grad norm 3.67710, param norm 60.01141
epoch 3, iter 2570, loss 5.61312, smoothed loss 5.48121, grad norm 3.26794, param norm 60.02973
epoch 3, iter 2575, loss 5.45561, smoothed loss 5.47890, grad norm 3.17999, param norm 60.05016
epoch 3, iter 2580, loss 5.40507, smoothed loss 5.46036, grad norm 3.90513, param norm 60.07798
epoch 3, iter 2585, loss 5.75185, smoothed loss 5.46239, grad norm 3.69888, param norm 60.10500
epoch 3, iter 2590, loss 5.77034, smoothed loss 5.46246, grad norm 3.93449, param norm 60.13197
epoch 3, iter 2595, loss 5.30613, smoothed loss 5.45178, grad norm 3.49512, param norm 60.15540
epoch 3, iter 2600, loss 5.48605, smoothed loss 5.46500, grad norm 3.29874, param norm 60.17790
epoch 3, iter 2605, loss 4.91964, smoothed loss 5.46344, grad norm 3.69636, param norm 60.20032
epoch 3, iter 2610, loss 5.91307, smoothed loss 5.46914, grad norm 3.41988, param norm 60.22840
epoch 3, iter 2615, loss 5.32697, smoothed loss 5.46221, grad norm 3.20173, param norm 60.25853
epoch 3, iter 2620, loss 5.54495, smoothed loss 5.46595, grad norm 4.86313, param norm 60.28502
epoch 3, iter 2625, loss 5.75817, smoothed loss 5.47625, grad norm 3.68391, param norm 60.30880
epoch 3, iter 2630, loss 5.68029, smoothed loss 5.47807, grad norm 3.74822, param norm 60.32940
epoch 3, iter 2635, loss 4.96664, smoothed loss 5.48520, grad norm 3.62120, param norm 60.35432
epoch 3, iter 2640, loss 5.50286, smoothed loss 5.49528, grad norm 3.07476, param norm 60.38094
epoch 3, iter 2645, loss 5.59918, smoothed loss 5.50146, grad norm 3.65692, param norm 60.41063
epoch 3, iter 2650, loss 5.44628, smoothed loss 5.51261, grad norm 3.19667, param norm 60.43994
epoch 3, iter 2655, loss 5.52632, smoothed loss 5.50485, grad norm 3.64058, param norm 60.46622
epoch 3, iter 2660, loss 4.68474, smoothed loss 5.49219, grad norm 3.27873, param norm 60.49037
epoch 3, iter 2665, loss 4.39758, smoothed loss 5.48428, grad norm 3.14216, param norm 60.51141
epoch 3, iter 2670, loss 4.82425, smoothed loss 5.47939, grad norm 3.56159, param norm 60.53188
epoch 3, iter 2675, loss 5.51065, smoothed loss 5.48528, grad norm 3.90648, param norm 60.55529
epoch 3, iter 2680, loss 5.83173, smoothed loss 5.50125, grad norm 3.58013, param norm 60.58021
epoch 3, iter 2685, loss 6.32480, smoothed loss 5.49362, grad norm 3.07181, param norm 60.60485
epoch 3, iter 2690, loss 5.54947, smoothed loss 5.49781, grad norm 3.71452, param norm 60.63142
epoch 3, iter 2695, loss 5.53633, smoothed loss 5.49717, grad norm 3.32841, param norm 60.66146
epoch 3, iter 2700, loss 5.49917, smoothed loss 5.49904, grad norm 3.72503, param norm 60.69016
epoch 3, iter 2705, loss 5.23239, smoothed loss 5.49848, grad norm 3.62453, param norm 60.71114
epoch 3, iter 2710, loss 5.22386, smoothed loss 5.49447, grad norm 3.86327, param norm 60.73253
epoch 3, iter 2715, loss 5.91611, smoothed loss 5.48788, grad norm 4.01130, param norm 60.75762
epoch 3, iter 2720, loss 4.99156, smoothed loss 5.48810, grad norm 3.55167, param norm 60.78051
epoch 3, iter 2725, loss 5.59862, smoothed loss 5.49286, grad norm 3.29230, param norm 60.81068
epoch 3, iter 2730, loss 5.05659, smoothed loss 5.48418, grad norm 3.59746, param norm 60.84119
epoch 3, iter 2735, loss 5.19753, smoothed loss 5.48619, grad norm 3.64123, param norm 60.86702
epoch 3, iter 2740, loss 5.44972, smoothed loss 5.48164, grad norm 3.77545, param norm 60.89149
epoch 3, iter 2745, loss 5.79876, smoothed loss 5.48723, grad norm 4.22268, param norm 60.91454
epoch 3, iter 2750, loss 5.68704, smoothed loss 5.48240, grad norm 4.82113, param norm 60.93871
epoch 3, iter 2755, loss 5.65283, smoothed loss 5.47784, grad norm 3.54442, param norm 60.96778
epoch 3, iter 2760, loss 4.40183, smoothed loss 5.46902, grad norm 3.69968, param norm 60.99448
epoch 3, iter 2765, loss 5.97877, smoothed loss 5.48280, grad norm 3.27515, param norm 61.02478
epoch 3, iter 2770, loss 5.73638, smoothed loss 5.47693, grad norm 3.36460, param norm 61.05535
epoch 3, iter 2775, loss 5.26882, smoothed loss 5.46690, grad norm 3.48728, param norm 61.08582
epoch 3, iter 2780, loss 5.44065, smoothed loss 5.46702, grad norm 4.07304, param norm 61.11692
epoch 3, iter 2785, loss 4.99132, smoothed loss 5.46351, grad norm 4.02876, param norm 61.14025
epoch 3, iter 2790, loss 5.63606, smoothed loss 5.48124, grad norm 3.57084, param norm 61.16126
epoch 3, iter 2795, loss 5.83155, smoothed loss 5.49261, grad norm 3.98582, param norm 61.18538
epoch 3, iter 2800, loss 5.43951, smoothed loss 5.48589, grad norm 3.61504, param norm 61.21443
epoch 3, iter 2805, loss 4.53727, smoothed loss 5.47245, grad norm 3.17323, param norm 61.24534
epoch 3, iter 2810, loss 5.06303, smoothed loss 5.47353, grad norm 3.50990, param norm 61.27524
epoch 3, iter 2815, loss 5.65155, smoothed loss 5.48663, grad norm 3.83219, param norm 61.30331
epoch 3, iter 2820, loss 4.77934, smoothed loss 5.48173, grad norm 3.36137, param norm 61.33321
epoch 3, iter 2825, loss 5.61224, smoothed loss 5.47988, grad norm 3.54502, param norm 61.36141
epoch 3, iter 2830, loss 4.76627, smoothed loss 5.46235, grad norm 3.51261, param norm 61.38736
epoch 4, iter 2835, loss 5.50809, smoothed loss 5.45903, grad norm 3.73483, param norm 61.40940
epoch 4, iter 2840, loss 6.15046, smoothed loss 5.45878, grad norm 3.94298, param norm 61.43212
epoch 4, iter 2845, loss 4.90254, smoothed loss 5.46478, grad norm 3.81946, param norm 61.45255
epoch 4, iter 2850, loss 5.05116, smoothed loss 5.45577, grad norm 3.91916, param norm 61.47588
epoch 4, iter 2855, loss 5.56107, smoothed loss 5.46910, grad norm 4.56111, param norm 61.50085
epoch 4, iter 2860, loss 4.97896, smoothed loss 5.45213, grad norm 3.66064, param norm 61.52377
epoch 4, iter 2865, loss 5.33072, smoothed loss 5.44276, grad norm 4.37515, param norm 61.54895
epoch 4, iter 2870, loss 5.60321, smoothed loss 5.43609, grad norm 3.52906, param norm 61.57242
epoch 4, iter 2875, loss 5.88488, smoothed loss 5.44492, grad norm 3.97452, param norm 61.59407
epoch 4, iter 2880, loss 5.10147, smoothed loss 5.43739, grad norm 3.77108, param norm 61.61678
epoch 4, iter 2885, loss 5.09412, smoothed loss 5.42895, grad norm 3.30663, param norm 61.64164
epoch 4, iter 2890, loss 6.45329, smoothed loss 5.43603, grad norm 4.29814, param norm 61.66879
epoch 4, iter 2895, loss 5.13833, smoothed loss 5.43167, grad norm 3.28674, param norm 61.69407
epoch 4, iter 2900, loss 5.15872, smoothed loss 5.41917, grad norm 3.76633, param norm 61.72002
epoch 4, iter 2905, loss 5.34172, smoothed loss 5.43483, grad norm 3.61608, param norm 61.74402
epoch 4, iter 2910, loss 4.87115, smoothed loss 5.42527, grad norm 4.00831, param norm 61.76768
epoch 4, iter 2915, loss 5.66604, smoothed loss 5.42980, grad norm 3.75890, param norm 61.79564
epoch 4, iter 2920, loss 5.33911, smoothed loss 5.43281, grad norm 4.28744, param norm 61.82125
epoch 4, iter 2925, loss 5.17264, smoothed loss 5.42207, grad norm 3.41159, param norm 61.84740
epoch 4, iter 2930, loss 6.04301, smoothed loss 5.42615, grad norm 4.41253, param norm 61.87501
epoch 4, iter 2935, loss 5.15280, smoothed loss 5.43038, grad norm 3.64130, param norm 61.89883
epoch 4, iter 2940, loss 5.05815, smoothed loss 5.42204, grad norm 3.97724, param norm 61.92497
epoch 4, iter 2945, loss 6.06932, smoothed loss 5.44237, grad norm 4.01824, param norm 61.95135
epoch 4, iter 2950, loss 5.22290, smoothed loss 5.43277, grad norm 3.99265, param norm 61.97523
epoch 4, iter 2955, loss 5.21051, smoothed loss 5.43908, grad norm 3.91468, param norm 61.99961
epoch 4, iter 2960, loss 5.82767, smoothed loss 5.43157, grad norm 4.54794, param norm 62.02465
epoch 4, iter 2965, loss 5.29901, smoothed loss 5.43173, grad norm 3.72446, param norm 62.05021
epoch 4, iter 2970, loss 5.16260, smoothed loss 5.42281, grad norm 4.02454, param norm 62.07420
epoch 4, iter 2975, loss 5.45864, smoothed loss 5.42422, grad norm 3.76148, param norm 62.09936
epoch 4, iter 2980, loss 5.47470, smoothed loss 5.42113, grad norm 4.00481, param norm 62.12707
epoch 4, iter 2985, loss 5.55969, smoothed loss 5.42034, grad norm 3.89422, param norm 62.15322
epoch 4, iter 2990, loss 5.21616, smoothed loss 5.40469, grad norm 3.71228, param norm 62.18111
epoch 4, iter 2995, loss 5.17851, smoothed loss 5.39827, grad norm 3.33058, param norm 62.20954
epoch 4, iter 3000, loss 6.54697, smoothed loss 5.41837, grad norm 4.41106, param norm 62.23413
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 4, Iter 3000, dev loss: 5.382887
Calculating Train F1/EM...
F1 train: 1000 examples took 8.94720 seconds [Score: 0.34941]
Exact Match train: 1000 examples took 8.80193 seconds [Score: 0.26500]
Epoch 4, Iter 3000, Train F1 score: 0.349411, Train EM score: 0.265000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.11571 seconds [Score: 0.29835]
Exact Match dev: 7118 examples took 58.04185 seconds [Score: 0.20539]
Epoch 4, Iter 3000, Dev F1 score: 0.298353, Dev EM score: 0.205395
End of epoch 4
epoch 4, iter 3005, loss 5.72584, smoothed loss 5.42869, grad norm 3.49274, param norm 62.25570
epoch 4, iter 3010, loss 4.73233, smoothed loss 5.42483, grad norm 3.47025, param norm 62.27948
epoch 4, iter 3015, loss 5.90579, smoothed loss 5.42655, grad norm 4.44877, param norm 62.30904
epoch 4, iter 3020, loss 4.59487, smoothed loss 5.41632, grad norm 3.33550, param norm 62.33942
epoch 4, iter 3025, loss 5.95193, smoothed loss 5.41298, grad norm 3.84479, param norm 62.36529
epoch 4, iter 3030, loss 5.37569, smoothed loss 5.40774, grad norm 4.56299, param norm 62.38945
epoch 4, iter 3035, loss 5.38562, smoothed loss 5.41268, grad norm 3.73366, param norm 62.41072
epoch 4, iter 3040, loss 5.00296, smoothed loss 5.41005, grad norm 3.79162, param norm 62.42999
epoch 4, iter 3045, loss 5.40542, smoothed loss 5.39524, grad norm 3.77541, param norm 62.45327
epoch 4, iter 3050, loss 5.84136, smoothed loss 5.41110, grad norm 3.83262, param norm 62.47751
epoch 4, iter 3055, loss 5.24806, smoothed loss 5.42246, grad norm 3.41208, param norm 62.49768
epoch 4, iter 3060, loss 4.96186, smoothed loss 5.41834, grad norm 3.50894, param norm 62.52160
epoch 4, iter 3065, loss 5.01731, smoothed loss 5.39708, grad norm 3.65510, param norm 62.55111
epoch 4, iter 3070, loss 5.08121, smoothed loss 5.40527, grad norm 3.77718, param norm 62.58113
epoch 4, iter 3075, loss 5.20548, smoothed loss 5.40394, grad norm 3.46478, param norm 62.60819
epoch 4, iter 3080, loss 6.07263, smoothed loss 5.40188, grad norm 3.83264, param norm 62.63138
epoch 4, iter 3085, loss 5.53760, smoothed loss 5.40861, grad norm 3.72809, param norm 62.65265
epoch 4, iter 3090, loss 5.01686, smoothed loss 5.40016, grad norm 3.30443, param norm 62.67222
epoch 4, iter 3095, loss 5.31489, smoothed loss 5.39727, grad norm 4.48698, param norm 62.69464
epoch 4, iter 3100, loss 5.17564, smoothed loss 5.40106, grad norm 3.84315, param norm 62.71345
epoch 4, iter 3105, loss 5.70448, smoothed loss 5.40479, grad norm 4.08408, param norm 62.73878
epoch 4, iter 3110, loss 4.91676, smoothed loss 5.41182, grad norm 3.56683, param norm 62.76394
epoch 4, iter 3115, loss 5.16952, smoothed loss 5.40546, grad norm 3.23925, param norm 62.79067
epoch 4, iter 3120, loss 5.35478, smoothed loss 5.40246, grad norm 4.33842, param norm 62.81660
epoch 4, iter 3125, loss 5.75526, smoothed loss 5.41363, grad norm 3.75930, param norm 62.83883
epoch 4, iter 3130, loss 5.46505, smoothed loss 5.41596, grad norm 4.46340, param norm 62.86237
epoch 4, iter 3135, loss 5.58185, smoothed loss 5.42777, grad norm 3.84691, param norm 62.88560
epoch 4, iter 3140, loss 5.26195, smoothed loss 5.42708, grad norm 4.10082, param norm 62.90950
epoch 4, iter 3145, loss 5.59027, smoothed loss 5.42326, grad norm 3.13767, param norm 62.93478
epoch 4, iter 3150, loss 5.46547, smoothed loss 5.42816, grad norm 4.11106, param norm 62.95721
epoch 4, iter 3155, loss 4.78521, smoothed loss 5.42014, grad norm 3.27057, param norm 62.97696
epoch 4, iter 3160, loss 5.44762, smoothed loss 5.41128, grad norm 3.44728, param norm 63.00075
epoch 4, iter 3165, loss 4.71096, smoothed loss 5.41035, grad norm 3.56162, param norm 63.02465
epoch 4, iter 3170, loss 4.70628, smoothed loss 5.39043, grad norm 3.38798, param norm 63.05610
epoch 4, iter 3175, loss 5.11256, smoothed loss 5.37907, grad norm 4.07890, param norm 63.09151
epoch 4, iter 3180, loss 4.70627, smoothed loss 5.37720, grad norm 3.33309, param norm 63.11950
epoch 4, iter 3185, loss 4.64630, smoothed loss 5.38663, grad norm 3.60459, param norm 63.14120
epoch 4, iter 3190, loss 5.02515, smoothed loss 5.37222, grad norm 3.88266, param norm 63.16345
epoch 4, iter 3195, loss 5.23942, smoothed loss 5.36892, grad norm 3.33322, param norm 63.18970
epoch 4, iter 3200, loss 5.58948, smoothed loss 5.35979, grad norm 3.76388, param norm 63.21759
epoch 4, iter 3205, loss 5.76732, smoothed loss 5.35081, grad norm 3.52297, param norm 63.24355
epoch 4, iter 3210, loss 6.01849, smoothed loss 5.35709, grad norm 4.10408, param norm 63.26739
epoch 4, iter 3215, loss 5.86074, smoothed loss 5.36519, grad norm 4.33131, param norm 63.29054
epoch 4, iter 3220, loss 5.85002, smoothed loss 5.37710, grad norm 3.78281, param norm 63.31411
epoch 4, iter 3225, loss 4.99469, smoothed loss 5.37614, grad norm 3.30953, param norm 63.33168
epoch 4, iter 3230, loss 5.21926, smoothed loss 5.37021, grad norm 3.65674, param norm 63.35415
epoch 4, iter 3235, loss 5.19703, smoothed loss 5.36887, grad norm 3.95854, param norm 63.37873
epoch 4, iter 3240, loss 5.18727, smoothed loss 5.35803, grad norm 3.26121, param norm 63.40561
epoch 4, iter 3245, loss 5.29718, smoothed loss 5.34785, grad norm 3.27554, param norm 63.43266
epoch 4, iter 3250, loss 5.11107, smoothed loss 5.34670, grad norm 3.52865, param norm 63.45861
epoch 4, iter 3255, loss 4.95031, smoothed loss 5.34090, grad norm 3.55050, param norm 63.48283
epoch 4, iter 3260, loss 5.30865, smoothed loss 5.32968, grad norm 3.78825, param norm 63.50601
epoch 4, iter 3265, loss 5.33731, smoothed loss 5.33438, grad norm 4.23474, param norm 63.52677
epoch 4, iter 3270, loss 5.83276, smoothed loss 5.34812, grad norm 3.90111, param norm 63.54704
epoch 4, iter 3275, loss 5.33734, smoothed loss 5.34564, grad norm 4.02657, param norm 63.56592
epoch 4, iter 3280, loss 5.59749, smoothed loss 5.35211, grad norm 3.96324, param norm 63.58600
epoch 4, iter 3285, loss 5.20467, smoothed loss 5.35195, grad norm 3.48881, param norm 63.61123
epoch 4, iter 3290, loss 5.28332, smoothed loss 5.35802, grad norm 3.97785, param norm 63.63773
epoch 4, iter 3295, loss 4.86619, smoothed loss 5.35485, grad norm 4.07423, param norm 63.66568
epoch 4, iter 3300, loss 5.77364, smoothed loss 5.33619, grad norm 3.99551, param norm 63.69920
epoch 4, iter 3305, loss 5.78054, smoothed loss 5.34015, grad norm 3.96902, param norm 63.72831
epoch 4, iter 3310, loss 4.82489, smoothed loss 5.34500, grad norm 3.74662, param norm 63.75317
epoch 4, iter 3315, loss 5.18134, smoothed loss 5.35477, grad norm 3.73787, param norm 63.77734
epoch 4, iter 3320, loss 4.86846, smoothed loss 5.34365, grad norm 3.35363, param norm 63.80095
epoch 4, iter 3325, loss 5.60299, smoothed loss 5.33608, grad norm 3.85858, param norm 63.82418
epoch 4, iter 3330, loss 5.22232, smoothed loss 5.32983, grad norm 3.42457, param norm 63.84692
epoch 4, iter 3335, loss 5.26413, smoothed loss 5.32497, grad norm 3.31987, param norm 63.86839
epoch 4, iter 3340, loss 5.27407, smoothed loss 5.32793, grad norm 3.93411, param norm 63.89077
epoch 4, iter 3345, loss 5.10762, smoothed loss 5.32420, grad norm 4.14266, param norm 63.91158
epoch 4, iter 3350, loss 5.28666, smoothed loss 5.33148, grad norm 3.71747, param norm 63.92876
epoch 4, iter 3355, loss 4.84388, smoothed loss 5.34190, grad norm 3.48323, param norm 63.94361
epoch 4, iter 3360, loss 5.29514, smoothed loss 5.34390, grad norm 3.66448, param norm 63.96252
epoch 4, iter 3365, loss 5.53690, smoothed loss 5.34312, grad norm 4.11245, param norm 63.98346
epoch 4, iter 3370, loss 4.96468, smoothed loss 5.33427, grad norm 3.63152, param norm 64.00423
epoch 4, iter 3375, loss 5.05619, smoothed loss 5.33257, grad norm 3.89826, param norm 64.02485
epoch 4, iter 3380, loss 5.75013, smoothed loss 5.34150, grad norm 3.50256, param norm 64.04421
epoch 4, iter 3385, loss 4.59188, smoothed loss 5.31523, grad norm 3.86017, param norm 64.06323
epoch 4, iter 3390, loss 5.21838, smoothed loss 5.31428, grad norm 4.75070, param norm 64.08689
epoch 4, iter 3395, loss 4.71631, smoothed loss 5.30895, grad norm 3.94345, param norm 64.10748
epoch 4, iter 3400, loss 5.15934, smoothed loss 5.29414, grad norm 4.27987, param norm 64.13277
epoch 4, iter 3405, loss 4.62942, smoothed loss 5.27882, grad norm 3.87525, param norm 64.15526
epoch 4, iter 3410, loss 5.25752, smoothed loss 5.27914, grad norm 4.12876, param norm 64.17681
epoch 4, iter 3415, loss 5.09422, smoothed loss 5.28071, grad norm 3.76579, param norm 64.19798
epoch 4, iter 3420, loss 5.44579, smoothed loss 5.27428, grad norm 4.21318, param norm 64.22439
epoch 4, iter 3425, loss 5.08645, smoothed loss 5.26937, grad norm 3.75939, param norm 64.24899
epoch 4, iter 3430, loss 5.69458, smoothed loss 5.27704, grad norm 4.03481, param norm 64.27156
epoch 4, iter 3435, loss 4.69336, smoothed loss 5.24777, grad norm 3.79179, param norm 64.29827
epoch 4, iter 3440, loss 5.15319, smoothed loss 5.24320, grad norm 3.44507, param norm 64.32513
epoch 4, iter 3445, loss 4.99700, smoothed loss 5.24423, grad norm 3.78451, param norm 64.34756
epoch 4, iter 3450, loss 4.94629, smoothed loss 5.24303, grad norm 3.64924, param norm 64.37155
epoch 4, iter 3455, loss 5.53219, smoothed loss 5.23752, grad norm 3.91571, param norm 64.39552
epoch 4, iter 3460, loss 5.07206, smoothed loss 5.23254, grad norm 4.14920, param norm 64.41975
epoch 4, iter 3465, loss 4.24441, smoothed loss 5.21290, grad norm 3.68691, param norm 64.44630
epoch 4, iter 3470, loss 4.36993, smoothed loss 5.22109, grad norm 3.52501, param norm 64.46854
epoch 4, iter 3475, loss 5.06959, smoothed loss 5.22748, grad norm 3.97667, param norm 64.48523
epoch 4, iter 3480, loss 5.20943, smoothed loss 5.23757, grad norm 4.15391, param norm 64.49929
epoch 4, iter 3485, loss 5.14421, smoothed loss 5.24456, grad norm 3.69055, param norm 64.51562
epoch 4, iter 3490, loss 5.36445, smoothed loss 5.26126, grad norm 3.72071, param norm 64.53297
epoch 4, iter 3495, loss 5.26775, smoothed loss 5.27010, grad norm 3.75673, param norm 64.55732
epoch 4, iter 3500, loss 4.66168, smoothed loss 5.25382, grad norm 3.39431, param norm 64.58378
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 4, Iter 3500, dev loss: 5.315671
Calculating Train F1/EM...
F1 train: 1000 examples took 9.25833 seconds [Score: 0.37016]
Exact Match train: 1000 examples took 8.84674 seconds [Score: 0.27900]
Epoch 4, Iter 3500, Train F1 score: 0.370160, Train EM score: 0.279000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.34096 seconds [Score: 0.30263]
Exact Match dev: 7118 examples took 58.02308 seconds [Score: 0.20820]
Epoch 4, Iter 3500, Dev F1 score: 0.302630, Dev EM score: 0.208205
End of epoch 4
epoch 4, iter 3505, loss 4.61512, smoothed loss 5.24635, grad norm 4.22872, param norm 64.61131
epoch 4, iter 3510, loss 5.14455, smoothed loss 5.24618, grad norm 4.30507, param norm 64.63388
epoch 4, iter 3515, loss 5.16051, smoothed loss 5.25532, grad norm 3.87231, param norm 64.65401
epoch 4, iter 3520, loss 5.69199, smoothed loss 5.26168, grad norm 4.31029, param norm 64.67661
epoch 4, iter 3525, loss 5.34790, smoothed loss 5.26922, grad norm 3.97135, param norm 64.69951
epoch 4, iter 3530, loss 5.20620, smoothed loss 5.25567, grad norm 3.39082, param norm 64.72381
epoch 4, iter 3535, loss 4.92877, smoothed loss 5.25342, grad norm 4.18658, param norm 64.74773
epoch 4, iter 3540, loss 5.50629, smoothed loss 5.23508, grad norm 3.71213, param norm 64.77046
epoch 4, iter 3545, loss 5.39321, smoothed loss 5.23703, grad norm 3.62540, param norm 64.79343
epoch 4, iter 3550, loss 5.31561, smoothed loss 5.23471, grad norm 4.10144, param norm 64.81372
epoch 4, iter 3555, loss 5.46837, smoothed loss 5.24037, grad norm 3.65002, param norm 64.83228
epoch 4, iter 3560, loss 4.88270, smoothed loss 5.24772, grad norm 3.52453, param norm 64.85130
epoch 4, iter 3565, loss 5.32860, smoothed loss 5.23933, grad norm 3.60104, param norm 64.87772
epoch 4, iter 3570, loss 5.78972, smoothed loss 5.24395, grad norm 3.90574, param norm 64.90629
epoch 4, iter 3575, loss 5.28916, smoothed loss 5.23871, grad norm 3.55746, param norm 64.93422
epoch 4, iter 3580, loss 5.72266, smoothed loss 5.24277, grad norm 4.66405, param norm 64.96190
epoch 4, iter 3585, loss 5.20666, smoothed loss 5.24571, grad norm 4.55809, param norm 64.98700
epoch 4, iter 3590, loss 5.36756, smoothed loss 5.26264, grad norm 3.95524, param norm 65.00452
epoch 4, iter 3595, loss 5.59851, smoothed loss 5.26160, grad norm 3.57864, param norm 65.02599
epoch 4, iter 3600, loss 4.68426, smoothed loss 5.24739, grad norm 3.42534, param norm 65.05077
epoch 4, iter 3605, loss 5.71812, smoothed loss 5.25851, grad norm 3.95516, param norm 65.07288
epoch 4, iter 3610, loss 5.24661, smoothed loss 5.25893, grad norm 3.47943, param norm 65.09055
epoch 4, iter 3615, loss 5.30037, smoothed loss 5.25194, grad norm 4.52417, param norm 65.11056
epoch 4, iter 3620, loss 5.30828, smoothed loss 5.24715, grad norm 4.05574, param norm 65.13287
epoch 4, iter 3625, loss 5.10098, smoothed loss 5.25452, grad norm 3.56144, param norm 65.15256
epoch 4, iter 3630, loss 4.88473, smoothed loss 5.25643, grad norm 3.40218, param norm 65.17061
epoch 4, iter 3635, loss 4.85449, smoothed loss 5.25180, grad norm 3.50989, param norm 65.19146
epoch 4, iter 3640, loss 4.98139, smoothed loss 5.23642, grad norm 3.58137, param norm 65.21639
epoch 4, iter 3645, loss 5.21959, smoothed loss 5.25245, grad norm 4.26849, param norm 65.24336
epoch 4, iter 3650, loss 5.23292, smoothed loss 5.25737, grad norm 3.83361, param norm 65.26722
epoch 4, iter 3655, loss 5.32868, smoothed loss 5.25143, grad norm 3.84390, param norm 65.29246
epoch 4, iter 3660, loss 5.09216, smoothed loss 5.25504, grad norm 3.50495, param norm 65.31551
epoch 4, iter 3665, loss 5.34440, smoothed loss 5.26721, grad norm 4.23970, param norm 65.33807
epoch 4, iter 3670, loss 5.88865, smoothed loss 5.26575, grad norm 4.60797, param norm 65.36098
epoch 4, iter 3675, loss 5.20346, smoothed loss 5.25098, grad norm 3.63031, param norm 65.38235
epoch 4, iter 3680, loss 5.46652, smoothed loss 5.25921, grad norm 3.47413, param norm 65.40432
epoch 4, iter 3685, loss 4.89433, smoothed loss 5.25156, grad norm 3.99330, param norm 65.42550
epoch 4, iter 3690, loss 5.23119, smoothed loss 5.26156, grad norm 4.05000, param norm 65.44613
epoch 4, iter 3695, loss 4.82206, smoothed loss 5.26710, grad norm 3.53111, param norm 65.46632
epoch 4, iter 3700, loss 4.65565, smoothed loss 5.26099, grad norm 3.52172, param norm 65.49178
epoch 4, iter 3705, loss 5.33556, smoothed loss 5.26014, grad norm 4.11344, param norm 65.51987
epoch 4, iter 3710, loss 5.10904, smoothed loss 5.25810, grad norm 4.59905, param norm 65.54345
epoch 4, iter 3715, loss 5.80075, smoothed loss 5.25747, grad norm 4.45009, param norm 65.56738
epoch 4, iter 3720, loss 5.01724, smoothed loss 5.25047, grad norm 3.85417, param norm 65.59008
epoch 4, iter 3725, loss 5.04515, smoothed loss 5.23844, grad norm 4.12122, param norm 65.61685
epoch 4, iter 3730, loss 5.59340, smoothed loss 5.24595, grad norm 3.87832, param norm 65.64434
epoch 4, iter 3735, loss 5.53802, smoothed loss 5.24373, grad norm 4.34504, param norm 65.66807
epoch 4, iter 3740, loss 5.48565, smoothed loss 5.25153, grad norm 4.29360, param norm 65.69090
epoch 4, iter 3745, loss 5.73239, smoothed loss 5.24324, grad norm 3.96454, param norm 65.71709
epoch 4, iter 3750, loss 5.43764, smoothed loss 5.23069, grad norm 4.04448, param norm 65.73717
epoch 4, iter 3755, loss 5.68061, smoothed loss 5.23432, grad norm 4.47784, param norm 65.75336
epoch 4, iter 3760, loss 5.64439, smoothed loss 5.24041, grad norm 4.62019, param norm 65.76969
epoch 4, iter 3765, loss 4.46309, smoothed loss 5.24452, grad norm 3.39607, param norm 65.79026
epoch 4, iter 3770, loss 4.80397, smoothed loss 5.24390, grad norm 3.55540, param norm 65.81231
epoch 4, iter 3775, loss 5.26667, smoothed loss 5.24995, grad norm 4.08410, param norm 65.83190
epoch 5, iter 3780, loss 4.56002, smoothed loss 5.23976, grad norm 3.69475, param norm 65.85094
epoch 5, iter 3785, loss 5.06402, smoothed loss 5.24843, grad norm 3.21851, param norm 65.87241
epoch 5, iter 3790, loss 4.97966, smoothed loss 5.24225, grad norm 4.08827, param norm 65.89635
epoch 5, iter 3795, loss 5.17981, smoothed loss 5.24293, grad norm 3.40689, param norm 65.92220
epoch 5, iter 3800, loss 4.41986, smoothed loss 5.24934, grad norm 3.86672, param norm 65.94560
epoch 5, iter 3805, loss 4.94924, smoothed loss 5.25229, grad norm 3.77172, param norm 65.96737
epoch 5, iter 3810, loss 5.06841, smoothed loss 5.24932, grad norm 3.78865, param norm 65.98623
epoch 5, iter 3815, loss 5.69124, smoothed loss 5.24165, grad norm 4.00030, param norm 66.00912
epoch 5, iter 3820, loss 5.49810, smoothed loss 5.23626, grad norm 4.30759, param norm 66.03008
epoch 5, iter 3825, loss 4.75885, smoothed loss 5.21779, grad norm 3.97501, param norm 66.05236
epoch 5, iter 3830, loss 5.11922, smoothed loss 5.21942, grad norm 4.06136, param norm 66.07520
epoch 5, iter 3835, loss 5.50058, smoothed loss 5.22429, grad norm 3.97751, param norm 66.09419
epoch 5, iter 3840, loss 4.80641, smoothed loss 5.22566, grad norm 4.32619, param norm 66.10850
epoch 5, iter 3845, loss 5.21884, smoothed loss 5.21777, grad norm 4.02228, param norm 66.12933
epoch 5, iter 3850, loss 4.53758, smoothed loss 5.20755, grad norm 3.89626, param norm 66.15200
epoch 5, iter 3855, loss 5.09701, smoothed loss 5.20297, grad norm 3.36696, param norm 66.17559
epoch 5, iter 3860, loss 4.82462, smoothed loss 5.19438, grad norm 3.53667, param norm 66.19630
epoch 5, iter 3865, loss 5.45069, smoothed loss 5.18086, grad norm 3.95880, param norm 66.21855
epoch 5, iter 3870, loss 5.12010, smoothed loss 5.17833, grad norm 3.57083, param norm 66.23979
epoch 5, iter 3875, loss 5.71901, smoothed loss 5.19596, grad norm 3.48224, param norm 66.26020
epoch 5, iter 3880, loss 5.07562, smoothed loss 5.20120, grad norm 3.48884, param norm 66.27884
epoch 5, iter 3885, loss 4.73827, smoothed loss 5.20149, grad norm 3.76775, param norm 66.30124
epoch 5, iter 3890, loss 4.88430, smoothed loss 5.18957, grad norm 3.62102, param norm 66.32806
epoch 5, iter 3895, loss 5.18341, smoothed loss 5.18641, grad norm 3.71928, param norm 66.35626
epoch 5, iter 3900, loss 5.85829, smoothed loss 5.20518, grad norm 3.87885, param norm 66.37856
epoch 5, iter 3905, loss 4.83501, smoothed loss 5.20162, grad norm 3.79550, param norm 66.40086
epoch 5, iter 3910, loss 5.40481, smoothed loss 5.20085, grad norm 4.40378, param norm 66.42332
epoch 5, iter 3915, loss 4.95040, smoothed loss 5.21225, grad norm 3.82488, param norm 66.44330
epoch 5, iter 3920, loss 5.38511, smoothed loss 5.20870, grad norm 3.87408, param norm 66.46398
epoch 5, iter 3925, loss 4.93864, smoothed loss 5.20574, grad norm 4.09839, param norm 66.48305
epoch 5, iter 3930, loss 5.15177, smoothed loss 5.20763, grad norm 4.42542, param norm 66.50134
epoch 5, iter 3935, loss 5.18129, smoothed loss 5.20808, grad norm 3.73568, param norm 66.52225
epoch 5, iter 3940, loss 5.37021, smoothed loss 5.21536, grad norm 3.71521, param norm 66.54884
epoch 5, iter 3945, loss 4.92387, smoothed loss 5.20699, grad norm 3.71552, param norm 66.57433
epoch 5, iter 3950, loss 5.55052, smoothed loss 5.20268, grad norm 4.39301, param norm 66.59935
epoch 5, iter 3955, loss 4.61003, smoothed loss 5.18867, grad norm 3.55526, param norm 66.62140
epoch 5, iter 3960, loss 5.96247, smoothed loss 5.20303, grad norm 3.60634, param norm 66.64235
epoch 5, iter 3965, loss 5.78135, smoothed loss 5.20742, grad norm 3.82369, param norm 66.66257
epoch 5, iter 3970, loss 5.26299, smoothed loss 5.20732, grad norm 3.90712, param norm 66.68307
epoch 5, iter 3975, loss 4.74929, smoothed loss 5.19737, grad norm 3.88819, param norm 66.70168
epoch 5, iter 3980, loss 5.84756, smoothed loss 5.22022, grad norm 4.25092, param norm 66.71841
epoch 5, iter 3985, loss 5.23661, smoothed loss 5.23117, grad norm 3.95980, param norm 66.73473
epoch 5, iter 3990, loss 5.88048, smoothed loss 5.23214, grad norm 3.89647, param norm 66.75705
epoch 5, iter 3995, loss 5.15346, smoothed loss 5.22931, grad norm 4.02363, param norm 66.78195
epoch 5, iter 4000, loss 5.17224, smoothed loss 5.21624, grad norm 3.97426, param norm 66.81027
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 4000, dev loss: 5.234631
Calculating Train F1/EM...
F1 train: 1000 examples took 9.00716 seconds [Score: 0.40469]
Exact Match train: 1000 examples took 8.83601 seconds [Score: 0.29000]
Epoch 5, Iter 4000, Train F1 score: 0.404688, Train EM score: 0.290000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.22902 seconds [Score: 0.31294]
Exact Match dev: 7118 examples took 58.33860 seconds [Score: 0.21776]
Epoch 5, Iter 4000, Dev F1 score: 0.312937, Dev EM score: 0.217758
End of epoch 5
epoch 5, iter 4005, loss 5.36213, smoothed loss 5.21731, grad norm 4.08936, param norm 66.83695
epoch 5, iter 4010, loss 6.28811, smoothed loss 5.21480, grad norm 4.11425, param norm 66.86057
epoch 5, iter 4015, loss 5.34104, smoothed loss 5.23131, grad norm 3.67612, param norm 66.88010
epoch 5, iter 4020, loss 4.95420, smoothed loss 5.22193, grad norm 3.62926, param norm 66.90096
epoch 5, iter 4025, loss 5.04371, smoothed loss 5.20764, grad norm 3.43050, param norm 66.92578
epoch 5, iter 4030, loss 5.35070, smoothed loss 5.20649, grad norm 3.95057, param norm 66.95052
epoch 5, iter 4035, loss 5.68816, smoothed loss 5.22554, grad norm 4.17695, param norm 66.97440
epoch 5, iter 4040, loss 5.49838, smoothed loss 5.23160, grad norm 4.33808, param norm 66.99787
epoch 5, iter 4045, loss 5.62985, smoothed loss 5.23574, grad norm 3.61198, param norm 67.02080
epoch 5, iter 4050, loss 5.13392, smoothed loss 5.23788, grad norm 3.70290, param norm 67.03850
epoch 5, iter 4055, loss 4.80578, smoothed loss 5.22036, grad norm 3.98403, param norm 67.05839
epoch 5, iter 4060, loss 5.63412, smoothed loss 5.21419, grad norm 4.03752, param norm 67.08361
epoch 5, iter 4065, loss 4.91417, smoothed loss 5.19956, grad norm 3.75599, param norm 67.10320
epoch 5, iter 4070, loss 4.67747, smoothed loss 5.20155, grad norm 3.66373, param norm 67.12136
epoch 5, iter 4075, loss 5.46968, smoothed loss 5.20743, grad norm 3.94124, param norm 67.13786
epoch 5, iter 4080, loss 5.19686, smoothed loss 5.20954, grad norm 4.34361, param norm 67.15430
epoch 5, iter 4085, loss 4.81686, smoothed loss 5.19877, grad norm 3.72408, param norm 67.17314
epoch 5, iter 4090, loss 5.43186, smoothed loss 5.20389, grad norm 3.59480, param norm 67.19652
epoch 5, iter 4095, loss 4.81168, smoothed loss 5.18987, grad norm 3.64023, param norm 67.22152
epoch 5, iter 4100, loss 5.19272, smoothed loss 5.19911, grad norm 3.78126, param norm 67.24617
epoch 5, iter 4105, loss 5.07915, smoothed loss 5.18295, grad norm 3.59208, param norm 67.27094
epoch 5, iter 4110, loss 4.78451, smoothed loss 5.17861, grad norm 3.61996, param norm 67.29444
epoch 5, iter 4115, loss 5.51271, smoothed loss 5.17473, grad norm 4.01669, param norm 67.31815
epoch 5, iter 4120, loss 5.10018, smoothed loss 5.16683, grad norm 3.92096, param norm 67.33798
epoch 5, iter 4125, loss 5.91264, smoothed loss 5.18644, grad norm 4.44399, param norm 67.35777
epoch 5, iter 4130, loss 5.23977, smoothed loss 5.18399, grad norm 3.45499, param norm 67.37820
epoch 5, iter 4135, loss 5.66823, smoothed loss 5.19934, grad norm 4.21520, param norm 67.40115
epoch 5, iter 4140, loss 4.71855, smoothed loss 5.19055, grad norm 3.73960, param norm 67.42291
epoch 5, iter 4145, loss 5.03503, smoothed loss 5.17615, grad norm 3.40579, param norm 67.44805
epoch 5, iter 4150, loss 5.22477, smoothed loss 5.17701, grad norm 3.67262, param norm 67.47529
epoch 5, iter 4155, loss 5.22086, smoothed loss 5.17363, grad norm 4.09847, param norm 67.50197
epoch 5, iter 4160, loss 5.16388, smoothed loss 5.17670, grad norm 4.93039, param norm 67.52586
epoch 5, iter 4165, loss 4.68518, smoothed loss 5.16698, grad norm 3.74981, param norm 67.55150
epoch 5, iter 4170, loss 5.02112, smoothed loss 5.15564, grad norm 4.59355, param norm 67.57867
epoch 5, iter 4175, loss 4.47113, smoothed loss 5.14470, grad norm 3.53207, param norm 67.60167
epoch 5, iter 4180, loss 4.73970, smoothed loss 5.13148, grad norm 4.47398, param norm 67.61942
epoch 5, iter 4185, loss 5.59797, smoothed loss 5.13166, grad norm 3.57628, param norm 67.63580
epoch 5, iter 4190, loss 4.62438, smoothed loss 5.13536, grad norm 3.38466, param norm 67.65611
epoch 5, iter 4195, loss 6.24893, smoothed loss 5.15203, grad norm 4.41104, param norm 67.67901
epoch 5, iter 4200, loss 4.65827, smoothed loss 5.14987, grad norm 3.92152, param norm 67.70136
epoch 5, iter 4205, loss 4.86526, smoothed loss 5.14405, grad norm 3.95508, param norm 67.72530
epoch 5, iter 4210, loss 5.12781, smoothed loss 5.15155, grad norm 3.98828, param norm 67.74712
epoch 5, iter 4215, loss 5.28412, smoothed loss 5.15861, grad norm 3.51973, param norm 67.76734
epoch 5, iter 4220, loss 5.04557, smoothed loss 5.16610, grad norm 3.81959, param norm 67.78910
epoch 5, iter 4225, loss 5.18886, smoothed loss 5.15779, grad norm 3.53059, param norm 67.81208
epoch 5, iter 4230, loss 5.87384, smoothed loss 5.16054, grad norm 3.86403, param norm 67.83597
epoch 5, iter 4235, loss 4.93665, smoothed loss 5.15667, grad norm 3.37614, param norm 67.85622
epoch 5, iter 4240, loss 5.08348, smoothed loss 5.15100, grad norm 4.25118, param norm 67.87920
epoch 5, iter 4245, loss 5.10477, smoothed loss 5.15475, grad norm 4.04045, param norm 67.90116
epoch 5, iter 4250, loss 5.07017, smoothed loss 5.15463, grad norm 3.60114, param norm 67.92530
epoch 5, iter 4255, loss 4.85068, smoothed loss 5.14368, grad norm 3.43532, param norm 67.94835
epoch 5, iter 4260, loss 4.65269, smoothed loss 5.14354, grad norm 3.39296, param norm 67.97138
epoch 5, iter 4265, loss 4.97164, smoothed loss 5.14745, grad norm 4.28896, param norm 67.99171
epoch 5, iter 4270, loss 5.28872, smoothed loss 5.14468, grad norm 4.20609, param norm 68.00941
epoch 5, iter 4275, loss 4.82803, smoothed loss 5.14210, grad norm 3.91492, param norm 68.02789
epoch 5, iter 4280, loss 4.65353, smoothed loss 5.13366, grad norm 3.83617, param norm 68.04624
epoch 5, iter 4285, loss 4.89940, smoothed loss 5.13189, grad norm 4.06342, param norm 68.06889
epoch 5, iter 4290, loss 4.28542, smoothed loss 5.11805, grad norm 3.51871, param norm 68.08992
epoch 5, iter 4295, loss 5.16751, smoothed loss 5.10940, grad norm 4.05511, param norm 68.10924
epoch 5, iter 4300, loss 4.86405, smoothed loss 5.11251, grad norm 3.34855, param norm 68.12811
epoch 5, iter 4305, loss 4.38930, smoothed loss 5.10099, grad norm 3.45977, param norm 68.14773
epoch 5, iter 4310, loss 5.26054, smoothed loss 5.09207, grad norm 3.81774, param norm 68.17153
epoch 5, iter 4315, loss 4.75851, smoothed loss 5.09104, grad norm 4.16802, param norm 68.19303
epoch 5, iter 4320, loss 4.25184, smoothed loss 5.08842, grad norm 3.23464, param norm 68.21079
epoch 5, iter 4325, loss 4.89043, smoothed loss 5.07842, grad norm 4.12273, param norm 68.23096
epoch 5, iter 4330, loss 4.89132, smoothed loss 5.08521, grad norm 3.79054, param norm 68.24951
epoch 5, iter 4335, loss 5.21809, smoothed loss 5.09069, grad norm 4.24154, param norm 68.26849
epoch 5, iter 4340, loss 4.34372, smoothed loss 5.08353, grad norm 3.94431, param norm 68.28743
epoch 5, iter 4345, loss 4.03615, smoothed loss 5.08061, grad norm 3.84127, param norm 68.30526
epoch 5, iter 4350, loss 5.40875, smoothed loss 5.08606, grad norm 3.64534, param norm 68.32042
epoch 5, iter 4355, loss 5.66431, smoothed loss 5.09577, grad norm 4.33467, param norm 68.33479
epoch 5, iter 4360, loss 5.20188, smoothed loss 5.08520, grad norm 3.37114, param norm 68.35229
epoch 5, iter 4365, loss 4.79383, smoothed loss 5.08425, grad norm 3.75614, param norm 68.37384
epoch 5, iter 4370, loss 4.67178, smoothed loss 5.07181, grad norm 3.78943, param norm 68.40132
epoch 5, iter 4375, loss 5.24479, smoothed loss 5.07723, grad norm 4.63319, param norm 68.42810
epoch 5, iter 4380, loss 5.09602, smoothed loss 5.08344, grad norm 3.67175, param norm 68.45046
epoch 5, iter 4385, loss 5.47694, smoothed loss 5.08498, grad norm 4.23743, param norm 68.46856
epoch 5, iter 4390, loss 5.28878, smoothed loss 5.07913, grad norm 4.41511, param norm 68.48254
epoch 5, iter 4395, loss 5.14282, smoothed loss 5.08377, grad norm 3.81491, param norm 68.49738
epoch 5, iter 4400, loss 5.07573, smoothed loss 5.09836, grad norm 4.28794, param norm 68.51431
epoch 5, iter 4405, loss 4.61572, smoothed loss 5.10060, grad norm 3.47882, param norm 68.53056
epoch 5, iter 4410, loss 5.04604, smoothed loss 5.09506, grad norm 3.96467, param norm 68.54928
epoch 5, iter 4415, loss 5.52346, smoothed loss 5.09949, grad norm 3.86935, param norm 68.56959
epoch 5, iter 4420, loss 5.89507, smoothed loss 5.10445, grad norm 4.36957, param norm 68.58665
epoch 5, iter 4425, loss 5.20852, smoothed loss 5.09443, grad norm 3.65604, param norm 68.60298
epoch 5, iter 4430, loss 5.35174, smoothed loss 5.09601, grad norm 3.79801, param norm 68.62422
epoch 5, iter 4435, loss 4.85647, smoothed loss 5.08764, grad norm 3.20849, param norm 68.64558
epoch 5, iter 4440, loss 4.19452, smoothed loss 5.07364, grad norm 3.68038, param norm 68.66785
epoch 5, iter 4445, loss 5.43779, smoothed loss 5.07243, grad norm 4.09816, param norm 68.69221
epoch 5, iter 4450, loss 5.24970, smoothed loss 5.07143, grad norm 4.06369, param norm 68.71686
epoch 5, iter 4455, loss 4.54438, smoothed loss 5.06457, grad norm 3.70816, param norm 68.74157
epoch 5, iter 4460, loss 5.07534, smoothed loss 5.07278, grad norm 3.85998, param norm 68.76151
epoch 5, iter 4465, loss 4.83102, smoothed loss 5.06913, grad norm 3.67917, param norm 68.77983
epoch 5, iter 4470, loss 5.17509, smoothed loss 5.06669, grad norm 3.71834, param norm 68.79956
epoch 5, iter 4475, loss 5.19210, smoothed loss 5.06800, grad norm 3.44323, param norm 68.81758
epoch 5, iter 4480, loss 4.27931, smoothed loss 5.05625, grad norm 3.86784, param norm 68.83778
epoch 5, iter 4485, loss 4.77425, smoothed loss 5.05089, grad norm 4.04100, param norm 68.86197
epoch 5, iter 4490, loss 5.43249, smoothed loss 5.06274, grad norm 4.08634, param norm 68.88187
epoch 5, iter 4495, loss 5.11567, smoothed loss 5.07615, grad norm 3.89451, param norm 68.89934
epoch 5, iter 4500, loss 5.49294, smoothed loss 5.08206, grad norm 4.13548, param norm 68.91662
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 4500, dev loss: 5.158506
Calculating Train F1/EM...
F1 train: 1000 examples took 9.18814 seconds [Score: 0.38030]
Exact Match train: 1000 examples took 8.98138 seconds [Score: 0.27900]
Epoch 5, Iter 4500, Train F1 score: 0.380302, Train EM score: 0.279000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.32901 seconds [Score: 0.31660]
Exact Match dev: 7118 examples took 58.15328 seconds [Score: 0.22071]
Epoch 5, Iter 4500, Dev F1 score: 0.316596, Dev EM score: 0.220708
End of epoch 5
epoch 5, iter 4505, loss 5.06875, smoothed loss 5.08614, grad norm 4.36279, param norm 68.93527
epoch 5, iter 4510, loss 4.84058, smoothed loss 5.07647, grad norm 4.05911, param norm 68.95702
epoch 5, iter 4515, loss 5.23657, smoothed loss 5.07454, grad norm 3.96784, param norm 68.97759
epoch 5, iter 4520, loss 4.91332, smoothed loss 5.07810, grad norm 4.02719, param norm 68.99772
epoch 5, iter 4525, loss 5.54945, smoothed loss 5.07386, grad norm 4.10635, param norm 69.01706
epoch 5, iter 4530, loss 4.27724, smoothed loss 5.08004, grad norm 3.44641, param norm 69.03571
epoch 5, iter 4535, loss 4.71614, smoothed loss 5.08354, grad norm 4.07940, param norm 69.05597
epoch 5, iter 4540, loss 4.42013, smoothed loss 5.08684, grad norm 3.81831, param norm 69.07752
epoch 5, iter 4545, loss 5.08946, smoothed loss 5.10507, grad norm 3.75570, param norm 69.09548
epoch 5, iter 4550, loss 5.27200, smoothed loss 5.09997, grad norm 4.12100, param norm 69.11284
epoch 5, iter 4555, loss 5.16504, smoothed loss 5.10831, grad norm 3.46204, param norm 69.13248
epoch 5, iter 4560, loss 5.33624, smoothed loss 5.11885, grad norm 3.41065, param norm 69.15452
epoch 5, iter 4565, loss 4.93478, smoothed loss 5.10840, grad norm 3.67166, param norm 69.17952
epoch 5, iter 4570, loss 4.08565, smoothed loss 5.09500, grad norm 3.36405, param norm 69.20672
epoch 5, iter 4575, loss 5.04770, smoothed loss 5.09245, grad norm 4.26668, param norm 69.23647
epoch 5, iter 4580, loss 5.78710, smoothed loss 5.10857, grad norm 3.81314, param norm 69.25792
epoch 5, iter 4585, loss 4.95572, smoothed loss 5.08674, grad norm 3.73517, param norm 69.27693
epoch 5, iter 4590, loss 4.52010, smoothed loss 5.08978, grad norm 3.87549, param norm 69.29814
epoch 5, iter 4595, loss 5.58345, smoothed loss 5.09805, grad norm 4.13898, param norm 69.31637
epoch 5, iter 4600, loss 5.37217, smoothed loss 5.09762, grad norm 4.06273, param norm 69.33346
epoch 5, iter 4605, loss 4.87860, smoothed loss 5.08964, grad norm 3.86119, param norm 69.35423
epoch 5, iter 4610, loss 4.78315, smoothed loss 5.08345, grad norm 3.65538, param norm 69.37820
epoch 5, iter 4615, loss 5.39261, smoothed loss 5.07598, grad norm 4.42715, param norm 69.39818
epoch 5, iter 4620, loss 4.96469, smoothed loss 5.08539, grad norm 3.44230, param norm 69.41545
epoch 5, iter 4625, loss 5.17102, smoothed loss 5.08613, grad norm 4.13866, param norm 69.43205
epoch 5, iter 4630, loss 5.19580, smoothed loss 5.08487, grad norm 4.00621, param norm 69.45079
epoch 5, iter 4635, loss 5.57237, smoothed loss 5.09263, grad norm 4.41322, param norm 69.47131
epoch 5, iter 4640, loss 5.46243, smoothed loss 5.09800, grad norm 4.63228, param norm 69.49311
epoch 5, iter 4645, loss 5.01066, smoothed loss 5.10099, grad norm 3.92300, param norm 69.51821
epoch 5, iter 4650, loss 4.60803, smoothed loss 5.09494, grad norm 3.64272, param norm 69.54219
epoch 5, iter 4655, loss 5.43793, smoothed loss 5.08895, grad norm 3.98598, param norm 69.56551
epoch 5, iter 4660, loss 5.35973, smoothed loss 5.08570, grad norm 4.13515, param norm 69.58778
epoch 5, iter 4665, loss 5.39906, smoothed loss 5.09229, grad norm 3.82491, param norm 69.60857
epoch 5, iter 4670, loss 5.11939, smoothed loss 5.08416, grad norm 4.26415, param norm 69.62704
epoch 5, iter 4675, loss 5.74174, smoothed loss 5.10042, grad norm 4.47556, param norm 69.64544
epoch 5, iter 4680, loss 4.68049, smoothed loss 5.09655, grad norm 3.64650, param norm 69.65994
epoch 5, iter 4685, loss 4.78889, smoothed loss 5.10339, grad norm 3.82605, param norm 69.67620
epoch 5, iter 4690, loss 4.47864, smoothed loss 5.08346, grad norm 3.87311, param norm 69.69638
epoch 5, iter 4695, loss 5.89708, smoothed loss 5.09696, grad norm 4.14821, param norm 69.71900
epoch 5, iter 4700, loss 5.26367, smoothed loss 5.09155, grad norm 3.62574, param norm 69.73871
epoch 5, iter 4705, loss 5.31669, smoothed loss 5.08752, grad norm 3.86638, param norm 69.76001
epoch 5, iter 4710, loss 5.16825, smoothed loss 5.08546, grad norm 4.45118, param norm 69.78110
epoch 5, iter 4715, loss 4.44774, smoothed loss 5.08045, grad norm 3.77386, param norm 69.80265
epoch 5, iter 4720, loss 5.07677, smoothed loss 5.08391, grad norm 4.07547, param norm 69.82241
epoch 6, iter 4725, loss 5.60523, smoothed loss 5.09537, grad norm 4.08165, param norm 69.84260
epoch 6, iter 4730, loss 4.89312, smoothed loss 5.08232, grad norm 4.02424, param norm 69.86182
epoch 6, iter 4735, loss 4.91811, smoothed loss 5.09529, grad norm 4.56839, param norm 69.88148
epoch 6, iter 4740, loss 4.85672, smoothed loss 5.08329, grad norm 3.98923, param norm 69.90228
epoch 6, iter 4745, loss 4.89291, smoothed loss 5.09295, grad norm 4.06276, param norm 69.92179
epoch 6, iter 4750, loss 4.81767, smoothed loss 5.09274, grad norm 4.04004, param norm 69.93845
epoch 6, iter 4755, loss 4.40565, smoothed loss 5.06640, grad norm 3.49089, param norm 69.95723
epoch 6, iter 4760, loss 5.01144, smoothed loss 5.05022, grad norm 4.15638, param norm 69.98277
epoch 6, iter 4765, loss 4.62092, smoothed loss 5.04114, grad norm 3.99459, param norm 70.00940
epoch 6, iter 4770, loss 4.85787, smoothed loss 5.02465, grad norm 4.44802, param norm 70.03508
epoch 6, iter 4775, loss 4.13180, smoothed loss 5.00987, grad norm 4.13969, param norm 70.05638
epoch 6, iter 4780, loss 5.87510, smoothed loss 5.03585, grad norm 4.34161, param norm 70.07397
epoch 6, iter 4785, loss 4.96302, smoothed loss 5.04521, grad norm 3.92910, param norm 70.08593
epoch 6, iter 4790, loss 4.79127, smoothed loss 5.04066, grad norm 3.95935, param norm 70.09685
epoch 6, iter 4795, loss 5.43510, smoothed loss 5.05151, grad norm 3.58861, param norm 70.11150
epoch 6, iter 4800, loss 5.23919, smoothed loss 5.04749, grad norm 3.88546, param norm 70.12611
epoch 6, iter 4805, loss 5.91740, smoothed loss 5.04898, grad norm 4.43752, param norm 70.14356
epoch 6, iter 4810, loss 5.15551, smoothed loss 5.05000, grad norm 4.14354, param norm 70.16045
epoch 6, iter 4815, loss 5.65798, smoothed loss 5.04457, grad norm 3.85616, param norm 70.18280
epoch 6, iter 4820, loss 4.72732, smoothed loss 5.03345, grad norm 4.33414, param norm 70.20599
epoch 6, iter 4825, loss 4.67032, smoothed loss 5.01948, grad norm 3.89565, param norm 70.22697
epoch 6, iter 4830, loss 5.29879, smoothed loss 5.01736, grad norm 3.82503, param norm 70.25158
epoch 6, iter 4835, loss 4.77791, smoothed loss 5.00113, grad norm 3.95652, param norm 70.27589
epoch 6, iter 4840, loss 4.75992, smoothed loss 5.00022, grad norm 4.09040, param norm 70.29893
epoch 6, iter 4845, loss 4.62983, smoothed loss 4.99488, grad norm 3.83813, param norm 70.31870
epoch 6, iter 4850, loss 5.79828, smoothed loss 5.01214, grad norm 4.78894, param norm 70.33990
epoch 6, iter 4855, loss 4.84848, smoothed loss 5.01339, grad norm 5.30645, param norm 70.35796
epoch 6, iter 4860, loss 5.03236, smoothed loss 5.01464, grad norm 3.37464, param norm 70.37643
epoch 6, iter 4865, loss 5.50878, smoothed loss 5.01368, grad norm 4.06452, param norm 70.39445
epoch 6, iter 4870, loss 5.54847, smoothed loss 5.02856, grad norm 4.03240, param norm 70.41126
epoch 6, iter 4875, loss 4.66019, smoothed loss 5.03249, grad norm 3.95765, param norm 70.42910
epoch 6, iter 4880, loss 5.39361, smoothed loss 5.04642, grad norm 3.72446, param norm 70.44756
epoch 6, iter 4885, loss 5.37043, smoothed loss 5.05165, grad norm 3.72152, param norm 70.46382
epoch 6, iter 4890, loss 4.97576, smoothed loss 5.05016, grad norm 3.73911, param norm 70.48069
epoch 6, iter 4895, loss 4.88463, smoothed loss 5.04267, grad norm 3.52890, param norm 70.49833
epoch 6, iter 4900, loss 4.30081, smoothed loss 5.02423, grad norm 3.99043, param norm 70.51910
epoch 6, iter 4905, loss 4.73699, smoothed loss 5.01128, grad norm 4.10178, param norm 70.54018
epoch 6, iter 4910, loss 4.82200, smoothed loss 5.01821, grad norm 3.63088, param norm 70.56104
epoch 6, iter 4915, loss 4.75965, smoothed loss 5.01610, grad norm 3.90185, param norm 70.58086
epoch 6, iter 4920, loss 4.87765, smoothed loss 5.00781, grad norm 3.66269, param norm 70.60322
epoch 6, iter 4925, loss 5.00336, smoothed loss 4.99645, grad norm 4.15801, param norm 70.62529
epoch 6, iter 4930, loss 5.43801, smoothed loss 5.01822, grad norm 4.51077, param norm 70.64233
epoch 6, iter 4935, loss 4.94834, smoothed loss 5.02725, grad norm 3.72913, param norm 70.65750
epoch 6, iter 4940, loss 5.20953, smoothed loss 5.04031, grad norm 3.57219, param norm 70.67558
epoch 6, iter 4945, loss 4.50915, smoothed loss 5.03566, grad norm 3.59976, param norm 70.69765
epoch 6, iter 4950, loss 5.62814, smoothed loss 5.04901, grad norm 4.52979, param norm 70.72044
epoch 6, iter 4955, loss 4.25744, smoothed loss 5.03871, grad norm 3.91714, param norm 70.74022
epoch 6, iter 4960, loss 5.10163, smoothed loss 5.04009, grad norm 4.07936, param norm 70.76151
epoch 6, iter 4965, loss 5.25842, smoothed loss 5.05471, grad norm 4.01635, param norm 70.78165
epoch 6, iter 4970, loss 5.12204, smoothed loss 5.06515, grad norm 4.62334, param norm 70.79681
epoch 6, iter 4975, loss 5.21585, smoothed loss 5.06784, grad norm 3.85213, param norm 70.81078
epoch 6, iter 4980, loss 5.19153, smoothed loss 5.06693, grad norm 3.93333, param norm 70.82750
epoch 6, iter 4985, loss 4.78062, smoothed loss 5.07137, grad norm 4.27197, param norm 70.84564
epoch 6, iter 4990, loss 4.91489, smoothed loss 5.08607, grad norm 4.02830, param norm 70.86347
epoch 6, iter 4995, loss 4.75221, smoothed loss 5.08188, grad norm 3.71023, param norm 70.88409
epoch 6, iter 5000, loss 4.91633, smoothed loss 5.06203, grad norm 3.79604, param norm 70.90780
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 6, Iter 5000, dev loss: 5.157711
Calculating Train F1/EM...
F1 train: 1000 examples took 9.38406 seconds [Score: 0.43254]
Exact Match train: 1000 examples took 8.96311 seconds [Score: 0.29100]
Epoch 6, Iter 5000, Train F1 score: 0.432541, Train EM score: 0.291000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.62011 seconds [Score: 0.31644]
Exact Match dev: 7118 examples took 58.02799 seconds [Score: 0.22183]
Epoch 6, Iter 5000, Dev F1 score: 0.316444, Dev EM score: 0.221832
End of epoch 6
epoch 6, iter 5005, loss 4.76600, smoothed loss 5.06630, grad norm 4.07236, param norm 70.93041
epoch 6, iter 5010, loss 5.37414, smoothed loss 5.06940, grad norm 4.06091, param norm 70.95022
epoch 6, iter 5015, loss 5.32258, smoothed loss 5.06608, grad norm 3.93849, param norm 70.96809
epoch 6, iter 5020, loss 4.96888, smoothed loss 5.06760, grad norm 4.01858, param norm 70.98193
epoch 6, iter 5025, loss 5.22957, smoothed loss 5.07381, grad norm 3.62619, param norm 70.99527
epoch 6, iter 5030, loss 5.22426, smoothed loss 5.08323, grad norm 4.01842, param norm 71.01241
epoch 6, iter 5035, loss 5.01046, smoothed loss 5.08611, grad norm 3.88203, param norm 71.03273
epoch 6, iter 5040, loss 4.78068, smoothed loss 5.07317, grad norm 3.82081, param norm 71.05121
epoch 6, iter 5045, loss 4.95701, smoothed loss 5.07009, grad norm 3.58445, param norm 71.06967
epoch 6, iter 5050, loss 4.86161, smoothed loss 5.07113, grad norm 4.24283, param norm 71.08676
epoch 6, iter 5055, loss 5.24455, smoothed loss 5.06178, grad norm 3.70673, param norm 71.10094
epoch 6, iter 5060, loss 5.06753, smoothed loss 5.04716, grad norm 3.96637, param norm 71.11746
epoch 6, iter 5065, loss 5.25630, smoothed loss 5.05422, grad norm 4.56882, param norm 71.13448
epoch 6, iter 5070, loss 4.99106, smoothed loss 5.04368, grad norm 3.38230, param norm 71.15147
epoch 6, iter 5075, loss 5.12706, smoothed loss 5.02072, grad norm 4.00091, param norm 71.17168
epoch 6, iter 5080, loss 5.52759, smoothed loss 5.02595, grad norm 4.13646, param norm 71.19444
epoch 6, iter 5085, loss 5.09329, smoothed loss 5.03264, grad norm 4.19284, param norm 71.21371
epoch 6, iter 5090, loss 4.82164, smoothed loss 5.01975, grad norm 3.86523, param norm 71.23376
epoch 6, iter 5095, loss 4.51657, smoothed loss 5.02129, grad norm 3.52898, param norm 71.25524
epoch 6, iter 5100, loss 4.83029, smoothed loss 5.01130, grad norm 3.53279, param norm 71.27617
epoch 6, iter 5105, loss 5.28809, smoothed loss 4.99938, grad norm 4.33567, param norm 71.29794
epoch 6, iter 5110, loss 4.74503, smoothed loss 5.00147, grad norm 3.98694, param norm 71.31765
epoch 6, iter 5115, loss 4.68793, smoothed loss 4.99305, grad norm 4.17090, param norm 71.33851
epoch 6, iter 5120, loss 5.23944, smoothed loss 4.99501, grad norm 3.87454, param norm 71.35648
epoch 6, iter 5125, loss 5.09445, smoothed loss 4.98552, grad norm 4.04156, param norm 71.37744
epoch 6, iter 5130, loss 5.37371, smoothed loss 4.99250, grad norm 4.14265, param norm 71.39789
epoch 6, iter 5135, loss 5.68072, smoothed loss 4.99036, grad norm 4.28897, param norm 71.41695
epoch 6, iter 5140, loss 5.17553, smoothed loss 5.00878, grad norm 4.04354, param norm 71.43100
epoch 6, iter 5145, loss 5.59573, smoothed loss 5.00389, grad norm 4.11899, param norm 71.44779
epoch 6, iter 5150, loss 5.52264, smoothed loss 5.00659, grad norm 4.49585, param norm 71.46790
epoch 6, iter 5155, loss 5.34575, smoothed loss 5.00884, grad norm 4.70259, param norm 71.48884
epoch 6, iter 5160, loss 5.46521, smoothed loss 5.01319, grad norm 4.00743, param norm 71.51004
epoch 6, iter 5165, loss 4.69070, smoothed loss 4.99837, grad norm 3.72296, param norm 71.52893
epoch 6, iter 5170, loss 4.86718, smoothed loss 4.99913, grad norm 4.02148, param norm 71.54807
epoch 6, iter 5175, loss 5.60876, smoothed loss 5.02222, grad norm 4.05130, param norm 71.56664
epoch 6, iter 5180, loss 4.75154, smoothed loss 5.01981, grad norm 3.71606, param norm 71.58385
epoch 6, iter 5185, loss 5.06564, smoothed loss 5.02029, grad norm 3.84885, param norm 71.60517
epoch 6, iter 5190, loss 5.30161, smoothed loss 5.02100, grad norm 4.12555, param norm 71.62912
epoch 6, iter 5195, loss 5.04372, smoothed loss 5.00265, grad norm 3.92497, param norm 71.65131
epoch 6, iter 5200, loss 4.88891, smoothed loss 4.99940, grad norm 3.85843, param norm 71.67310
epoch 6, iter 5205, loss 5.11491, smoothed loss 4.98399, grad norm 4.23253, param norm 71.69433
epoch 6, iter 5210, loss 4.68328, smoothed loss 4.97646, grad norm 4.26620, param norm 71.71236
epoch 6, iter 5215, loss 5.04137, smoothed loss 4.97799, grad norm 3.74533, param norm 71.72938
epoch 6, iter 5220, loss 4.88021, smoothed loss 4.98770, grad norm 4.16532, param norm 71.74552
epoch 6, iter 5225, loss 4.83160, smoothed loss 4.99064, grad norm 3.80587, param norm 71.76201
epoch 6, iter 5230, loss 4.94208, smoothed loss 4.98505, grad norm 4.30588, param norm 71.78042
epoch 6, iter 5235, loss 5.52537, smoothed loss 4.99451, grad norm 4.60523, param norm 71.79927
epoch 6, iter 5240, loss 5.01671, smoothed loss 4.99066, grad norm 4.29484, param norm 71.81619
epoch 6, iter 5245, loss 4.45437, smoothed loss 4.96945, grad norm 3.85714, param norm 71.83395
epoch 6, iter 5250, loss 4.62693, smoothed loss 4.97462, grad norm 3.82828, param norm 71.84969
epoch 6, iter 5255, loss 5.05245, smoothed loss 4.97918, grad norm 4.37344, param norm 71.86481
epoch 6, iter 5260, loss 4.99777, smoothed loss 4.96810, grad norm 3.82459, param norm 71.88114
epoch 6, iter 5265, loss 5.93695, smoothed loss 4.99029, grad norm 3.87891, param norm 71.90221
epoch 6, iter 5270, loss 4.99637, smoothed loss 4.97790, grad norm 4.16850, param norm 71.92725
epoch 6, iter 5275, loss 4.94797, smoothed loss 4.98063, grad norm 4.42609, param norm 71.95368
epoch 6, iter 5280, loss 5.02410, smoothed loss 4.96752, grad norm 4.21436, param norm 71.97923
epoch 6, iter 5285, loss 4.80865, smoothed loss 4.96191, grad norm 4.33291, param norm 72.00110
epoch 6, iter 5290, loss 5.13043, smoothed loss 4.96919, grad norm 3.58065, param norm 72.02155
epoch 6, iter 5295, loss 4.98320, smoothed loss 4.96018, grad norm 3.76180, param norm 72.03953
epoch 6, iter 5300, loss 5.07437, smoothed loss 4.95043, grad norm 4.05418, param norm 72.05816
epoch 6, iter 5305, loss 5.53491, smoothed loss 4.95669, grad norm 3.96908, param norm 72.07474
epoch 6, iter 5310, loss 4.72552, smoothed loss 4.96102, grad norm 4.04509, param norm 72.08737
epoch 6, iter 5315, loss 4.86219, smoothed loss 4.95575, grad norm 4.16521, param norm 72.10311
epoch 6, iter 5320, loss 5.21201, smoothed loss 4.95215, grad norm 3.94005, param norm 72.11842
epoch 6, iter 5325, loss 4.89669, smoothed loss 4.95576, grad norm 4.05962, param norm 72.13381
epoch 6, iter 5330, loss 5.11840, smoothed loss 4.94467, grad norm 4.13129, param norm 72.15103
epoch 6, iter 5335, loss 4.93765, smoothed loss 4.94330, grad norm 3.73080, param norm 72.16865
epoch 6, iter 5340, loss 4.84749, smoothed loss 4.94364, grad norm 3.79691, param norm 72.18693
epoch 6, iter 5345, loss 5.20936, smoothed loss 4.95342, grad norm 3.96683, param norm 72.20345
epoch 6, iter 5350, loss 4.88486, smoothed loss 4.95740, grad norm 4.54960, param norm 72.21651
epoch 6, iter 5355, loss 4.17662, smoothed loss 4.95135, grad norm 3.69651, param norm 72.23183
epoch 6, iter 5360, loss 4.97355, smoothed loss 4.95216, grad norm 4.28609, param norm 72.25181
epoch 6, iter 5365, loss 5.62145, smoothed loss 4.94984, grad norm 4.60181, param norm 72.27290
epoch 6, iter 5370, loss 4.39630, smoothed loss 4.94878, grad norm 3.40878, param norm 72.28916
epoch 6, iter 5375, loss 5.30605, smoothed loss 4.94794, grad norm 3.91334, param norm 72.30862
epoch 6, iter 5380, loss 5.27951, smoothed loss 4.95389, grad norm 3.80335, param norm 72.32661
epoch 6, iter 5385, loss 4.90016, smoothed loss 4.95527, grad norm 3.60052, param norm 72.34389
epoch 6, iter 5390, loss 5.23541, smoothed loss 4.96064, grad norm 4.02164, param norm 72.36395
epoch 6, iter 5395, loss 5.30113, smoothed loss 4.95867, grad norm 4.37632, param norm 72.38746
epoch 6, iter 5400, loss 4.96401, smoothed loss 4.96444, grad norm 3.61716, param norm 72.40784
epoch 6, iter 5405, loss 4.69621, smoothed loss 4.96318, grad norm 3.68059, param norm 72.42491
epoch 6, iter 5410, loss 5.61904, smoothed loss 4.96899, grad norm 3.42661, param norm 72.44572
epoch 6, iter 5415, loss 5.37243, smoothed loss 4.98897, grad norm 3.82817, param norm 72.46481
epoch 6, iter 5420, loss 5.29440, smoothed loss 4.98740, grad norm 3.85258, param norm 72.48019
epoch 6, iter 5425, loss 4.73917, smoothed loss 4.98983, grad norm 4.04288, param norm 72.49439
epoch 6, iter 5430, loss 4.32281, smoothed loss 4.98351, grad norm 3.60986, param norm 72.51000
epoch 6, iter 5435, loss 5.20075, smoothed loss 4.99316, grad norm 4.23507, param norm 72.52689
epoch 6, iter 5440, loss 4.81494, smoothed loss 4.99604, grad norm 3.77211, param norm 72.54344
epoch 6, iter 5445, loss 4.09056, smoothed loss 4.99271, grad norm 3.94851, param norm 72.55795
epoch 6, iter 5450, loss 4.77574, smoothed loss 4.98841, grad norm 3.49443, param norm 72.57417
epoch 6, iter 5455, loss 4.52208, smoothed loss 4.98569, grad norm 3.63255, param norm 72.59161
epoch 6, iter 5460, loss 5.76531, smoothed loss 4.99844, grad norm 3.66640, param norm 72.60934
epoch 6, iter 5465, loss 5.03283, smoothed loss 5.00102, grad norm 3.53989, param norm 72.62720
epoch 6, iter 5470, loss 5.51031, smoothed loss 5.00675, grad norm 3.92589, param norm 72.64209
epoch 6, iter 5475, loss 4.39291, smoothed loss 4.97807, grad norm 3.90960, param norm 72.65891
epoch 6, iter 5480, loss 5.27367, smoothed loss 4.98378, grad norm 4.50338, param norm 72.67943
epoch 6, iter 5485, loss 4.68397, smoothed loss 4.97279, grad norm 4.24550, param norm 72.70077
epoch 6, iter 5490, loss 5.07442, smoothed loss 4.97648, grad norm 4.45071, param norm 72.72112
epoch 6, iter 5495, loss 5.18408, smoothed loss 4.96685, grad norm 3.92310, param norm 72.73925
epoch 6, iter 5500, loss 4.79006, smoothed loss 4.96206, grad norm 3.44089, param norm 72.75941
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 6, Iter 5500, dev loss: 5.105126
Calculating Train F1/EM...
F1 train: 1000 examples took 9.10233 seconds [Score: 0.41341]
Exact Match train: 1000 examples took 8.69331 seconds [Score: 0.31800]
Epoch 6, Iter 5500, Train F1 score: 0.413406, Train EM score: 0.318000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.65094 seconds [Score: 0.32653]
Exact Match dev: 7118 examples took 58.00545 seconds [Score: 0.22633]
Epoch 6, Iter 5500, Dev F1 score: 0.326527, Dev EM score: 0.226328
End of epoch 6
epoch 6, iter 5505, loss 4.56662, smoothed loss 4.96215, grad norm 3.75328, param norm 72.77691
epoch 6, iter 5510, loss 4.35177, smoothed loss 4.95471, grad norm 4.27508, param norm 72.79350
epoch 6, iter 5515, loss 4.06676, smoothed loss 4.93614, grad norm 3.84729, param norm 72.81315
epoch 6, iter 5520, loss 4.86115, smoothed loss 4.94007, grad norm 4.08465, param norm 72.83401
epoch 6, iter 5525, loss 5.51456, smoothed loss 4.93939, grad norm 4.10934, param norm 72.85475
epoch 6, iter 5530, loss 4.59879, smoothed loss 4.94472, grad norm 3.76974, param norm 72.87402
epoch 6, iter 5535, loss 4.42464, smoothed loss 4.94162, grad norm 3.69109, param norm 72.89329
epoch 6, iter 5540, loss 5.95655, smoothed loss 4.94845, grad norm 3.66124, param norm 72.91046
epoch 6, iter 5545, loss 4.58087, smoothed loss 4.96071, grad norm 3.96301, param norm 72.92567
epoch 6, iter 5550, loss 4.69446, smoothed loss 4.94177, grad norm 3.81915, param norm 72.94048
epoch 6, iter 5555, loss 4.81717, smoothed loss 4.95238, grad norm 4.31730, param norm 72.95833
epoch 6, iter 5560, loss 4.85616, smoothed loss 4.95766, grad norm 4.68629, param norm 72.97241
epoch 6, iter 5565, loss 5.25437, smoothed loss 4.96073, grad norm 4.06896, param norm 72.99078
epoch 6, iter 5570, loss 4.99694, smoothed loss 4.96158, grad norm 4.47723, param norm 73.00900
epoch 6, iter 5575, loss 5.30700, smoothed loss 4.94954, grad norm 3.82634, param norm 73.02505
epoch 6, iter 5580, loss 5.21853, smoothed loss 4.94595, grad norm 3.99294, param norm 73.04355
epoch 6, iter 5585, loss 5.56903, smoothed loss 4.95022, grad norm 4.35608, param norm 73.06423
epoch 6, iter 5590, loss 4.32758, smoothed loss 4.94015, grad norm 3.69711, param norm 73.08013
epoch 6, iter 5595, loss 4.66840, smoothed loss 4.94994, grad norm 4.70099, param norm 73.09828
epoch 6, iter 5600, loss 4.95692, smoothed loss 4.95400, grad norm 4.20878, param norm 73.11764
epoch 6, iter 5605, loss 4.75480, smoothed loss 4.94791, grad norm 3.95978, param norm 73.13791
epoch 6, iter 5610, loss 5.34943, smoothed loss 4.95300, grad norm 4.31062, param norm 73.15759
epoch 6, iter 5615, loss 5.39103, smoothed loss 4.94313, grad norm 4.25497, param norm 73.17513
epoch 6, iter 5620, loss 5.07701, smoothed loss 4.95348, grad norm 4.05587, param norm 73.18987
epoch 6, iter 5625, loss 4.87183, smoothed loss 4.94141, grad norm 4.14117, param norm 73.20367
epoch 6, iter 5630, loss 3.74147, smoothed loss 4.92677, grad norm 3.50669, param norm 73.22189
epoch 6, iter 5635, loss 5.15321, smoothed loss 4.92858, grad norm 4.11103, param norm 73.24339
epoch 6, iter 5640, loss 4.43948, smoothed loss 4.91818, grad norm 3.60203, param norm 73.26152
epoch 6, iter 5645, loss 5.72732, smoothed loss 4.92905, grad norm 4.11469, param norm 73.27779
epoch 6, iter 5650, loss 4.52602, smoothed loss 4.93105, grad norm 3.85493, param norm 73.28931
epoch 6, iter 5655, loss 5.04482, smoothed loss 4.94383, grad norm 4.26422, param norm 73.30229
epoch 6, iter 5660, loss 5.28408, smoothed loss 4.94518, grad norm 3.78509, param norm 73.31506
epoch 7, iter 5665, loss 5.05892, smoothed loss 4.94647, grad norm 4.68195, param norm 73.32922
epoch 7, iter 5670, loss 5.71163, smoothed loss 4.94631, grad norm 4.14448, param norm 73.34784
epoch 7, iter 5675, loss 4.97912, smoothed loss 4.94045, grad norm 3.69888, param norm 73.36987
epoch 7, iter 5680, loss 4.46448, smoothed loss 4.93801, grad norm 3.85272, param norm 73.38886
epoch 7, iter 5685, loss 5.06830, smoothed loss 4.94744, grad norm 3.77215, param norm 73.40347
epoch 7, iter 5690, loss 4.91828, smoothed loss 4.94267, grad norm 3.95144, param norm 73.41904
epoch 7, iter 5695, loss 5.15644, smoothed loss 4.94015, grad norm 3.80033, param norm 73.43851
epoch 7, iter 5700, loss 4.98773, smoothed loss 4.93619, grad norm 3.63190, param norm 73.46094
epoch 7, iter 5705, loss 5.05359, smoothed loss 4.93061, grad norm 4.49660, param norm 73.48511
epoch 7, iter 5710, loss 4.85981, smoothed loss 4.93176, grad norm 4.37318, param norm 73.50602
epoch 7, iter 5715, loss 4.80847, smoothed loss 4.91562, grad norm 4.42216, param norm 73.52120
epoch 7, iter 5720, loss 5.00257, smoothed loss 4.90114, grad norm 4.17798, param norm 73.53532
epoch 7, iter 5725, loss 4.76116, smoothed loss 4.89946, grad norm 3.85711, param norm 73.54533
epoch 7, iter 5730, loss 4.59957, smoothed loss 4.90650, grad norm 3.80668, param norm 73.55988
epoch 7, iter 5735, loss 5.63278, smoothed loss 4.91514, grad norm 3.37112, param norm 73.57738
epoch 7, iter 5740, loss 4.54858, smoothed loss 4.91496, grad norm 4.01249, param norm 73.59406
epoch 7, iter 5745, loss 4.93765, smoothed loss 4.93622, grad norm 3.82484, param norm 73.60766
epoch 7, iter 5750, loss 4.96416, smoothed loss 4.92811, grad norm 4.76159, param norm 73.62172
epoch 7, iter 5755, loss 5.26223, smoothed loss 4.93535, grad norm 4.17322, param norm 73.63634
epoch 7, iter 5760, loss 4.95261, smoothed loss 4.92846, grad norm 3.82572, param norm 73.65095
epoch 7, iter 5765, loss 4.96225, smoothed loss 4.93095, grad norm 3.94473, param norm 73.66826
epoch 7, iter 5770, loss 5.06159, smoothed loss 4.92469, grad norm 4.62159, param norm 73.68813
epoch 7, iter 5775, loss 4.58526, smoothed loss 4.91915, grad norm 3.84854, param norm 73.70982
epoch 7, iter 5780, loss 4.36998, smoothed loss 4.92093, grad norm 4.05755, param norm 73.73393
epoch 7, iter 5785, loss 4.95981, smoothed loss 4.92112, grad norm 4.05302, param norm 73.75748
epoch 7, iter 5790, loss 5.50563, smoothed loss 4.92303, grad norm 4.24339, param norm 73.78114
epoch 7, iter 5795, loss 5.84198, smoothed loss 4.91504, grad norm 4.94300, param norm 73.80476
epoch 7, iter 5800, loss 4.74324, smoothed loss 4.91370, grad norm 4.86026, param norm 73.82392
epoch 7, iter 5805, loss 4.62029, smoothed loss 4.90804, grad norm 3.99658, param norm 73.84312
epoch 7, iter 5810, loss 4.48342, smoothed loss 4.89962, grad norm 4.04261, param norm 73.86248
epoch 7, iter 5815, loss 4.42209, smoothed loss 4.89336, grad norm 3.86113, param norm 73.88667
epoch 7, iter 5820, loss 5.10605, smoothed loss 4.90066, grad norm 4.34323, param norm 73.91088
epoch 7, iter 5825, loss 4.90383, smoothed loss 4.91246, grad norm 4.12861, param norm 73.93029
epoch 7, iter 5830, loss 5.13211, smoothed loss 4.91765, grad norm 4.57271, param norm 73.94669
epoch 7, iter 5835, loss 4.70719, smoothed loss 4.90907, grad norm 3.87771, param norm 73.96358
epoch 7, iter 5840, loss 4.28394, smoothed loss 4.89159, grad norm 4.12321, param norm 73.98353
epoch 7, iter 5845, loss 4.59042, smoothed loss 4.89206, grad norm 3.84967, param norm 74.00520
epoch 7, iter 5850, loss 5.37741, smoothed loss 4.90539, grad norm 4.20784, param norm 74.02203
epoch 7, iter 5855, loss 4.40848, smoothed loss 4.90186, grad norm 3.94047, param norm 74.03737
epoch 7, iter 5860, loss 5.05758, smoothed loss 4.90559, grad norm 4.12446, param norm 74.05518
epoch 7, iter 5865, loss 4.40923, smoothed loss 4.90634, grad norm 3.93812, param norm 74.07170
epoch 7, iter 5870, loss 5.11239, smoothed loss 4.90347, grad norm 4.00336, param norm 74.09158
epoch 7, iter 5875, loss 5.26942, smoothed loss 4.91804, grad norm 3.98582, param norm 74.10904
epoch 7, iter 5880, loss 4.58527, smoothed loss 4.91904, grad norm 4.03836, param norm 74.12370
epoch 7, iter 5885, loss 4.53441, smoothed loss 4.91630, grad norm 4.08958, param norm 74.14119
epoch 7, iter 5890, loss 5.36151, smoothed loss 4.92552, grad norm 4.45658, param norm 74.16239
epoch 7, iter 5895, loss 4.96656, smoothed loss 4.91322, grad norm 3.51277, param norm 74.18489
epoch 7, iter 5900, loss 4.75440, smoothed loss 4.91541, grad norm 3.61440, param norm 74.20718
epoch 7, iter 5905, loss 4.47181, smoothed loss 4.92504, grad norm 3.69527, param norm 74.22905
epoch 7, iter 5910, loss 4.99400, smoothed loss 4.93674, grad norm 3.92465, param norm 74.24770
epoch 7, iter 5915, loss 4.16633, smoothed loss 4.92457, grad norm 3.91840, param norm 74.26506
epoch 7, iter 5920, loss 4.84121, smoothed loss 4.91554, grad norm 3.86566, param norm 74.28410
epoch 7, iter 5925, loss 4.71756, smoothed loss 4.90759, grad norm 3.89223, param norm 74.30000
epoch 7, iter 5930, loss 5.07984, smoothed loss 4.90480, grad norm 4.03601, param norm 74.31445
epoch 7, iter 5935, loss 4.45610, smoothed loss 4.89739, grad norm 4.31262, param norm 74.32744
epoch 7, iter 5940, loss 4.67993, smoothed loss 4.89605, grad norm 3.96129, param norm 74.34132
epoch 7, iter 5945, loss 4.21973, smoothed loss 4.89654, grad norm 3.82409, param norm 74.35903
epoch 7, iter 5950, loss 4.61924, smoothed loss 4.88414, grad norm 3.76379, param norm 74.37863
epoch 7, iter 5955, loss 5.17448, smoothed loss 4.88465, grad norm 3.98920, param norm 74.40198
epoch 7, iter 5960, loss 5.18184, smoothed loss 4.90184, grad norm 3.83300, param norm 74.42096
epoch 7, iter 5965, loss 5.23747, smoothed loss 4.90360, grad norm 4.28297, param norm 74.43514
epoch 7, iter 5970, loss 4.84934, smoothed loss 4.89764, grad norm 4.37395, param norm 74.45135
epoch 7, iter 5975, loss 5.04677, smoothed loss 4.89696, grad norm 4.02004, param norm 74.47153
epoch 7, iter 5980, loss 5.16110, smoothed loss 4.90848, grad norm 4.35674, param norm 74.48772
epoch 7, iter 5985, loss 4.91773, smoothed loss 4.90757, grad norm 3.81968, param norm 74.50284
epoch 7, iter 5990, loss 4.41318, smoothed loss 4.91846, grad norm 3.52178, param norm 74.51909
epoch 7, iter 5995, loss 5.28362, smoothed loss 4.92584, grad norm 4.58229, param norm 74.53703
epoch 7, iter 6000, loss 4.65445, smoothed loss 4.93501, grad norm 4.38901, param norm 74.55501
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 6000, dev loss: 5.048331
Calculating Train F1/EM...
F1 train: 1000 examples took 9.18841 seconds [Score: 0.48395]
Exact Match train: 1000 examples took 9.24181 seconds [Score: 0.33800]
Epoch 7, Iter 6000, Train F1 score: 0.483946, Train EM score: 0.338000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.37075 seconds [Score: 0.33158]
Exact Match dev: 7118 examples took 58.56272 seconds [Score: 0.23181]
Epoch 7, Iter 6000, Dev F1 score: 0.331579, Dev EM score: 0.231807
End of epoch 7
epoch 7, iter 6005, loss 5.11310, smoothed loss 4.94486, grad norm 4.45944, param norm 74.57467
epoch 7, iter 6010, loss 4.76025, smoothed loss 4.94331, grad norm 3.90047, param norm 74.59502
epoch 7, iter 6015, loss 5.13707, smoothed loss 4.93338, grad norm 3.95311, param norm 74.61685
epoch 7, iter 6020, loss 5.15115, smoothed loss 4.94220, grad norm 4.66525, param norm 74.63489
epoch 7, iter 6025, loss 5.71968, smoothed loss 4.93668, grad norm 4.77759, param norm 74.64787
epoch 7, iter 6030, loss 4.93657, smoothed loss 4.92794, grad norm 4.22189, param norm 74.66206
epoch 7, iter 6035, loss 4.97030, smoothed loss 4.93381, grad norm 4.06368, param norm 74.68103
epoch 7, iter 6040, loss 4.75887, smoothed loss 4.90335, grad norm 4.02096, param norm 74.70480
epoch 7, iter 6045, loss 4.93950, smoothed loss 4.90883, grad norm 4.29091, param norm 74.72706
epoch 7, iter 6050, loss 4.96012, smoothed loss 4.91082, grad norm 4.51465, param norm 74.74577
epoch 7, iter 6055, loss 5.89583, smoothed loss 4.91368, grad norm 4.62792, param norm 74.76342
epoch 7, iter 6060, loss 4.61121, smoothed loss 4.90870, grad norm 4.03990, param norm 74.77636
epoch 7, iter 6065, loss 4.59930, smoothed loss 4.89245, grad norm 4.24266, param norm 74.79044
epoch 7, iter 6070, loss 4.70204, smoothed loss 4.88673, grad norm 3.46223, param norm 74.80799
epoch 7, iter 6075, loss 4.74760, smoothed loss 4.88369, grad norm 3.64335, param norm 74.82643
epoch 7, iter 6080, loss 4.64838, smoothed loss 4.88121, grad norm 4.10274, param norm 74.84631
epoch 7, iter 6085, loss 4.84314, smoothed loss 4.88919, grad norm 4.04804, param norm 74.86730
epoch 7, iter 6090, loss 5.22576, smoothed loss 4.88674, grad norm 3.31597, param norm 74.88716
epoch 7, iter 6095, loss 4.86615, smoothed loss 4.88338, grad norm 3.84583, param norm 74.90673
epoch 7, iter 6100, loss 4.55674, smoothed loss 4.87856, grad norm 4.27601, param norm 74.92510
epoch 7, iter 6105, loss 4.84264, smoothed loss 4.87993, grad norm 3.89849, param norm 74.94473
epoch 7, iter 6110, loss 5.15981, smoothed loss 4.87471, grad norm 4.12127, param norm 74.96187
epoch 7, iter 6115, loss 4.00403, smoothed loss 4.87232, grad norm 3.90769, param norm 74.97532
epoch 7, iter 6120, loss 4.22355, smoothed loss 4.87568, grad norm 3.65136, param norm 74.99064
epoch 7, iter 6125, loss 5.45613, smoothed loss 4.87508, grad norm 3.92738, param norm 75.00926
epoch 7, iter 6130, loss 4.80813, smoothed loss 4.86799, grad norm 4.11038, param norm 75.02676
epoch 7, iter 6135, loss 5.22867, smoothed loss 4.86079, grad norm 4.36705, param norm 75.04758
epoch 7, iter 6140, loss 4.25989, smoothed loss 4.84878, grad norm 4.55410, param norm 75.06930
epoch 7, iter 6145, loss 4.78675, smoothed loss 4.84389, grad norm 4.19037, param norm 75.08958
epoch 7, iter 6150, loss 5.20323, smoothed loss 4.83127, grad norm 4.86614, param norm 75.10672
epoch 7, iter 6155, loss 5.01515, smoothed loss 4.83298, grad norm 3.96302, param norm 75.12190
epoch 7, iter 6160, loss 4.80903, smoothed loss 4.83563, grad norm 4.28417, param norm 75.13934
epoch 7, iter 6165, loss 4.89229, smoothed loss 4.83038, grad norm 3.68692, param norm 75.15791
epoch 7, iter 6170, loss 4.48180, smoothed loss 4.83056, grad norm 4.23516, param norm 75.18090
epoch 7, iter 6175, loss 4.38530, smoothed loss 4.82293, grad norm 3.92116, param norm 75.19997
epoch 7, iter 6180, loss 4.81216, smoothed loss 4.82095, grad norm 3.96937, param norm 75.21853
epoch 7, iter 6185, loss 5.50871, smoothed loss 4.83025, grad norm 4.68398, param norm 75.23651
epoch 7, iter 6190, loss 4.56477, smoothed loss 4.82880, grad norm 3.66928, param norm 75.24922
epoch 7, iter 6195, loss 4.29052, smoothed loss 4.82269, grad norm 4.09701, param norm 75.26198
epoch 7, iter 6200, loss 5.00822, smoothed loss 4.83003, grad norm 3.74496, param norm 75.27687
epoch 7, iter 6205, loss 5.22283, smoothed loss 4.83190, grad norm 3.98287, param norm 75.29675
epoch 7, iter 6210, loss 5.11339, smoothed loss 4.84840, grad norm 4.27103, param norm 75.31617
epoch 7, iter 6215, loss 4.96188, smoothed loss 4.85047, grad norm 4.08478, param norm 75.33261
epoch 7, iter 6220, loss 5.19964, smoothed loss 4.84322, grad norm 4.11101, param norm 75.34838
epoch 7, iter 6225, loss 4.92471, smoothed loss 4.84373, grad norm 4.13540, param norm 75.36433
epoch 7, iter 6230, loss 5.06917, smoothed loss 4.84916, grad norm 4.93330, param norm 75.38336
epoch 7, iter 6235, loss 4.71968, smoothed loss 4.83990, grad norm 3.67893, param norm 75.40259
epoch 7, iter 6240, loss 3.86210, smoothed loss 4.82443, grad norm 3.70844, param norm 75.42145
epoch 7, iter 6245, loss 4.36719, smoothed loss 4.80171, grad norm 4.15802, param norm 75.43727
epoch 7, iter 6250, loss 4.62957, smoothed loss 4.80336, grad norm 3.81629, param norm 75.45287
epoch 7, iter 6255, loss 4.02669, smoothed loss 4.78403, grad norm 3.47258, param norm 75.46445
epoch 7, iter 6260, loss 5.28803, smoothed loss 4.79820, grad norm 4.38373, param norm 75.47498
epoch 7, iter 6265, loss 4.91297, smoothed loss 4.80104, grad norm 4.29387, param norm 75.48726
epoch 7, iter 6270, loss 4.65934, smoothed loss 4.81218, grad norm 4.41302, param norm 75.50128
epoch 7, iter 6275, loss 4.82382, smoothed loss 4.80164, grad norm 3.71578, param norm 75.51643
epoch 7, iter 6280, loss 4.77012, smoothed loss 4.79969, grad norm 4.24449, param norm 75.53724
epoch 7, iter 6285, loss 4.72529, smoothed loss 4.79144, grad norm 4.13744, param norm 75.55791
epoch 7, iter 6290, loss 4.41574, smoothed loss 4.79136, grad norm 4.26642, param norm 75.57465
epoch 7, iter 6295, loss 5.04574, smoothed loss 4.79382, grad norm 4.16748, param norm 75.58886
epoch 7, iter 6300, loss 5.13013, smoothed loss 4.80196, grad norm 5.10699, param norm 75.60149
epoch 7, iter 6305, loss 4.41214, smoothed loss 4.80253, grad norm 4.05652, param norm 75.61404
epoch 7, iter 6310, loss 5.07274, smoothed loss 4.79986, grad norm 4.25519, param norm 75.63014
epoch 7, iter 6315, loss 5.19130, smoothed loss 4.81547, grad norm 3.83674, param norm 75.64674
epoch 7, iter 6320, loss 4.95558, smoothed loss 4.82968, grad norm 3.87627, param norm 75.66232
epoch 7, iter 6325, loss 4.67841, smoothed loss 4.82055, grad norm 3.98421, param norm 75.67792
epoch 7, iter 6330, loss 4.65935, smoothed loss 4.83086, grad norm 3.41314, param norm 75.69303
epoch 7, iter 6335, loss 5.36017, smoothed loss 4.83233, grad norm 4.01979, param norm 75.70946
epoch 7, iter 6340, loss 4.39830, smoothed loss 4.82320, grad norm 3.39552, param norm 75.72856
epoch 7, iter 6345, loss 4.79357, smoothed loss 4.81422, grad norm 4.35817, param norm 75.75048
epoch 7, iter 6350, loss 5.14090, smoothed loss 4.81349, grad norm 3.93579, param norm 75.77078
epoch 7, iter 6355, loss 4.53789, smoothed loss 4.80683, grad norm 3.94469, param norm 75.79101
epoch 7, iter 6360, loss 5.21163, smoothed loss 4.82066, grad norm 4.20413, param norm 75.80445
epoch 7, iter 6365, loss 4.99600, smoothed loss 4.81603, grad norm 3.78202, param norm 75.81699
epoch 7, iter 6370, loss 4.67608, smoothed loss 4.80473, grad norm 3.59708, param norm 75.83678
epoch 7, iter 6375, loss 4.55123, smoothed loss 4.80823, grad norm 3.73112, param norm 75.85432
epoch 7, iter 6380, loss 5.02901, smoothed loss 4.82087, grad norm 3.50844, param norm 75.87009
epoch 7, iter 6385, loss 5.03971, smoothed loss 4.82617, grad norm 3.59332, param norm 75.88712
epoch 7, iter 6390, loss 4.59148, smoothed loss 4.82743, grad norm 3.82243, param norm 75.90581
epoch 7, iter 6395, loss 5.10641, smoothed loss 4.81904, grad norm 4.03132, param norm 75.92802
epoch 7, iter 6400, loss 4.60954, smoothed loss 4.80836, grad norm 3.64815, param norm 75.95034
epoch 7, iter 6405, loss 4.46609, smoothed loss 4.79695, grad norm 3.80097, param norm 75.97081
epoch 7, iter 6410, loss 5.76293, smoothed loss 4.80621, grad norm 4.60471, param norm 75.98978
epoch 7, iter 6415, loss 5.54203, smoothed loss 4.82541, grad norm 4.19165, param norm 76.00310
epoch 7, iter 6420, loss 5.66820, smoothed loss 4.83039, grad norm 4.67866, param norm 76.01620
epoch 7, iter 6425, loss 5.22392, smoothed loss 4.82943, grad norm 3.99217, param norm 76.03350
epoch 7, iter 6430, loss 4.80344, smoothed loss 4.82832, grad norm 4.04312, param norm 76.05247
epoch 7, iter 6435, loss 4.56241, smoothed loss 4.83670, grad norm 4.13643, param norm 76.06894
epoch 7, iter 6440, loss 4.68743, smoothed loss 4.84516, grad norm 4.32310, param norm 76.08270
epoch 7, iter 6445, loss 4.32561, smoothed loss 4.84685, grad norm 3.73296, param norm 76.09664
epoch 7, iter 6450, loss 4.82290, smoothed loss 4.84156, grad norm 4.21713, param norm 76.11389
epoch 7, iter 6455, loss 4.26733, smoothed loss 4.82681, grad norm 3.71603, param norm 76.13241
epoch 7, iter 6460, loss 4.69383, smoothed loss 4.82475, grad norm 3.98428, param norm 76.15192
epoch 7, iter 6465, loss 5.01516, smoothed loss 4.83088, grad norm 4.37796, param norm 76.16628
epoch 7, iter 6470, loss 4.69184, smoothed loss 4.83984, grad norm 4.13923, param norm 76.18365
epoch 7, iter 6475, loss 4.50186, smoothed loss 4.81961, grad norm 4.08705, param norm 76.20187
epoch 7, iter 6480, loss 4.14739, smoothed loss 4.81124, grad norm 3.85546, param norm 76.21865
epoch 7, iter 6485, loss 5.31411, smoothed loss 4.81714, grad norm 4.22442, param norm 76.23469
epoch 7, iter 6490, loss 4.75837, smoothed loss 4.81578, grad norm 4.16756, param norm 76.25198
epoch 7, iter 6495, loss 4.39936, smoothed loss 4.80581, grad norm 4.28255, param norm 76.27040
epoch 7, iter 6500, loss 3.63163, smoothed loss 4.80474, grad norm 3.57612, param norm 76.28922
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 6500, dev loss: 5.029021
Calculating Train F1/EM...
F1 train: 1000 examples took 8.80682 seconds [Score: 0.40743]
Exact Match train: 1000 examples took 9.36549 seconds [Score: 0.33900]
Epoch 7, Iter 6500, Train F1 score: 0.407429, Train EM score: 0.339000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.01541 seconds [Score: 0.33514]
Exact Match dev: 7118 examples took 58.47577 seconds [Score: 0.23532]
Epoch 7, Iter 6500, Dev F1 score: 0.335136, Dev EM score: 0.235319
End of epoch 7
epoch 7, iter 6505, loss 4.15886, smoothed loss 4.79840, grad norm 3.98968, param norm 76.30698
epoch 7, iter 6510, loss 4.09662, smoothed loss 4.79756, grad norm 3.85382, param norm 76.32419
epoch 7, iter 6515, loss 5.30845, smoothed loss 4.79143, grad norm 4.23693, param norm 76.34160
epoch 7, iter 6520, loss 4.74721, smoothed loss 4.80532, grad norm 4.12926, param norm 76.35634
epoch 7, iter 6525, loss 4.67310, smoothed loss 4.80607, grad norm 3.87953, param norm 76.37117
epoch 7, iter 6530, loss 5.25473, smoothed loss 4.82476, grad norm 4.45759, param norm 76.38409
epoch 7, iter 6535, loss 5.06190, smoothed loss 4.82397, grad norm 4.97748, param norm 76.39623
epoch 7, iter 6540, loss 4.65803, smoothed loss 4.82039, grad norm 4.22672, param norm 76.40892
epoch 7, iter 6545, loss 4.64702, smoothed loss 4.81713, grad norm 3.82186, param norm 76.42534
epoch 7, iter 6550, loss 4.62633, smoothed loss 4.81730, grad norm 3.91417, param norm 76.44532
epoch 7, iter 6555, loss 5.17750, smoothed loss 4.81210, grad norm 4.43665, param norm 76.46469
epoch 7, iter 6560, loss 4.56776, smoothed loss 4.81445, grad norm 3.77643, param norm 76.48187
epoch 7, iter 6565, loss 5.21787, smoothed loss 4.81470, grad norm 4.04037, param norm 76.49928
epoch 7, iter 6570, loss 4.46622, smoothed loss 4.80974, grad norm 4.03505, param norm 76.51463
epoch 7, iter 6575, loss 4.32280, smoothed loss 4.81122, grad norm 3.66273, param norm 76.52898
epoch 7, iter 6580, loss 4.38269, smoothed loss 4.81472, grad norm 4.41968, param norm 76.54560
epoch 7, iter 6585, loss 4.86539, smoothed loss 4.82624, grad norm 4.16098, param norm 76.56334
epoch 7, iter 6590, loss 5.34872, smoothed loss 4.83773, grad norm 4.74074, param norm 76.58188
epoch 7, iter 6595, loss 4.94205, smoothed loss 4.84260, grad norm 4.38933, param norm 76.59906
epoch 7, iter 6600, loss 4.44867, smoothed loss 4.83184, grad norm 3.73236, param norm 76.61929
epoch 7, iter 6605, loss 4.22975, smoothed loss 4.81667, grad norm 3.83090, param norm 76.63884
epoch 8, iter 6610, loss 5.31579, smoothed loss 4.82151, grad norm 4.32629, param norm 76.65598
epoch 8, iter 6615, loss 5.15841, smoothed loss 4.83366, grad norm 4.56588, param norm 76.66996
epoch 8, iter 6620, loss 4.79577, smoothed loss 4.81377, grad norm 4.01915, param norm 76.68406
epoch 8, iter 6625, loss 4.74288, smoothed loss 4.80581, grad norm 3.84307, param norm 76.70394
epoch 8, iter 6630, loss 5.47436, smoothed loss 4.83093, grad norm 3.48550, param norm 76.72231
epoch 8, iter 6635, loss 5.02277, smoothed loss 4.84083, grad norm 4.08594, param norm 76.73840
epoch 8, iter 6640, loss 4.85204, smoothed loss 4.83271, grad norm 4.82944, param norm 76.75377
epoch 8, iter 6645, loss 5.17290, smoothed loss 4.83381, grad norm 4.77835, param norm 76.77036
epoch 8, iter 6650, loss 4.11379, smoothed loss 4.81967, grad norm 4.24403, param norm 76.78565
epoch 8, iter 6655, loss 4.81459, smoothed loss 4.81911, grad norm 4.44580, param norm 76.80122
epoch 8, iter 6660, loss 4.89876, smoothed loss 4.81923, grad norm 3.89391, param norm 76.81712
epoch 8, iter 6665, loss 4.58189, smoothed loss 4.81287, grad norm 3.78925, param norm 76.83463
epoch 8, iter 6670, loss 4.46558, smoothed loss 4.80476, grad norm 3.80359, param norm 76.85394
epoch 8, iter 6675, loss 4.70814, smoothed loss 4.82476, grad norm 4.16325, param norm 76.87131
epoch 8, iter 6680, loss 4.21135, smoothed loss 4.82897, grad norm 3.68708, param norm 76.88533
epoch 8, iter 6685, loss 4.01166, smoothed loss 4.81138, grad norm 3.85450, param norm 76.90115
epoch 8, iter 6690, loss 5.41114, smoothed loss 4.83155, grad norm 4.45191, param norm 76.91608
epoch 8, iter 6695, loss 4.24297, smoothed loss 4.83986, grad norm 3.90171, param norm 76.92995
epoch 8, iter 6700, loss 4.61697, smoothed loss 4.83340, grad norm 4.26476, param norm 76.94556
epoch 8, iter 6705, loss 4.32508, smoothed loss 4.82446, grad norm 3.85391, param norm 76.96079
epoch 8, iter 6710, loss 5.07198, smoothed loss 4.81559, grad norm 4.18327, param norm 76.97801
epoch 8, iter 6715, loss 4.47414, smoothed loss 4.81105, grad norm 4.43653, param norm 76.99701
epoch 8, iter 6720, loss 5.06270, smoothed loss 4.81625, grad norm 4.08755, param norm 77.01764
epoch 8, iter 6725, loss 5.04785, smoothed loss 4.81475, grad norm 4.33549, param norm 77.04006
epoch 8, iter 6730, loss 4.76714, smoothed loss 4.80786, grad norm 4.82458, param norm 77.06249
epoch 8, iter 6735, loss 4.79031, smoothed loss 4.79470, grad norm 4.04928, param norm 77.08114
epoch 8, iter 6740, loss 4.83938, smoothed loss 4.78347, grad norm 4.56205, param norm 77.09882
epoch 8, iter 6745, loss 5.18009, smoothed loss 4.78663, grad norm 4.71811, param norm 77.11681
epoch 8, iter 6750, loss 5.22896, smoothed loss 4.79161, grad norm 4.10908, param norm 77.13068
epoch 8, iter 6755, loss 4.55240, smoothed loss 4.78424, grad norm 3.78670, param norm 77.14659
epoch 8, iter 6760, loss 4.50408, smoothed loss 4.78978, grad norm 4.30540, param norm 77.16386
epoch 8, iter 6765, loss 4.50407, smoothed loss 4.77844, grad norm 3.83275, param norm 77.18140
epoch 8, iter 6770, loss 4.75934, smoothed loss 4.77547, grad norm 4.26995, param norm 77.20188
epoch 8, iter 6775, loss 5.34537, smoothed loss 4.78312, grad norm 4.50937, param norm 77.21953
epoch 8, iter 6780, loss 5.14268, smoothed loss 4.77698, grad norm 4.72671, param norm 77.23719
epoch 8, iter 6785, loss 4.81079, smoothed loss 4.78235, grad norm 3.72667, param norm 77.25672
epoch 8, iter 6790, loss 4.90373, smoothed loss 4.79409, grad norm 4.04301, param norm 77.27612
epoch 8, iter 6795, loss 4.78372, smoothed loss 4.79758, grad norm 4.31661, param norm 77.29816
epoch 8, iter 6800, loss 4.74913, smoothed loss 4.79445, grad norm 4.41042, param norm 77.31779
epoch 8, iter 6805, loss 4.92032, smoothed loss 4.79838, grad norm 3.91894, param norm 77.33521
epoch 8, iter 6810, loss 4.26170, smoothed loss 4.78750, grad norm 3.60314, param norm 77.35484
epoch 8, iter 6815, loss 5.05629, smoothed loss 4.79031, grad norm 3.87657, param norm 77.37209
epoch 8, iter 6820, loss 5.69720, smoothed loss 4.81145, grad norm 3.85976, param norm 77.38503
epoch 8, iter 6825, loss 4.64304, smoothed loss 4.82491, grad norm 3.52271, param norm 77.39698
epoch 8, iter 6830, loss 4.56364, smoothed loss 4.81127, grad norm 4.02163, param norm 77.41211
epoch 8, iter 6835, loss 4.71783, smoothed loss 4.81073, grad norm 3.97284, param norm 77.42919
epoch 8, iter 6840, loss 4.44847, smoothed loss 4.79917, grad norm 4.12926, param norm 77.44650
epoch 8, iter 6845, loss 4.44458, smoothed loss 4.79043, grad norm 3.99485, param norm 77.46098
epoch 8, iter 6850, loss 3.91054, smoothed loss 4.78311, grad norm 4.25152, param norm 77.47688
epoch 8, iter 6855, loss 5.43139, smoothed loss 4.78776, grad norm 4.92622, param norm 77.49429
epoch 8, iter 6860, loss 4.81376, smoothed loss 4.80011, grad norm 4.36657, param norm 77.51063
epoch 8, iter 6865, loss 4.57923, smoothed loss 4.78421, grad norm 4.13403, param norm 77.52625
epoch 8, iter 6870, loss 4.27748, smoothed loss 4.78078, grad norm 3.82466, param norm 77.54156
epoch 8, iter 6875, loss 5.07672, smoothed loss 4.78156, grad norm 3.78500, param norm 77.55902
epoch 8, iter 6880, loss 4.75556, smoothed loss 4.77401, grad norm 4.65357, param norm 77.57986
epoch 8, iter 6885, loss 4.34251, smoothed loss 4.77758, grad norm 3.90794, param norm 77.59930
epoch 8, iter 6890, loss 4.77032, smoothed loss 4.76944, grad norm 4.19541, param norm 77.61548
epoch 8, iter 6895, loss 5.11521, smoothed loss 4.77014, grad norm 4.22505, param norm 77.63300
epoch 8, iter 6900, loss 4.60836, smoothed loss 4.75994, grad norm 4.32521, param norm 77.64912
epoch 8, iter 6905, loss 4.64812, smoothed loss 4.76379, grad norm 4.15671, param norm 77.66415
epoch 8, iter 6910, loss 5.36784, smoothed loss 4.78109, grad norm 4.59181, param norm 77.67780
epoch 8, iter 6915, loss 4.48643, smoothed loss 4.77998, grad norm 4.15640, param norm 77.69220
epoch 8, iter 6920, loss 5.11709, smoothed loss 4.76856, grad norm 4.20333, param norm 77.71181
epoch 8, iter 6925, loss 4.47855, smoothed loss 4.77661, grad norm 4.23097, param norm 77.72961
epoch 8, iter 6930, loss 4.73378, smoothed loss 4.78005, grad norm 4.34222, param norm 77.74448
epoch 8, iter 6935, loss 4.83559, smoothed loss 4.77515, grad norm 4.37857, param norm 77.75999
epoch 8, iter 6940, loss 5.00785, smoothed loss 4.77002, grad norm 4.20017, param norm 77.77548
epoch 8, iter 6945, loss 4.84126, smoothed loss 4.76338, grad norm 4.29603, param norm 77.79028
epoch 8, iter 6950, loss 4.78115, smoothed loss 4.75809, grad norm 3.91797, param norm 77.80724
epoch 8, iter 6955, loss 4.94396, smoothed loss 4.76596, grad norm 4.51771, param norm 77.82335
epoch 8, iter 6960, loss 4.81890, smoothed loss 4.75308, grad norm 4.39021, param norm 77.83823
epoch 8, iter 6965, loss 5.46572, smoothed loss 4.75899, grad norm 4.46334, param norm 77.85529
epoch 8, iter 6970, loss 5.18508, smoothed loss 4.77094, grad norm 4.10642, param norm 77.86942
epoch 8, iter 6975, loss 4.24288, smoothed loss 4.75622, grad norm 3.81577, param norm 77.88734
epoch 8, iter 6980, loss 4.88927, smoothed loss 4.74950, grad norm 4.04364, param norm 77.90842
epoch 8, iter 6985, loss 4.70711, smoothed loss 4.75808, grad norm 4.06972, param norm 77.92707
epoch 8, iter 6990, loss 4.83785, smoothed loss 4.75597, grad norm 4.07663, param norm 77.94498
epoch 8, iter 6995, loss 5.25363, smoothed loss 4.75667, grad norm 4.21122, param norm 77.95962
epoch 8, iter 7000, loss 5.59965, smoothed loss 4.76300, grad norm 4.46503, param norm 77.97156
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 8, Iter 7000, dev loss: 5.002495
Calculating Train F1/EM...
F1 train: 1000 examples took 9.14062 seconds [Score: 0.47222]
Exact Match train: 1000 examples took 8.87626 seconds [Score: 0.35100]
Epoch 8, Iter 7000, Train F1 score: 0.472217, Train EM score: 0.351000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.98766 seconds [Score: 0.33879]
Exact Match dev: 7118 examples took 58.45561 seconds [Score: 0.23925]
Epoch 8, Iter 7000, Dev F1 score: 0.338791, Dev EM score: 0.239253
End of epoch 8
epoch 8, iter 7005, loss 4.74624, smoothed loss 4.76538, grad norm 4.13444, param norm 77.98454
epoch 8, iter 7010, loss 5.10073, smoothed loss 4.76892, grad norm 4.61533, param norm 77.99930
epoch 8, iter 7015, loss 4.24659, smoothed loss 4.75438, grad norm 3.68292, param norm 78.01615
epoch 8, iter 7020, loss 3.75856, smoothed loss 4.75898, grad norm 4.00890, param norm 78.03622
epoch 8, iter 7025, loss 4.68343, smoothed loss 4.75005, grad norm 4.06043, param norm 78.05982
epoch 8, iter 7030, loss 4.63760, smoothed loss 4.74702, grad norm 4.26625, param norm 78.08404
epoch 8, iter 7035, loss 3.98849, smoothed loss 4.73616, grad norm 3.84712, param norm 78.10471
epoch 8, iter 7040, loss 5.00380, smoothed loss 4.74347, grad norm 4.40830, param norm 78.12240
epoch 8, iter 7045, loss 4.08272, smoothed loss 4.74157, grad norm 3.84942, param norm 78.13474
epoch 8, iter 7050, loss 4.98776, smoothed loss 4.74856, grad norm 4.46320, param norm 78.14705
epoch 8, iter 7055, loss 5.52210, smoothed loss 4.75567, grad norm 4.25510, param norm 78.16300
epoch 8, iter 7060, loss 4.16596, smoothed loss 4.74504, grad norm 3.73743, param norm 78.17860
epoch 8, iter 7065, loss 4.24954, smoothed loss 4.75394, grad norm 4.01666, param norm 78.19417
epoch 8, iter 7070, loss 4.74721, smoothed loss 4.75621, grad norm 3.99237, param norm 78.21305
epoch 8, iter 7075, loss 4.64006, smoothed loss 4.75141, grad norm 3.83201, param norm 78.23145
epoch 8, iter 7080, loss 4.79602, smoothed loss 4.77075, grad norm 4.21692, param norm 78.24483
epoch 8, iter 7085, loss 5.33431, smoothed loss 4.76817, grad norm 3.56225, param norm 78.25944
epoch 8, iter 7090, loss 4.17462, smoothed loss 4.76244, grad norm 3.99901, param norm 78.27502
epoch 8, iter 7095, loss 4.29500, smoothed loss 4.75919, grad norm 3.59598, param norm 78.29121
epoch 8, iter 7100, loss 5.01783, smoothed loss 4.76024, grad norm 4.48279, param norm 78.30828
epoch 8, iter 7105, loss 4.90749, smoothed loss 4.75857, grad norm 4.29909, param norm 78.32413
epoch 8, iter 7110, loss 4.53931, smoothed loss 4.76309, grad norm 3.79653, param norm 78.34007
epoch 8, iter 7115, loss 4.91435, smoothed loss 4.75935, grad norm 4.31763, param norm 78.35787
epoch 8, iter 7120, loss 5.17953, smoothed loss 4.76635, grad norm 4.29084, param norm 78.37316
epoch 8, iter 7125, loss 4.97058, smoothed loss 4.76372, grad norm 4.29058, param norm 78.38654
epoch 8, iter 7130, loss 3.96249, smoothed loss 4.74542, grad norm 4.10057, param norm 78.40176
epoch 8, iter 7135, loss 4.79678, smoothed loss 4.74431, grad norm 4.22552, param norm 78.41845
epoch 8, iter 7140, loss 4.39852, smoothed loss 4.72721, grad norm 3.94322, param norm 78.43748
epoch 8, iter 7145, loss 5.04035, smoothed loss 4.73386, grad norm 4.70161, param norm 78.45557
epoch 8, iter 7150, loss 4.44748, smoothed loss 4.73784, grad norm 4.27386, param norm 78.46681
epoch 8, iter 7155, loss 4.91956, smoothed loss 4.73766, grad norm 4.55756, param norm 78.47980
epoch 8, iter 7160, loss 4.80972, smoothed loss 4.72728, grad norm 3.94888, param norm 78.49511
epoch 8, iter 7165, loss 5.15804, smoothed loss 4.73185, grad norm 5.00089, param norm 78.51255
epoch 8, iter 7170, loss 4.18917, smoothed loss 4.73283, grad norm 4.02082, param norm 78.53024
epoch 8, iter 7175, loss 4.42252, smoothed loss 4.72370, grad norm 4.18074, param norm 78.55185
epoch 8, iter 7180, loss 4.58208, smoothed loss 4.72496, grad norm 4.55199, param norm 78.57371
epoch 8, iter 7185, loss 5.21671, smoothed loss 4.72223, grad norm 4.29740, param norm 78.59496
epoch 8, iter 7190, loss 4.73847, smoothed loss 4.71651, grad norm 4.28430, param norm 78.61366
epoch 8, iter 7195, loss 4.55110, smoothed loss 4.72234, grad norm 4.80489, param norm 78.62466
epoch 8, iter 7200, loss 5.47727, smoothed loss 4.73050, grad norm 3.94981, param norm 78.63453
epoch 8, iter 7205, loss 4.34469, smoothed loss 4.71425, grad norm 3.91650, param norm 78.64661
epoch 8, iter 7210, loss 5.05725, smoothed loss 4.71996, grad norm 4.90317, param norm 78.66241
epoch 8, iter 7215, loss 4.91440, smoothed loss 4.71604, grad norm 4.83041, param norm 78.67759
epoch 8, iter 7220, loss 4.22966, smoothed loss 4.71467, grad norm 4.09129, param norm 78.69147
epoch 8, iter 7225, loss 4.32607, smoothed loss 4.71564, grad norm 4.04138, param norm 78.70499
epoch 8, iter 7230, loss 4.52564, smoothed loss 4.70290, grad norm 3.90452, param norm 78.71870
epoch 8, iter 7235, loss 4.55258, smoothed loss 4.70407, grad norm 4.12217, param norm 78.73215
epoch 8, iter 7240, loss 4.69378, smoothed loss 4.70096, grad norm 3.87169, param norm 78.74432
epoch 8, iter 7245, loss 4.88737, smoothed loss 4.69222, grad norm 3.85339, param norm 78.75896
epoch 8, iter 7250, loss 4.72262, smoothed loss 4.67969, grad norm 4.25477, param norm 78.77215
epoch 8, iter 7255, loss 4.06545, smoothed loss 4.67221, grad norm 3.91824, param norm 78.78837
epoch 8, iter 7260, loss 3.76657, smoothed loss 4.67681, grad norm 3.64405, param norm 78.80446
epoch 8, iter 7265, loss 5.03062, smoothed loss 4.68440, grad norm 3.94741, param norm 78.81887
epoch 8, iter 7270, loss 4.40560, smoothed loss 4.67615, grad norm 4.06993, param norm 78.83542
epoch 8, iter 7275, loss 5.36610, smoothed loss 4.68585, grad norm 4.13579, param norm 78.85339
epoch 8, iter 7280, loss 5.28805, smoothed loss 4.69917, grad norm 4.35413, param norm 78.86826
epoch 8, iter 7285, loss 4.71982, smoothed loss 4.69825, grad norm 4.03105, param norm 78.88023
epoch 8, iter 7290, loss 4.44736, smoothed loss 4.69169, grad norm 3.78899, param norm 78.89240
epoch 8, iter 7295, loss 4.81596, smoothed loss 4.67662, grad norm 4.18657, param norm 78.90545
epoch 8, iter 7300, loss 4.01927, smoothed loss 4.70190, grad norm 3.93666, param norm 78.91850
epoch 8, iter 7305, loss 5.58228, smoothed loss 4.71100, grad norm 3.73239, param norm 78.93252
epoch 8, iter 7310, loss 4.93094, smoothed loss 4.72085, grad norm 4.61945, param norm 78.94858
epoch 8, iter 7315, loss 4.88992, smoothed loss 4.71342, grad norm 4.32551, param norm 78.96512
epoch 8, iter 7320, loss 4.75492, smoothed loss 4.70858, grad norm 3.93798, param norm 78.97978
epoch 8, iter 7325, loss 5.20033, smoothed loss 4.72027, grad norm 5.14040, param norm 78.99471
epoch 8, iter 7330, loss 4.88305, smoothed loss 4.72386, grad norm 4.16036, param norm 79.00753
epoch 8, iter 7335, loss 4.63878, smoothed loss 4.72950, grad norm 4.22603, param norm 79.02103
epoch 8, iter 7340, loss 4.60011, smoothed loss 4.72716, grad norm 4.00836, param norm 79.03719
epoch 8, iter 7345, loss 4.86057, smoothed loss 4.72204, grad norm 4.02622, param norm 79.05554
epoch 8, iter 7350, loss 4.56674, smoothed loss 4.73242, grad norm 3.88124, param norm 79.07338
epoch 8, iter 7355, loss 4.69053, smoothed loss 4.72251, grad norm 4.17881, param norm 79.09194
epoch 8, iter 7360, loss 4.66595, smoothed loss 4.71849, grad norm 4.24681, param norm 79.11177
epoch 8, iter 7365, loss 4.55473, smoothed loss 4.71065, grad norm 3.74492, param norm 79.13393
epoch 8, iter 7370, loss 4.62541, smoothed loss 4.71778, grad norm 4.36998, param norm 79.15289
epoch 8, iter 7375, loss 4.19742, smoothed loss 4.71242, grad norm 3.72492, param norm 79.16753
epoch 8, iter 7380, loss 4.62421, smoothed loss 4.70583, grad norm 5.07825, param norm 79.18120
epoch 8, iter 7385, loss 4.16847, smoothed loss 4.70418, grad norm 3.66194, param norm 79.19541
epoch 8, iter 7390, loss 5.12044, smoothed loss 4.70920, grad norm 4.76818, param norm 79.21121
epoch 8, iter 7395, loss 4.45650, smoothed loss 4.70481, grad norm 4.46476, param norm 79.22797
epoch 8, iter 7400, loss 4.33437, smoothed loss 4.70096, grad norm 3.73093, param norm 79.24580
epoch 8, iter 7405, loss 4.63141, smoothed loss 4.70066, grad norm 3.95971, param norm 79.26267
epoch 8, iter 7410, loss 4.34408, smoothed loss 4.69555, grad norm 4.12765, param norm 79.27782
epoch 8, iter 7415, loss 4.58363, smoothed loss 4.68649, grad norm 3.87964, param norm 79.29329
epoch 8, iter 7420, loss 4.38196, smoothed loss 4.66023, grad norm 4.47558, param norm 79.31071
epoch 8, iter 7425, loss 4.94389, smoothed loss 4.66989, grad norm 4.16139, param norm 79.32935
epoch 8, iter 7430, loss 4.97270, smoothed loss 4.66963, grad norm 4.37141, param norm 79.34486
epoch 8, iter 7435, loss 4.77717, smoothed loss 4.67110, grad norm 4.27268, param norm 79.36142
epoch 8, iter 7440, loss 4.66097, smoothed loss 4.67880, grad norm 3.98225, param norm 79.37634
epoch 8, iter 7445, loss 4.59478, smoothed loss 4.68077, grad norm 4.04031, param norm 79.39218
epoch 8, iter 7450, loss 5.08920, smoothed loss 4.67136, grad norm 4.38534, param norm 79.40969
epoch 8, iter 7455, loss 4.92629, smoothed loss 4.68116, grad norm 5.40136, param norm 79.42303
epoch 8, iter 7460, loss 4.48665, smoothed loss 4.68943, grad norm 4.69513, param norm 79.43663
epoch 8, iter 7465, loss 4.81591, smoothed loss 4.68918, grad norm 4.78060, param norm 79.45319
epoch 8, iter 7470, loss 4.98039, smoothed loss 4.69548, grad norm 4.09725, param norm 79.47157
epoch 8, iter 7475, loss 4.83885, smoothed loss 4.69385, grad norm 4.21263, param norm 79.48936
epoch 8, iter 7480, loss 4.81896, smoothed loss 4.69476, grad norm 3.92709, param norm 79.50678
epoch 8, iter 7485, loss 4.99246, smoothed loss 4.69319, grad norm 4.68207, param norm 79.52393
epoch 8, iter 7490, loss 4.71070, smoothed loss 4.69009, grad norm 4.69678, param norm 79.53930
epoch 8, iter 7495, loss 4.88520, smoothed loss 4.67511, grad norm 3.95285, param norm 79.55643
epoch 8, iter 7500, loss 5.19395, smoothed loss 4.68942, grad norm 4.43674, param norm 79.57283
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 8, Iter 7500, dev loss: 5.009585
Calculating Train F1/EM...
F1 train: 1000 examples took 9.10226 seconds [Score: 0.43552]
Exact Match train: 1000 examples took 9.20501 seconds [Score: 0.33000]
Epoch 8, Iter 7500, Train F1 score: 0.435517, Train EM score: 0.330000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.50945 seconds [Score: 0.34259]
Exact Match dev: 7118 examples took 58.22151 seconds [Score: 0.24291]
Epoch 8, Iter 7500, Dev F1 score: 0.342590, Dev EM score: 0.242905
End of epoch 8
epoch 8, iter 7505, loss 5.07313, smoothed loss 4.70037, grad norm 4.17325, param norm 79.58846
epoch 8, iter 7510, loss 4.61600, smoothed loss 4.70406, grad norm 4.61559, param norm 79.60195
epoch 8, iter 7515, loss 5.01111, smoothed loss 4.69611, grad norm 4.17425, param norm 79.61839
epoch 8, iter 7520, loss 4.94663, smoothed loss 4.71015, grad norm 4.28184, param norm 79.63258
epoch 8, iter 7525, loss 5.12746, smoothed loss 4.71133, grad norm 3.85149, param norm 79.64728
epoch 8, iter 7530, loss 4.62887, smoothed loss 4.70928, grad norm 3.87656, param norm 79.66724
epoch 8, iter 7535, loss 5.45416, smoothed loss 4.73017, grad norm 4.69314, param norm 79.68534
epoch 8, iter 7540, loss 4.73909, smoothed loss 4.72828, grad norm 4.44606, param norm 79.70141
epoch 8, iter 7545, loss 5.15768, smoothed loss 4.73861, grad norm 4.29156, param norm 79.71677
epoch 8, iter 7550, loss 5.17882, smoothed loss 4.74035, grad norm 4.54986, param norm 79.73122
epoch 9, iter 7555, loss 4.88888, smoothed loss 4.74254, grad norm 3.82095, param norm 79.74818
epoch 9, iter 7560, loss 4.94979, smoothed loss 4.72755, grad norm 3.77509, param norm 79.76900
epoch 9, iter 7565, loss 4.32573, smoothed loss 4.72720, grad norm 4.61934, param norm 79.79240
epoch 9, iter 7570, loss 4.18469, smoothed loss 4.72245, grad norm 3.80996, param norm 79.81191
epoch 9, iter 7575, loss 4.02079, smoothed loss 4.73070, grad norm 4.39301, param norm 79.82703
epoch 9, iter 7580, loss 4.44171, smoothed loss 4.73985, grad norm 3.86013, param norm 79.84122
epoch 9, iter 7585, loss 4.21242, smoothed loss 4.73087, grad norm 3.79482, param norm 79.85618
epoch 9, iter 7590, loss 4.47122, smoothed loss 4.73412, grad norm 4.20602, param norm 79.87096
epoch 9, iter 7595, loss 4.67692, smoothed loss 4.72956, grad norm 4.36271, param norm 79.88676
epoch 9, iter 7600, loss 4.79985, smoothed loss 4.73407, grad norm 4.01150, param norm 79.90188
epoch 9, iter 7605, loss 4.56095, smoothed loss 4.72563, grad norm 3.91478, param norm 79.91843
epoch 9, iter 7610, loss 4.29162, smoothed loss 4.71898, grad norm 3.97891, param norm 79.93578
epoch 9, iter 7615, loss 4.69381, smoothed loss 4.71186, grad norm 4.59670, param norm 79.95308
epoch 9, iter 7620, loss 5.16647, smoothed loss 4.71268, grad norm 4.68468, param norm 79.96752
epoch 9, iter 7625, loss 4.41364, smoothed loss 4.70741, grad norm 4.48744, param norm 79.98523
epoch 9, iter 7630, loss 4.10950, smoothed loss 4.70108, grad norm 3.72457, param norm 80.00002
epoch 9, iter 7635, loss 4.71371, smoothed loss 4.69271, grad norm 4.17513, param norm 80.01747
epoch 9, iter 7640, loss 5.33002, smoothed loss 4.70875, grad norm 5.08888, param norm 80.03323
epoch 9, iter 7645, loss 4.09847, smoothed loss 4.69749, grad norm 4.03467, param norm 80.04691
epoch 9, iter 7650, loss 4.34548, smoothed loss 4.68610, grad norm 4.00498, param norm 80.06345
epoch 9, iter 7655, loss 4.79630, smoothed loss 4.69092, grad norm 3.83668, param norm 80.08000
epoch 9, iter 7660, loss 4.90400, smoothed loss 4.68011, grad norm 4.50623, param norm 80.09814
epoch 9, iter 7665, loss 5.60951, smoothed loss 4.68255, grad norm 5.01397, param norm 80.11819
epoch 9, iter 7670, loss 3.89219, smoothed loss 4.66741, grad norm 3.90403, param norm 80.13639
epoch 9, iter 7675, loss 5.25119, smoothed loss 4.67088, grad norm 4.08827, param norm 80.15272
epoch 9, iter 7680, loss 4.41390, smoothed loss 4.67246, grad norm 4.58749, param norm 80.16583
epoch 9, iter 7685, loss 4.67627, smoothed loss 4.69646, grad norm 3.70269, param norm 80.17709
epoch 9, iter 7690, loss 4.70374, smoothed loss 4.68755, grad norm 3.99412, param norm 80.18755
epoch 9, iter 7695, loss 4.73335, smoothed loss 4.69134, grad norm 4.42609, param norm 80.20005
epoch 9, iter 7700, loss 4.46810, smoothed loss 4.68727, grad norm 4.26237, param norm 80.21211
epoch 9, iter 7705, loss 4.64146, smoothed loss 4.69353, grad norm 4.43266, param norm 80.22782
epoch 9, iter 7710, loss 4.40072, smoothed loss 4.68109, grad norm 3.92286, param norm 80.24369
epoch 9, iter 7715, loss 4.62405, smoothed loss 4.68497, grad norm 4.27959, param norm 80.26064
epoch 9, iter 7720, loss 4.53831, smoothed loss 4.68495, grad norm 4.19320, param norm 80.27688
epoch 9, iter 7725, loss 3.84010, smoothed loss 4.67926, grad norm 4.02207, param norm 80.29060
epoch 9, iter 7730, loss 4.98436, smoothed loss 4.68595, grad norm 4.54056, param norm 80.30293
epoch 9, iter 7735, loss 5.04673, smoothed loss 4.67496, grad norm 4.09452, param norm 80.32042
epoch 9, iter 7740, loss 4.80464, smoothed loss 4.65665, grad norm 4.16597, param norm 80.34021
epoch 9, iter 7745, loss 4.11372, smoothed loss 4.65954, grad norm 3.93944, param norm 80.35857
epoch 9, iter 7750, loss 4.63886, smoothed loss 4.64461, grad norm 4.14608, param norm 80.37567
epoch 9, iter 7755, loss 4.61788, smoothed loss 4.64696, grad norm 3.91109, param norm 80.39207
epoch 9, iter 7760, loss 4.83345, smoothed loss 4.64612, grad norm 4.81050, param norm 80.40976
epoch 9, iter 7765, loss 4.15631, smoothed loss 4.66730, grad norm 3.53833, param norm 80.42532
epoch 9, iter 7770, loss 4.22464, smoothed loss 4.65993, grad norm 4.02602, param norm 80.43985
epoch 9, iter 7775, loss 4.83456, smoothed loss 4.67405, grad norm 4.40250, param norm 80.45574
epoch 9, iter 7780, loss 4.46073, smoothed loss 4.67209, grad norm 3.92763, param norm 80.47096
epoch 9, iter 7785, loss 4.55515, smoothed loss 4.67419, grad norm 4.05907, param norm 80.48438
epoch 9, iter 7790, loss 5.28159, smoothed loss 4.68648, grad norm 5.27468, param norm 80.49754
epoch 9, iter 7795, loss 4.59932, smoothed loss 4.68085, grad norm 4.81727, param norm 80.51331
epoch 9, iter 7800, loss 4.71811, smoothed loss 4.68911, grad norm 4.17297, param norm 80.52611
epoch 9, iter 7805, loss 4.47946, smoothed loss 4.68331, grad norm 4.42288, param norm 80.53936
epoch 9, iter 7810, loss 4.45861, smoothed loss 4.69304, grad norm 3.86487, param norm 80.55413
epoch 9, iter 7815, loss 5.43177, smoothed loss 4.70116, grad norm 3.75554, param norm 80.56887
epoch 9, iter 7820, loss 4.20355, smoothed loss 4.69453, grad norm 3.91389, param norm 80.58479
epoch 9, iter 7825, loss 5.13631, smoothed loss 4.69949, grad norm 4.21325, param norm 80.60175
epoch 9, iter 7830, loss 5.28149, smoothed loss 4.70817, grad norm 4.78715, param norm 80.61855
epoch 9, iter 7835, loss 4.80846, smoothed loss 4.70288, grad norm 4.57986, param norm 80.63435
epoch 9, iter 7840, loss 4.67817, smoothed loss 4.69666, grad norm 4.20235, param norm 80.64990
epoch 9, iter 7845, loss 4.70587, smoothed loss 4.69792, grad norm 4.21777, param norm 80.66499
epoch 9, iter 7850, loss 5.12226, smoothed loss 4.70977, grad norm 4.79635, param norm 80.68115
epoch 9, iter 7855, loss 4.32859, smoothed loss 4.70869, grad norm 4.23293, param norm 80.69632
epoch 9, iter 7860, loss 4.90140, smoothed loss 4.73297, grad norm 4.44684, param norm 80.71117
epoch 9, iter 7865, loss 4.43272, smoothed loss 4.73405, grad norm 4.17855, param norm 80.72278
epoch 9, iter 7870, loss 4.43873, smoothed loss 4.72172, grad norm 3.86364, param norm 80.73835
epoch 9, iter 7875, loss 5.48826, smoothed loss 4.72179, grad norm 4.53085, param norm 80.75446
epoch 9, iter 7880, loss 4.86013, smoothed loss 4.70956, grad norm 4.49180, param norm 80.76875
epoch 9, iter 7885, loss 4.44886, smoothed loss 4.70186, grad norm 4.32893, param norm 80.78428
epoch 9, iter 7890, loss 4.38517, smoothed loss 4.70635, grad norm 4.39508, param norm 80.79945
epoch 9, iter 7895, loss 4.73785, smoothed loss 4.70579, grad norm 3.79116, param norm 80.81540
epoch 9, iter 7900, loss 4.39032, smoothed loss 4.69669, grad norm 4.64479, param norm 80.83269
epoch 9, iter 7905, loss 4.34635, smoothed loss 4.69483, grad norm 3.90262, param norm 80.85309
epoch 9, iter 7910, loss 4.64255, smoothed loss 4.69321, grad norm 3.63121, param norm 80.87104
epoch 9, iter 7915, loss 4.49163, smoothed loss 4.67447, grad norm 4.05097, param norm 80.88859
epoch 9, iter 7920, loss 4.34054, smoothed loss 4.67237, grad norm 4.19005, param norm 80.90751
epoch 9, iter 7925, loss 4.18407, smoothed loss 4.66211, grad norm 3.98894, param norm 80.92693
epoch 9, iter 7930, loss 5.02383, smoothed loss 4.65859, grad norm 4.50072, param norm 80.94737
epoch 9, iter 7935, loss 5.59268, smoothed loss 4.68507, grad norm 3.94902, param norm 80.96403
epoch 9, iter 7940, loss 4.32290, smoothed loss 4.67623, grad norm 4.47265, param norm 80.97879
epoch 9, iter 7945, loss 4.71241, smoothed loss 4.66435, grad norm 4.20100, param norm 80.99520
epoch 9, iter 7950, loss 5.39542, smoothed loss 4.66987, grad norm 4.72206, param norm 81.01166
epoch 9, iter 7955, loss 4.43380, smoothed loss 4.66431, grad norm 4.62480, param norm 81.02447
epoch 9, iter 7960, loss 4.95738, smoothed loss 4.65838, grad norm 4.12141, param norm 81.03598
epoch 9, iter 7965, loss 4.30599, smoothed loss 4.66729, grad norm 4.78287, param norm 81.04773
epoch 9, iter 7970, loss 4.48021, smoothed loss 4.66756, grad norm 3.90936, param norm 81.05990
epoch 9, iter 7975, loss 4.43007, smoothed loss 4.65625, grad norm 4.17866, param norm 81.07491
epoch 9, iter 7980, loss 4.23191, smoothed loss 4.64920, grad norm 4.06153, param norm 81.09066
epoch 9, iter 7985, loss 4.12875, smoothed loss 4.64968, grad norm 4.06577, param norm 81.10431
epoch 9, iter 7990, loss 4.75768, smoothed loss 4.64601, grad norm 4.02198, param norm 81.11852
epoch 9, iter 7995, loss 5.37324, smoothed loss 4.65816, grad norm 4.86169, param norm 81.13071
epoch 9, iter 8000, loss 4.64570, smoothed loss 4.65989, grad norm 4.33461, param norm 81.14214
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 9, Iter 8000, dev loss: 4.977103
Calculating Train F1/EM...
F1 train: 1000 examples took 9.15977 seconds [Score: 0.47791]
Exact Match train: 1000 examples took 8.68712 seconds [Score: 0.34300]
Epoch 9, Iter 8000, Train F1 score: 0.477913, Train EM score: 0.343000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.01732 seconds [Score: 0.34021]
Exact Match dev: 7118 examples took 57.94038 seconds [Score: 0.23855]
Epoch 9, Iter 8000, Dev F1 score: 0.340213, Dev EM score: 0.238550
End of epoch 9
epoch 9, iter 8005, loss 5.58718, smoothed loss 4.68401, grad norm 5.20366, param norm 81.15842
epoch 9, iter 8010, loss 5.32338, smoothed loss 4.70298, grad norm 4.49009, param norm 81.17257
epoch 9, iter 8015, loss 4.19265, smoothed loss 4.68823, grad norm 3.62565, param norm 81.18755
epoch 9, iter 8020, loss 4.23735, smoothed loss 4.67775, grad norm 3.83302, param norm 81.20554
epoch 9, iter 8025, loss 4.75733, smoothed loss 4.68080, grad norm 4.32268, param norm 81.22375
epoch 9, iter 8030, loss 4.38319, smoothed loss 4.66674, grad norm 4.30213, param norm 81.24151
epoch 9, iter 8035, loss 4.10390, smoothed loss 4.65696, grad norm 4.32270, param norm 81.26049
epoch 9, iter 8040, loss 4.48713, smoothed loss 4.64930, grad norm 4.42102, param norm 81.27734
epoch 9, iter 8045, loss 4.80515, smoothed loss 4.63613, grad norm 4.53056, param norm 81.29129
epoch 9, iter 8050, loss 4.65827, smoothed loss 4.62287, grad norm 4.13058, param norm 81.30565
epoch 9, iter 8055, loss 4.54936, smoothed loss 4.62706, grad norm 4.35317, param norm 81.31727
epoch 9, iter 8060, loss 4.69782, smoothed loss 4.63495, grad norm 4.07489, param norm 81.32742
epoch 9, iter 8065, loss 4.65843, smoothed loss 4.62309, grad norm 3.87840, param norm 81.34097
epoch 9, iter 8070, loss 4.30380, smoothed loss 4.61441, grad norm 4.09384, param norm 81.35558
epoch 9, iter 8075, loss 3.49716, smoothed loss 4.60298, grad norm 3.89835, param norm 81.37334
epoch 9, iter 8080, loss 4.93683, smoothed loss 4.60899, grad norm 4.36891, param norm 81.39192
epoch 9, iter 8085, loss 4.52557, smoothed loss 4.61611, grad norm 4.07095, param norm 81.40620
epoch 9, iter 8090, loss 4.42743, smoothed loss 4.61789, grad norm 4.10164, param norm 81.41822
epoch 9, iter 8095, loss 4.60646, smoothed loss 4.61025, grad norm 4.37059, param norm 81.43153
epoch 9, iter 8100, loss 4.74513, smoothed loss 4.59817, grad norm 4.17182, param norm 81.44655
epoch 9, iter 8105, loss 4.56674, smoothed loss 4.59589, grad norm 4.75642, param norm 81.46285
epoch 9, iter 8110, loss 4.34249, smoothed loss 4.58594, grad norm 3.89298, param norm 81.48045
epoch 9, iter 8115, loss 4.64287, smoothed loss 4.60045, grad norm 4.49902, param norm 81.49812
epoch 9, iter 8120, loss 4.16787, smoothed loss 4.60938, grad norm 3.82688, param norm 81.51142
epoch 9, iter 8125, loss 5.06719, smoothed loss 4.61624, grad norm 4.77594, param norm 81.52586
epoch 9, iter 8130, loss 4.34595, smoothed loss 4.61878, grad norm 4.13666, param norm 81.54027
epoch 9, iter 8135, loss 5.10724, smoothed loss 4.62077, grad norm 4.52803, param norm 81.55183
epoch 9, iter 8140, loss 3.77496, smoothed loss 4.61082, grad norm 3.76261, param norm 81.56194
epoch 9, iter 8145, loss 4.38040, smoothed loss 4.60395, grad norm 4.55274, param norm 81.57689
epoch 9, iter 8150, loss 4.15222, smoothed loss 4.60422, grad norm 4.63630, param norm 81.59275
epoch 9, iter 8155, loss 4.99684, smoothed loss 4.61063, grad norm 4.05422, param norm 81.60964
epoch 9, iter 8160, loss 4.98591, smoothed loss 4.59711, grad norm 5.40197, param norm 81.62557
epoch 9, iter 8165, loss 4.47965, smoothed loss 4.60105, grad norm 4.16485, param norm 81.64143
epoch 9, iter 8170, loss 5.25136, smoothed loss 4.62235, grad norm 5.15711, param norm 81.65647
epoch 9, iter 8175, loss 3.86378, smoothed loss 4.62319, grad norm 4.10789, param norm 81.66846
epoch 9, iter 8180, loss 4.73005, smoothed loss 4.62828, grad norm 4.20521, param norm 81.68569
epoch 9, iter 8185, loss 4.40231, smoothed loss 4.62462, grad norm 4.15832, param norm 81.70378
epoch 9, iter 8190, loss 4.41609, smoothed loss 4.61260, grad norm 4.68481, param norm 81.72193
epoch 9, iter 8195, loss 4.38515, smoothed loss 4.60553, grad norm 4.29257, param norm 81.73992
epoch 9, iter 8200, loss 5.12931, smoothed loss 4.60183, grad norm 4.87442, param norm 81.75719
epoch 9, iter 8205, loss 3.80264, smoothed loss 4.60006, grad norm 5.07172, param norm 81.77215
epoch 9, iter 8210, loss 4.75765, smoothed loss 4.60225, grad norm 4.59876, param norm 81.78513
epoch 9, iter 8215, loss 4.40302, smoothed loss 4.60369, grad norm 4.23670, param norm 81.79716
epoch 9, iter 8220, loss 4.32553, smoothed loss 4.58854, grad norm 3.96893, param norm 81.81087
epoch 9, iter 8225, loss 4.75233, smoothed loss 4.60134, grad norm 4.20540, param norm 81.82438
epoch 9, iter 8230, loss 4.56752, smoothed loss 4.60057, grad norm 3.85193, param norm 81.83899
epoch 9, iter 8235, loss 4.87596, smoothed loss 4.60893, grad norm 3.93910, param norm 81.85351
epoch 9, iter 8240, loss 4.51912, smoothed loss 4.59890, grad norm 3.79834, param norm 81.86713
epoch 9, iter 8245, loss 4.85820, smoothed loss 4.59713, grad norm 4.07366, param norm 81.88208
epoch 9, iter 8250, loss 4.40480, smoothed loss 4.60065, grad norm 3.69843, param norm 81.89900
epoch 9, iter 8255, loss 4.74573, smoothed loss 4.60534, grad norm 4.32705, param norm 81.91613
epoch 9, iter 8260, loss 4.93245, smoothed loss 4.60714, grad norm 4.77702, param norm 81.92997
epoch 9, iter 8265, loss 5.11339, smoothed loss 4.60513, grad norm 4.74426, param norm 81.94329
epoch 9, iter 8270, loss 4.31170, smoothed loss 4.61267, grad norm 4.50316, param norm 81.95851
epoch 9, iter 8275, loss 4.75464, smoothed loss 4.60758, grad norm 4.16221, param norm 81.97239
epoch 9, iter 8280, loss 3.75194, smoothed loss 4.59081, grad norm 3.73348, param norm 81.98865
epoch 9, iter 8285, loss 5.12952, smoothed loss 4.59726, grad norm 4.54197, param norm 82.00809
epoch 9, iter 8290, loss 4.33739, smoothed loss 4.59762, grad norm 3.70825, param norm 82.02341
epoch 9, iter 8295, loss 4.74245, smoothed loss 4.60611, grad norm 4.35542, param norm 82.03795
epoch 9, iter 8300, loss 4.24332, smoothed loss 4.61551, grad norm 4.29280, param norm 82.04844
epoch 9, iter 8305, loss 5.47348, smoothed loss 4.62448, grad norm 3.89454, param norm 82.05893
epoch 9, iter 8310, loss 4.04552, smoothed loss 4.62022, grad norm 4.21690, param norm 82.07079
epoch 9, iter 8315, loss 4.44771, smoothed loss 4.61229, grad norm 4.23078, param norm 82.08665
epoch 9, iter 8320, loss 3.95776, smoothed loss 4.61701, grad norm 4.23630, param norm 82.10281
epoch 9, iter 8325, loss 4.90472, smoothed loss 4.61905, grad norm 4.61346, param norm 82.11765
epoch 9, iter 8330, loss 5.18313, smoothed loss 4.64407, grad norm 5.21864, param norm 82.12845
epoch 9, iter 8335, loss 4.52850, smoothed loss 4.64320, grad norm 3.92831, param norm 82.13940
epoch 9, iter 8340, loss 4.99499, smoothed loss 4.64744, grad norm 4.59102, param norm 82.15432
epoch 9, iter 8345, loss 4.59986, smoothed loss 4.64245, grad norm 3.89732, param norm 82.17040
epoch 9, iter 8350, loss 4.79135, smoothed loss 4.64559, grad norm 3.93851, param norm 82.18613
epoch 9, iter 8355, loss 4.85098, smoothed loss 4.65447, grad norm 4.46041, param norm 82.19855
epoch 9, iter 8360, loss 4.45733, smoothed loss 4.64437, grad norm 4.02752, param norm 82.21074
epoch 9, iter 8365, loss 4.85618, smoothed loss 4.63945, grad norm 4.24325, param norm 82.22182
epoch 9, iter 8370, loss 4.52544, smoothed loss 4.64489, grad norm 4.86278, param norm 82.23607
epoch 9, iter 8375, loss 5.00573, smoothed loss 4.64841, grad norm 4.48967, param norm 82.25313
epoch 9, iter 8380, loss 4.48728, smoothed loss 4.64524, grad norm 3.90057, param norm 82.26930
epoch 9, iter 8385, loss 4.68215, smoothed loss 4.63565, grad norm 4.57017, param norm 82.28624
epoch 9, iter 8390, loss 4.90805, smoothed loss 4.63374, grad norm 4.18552, param norm 82.30379
epoch 9, iter 8395, loss 4.37164, smoothed loss 4.63513, grad norm 4.82120, param norm 82.31792
epoch 9, iter 8400, loss 4.65654, smoothed loss 4.63123, grad norm 4.24846, param norm 82.33353
epoch 9, iter 8405, loss 4.33333, smoothed loss 4.63379, grad norm 4.55636, param norm 82.35162
epoch 9, iter 8410, loss 4.38734, smoothed loss 4.62833, grad norm 4.41865, param norm 82.36944
epoch 9, iter 8415, loss 4.87041, smoothed loss 4.63484, grad norm 4.26013, param norm 82.38791
epoch 9, iter 8420, loss 4.05370, smoothed loss 4.62839, grad norm 3.79980, param norm 82.40337
epoch 9, iter 8425, loss 5.37784, smoothed loss 4.64210, grad norm 4.25593, param norm 82.41642
epoch 9, iter 8430, loss 4.88801, smoothed loss 4.64444, grad norm 4.85932, param norm 82.43088
epoch 9, iter 8435, loss 4.26416, smoothed loss 4.63151, grad norm 4.05369, param norm 82.44805
epoch 9, iter 8440, loss 4.30416, smoothed loss 4.62823, grad norm 4.15380, param norm 82.46484
epoch 9, iter 8445, loss 3.48033, smoothed loss 4.61139, grad norm 3.50056, param norm 82.48257
epoch 9, iter 8450, loss 4.56491, smoothed loss 4.62120, grad norm 4.45454, param norm 82.50064
epoch 9, iter 8455, loss 4.07169, smoothed loss 4.61119, grad norm 3.80961, param norm 82.51725
epoch 9, iter 8460, loss 4.98202, smoothed loss 4.61466, grad norm 4.23984, param norm 82.53532
epoch 9, iter 8465, loss 4.65561, smoothed loss 4.61743, grad norm 4.34993, param norm 82.54857
epoch 9, iter 8470, loss 4.26360, smoothed loss 4.60557, grad norm 4.08660, param norm 82.55898
epoch 9, iter 8475, loss 4.24793, smoothed loss 4.60042, grad norm 4.11590, param norm 82.56911
epoch 9, iter 8480, loss 3.69007, smoothed loss 4.60219, grad norm 4.29693, param norm 82.57871
epoch 9, iter 8485, loss 4.71889, smoothed loss 4.61285, grad norm 4.36150, param norm 82.59048
epoch 9, iter 8490, loss 4.93923, smoothed loss 4.62460, grad norm 3.84194, param norm 82.60262
epoch 9, iter 8495, loss 4.85383, smoothed loss 4.63240, grad norm 4.88997, param norm 82.61562
epoch 10, iter 8500, loss 5.04077, smoothed loss 4.62623, grad norm 4.59766, param norm 82.63167
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 8500, dev loss: 4.971242
Calculating Train F1/EM...
F1 train: 1000 examples took 9.01051 seconds [Score: 0.45839]
Exact Match train: 1000 examples took 9.56314 seconds [Score: 0.35100]
Epoch 10, Iter 8500, Train F1 score: 0.458394, Train EM score: 0.351000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.98282 seconds [Score: 0.34484]
Exact Match dev: 7118 examples took 58.04950 seconds [Score: 0.24234]
Epoch 10, Iter 8500, Dev F1 score: 0.344840, Dev EM score: 0.242343
End of epoch 10
epoch 10, iter 8505, loss 4.35217, smoothed loss 4.62057, grad norm 4.16009, param norm 82.64753
epoch 10, iter 8510, loss 4.50414, smoothed loss 4.61271, grad norm 4.81801, param norm 82.66500
epoch 10, iter 8515, loss 4.56167, smoothed loss 4.62268, grad norm 4.03472, param norm 82.68162
epoch 10, iter 8520, loss 4.43717, smoothed loss 4.62435, grad norm 4.23681, param norm 82.69527
epoch 10, iter 8525, loss 4.03840, smoothed loss 4.61455, grad norm 4.20700, param norm 82.71207
epoch 10, iter 8530, loss 4.84673, smoothed loss 4.62191, grad norm 4.50904, param norm 82.72567
epoch 10, iter 8535, loss 4.65098, smoothed loss 4.61743, grad norm 4.23253, param norm 82.73781
epoch 10, iter 8540, loss 4.39493, smoothed loss 4.59187, grad norm 4.33430, param norm 82.75288
epoch 10, iter 8545, loss 4.15055, smoothed loss 4.59042, grad norm 3.96589, param norm 82.76785
epoch 10, iter 8550, loss 4.81979, smoothed loss 4.61619, grad norm 3.67008, param norm 82.78206
epoch 10, iter 8555, loss 4.93385, smoothed loss 4.60860, grad norm 4.33464, param norm 82.79492
epoch 10, iter 8560, loss 3.90852, smoothed loss 4.59080, grad norm 4.43989, param norm 82.81213
epoch 10, iter 8565, loss 4.95424, smoothed loss 4.58244, grad norm 4.45136, param norm 82.83215
epoch 10, iter 8570, loss 5.01291, smoothed loss 4.58924, grad norm 4.67181, param norm 82.84604
epoch 10, iter 8575, loss 5.15965, smoothed loss 4.59016, grad norm 5.49028, param norm 82.85905
epoch 10, iter 8580, loss 4.18612, smoothed loss 4.58647, grad norm 3.84710, param norm 82.87180
epoch 10, iter 8585, loss 4.31706, smoothed loss 4.58699, grad norm 3.95310, param norm 82.88604
epoch 10, iter 8590, loss 4.63502, smoothed loss 4.59198, grad norm 5.09254, param norm 82.90056
epoch 10, iter 8595, loss 4.75127, smoothed loss 4.59537, grad norm 4.82248, param norm 82.91578
epoch 10, iter 8600, loss 5.09234, smoothed loss 4.60015, grad norm 4.59241, param norm 82.92949
epoch 10, iter 8605, loss 4.06559, smoothed loss 4.60509, grad norm 4.67706, param norm 82.94185
epoch 10, iter 8610, loss 5.37984, smoothed loss 4.61627, grad norm 3.71860, param norm 82.95428
epoch 10, iter 8615, loss 5.01074, smoothed loss 4.62661, grad norm 3.38525, param norm 82.96761
epoch 10, iter 8620, loss 4.45002, smoothed loss 4.62351, grad norm 4.02542, param norm 82.98419
epoch 10, iter 8625, loss 3.77805, smoothed loss 4.61745, grad norm 3.55850, param norm 83.00134
epoch 10, iter 8630, loss 4.44942, smoothed loss 4.61131, grad norm 4.17371, param norm 83.01759
epoch 10, iter 8635, loss 4.21246, smoothed loss 4.60532, grad norm 3.83077, param norm 83.03304
epoch 10, iter 8640, loss 4.66364, smoothed loss 4.61037, grad norm 4.44587, param norm 83.04896
epoch 10, iter 8645, loss 4.97343, smoothed loss 4.60546, grad norm 4.78449, param norm 83.06267
epoch 10, iter 8650, loss 4.30933, smoothed loss 4.60245, grad norm 4.28213, param norm 83.07569
epoch 10, iter 8655, loss 4.53080, smoothed loss 4.60065, grad norm 3.83304, param norm 83.08811
epoch 10, iter 8660, loss 3.87630, smoothed loss 4.58595, grad norm 3.79819, param norm 83.10279
epoch 10, iter 8665, loss 4.94948, smoothed loss 4.58235, grad norm 4.44121, param norm 83.11871
epoch 10, iter 8670, loss 4.50308, smoothed loss 4.57976, grad norm 3.94807, param norm 83.13414
epoch 10, iter 8675, loss 4.09702, smoothed loss 4.58207, grad norm 3.83727, param norm 83.14621
epoch 10, iter 8680, loss 4.89488, smoothed loss 4.59150, grad norm 4.59490, param norm 83.15970
epoch 10, iter 8685, loss 4.21225, smoothed loss 4.58782, grad norm 4.38603, param norm 83.17445
epoch 10, iter 8690, loss 4.50889, smoothed loss 4.57918, grad norm 4.52184, param norm 83.19154
epoch 10, iter 8695, loss 4.81461, smoothed loss 4.57398, grad norm 4.95857, param norm 83.20899
epoch 10, iter 8700, loss 4.38213, smoothed loss 4.57442, grad norm 4.54580, param norm 83.22522
epoch 10, iter 8705, loss 4.29895, smoothed loss 4.56310, grad norm 4.27769, param norm 83.23886
epoch 10, iter 8710, loss 5.05119, smoothed loss 4.57277, grad norm 4.82289, param norm 83.25242
epoch 10, iter 8715, loss 4.15958, smoothed loss 4.56934, grad norm 3.93422, param norm 83.26610
epoch 10, iter 8720, loss 5.38756, smoothed loss 4.57185, grad norm 4.01594, param norm 83.28231
epoch 10, iter 8725, loss 4.18871, smoothed loss 4.56370, grad norm 4.16970, param norm 83.29817
epoch 10, iter 8730, loss 4.04693, smoothed loss 4.55338, grad norm 4.31330, param norm 83.31583
epoch 10, iter 8735, loss 4.20715, smoothed loss 4.55643, grad norm 4.58090, param norm 83.33521
epoch 10, iter 8740, loss 4.32592, smoothed loss 4.56316, grad norm 3.68388, param norm 83.35458
epoch 10, iter 8745, loss 4.66394, smoothed loss 4.56134, grad norm 3.85050, param norm 83.37346
epoch 10, iter 8750, loss 4.56313, smoothed loss 4.56337, grad norm 3.77126, param norm 83.39062
epoch 10, iter 8755, loss 4.31882, smoothed loss 4.57057, grad norm 4.11574, param norm 83.40939
epoch 10, iter 8760, loss 4.62083, smoothed loss 4.56807, grad norm 4.01291, param norm 83.42808
epoch 10, iter 8765, loss 4.58186, smoothed loss 4.56399, grad norm 4.32801, param norm 83.44357
epoch 10, iter 8770, loss 4.55061, smoothed loss 4.57270, grad norm 4.32386, param norm 83.45666
epoch 10, iter 8775, loss 4.48915, smoothed loss 4.57378, grad norm 4.39757, param norm 83.46853
epoch 10, iter 8780, loss 5.11506, smoothed loss 4.58647, grad norm 4.52270, param norm 83.48148
epoch 10, iter 8785, loss 5.58334, smoothed loss 4.59807, grad norm 5.37207, param norm 83.49339
epoch 10, iter 8790, loss 5.01936, smoothed loss 4.60521, grad norm 3.88422, param norm 83.50516
epoch 10, iter 8795, loss 4.25938, smoothed loss 4.60279, grad norm 3.87271, param norm 83.51723
epoch 10, iter 8800, loss 5.03676, smoothed loss 4.62076, grad norm 4.46200, param norm 83.52898
epoch 10, iter 8805, loss 4.67915, smoothed loss 4.62696, grad norm 4.78089, param norm 83.54174
epoch 10, iter 8810, loss 4.64449, smoothed loss 4.63380, grad norm 4.46385, param norm 83.55539
epoch 10, iter 8815, loss 4.15028, smoothed loss 4.63883, grad norm 3.84991, param norm 83.56915
epoch 10, iter 8820, loss 4.74649, smoothed loss 4.63466, grad norm 4.62772, param norm 83.58447
epoch 10, iter 8825, loss 4.37045, smoothed loss 4.63734, grad norm 4.04377, param norm 83.59826
epoch 10, iter 8830, loss 4.76612, smoothed loss 4.63823, grad norm 3.84746, param norm 83.61088
epoch 10, iter 8835, loss 4.80090, smoothed loss 4.63766, grad norm 4.15469, param norm 83.62159
epoch 10, iter 8840, loss 3.83373, smoothed loss 4.62371, grad norm 4.15132, param norm 83.63229
epoch 10, iter 8845, loss 5.08698, smoothed loss 4.62398, grad norm 4.49654, param norm 83.64692
epoch 10, iter 8850, loss 4.37758, smoothed loss 4.61504, grad norm 4.29164, param norm 83.66322
epoch 10, iter 8855, loss 4.79154, smoothed loss 4.61388, grad norm 4.11116, param norm 83.68000
epoch 10, iter 8860, loss 4.84702, smoothed loss 4.62681, grad norm 4.27659, param norm 83.69753
epoch 10, iter 8865, loss 4.65751, smoothed loss 4.62411, grad norm 3.94322, param norm 83.71478
epoch 10, iter 8870, loss 5.24575, smoothed loss 4.62451, grad norm 4.55064, param norm 83.73089
epoch 10, iter 8875, loss 4.31894, smoothed loss 4.62180, grad norm 3.42398, param norm 83.74701
epoch 10, iter 8880, loss 3.90542, smoothed loss 4.60758, grad norm 3.63544, param norm 83.76080
epoch 10, iter 8885, loss 4.99126, smoothed loss 4.60529, grad norm 4.54094, param norm 83.77464
epoch 10, iter 8890, loss 4.36068, smoothed loss 4.59781, grad norm 4.05026, param norm 83.78793
epoch 10, iter 8895, loss 5.33428, smoothed loss 4.60668, grad norm 4.38968, param norm 83.80466
epoch 10, iter 8900, loss 3.97427, smoothed loss 4.59996, grad norm 4.08859, param norm 83.82207
epoch 10, iter 8905, loss 5.15733, smoothed loss 4.60653, grad norm 3.90769, param norm 83.83835
epoch 10, iter 8910, loss 4.60897, smoothed loss 4.60099, grad norm 4.59238, param norm 83.85306
epoch 10, iter 8915, loss 4.88169, smoothed loss 4.59781, grad norm 4.90008, param norm 83.86544
epoch 10, iter 8920, loss 4.22907, smoothed loss 4.59108, grad norm 4.37116, param norm 83.87345
epoch 10, iter 8925, loss 4.41957, smoothed loss 4.58015, grad norm 4.56436, param norm 83.88486
epoch 10, iter 8930, loss 5.16927, smoothed loss 4.58954, grad norm 4.77218, param norm 83.89750
epoch 10, iter 8935, loss 4.88049, smoothed loss 4.58053, grad norm 4.22751, param norm 83.90923
epoch 10, iter 8940, loss 4.69487, smoothed loss 4.58800, grad norm 4.16062, param norm 83.92230
epoch 10, iter 8945, loss 4.79399, smoothed loss 4.59114, grad norm 3.71798, param norm 83.93370
epoch 10, iter 8950, loss 3.84192, smoothed loss 4.59119, grad norm 3.85963, param norm 83.94383
epoch 10, iter 8955, loss 4.61972, smoothed loss 4.58848, grad norm 4.29336, param norm 83.95790
epoch 10, iter 8960, loss 4.43702, smoothed loss 4.58435, grad norm 4.19356, param norm 83.97113
epoch 10, iter 8965, loss 4.51278, smoothed loss 4.58703, grad norm 4.68937, param norm 83.98402
epoch 10, iter 8970, loss 4.56284, smoothed loss 4.58614, grad norm 4.67573, param norm 83.99658
epoch 10, iter 8975, loss 4.24692, smoothed loss 4.59135, grad norm 4.16227, param norm 84.01075
epoch 10, iter 8980, loss 3.99488, smoothed loss 4.57466, grad norm 3.82795, param norm 84.02711
epoch 10, iter 8985, loss 4.51925, smoothed loss 4.57393, grad norm 4.87804, param norm 84.04729
epoch 10, iter 8990, loss 4.11395, smoothed loss 4.56848, grad norm 3.78576, param norm 84.06580
epoch 10, iter 8995, loss 4.53561, smoothed loss 4.56675, grad norm 4.49672, param norm 84.08387
epoch 10, iter 9000, loss 4.73843, smoothed loss 4.56411, grad norm 3.96287, param norm 84.10120
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 9000, dev loss: 4.961210
Calculating Train F1/EM...
F1 train: 1000 examples took 8.81704 seconds [Score: 0.49184]
Exact Match train: 1000 examples took 8.84655 seconds [Score: 0.37300]
Epoch 10, Iter 9000, Train F1 score: 0.491837, Train EM score: 0.373000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.22456 seconds [Score: 0.35505]
Exact Match dev: 7118 examples took 57.70075 seconds [Score: 0.24979]
Epoch 10, Iter 9000, Dev F1 score: 0.355045, Dev EM score: 0.249789
End of epoch 10
epoch 10, iter 9005, loss 3.97950, smoothed loss 4.55183, grad norm 4.24545, param norm 84.11738
epoch 10, iter 9010, loss 4.28126, smoothed loss 4.55140, grad norm 4.46564, param norm 84.13340
epoch 10, iter 9015, loss 4.65805, smoothed loss 4.55031, grad norm 3.90642, param norm 84.14783
epoch 10, iter 9020, loss 4.34028, smoothed loss 4.54831, grad norm 4.08165, param norm 84.16150
epoch 10, iter 9025, loss 4.27894, smoothed loss 4.52945, grad norm 3.87862, param norm 84.17683
epoch 10, iter 9030, loss 5.01901, smoothed loss 4.53448, grad norm 4.79494, param norm 84.19418
epoch 10, iter 9035, loss 4.67809, smoothed loss 4.51710, grad norm 3.79651, param norm 84.20889
epoch 10, iter 9040, loss 4.53017, smoothed loss 4.51145, grad norm 4.44120, param norm 84.22214
epoch 10, iter 9045, loss 4.34885, smoothed loss 4.50951, grad norm 4.42819, param norm 84.23326
epoch 10, iter 9050, loss 4.56243, smoothed loss 4.53027, grad norm 4.31297, param norm 84.24442
epoch 10, iter 9055, loss 4.66816, smoothed loss 4.52217, grad norm 4.11305, param norm 84.25686
epoch 10, iter 9060, loss 4.43234, smoothed loss 4.52153, grad norm 3.99121, param norm 84.27161
epoch 10, iter 9065, loss 3.95090, smoothed loss 4.52069, grad norm 4.13846, param norm 84.28835
epoch 10, iter 9070, loss 3.84670, smoothed loss 4.51333, grad norm 4.32926, param norm 84.30631
epoch 10, iter 9075, loss 4.75124, smoothed loss 4.50858, grad norm 4.63122, param norm 84.32504
epoch 10, iter 9080, loss 3.84975, smoothed loss 4.52447, grad norm 4.25796, param norm 84.34105
epoch 10, iter 9085, loss 3.94260, smoothed loss 4.52500, grad norm 3.93217, param norm 84.35516
epoch 10, iter 9090, loss 5.24789, smoothed loss 4.53686, grad norm 4.99370, param norm 84.36608
epoch 10, iter 9095, loss 3.93890, smoothed loss 4.52996, grad norm 3.78580, param norm 84.37896
epoch 10, iter 9100, loss 5.05409, smoothed loss 4.54288, grad norm 4.13402, param norm 84.39037
epoch 10, iter 9105, loss 4.82430, smoothed loss 4.54644, grad norm 4.72307, param norm 84.40140
epoch 10, iter 9110, loss 4.14620, smoothed loss 4.54494, grad norm 4.04302, param norm 84.41388
epoch 10, iter 9115, loss 4.32432, smoothed loss 4.54860, grad norm 4.04306, param norm 84.42629
epoch 10, iter 9120, loss 5.21989, smoothed loss 4.55037, grad norm 4.68996, param norm 84.43954
epoch 10, iter 9125, loss 4.95053, smoothed loss 4.54629, grad norm 4.98742, param norm 84.45627
epoch 10, iter 9130, loss 4.59643, smoothed loss 4.54432, grad norm 4.53815, param norm 84.47324
epoch 10, iter 9135, loss 4.71641, smoothed loss 4.53593, grad norm 4.12103, param norm 84.48898
epoch 10, iter 9140, loss 4.57000, smoothed loss 4.53069, grad norm 4.17144, param norm 84.50489
epoch 10, iter 9145, loss 5.01415, smoothed loss 4.54319, grad norm 4.08279, param norm 84.51912
epoch 10, iter 9150, loss 4.64806, smoothed loss 4.54079, grad norm 4.17336, param norm 84.53117
epoch 10, iter 9155, loss 4.38401, smoothed loss 4.54100, grad norm 3.69660, param norm 84.54424
epoch 10, iter 9160, loss 4.26108, smoothed loss 4.53741, grad norm 4.23884, param norm 84.56037
epoch 10, iter 9165, loss 4.11783, smoothed loss 4.53476, grad norm 4.20123, param norm 84.57471
epoch 10, iter 9170, loss 3.49549, smoothed loss 4.52107, grad norm 3.81460, param norm 84.58947
epoch 10, iter 9175, loss 4.79540, smoothed loss 4.51765, grad norm 4.27069, param norm 84.60535
epoch 10, iter 9180, loss 4.26734, smoothed loss 4.50695, grad norm 4.01608, param norm 84.62119
epoch 10, iter 9185, loss 4.72818, smoothed loss 4.50342, grad norm 4.45916, param norm 84.63619
epoch 10, iter 9190, loss 3.77156, smoothed loss 4.49754, grad norm 3.86578, param norm 84.64970
epoch 10, iter 9195, loss 5.12243, smoothed loss 4.52221, grad norm 4.58742, param norm 84.66312
epoch 10, iter 9200, loss 5.21400, smoothed loss 4.53951, grad norm 4.64674, param norm 84.67583
epoch 10, iter 9205, loss 4.50843, smoothed loss 4.54891, grad norm 3.92991, param norm 84.68852
epoch 10, iter 9210, loss 3.90690, smoothed loss 4.54103, grad norm 4.34620, param norm 84.70095
epoch 10, iter 9215, loss 4.18178, smoothed loss 4.53476, grad norm 4.35799, param norm 84.71386
epoch 10, iter 9220, loss 4.52566, smoothed loss 4.53270, grad norm 4.89389, param norm 84.72803
epoch 10, iter 9225, loss 4.73825, smoothed loss 4.52949, grad norm 4.51445, param norm 84.74044
epoch 10, iter 9230, loss 4.40411, smoothed loss 4.52587, grad norm 4.29463, param norm 84.75494
epoch 10, iter 9235, loss 5.11313, smoothed loss 4.53261, grad norm 5.22681, param norm 84.76947
epoch 10, iter 9240, loss 4.72867, smoothed loss 4.53527, grad norm 4.46677, param norm 84.78356
epoch 10, iter 9245, loss 5.31373, smoothed loss 4.52411, grad norm 3.95158, param norm 84.80038
epoch 10, iter 9250, loss 4.76020, smoothed loss 4.53482, grad norm 4.62437, param norm 84.81558
epoch 10, iter 9255, loss 4.05153, smoothed loss 4.53921, grad norm 4.39731, param norm 84.82928
epoch 10, iter 9260, loss 5.28449, smoothed loss 4.54646, grad norm 4.01061, param norm 84.84439
epoch 10, iter 9265, loss 5.19877, smoothed loss 4.56005, grad norm 3.92617, param norm 84.85856
epoch 10, iter 9270, loss 4.46406, smoothed loss 4.55926, grad norm 4.67912, param norm 84.87258
epoch 10, iter 9275, loss 5.25692, smoothed loss 4.56910, grad norm 4.97278, param norm 84.88694
epoch 10, iter 9280, loss 5.12446, smoothed loss 4.58444, grad norm 4.60084, param norm 84.90060
epoch 10, iter 9285, loss 4.70536, smoothed loss 4.57778, grad norm 3.78760, param norm 84.91203
epoch 10, iter 9290, loss 4.72166, smoothed loss 4.57216, grad norm 4.50384, param norm 84.92629
epoch 10, iter 9295, loss 5.55435, smoothed loss 4.56631, grad norm 4.36774, param norm 84.94147
epoch 10, iter 9300, loss 4.84070, smoothed loss 4.57515, grad norm 4.28288, param norm 84.95519
epoch 10, iter 9305, loss 4.40304, smoothed loss 4.58310, grad norm 4.12211, param norm 84.96900
epoch 10, iter 9310, loss 4.27210, smoothed loss 4.56783, grad norm 4.12688, param norm 84.98157
epoch 10, iter 9315, loss 4.29467, smoothed loss 4.56553, grad norm 4.03914, param norm 84.99255
epoch 10, iter 9320, loss 4.98329, smoothed loss 4.57331, grad norm 4.28543, param norm 85.00340
epoch 10, iter 9325, loss 5.17546, smoothed loss 4.57663, grad norm 4.08502, param norm 85.01552
epoch 10, iter 9330, loss 4.61634, smoothed loss 4.56449, grad norm 4.22254, param norm 85.03170
epoch 10, iter 9335, loss 4.40547, smoothed loss 4.56492, grad norm 3.88667, param norm 85.04691
epoch 10, iter 9340, loss 4.16040, smoothed loss 4.55934, grad norm 4.43309, param norm 85.06070
epoch 10, iter 9345, loss 4.75025, smoothed loss 4.56406, grad norm 4.23684, param norm 85.07523
epoch 10, iter 9350, loss 4.26666, smoothed loss 4.55774, grad norm 3.90398, param norm 85.08759
epoch 10, iter 9355, loss 4.16320, smoothed loss 4.55436, grad norm 4.27191, param norm 85.10169
epoch 10, iter 9360, loss 4.28181, smoothed loss 4.54652, grad norm 5.25340, param norm 85.11931
epoch 10, iter 9365, loss 4.52238, smoothed loss 4.55084, grad norm 4.30230, param norm 85.13632
epoch 10, iter 9370, loss 4.43097, smoothed loss 4.55524, grad norm 4.65531, param norm 85.15017
epoch 10, iter 9375, loss 4.44035, smoothed loss 4.55739, grad norm 4.43509, param norm 85.16261
epoch 10, iter 9380, loss 4.58696, smoothed loss 4.55245, grad norm 4.42813, param norm 85.17439
epoch 10, iter 9385, loss 4.94820, smoothed loss 4.55512, grad norm 4.24271, param norm 85.18782
epoch 10, iter 9390, loss 4.68642, smoothed loss 4.55022, grad norm 4.46299, param norm 85.20053
epoch 10, iter 9395, loss 4.38009, smoothed loss 4.56110, grad norm 4.97484, param norm 85.21295
epoch 10, iter 9400, loss 3.91205, smoothed loss 4.54990, grad norm 4.39715, param norm 85.22453
epoch 10, iter 9405, loss 4.13867, smoothed loss 4.54836, grad norm 4.14940, param norm 85.23815
epoch 10, iter 9410, loss 4.88733, smoothed loss 4.53789, grad norm 4.83015, param norm 85.25338
epoch 10, iter 9415, loss 4.57883, smoothed loss 4.53271, grad norm 4.11982, param norm 85.26707
epoch 10, iter 9420, loss 4.29343, smoothed loss 4.53828, grad norm 5.19700, param norm 85.27748
epoch 10, iter 9425, loss 3.83179, smoothed loss 4.52279, grad norm 3.95738, param norm 85.28809
epoch 10, iter 9430, loss 4.48903, smoothed loss 4.51556, grad norm 4.11728, param norm 85.29864
epoch 10, iter 9435, loss 4.31532, smoothed loss 4.51651, grad norm 4.36440, param norm 85.31067
epoch 10, iter 9440, loss 5.28024, smoothed loss 4.52419, grad norm 4.93634, param norm 85.32419
epoch 11, iter 9445, loss 5.02305, smoothed loss 4.52156, grad norm 4.11341, param norm 85.33625
epoch 11, iter 9450, loss 4.41027, smoothed loss 4.52458, grad norm 4.04092, param norm 85.34796
epoch 11, iter 9455, loss 4.35106, smoothed loss 4.52723, grad norm 4.77666, param norm 85.35801
epoch 11, iter 9460, loss 3.95006, smoothed loss 4.51355, grad norm 3.88663, param norm 85.37059
epoch 11, iter 9465, loss 4.01345, smoothed loss 4.50630, grad norm 4.23305, param norm 85.38613
epoch 11, iter 9470, loss 4.49365, smoothed loss 4.50172, grad norm 4.29594, param norm 85.40040
epoch 11, iter 9475, loss 4.16351, smoothed loss 4.48970, grad norm 4.39424, param norm 85.41254
epoch 11, iter 9480, loss 4.65109, smoothed loss 4.49067, grad norm 4.36447, param norm 85.42645
epoch 11, iter 9485, loss 5.20322, smoothed loss 4.50148, grad norm 4.13187, param norm 85.44189
epoch 11, iter 9490, loss 4.36059, smoothed loss 4.50433, grad norm 4.47928, param norm 85.45699
epoch 11, iter 9495, loss 4.52442, smoothed loss 4.51311, grad norm 4.35747, param norm 85.47070
epoch 11, iter 9500, loss 4.49990, smoothed loss 4.50572, grad norm 4.29804, param norm 85.48605
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 11, Iter 9500, dev loss: 4.955613
Calculating Train F1/EM...
F1 train: 1000 examples took 8.76801 seconds [Score: 0.50088]
Exact Match train: 1000 examples took 9.14260 seconds [Score: 0.37600]
Epoch 11, Iter 9500, Train F1 score: 0.500879, Train EM score: 0.376000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.48034 seconds [Score: 0.35202]
Exact Match dev: 7118 examples took 57.18992 seconds [Score: 0.24937]
Epoch 11, Iter 9500, Dev F1 score: 0.352024, Dev EM score: 0.249368
End of epoch 11
epoch 11, iter 9505, loss 5.27710, smoothed loss 4.51076, grad norm 4.97217, param norm 85.50200
epoch 11, iter 9510, loss 4.32832, smoothed loss 4.50976, grad norm 3.91279, param norm 85.51584
epoch 11, iter 9515, loss 4.63674, smoothed loss 4.50505, grad norm 4.49223, param norm 85.52990
epoch 11, iter 9520, loss 3.69304, smoothed loss 4.49134, grad norm 4.02272, param norm 85.54378
epoch 11, iter 9525, loss 4.79657, smoothed loss 4.48258, grad norm 4.79463, param norm 85.55669
epoch 11, iter 9530, loss 3.63083, smoothed loss 4.47576, grad norm 4.03415, param norm 85.56807
epoch 11, iter 9535, loss 4.65099, smoothed loss 4.47413, grad norm 4.48827, param norm 85.58113
epoch 11, iter 9540, loss 4.85100, smoothed loss 4.46245, grad norm 4.50991, param norm 85.59677
epoch 11, iter 9545, loss 4.25555, smoothed loss 4.46763, grad norm 4.57682, param norm 85.61285
epoch 11, iter 9550, loss 5.52342, smoothed loss 4.48262, grad norm 4.47642, param norm 85.62931
epoch 11, iter 9555, loss 4.63790, smoothed loss 4.49360, grad norm 4.68578, param norm 85.64108
epoch 11, iter 9560, loss 4.50084, smoothed loss 4.50672, grad norm 4.26171, param norm 85.65067
epoch 11, iter 9565, loss 5.21021, smoothed loss 4.51180, grad norm 4.44150, param norm 85.65966
epoch 11, iter 9570, loss 4.15898, smoothed loss 4.50437, grad norm 4.04658, param norm 85.66920
epoch 11, iter 9575, loss 4.43996, smoothed loss 4.50198, grad norm 4.13918, param norm 85.68248
epoch 11, iter 9580, loss 5.28423, smoothed loss 4.50610, grad norm 4.74316, param norm 85.69553
epoch 11, iter 9585, loss 4.04303, smoothed loss 4.49607, grad norm 3.84211, param norm 85.70863
epoch 11, iter 9590, loss 5.31025, smoothed loss 4.51252, grad norm 4.77225, param norm 85.72323
epoch 11, iter 9595, loss 5.14228, smoothed loss 4.52866, grad norm 4.97539, param norm 85.73773
epoch 11, iter 9600, loss 4.28622, smoothed loss 4.52231, grad norm 4.25372, param norm 85.75149
epoch 11, iter 9605, loss 4.07928, smoothed loss 4.52474, grad norm 3.99983, param norm 85.76479
epoch 11, iter 9610, loss 4.75204, smoothed loss 4.52589, grad norm 4.95129, param norm 85.78024
epoch 11, iter 9615, loss 3.80899, smoothed loss 4.50667, grad norm 3.93693, param norm 85.79546
epoch 11, iter 9620, loss 3.88287, smoothed loss 4.50522, grad norm 3.92582, param norm 85.81299
epoch 11, iter 9625, loss 4.79332, smoothed loss 4.50897, grad norm 4.37865, param norm 85.83035
epoch 11, iter 9630, loss 5.00291, smoothed loss 4.52531, grad norm 4.77240, param norm 85.84363
epoch 11, iter 9635, loss 5.31287, smoothed loss 4.53828, grad norm 5.08784, param norm 85.85544
epoch 11, iter 9640, loss 4.64262, smoothed loss 4.54145, grad norm 3.82081, param norm 85.86777
epoch 11, iter 9645, loss 3.91053, smoothed loss 4.54515, grad norm 4.55336, param norm 85.87950
epoch 11, iter 9650, loss 4.64257, smoothed loss 4.54323, grad norm 4.00597, param norm 85.89156
epoch 11, iter 9655, loss 4.61624, smoothed loss 4.54018, grad norm 4.49014, param norm 85.90575
epoch 11, iter 9660, loss 4.15755, smoothed loss 4.53422, grad norm 4.35886, param norm 85.92159
epoch 11, iter 9665, loss 5.10056, smoothed loss 4.53346, grad norm 4.44429, param norm 85.93840
epoch 11, iter 9670, loss 4.67237, smoothed loss 4.52081, grad norm 4.09110, param norm 85.95294
epoch 11, iter 9675, loss 4.06802, smoothed loss 4.51149, grad norm 4.14789, param norm 85.96809
epoch 11, iter 9680, loss 5.08418, smoothed loss 4.52189, grad norm 4.13920, param norm 85.98241
epoch 11, iter 9685, loss 4.40804, smoothed loss 4.50674, grad norm 4.36203, param norm 85.99863
epoch 11, iter 9690, loss 4.99167, smoothed loss 4.51489, grad norm 4.37085, param norm 86.01453
epoch 11, iter 9695, loss 4.85312, smoothed loss 4.52756, grad norm 4.84217, param norm 86.02598
epoch 11, iter 9700, loss 4.41978, smoothed loss 4.52796, grad norm 4.46895, param norm 86.03680
epoch 11, iter 9705, loss 4.49228, smoothed loss 4.52335, grad norm 4.22211, param norm 86.05023
epoch 11, iter 9710, loss 5.00702, smoothed loss 4.53190, grad norm 4.89593, param norm 86.06456
epoch 11, iter 9715, loss 5.14746, smoothed loss 4.53897, grad norm 4.75877, param norm 86.07746
epoch 11, iter 9720, loss 3.89036, smoothed loss 4.53809, grad norm 4.11332, param norm 86.09186
epoch 11, iter 9725, loss 4.42694, smoothed loss 4.54847, grad norm 4.12096, param norm 86.10780
epoch 11, iter 9730, loss 3.95351, smoothed loss 4.55104, grad norm 3.89012, param norm 86.12393
epoch 11, iter 9735, loss 4.07591, smoothed loss 4.54607, grad norm 4.47496, param norm 86.13848
epoch 11, iter 9740, loss 4.41186, smoothed loss 4.54134, grad norm 3.86179, param norm 86.15386
epoch 11, iter 9745, loss 4.98833, smoothed loss 4.54976, grad norm 4.87976, param norm 86.16772
epoch 11, iter 9750, loss 3.99357, smoothed loss 4.54676, grad norm 3.95599, param norm 86.17756
epoch 11, iter 9755, loss 4.21216, smoothed loss 4.54614, grad norm 3.96799, param norm 86.18912
epoch 11, iter 9760, loss 4.44690, smoothed loss 4.56047, grad norm 4.40117, param norm 86.20371
epoch 11, iter 9765, loss 3.87902, smoothed loss 4.54665, grad norm 4.63705, param norm 86.21701
epoch 11, iter 9770, loss 4.76655, smoothed loss 4.56354, grad norm 4.49225, param norm 86.23108
epoch 11, iter 9775, loss 4.36201, smoothed loss 4.56110, grad norm 4.24625, param norm 86.24425
epoch 11, iter 9780, loss 4.29431, smoothed loss 4.56151, grad norm 4.03086, param norm 86.25632
epoch 11, iter 9785, loss 4.46848, smoothed loss 4.56642, grad norm 4.34104, param norm 86.26924
epoch 11, iter 9790, loss 4.80436, smoothed loss 4.55919, grad norm 4.18076, param norm 86.28333
epoch 11, iter 9795, loss 4.41813, smoothed loss 4.56165, grad norm 4.27381, param norm 86.29765
epoch 11, iter 9800, loss 4.61699, smoothed loss 4.55126, grad norm 3.87240, param norm 86.31309
epoch 11, iter 9805, loss 4.39397, smoothed loss 4.54202, grad norm 4.42263, param norm 86.32796
epoch 11, iter 9810, loss 4.65389, smoothed loss 4.53228, grad norm 4.82295, param norm 86.34177
epoch 11, iter 9815, loss 5.20701, smoothed loss 4.53859, grad norm 4.57875, param norm 86.35532
epoch 11, iter 9820, loss 5.55144, smoothed loss 4.54689, grad norm 4.70463, param norm 86.36664
epoch 11, iter 9825, loss 4.50012, smoothed loss 4.54012, grad norm 3.85384, param norm 86.37718
epoch 11, iter 9830, loss 4.31721, smoothed loss 4.53497, grad norm 4.25910, param norm 86.38757
epoch 11, iter 9835, loss 3.86201, smoothed loss 4.53437, grad norm 3.85378, param norm 86.39986
epoch 11, iter 9840, loss 3.91106, smoothed loss 4.52951, grad norm 3.97164, param norm 86.41365
epoch 11, iter 9845, loss 4.82994, smoothed loss 4.54244, grad norm 4.51895, param norm 86.42748
epoch 11, iter 9850, loss 3.98222, smoothed loss 4.53035, grad norm 4.16987, param norm 86.43770
epoch 11, iter 9855, loss 4.59558, smoothed loss 4.52584, grad norm 4.16463, param norm 86.44781
epoch 11, iter 9860, loss 4.57000, smoothed loss 4.52858, grad norm 3.99624, param norm 86.45867
epoch 11, iter 9865, loss 4.34948, smoothed loss 4.52687, grad norm 3.97950, param norm 86.47254
epoch 11, iter 9870, loss 4.60390, smoothed loss 4.52287, grad norm 4.71847, param norm 86.48911
epoch 11, iter 9875, loss 4.86828, smoothed loss 4.52448, grad norm 4.16515, param norm 86.50393
epoch 11, iter 9880, loss 4.72172, smoothed loss 4.52736, grad norm 5.11867, param norm 86.51725
epoch 11, iter 9885, loss 4.37028, smoothed loss 4.51963, grad norm 5.00832, param norm 86.53008
epoch 11, iter 9890, loss 3.99029, smoothed loss 4.51681, grad norm 4.42897, param norm 86.54237
epoch 11, iter 9895, loss 4.18256, smoothed loss 4.51499, grad norm 4.36084, param norm 86.55443
epoch 11, iter 9900, loss 4.37426, smoothed loss 4.51307, grad norm 4.20569, param norm 86.56750
epoch 11, iter 9905, loss 4.85373, smoothed loss 4.52012, grad norm 4.76742, param norm 86.58125
epoch 11, iter 9910, loss 5.15579, smoothed loss 4.51229, grad norm 4.72790, param norm 86.59443
epoch 11, iter 9915, loss 4.44248, smoothed loss 4.50147, grad norm 4.58860, param norm 86.60725
epoch 11, iter 9920, loss 5.39607, smoothed loss 4.51473, grad norm 4.99207, param norm 86.62202
epoch 11, iter 9925, loss 4.94380, smoothed loss 4.51688, grad norm 5.15641, param norm 86.63729
epoch 11, iter 9930, loss 5.10852, smoothed loss 4.51783, grad norm 4.87826, param norm 86.65273
epoch 11, iter 9935, loss 4.31863, smoothed loss 4.51665, grad norm 4.18795, param norm 86.66315
epoch 11, iter 9940, loss 4.21199, smoothed loss 4.51251, grad norm 3.78074, param norm 86.67178
epoch 11, iter 9945, loss 4.13435, smoothed loss 4.50220, grad norm 3.84410, param norm 86.68113
epoch 11, iter 9950, loss 4.56110, smoothed loss 4.49992, grad norm 3.97108, param norm 86.69029
epoch 11, iter 9955, loss 3.94218, smoothed loss 4.49781, grad norm 4.42847, param norm 86.70100
epoch 11, iter 9960, loss 4.55622, smoothed loss 4.50170, grad norm 4.79988, param norm 86.71272
epoch 11, iter 9965, loss 4.44111, smoothed loss 4.51451, grad norm 4.18687, param norm 86.72433
epoch 11, iter 9970, loss 5.80431, smoothed loss 4.52232, grad norm 4.11538, param norm 86.73747
epoch 11, iter 9975, loss 4.21593, smoothed loss 4.51250, grad norm 4.36174, param norm 86.75214
epoch 11, iter 9980, loss 4.64562, smoothed loss 4.49833, grad norm 3.77180, param norm 86.76709
epoch 11, iter 9985, loss 4.24804, smoothed loss 4.49650, grad norm 4.49791, param norm 86.78065
epoch 11, iter 9990, loss 4.72615, smoothed loss 4.49145, grad norm 4.53840, param norm 86.79447
epoch 11, iter 9995, loss 4.39596, smoothed loss 4.49484, grad norm 4.20456, param norm 86.80817
epoch 11, iter 10000, loss 4.77757, smoothed loss 4.49500, grad norm 4.57973, param norm 86.82287
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 11, Iter 10000, dev loss: 4.934883
Calculating Train F1/EM...
F1 train: 1000 examples took 9.03892 seconds [Score: 0.49616]
Exact Match train: 1000 examples took 9.39031 seconds [Score: 0.39500]
Epoch 11, Iter 10000, Train F1 score: 0.496155, Train EM score: 0.395000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.07257 seconds [Score: 0.34862]
Exact Match dev: 7118 examples took 57.94304 seconds [Score: 0.24572]
Epoch 11, Iter 10000, Dev F1 score: 0.348624, Dev EM score: 0.245715
End of epoch 11
epoch 11, iter 10005, loss 4.70133, smoothed loss 4.49485, grad norm 5.67535, param norm 86.83981
epoch 11, iter 10010, loss 4.00393, smoothed loss 4.49735, grad norm 3.78733, param norm 86.85498
epoch 11, iter 10015, loss 4.61292, smoothed loss 4.49015, grad norm 4.28904, param norm 86.86885
epoch 11, iter 10020, loss 4.38128, smoothed loss 4.46991, grad norm 5.07142, param norm 86.88097
epoch 11, iter 10025, loss 5.25253, smoothed loss 4.48122, grad norm 4.01691, param norm 86.89309
epoch 11, iter 10030, loss 3.97693, smoothed loss 4.47339, grad norm 3.87122, param norm 86.90449
epoch 11, iter 10035, loss 4.10929, smoothed loss 4.45620, grad norm 5.16579, param norm 86.92022
epoch 11, iter 10040, loss 4.38912, smoothed loss 4.45657, grad norm 4.39754, param norm 86.93550
epoch 11, iter 10045, loss 4.47673, smoothed loss 4.44298, grad norm 5.09722, param norm 86.94949
epoch 11, iter 10050, loss 4.49350, smoothed loss 4.43515, grad norm 4.28107, param norm 86.96278
epoch 11, iter 10055, loss 4.52421, smoothed loss 4.43159, grad norm 4.44799, param norm 86.97531
epoch 11, iter 10060, loss 4.58781, smoothed loss 4.43224, grad norm 3.87911, param norm 86.98961
epoch 11, iter 10065, loss 3.97386, smoothed loss 4.42406, grad norm 4.00214, param norm 87.00576
epoch 11, iter 10070, loss 4.87796, smoothed loss 4.43819, grad norm 4.21970, param norm 87.02102
epoch 11, iter 10075, loss 4.44350, smoothed loss 4.43960, grad norm 3.87572, param norm 87.03509
epoch 11, iter 10080, loss 4.52947, smoothed loss 4.44355, grad norm 4.14558, param norm 87.04961
epoch 11, iter 10085, loss 4.48007, smoothed loss 4.45757, grad norm 4.45097, param norm 87.06219
epoch 11, iter 10090, loss 4.61868, smoothed loss 4.46198, grad norm 4.52887, param norm 87.07363
epoch 11, iter 10095, loss 4.51784, smoothed loss 4.46080, grad norm 4.16231, param norm 87.08506
epoch 11, iter 10100, loss 4.37423, smoothed loss 4.45644, grad norm 4.25669, param norm 87.09698
epoch 11, iter 10105, loss 4.59717, smoothed loss 4.45435, grad norm 4.58652, param norm 87.11059
epoch 11, iter 10110, loss 4.09070, smoothed loss 4.43313, grad norm 3.97176, param norm 87.12617
epoch 11, iter 10115, loss 4.03102, smoothed loss 4.42788, grad norm 4.03774, param norm 87.14145
epoch 11, iter 10120, loss 4.45496, smoothed loss 4.43400, grad norm 3.95249, param norm 87.15562
epoch 11, iter 10125, loss 4.74051, smoothed loss 4.45251, grad norm 4.61205, param norm 87.16846
epoch 11, iter 10130, loss 4.57816, smoothed loss 4.45364, grad norm 4.15290, param norm 87.18026
epoch 11, iter 10135, loss 4.41903, smoothed loss 4.44432, grad norm 4.81162, param norm 87.19379
epoch 11, iter 10140, loss 5.08473, smoothed loss 4.46229, grad norm 4.88232, param norm 87.20683
epoch 11, iter 10145, loss 4.25203, smoothed loss 4.44997, grad norm 4.07797, param norm 87.21864
epoch 11, iter 10150, loss 4.16769, smoothed loss 4.43650, grad norm 4.13840, param norm 87.23265
epoch 11, iter 10155, loss 3.80054, smoothed loss 4.43312, grad norm 4.26347, param norm 87.24764
epoch 11, iter 10160, loss 4.89204, smoothed loss 4.43534, grad norm 5.16647, param norm 87.26000
epoch 11, iter 10165, loss 3.63256, smoothed loss 4.42583, grad norm 4.51108, param norm 87.27435
epoch 11, iter 10170, loss 5.46684, smoothed loss 4.44037, grad norm 4.12494, param norm 87.28973
epoch 11, iter 10175, loss 4.25477, smoothed loss 4.44947, grad norm 4.38674, param norm 87.30330
epoch 11, iter 10180, loss 4.67802, smoothed loss 4.46316, grad norm 4.66602, param norm 87.31548
epoch 11, iter 10185, loss 4.33916, smoothed loss 4.47182, grad norm 4.99290, param norm 87.32821
epoch 11, iter 10190, loss 4.71784, smoothed loss 4.47035, grad norm 4.46583, param norm 87.34319
epoch 11, iter 10195, loss 4.63272, smoothed loss 4.47511, grad norm 5.67215, param norm 87.35763
epoch 11, iter 10200, loss 4.22884, smoothed loss 4.47493, grad norm 4.19861, param norm 87.36940
epoch 11, iter 10205, loss 4.72546, smoothed loss 4.47884, grad norm 3.91542, param norm 87.38227
epoch 11, iter 10210, loss 4.75916, smoothed loss 4.48118, grad norm 4.23162, param norm 87.39591
epoch 11, iter 10215, loss 4.70222, smoothed loss 4.48190, grad norm 4.06042, param norm 87.40874
epoch 11, iter 10220, loss 4.18776, smoothed loss 4.48275, grad norm 3.96395, param norm 87.42438
epoch 11, iter 10225, loss 4.39742, smoothed loss 4.48924, grad norm 3.89002, param norm 87.43980
epoch 11, iter 10230, loss 4.38553, smoothed loss 4.48848, grad norm 3.70216, param norm 87.45470
epoch 11, iter 10235, loss 3.85053, smoothed loss 4.49385, grad norm 4.15736, param norm 87.46889
epoch 11, iter 10240, loss 4.19918, smoothed loss 4.48973, grad norm 3.98068, param norm 87.48085
epoch 11, iter 10245, loss 3.76795, smoothed loss 4.50719, grad norm 3.82234, param norm 87.49068
epoch 11, iter 10250, loss 4.45117, smoothed loss 4.49999, grad norm 4.22563, param norm 87.50066
epoch 11, iter 10255, loss 4.59227, smoothed loss 4.49889, grad norm 4.70301, param norm 87.51228
epoch 11, iter 10260, loss 4.36788, smoothed loss 4.49863, grad norm 4.51048, param norm 87.52454
epoch 11, iter 10265, loss 4.60437, smoothed loss 4.49537, grad norm 4.40070, param norm 87.53699
epoch 11, iter 10270, loss 4.88055, smoothed loss 4.48320, grad norm 4.42788, param norm 87.55186
epoch 11, iter 10275, loss 3.93202, smoothed loss 4.47907, grad norm 4.02553, param norm 87.56800
epoch 11, iter 10280, loss 4.39491, smoothed loss 4.48905, grad norm 4.40968, param norm 87.58270
epoch 11, iter 10285, loss 4.42049, smoothed loss 4.47966, grad norm 4.37221, param norm 87.59534
epoch 11, iter 10290, loss 4.35621, smoothed loss 4.46509, grad norm 4.25912, param norm 87.60773
epoch 11, iter 10295, loss 4.09021, smoothed loss 4.45819, grad norm 4.20098, param norm 87.62128
epoch 11, iter 10300, loss 4.75488, smoothed loss 4.45400, grad norm 5.34190, param norm 87.63751
epoch 11, iter 10305, loss 5.15258, smoothed loss 4.46671, grad norm 3.77081, param norm 87.65302
epoch 11, iter 10310, loss 4.42729, smoothed loss 4.47804, grad norm 4.28434, param norm 87.66470
epoch 11, iter 10315, loss 4.45632, smoothed loss 4.47787, grad norm 4.63752, param norm 87.67719
epoch 11, iter 10320, loss 4.52841, smoothed loss 4.46876, grad norm 4.84233, param norm 87.68930
epoch 11, iter 10325, loss 4.13954, smoothed loss 4.46022, grad norm 4.33980, param norm 87.70158
epoch 11, iter 10330, loss 4.11467, smoothed loss 4.45713, grad norm 3.84986, param norm 87.71236
epoch 11, iter 10335, loss 3.28255, smoothed loss 4.44946, grad norm 4.07022, param norm 87.72427
epoch 11, iter 10340, loss 4.64174, smoothed loss 4.45116, grad norm 4.67155, param norm 87.73717
epoch 11, iter 10345, loss 4.97715, smoothed loss 4.46375, grad norm 4.50220, param norm 87.74938
epoch 11, iter 10350, loss 4.87035, smoothed loss 4.46432, grad norm 4.19535, param norm 87.76039
epoch 11, iter 10355, loss 4.67714, smoothed loss 4.47091, grad norm 4.70929, param norm 87.77165
epoch 11, iter 10360, loss 4.29752, smoothed loss 4.47370, grad norm 4.31922, param norm 87.78236
epoch 11, iter 10365, loss 4.54834, smoothed loss 4.47802, grad norm 4.55277, param norm 87.79260
epoch 11, iter 10370, loss 5.28266, smoothed loss 4.49458, grad norm 4.26150, param norm 87.80438
epoch 11, iter 10375, loss 4.41429, smoothed loss 4.50446, grad norm 4.53284, param norm 87.81585
epoch 11, iter 10380, loss 4.03380, smoothed loss 4.49172, grad norm 4.20399, param norm 87.82767
epoch 12, iter 10385, loss 5.69782, smoothed loss 4.50372, grad norm 5.00142, param norm 87.84261
epoch 12, iter 10390, loss 4.06573, smoothed loss 4.48443, grad norm 4.41157, param norm 87.85763
epoch 12, iter 10395, loss 4.20585, smoothed loss 4.46984, grad norm 4.18636, param norm 87.87566
epoch 12, iter 10400, loss 4.37320, smoothed loss 4.46822, grad norm 4.26035, param norm 87.89097
epoch 12, iter 10405, loss 4.47546, smoothed loss 4.47697, grad norm 4.77691, param norm 87.90433
epoch 12, iter 10410, loss 3.95890, smoothed loss 4.46945, grad norm 4.10233, param norm 87.91794
epoch 12, iter 10415, loss 4.47286, smoothed loss 4.46993, grad norm 4.44100, param norm 87.93211
epoch 12, iter 10420, loss 4.43459, smoothed loss 4.45961, grad norm 4.11687, param norm 87.94575
epoch 12, iter 10425, loss 4.66620, smoothed loss 4.47837, grad norm 4.54480, param norm 87.95615
epoch 12, iter 10430, loss 4.06454, smoothed loss 4.46912, grad norm 4.08064, param norm 87.96897
epoch 12, iter 10435, loss 4.13048, smoothed loss 4.46525, grad norm 3.81407, param norm 87.98494
epoch 12, iter 10440, loss 5.37091, smoothed loss 4.47447, grad norm 4.72226, param norm 88.00072
epoch 12, iter 10445, loss 4.88825, smoothed loss 4.46209, grad norm 4.67497, param norm 88.01565
epoch 12, iter 10450, loss 3.84710, smoothed loss 4.46166, grad norm 4.24289, param norm 88.03138
epoch 12, iter 10455, loss 4.51505, smoothed loss 4.45913, grad norm 4.58447, param norm 88.04817
epoch 12, iter 10460, loss 4.75777, smoothed loss 4.45925, grad norm 4.55552, param norm 88.06083
epoch 12, iter 10465, loss 4.51328, smoothed loss 4.46003, grad norm 4.53164, param norm 88.07313
epoch 12, iter 10470, loss 4.66183, smoothed loss 4.46313, grad norm 4.28189, param norm 88.08626
epoch 12, iter 10475, loss 4.02337, smoothed loss 4.45455, grad norm 4.11187, param norm 88.09910
epoch 12, iter 10480, loss 5.65173, smoothed loss 4.45690, grad norm 4.17699, param norm 88.11308
epoch 12, iter 10485, loss 3.80670, smoothed loss 4.45726, grad norm 3.87560, param norm 88.12484
epoch 12, iter 10490, loss 4.49963, smoothed loss 4.45717, grad norm 4.19431, param norm 88.13692
epoch 12, iter 10495, loss 4.68292, smoothed loss 4.46000, grad norm 4.68531, param norm 88.14845
epoch 12, iter 10500, loss 5.59407, smoothed loss 4.48158, grad norm 4.47502, param norm 88.15981
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 12, Iter 10500, dev loss: 4.936144
Calculating Train F1/EM...
F1 train: 1000 examples took 8.83331 seconds [Score: 0.51940]
Exact Match train: 1000 examples took 9.08768 seconds [Score: 0.39400]
Epoch 12, Iter 10500, Train F1 score: 0.519405, Train EM score: 0.394000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.93614 seconds [Score: 0.35073]
Exact Match dev: 7118 examples took 58.09594 seconds [Score: 0.24698]
Epoch 12, Iter 10500, Dev F1 score: 0.350731, Dev EM score: 0.246979
End of epoch 12
epoch 12, iter 10505, loss 3.72776, smoothed loss 4.45368, grad norm 4.37326, param norm 88.17166
epoch 12, iter 10510, loss 4.10589, smoothed loss 4.44273, grad norm 3.77803, param norm 88.18810
epoch 12, iter 10515, loss 4.54948, smoothed loss 4.44050, grad norm 4.38232, param norm 88.20641
epoch 12, iter 10520, loss 5.26788, smoothed loss 4.43919, grad norm 4.47264, param norm 88.22195
epoch 12, iter 10525, loss 4.69719, smoothed loss 4.43660, grad norm 4.66437, param norm 88.23536
epoch 12, iter 10530, loss 5.29672, smoothed loss 4.44702, grad norm 4.32172, param norm 88.24728
epoch 12, iter 10535, loss 4.71920, smoothed loss 4.44400, grad norm 4.77111, param norm 88.25980
epoch 12, iter 10540, loss 4.25154, smoothed loss 4.44574, grad norm 4.80741, param norm 88.27228
epoch 12, iter 10545, loss 4.48683, smoothed loss 4.44094, grad norm 4.19846, param norm 88.28660
epoch 12, iter 10550, loss 4.39768, smoothed loss 4.45698, grad norm 4.02190, param norm 88.30241
epoch 12, iter 10555, loss 4.86223, smoothed loss 4.45870, grad norm 4.10266, param norm 88.31721
epoch 12, iter 10560, loss 4.30830, smoothed loss 4.46915, grad norm 4.09897, param norm 88.33052
epoch 12, iter 10565, loss 3.78774, smoothed loss 4.46212, grad norm 4.68902, param norm 88.34512
epoch 12, iter 10570, loss 4.48686, smoothed loss 4.44874, grad norm 4.62294, param norm 88.36044
epoch 12, iter 10575, loss 3.85080, smoothed loss 4.43710, grad norm 4.49526, param norm 88.37589
epoch 12, iter 10580, loss 5.37329, smoothed loss 4.45356, grad norm 4.59066, param norm 88.38877
epoch 12, iter 10585, loss 4.08542, smoothed loss 4.44736, grad norm 4.22778, param norm 88.39664
epoch 12, iter 10590, loss 4.80437, smoothed loss 4.45152, grad norm 4.54525, param norm 88.40611
epoch 12, iter 10595, loss 4.00757, smoothed loss 4.44205, grad norm 4.23238, param norm 88.41528
epoch 12, iter 10600, loss 3.84528, smoothed loss 4.44002, grad norm 3.83407, param norm 88.42831
epoch 12, iter 10605, loss 5.09524, smoothed loss 4.44631, grad norm 3.80728, param norm 88.44228
epoch 12, iter 10610, loss 5.40758, smoothed loss 4.45570, grad norm 4.74968, param norm 88.45606
epoch 12, iter 10615, loss 4.29120, smoothed loss 4.44922, grad norm 4.25119, param norm 88.47109
epoch 12, iter 10620, loss 4.15342, smoothed loss 4.44697, grad norm 3.96712, param norm 88.48524
epoch 12, iter 10625, loss 4.50595, smoothed loss 4.44348, grad norm 3.95043, param norm 88.49897
epoch 12, iter 10630, loss 4.50765, smoothed loss 4.43053, grad norm 4.19543, param norm 88.51536
epoch 12, iter 10635, loss 4.48669, smoothed loss 4.43379, grad norm 4.08538, param norm 88.52815
epoch 12, iter 10640, loss 4.32963, smoothed loss 4.44062, grad norm 4.13993, param norm 88.53837
epoch 12, iter 10645, loss 5.08560, smoothed loss 4.44104, grad norm 4.36605, param norm 88.54868
epoch 12, iter 10650, loss 4.93240, smoothed loss 4.44234, grad norm 4.83573, param norm 88.56051
epoch 12, iter 10655, loss 4.70859, smoothed loss 4.44970, grad norm 4.81953, param norm 88.57455
epoch 12, iter 10660, loss 4.05616, smoothed loss 4.45626, grad norm 4.58339, param norm 88.58823
epoch 12, iter 10665, loss 4.37671, smoothed loss 4.45918, grad norm 4.56578, param norm 88.60102
epoch 12, iter 10670, loss 4.79528, smoothed loss 4.47287, grad norm 4.76402, param norm 88.61114
epoch 12, iter 10675, loss 4.34842, smoothed loss 4.47311, grad norm 3.77922, param norm 88.62292
epoch 12, iter 10680, loss 4.64633, smoothed loss 4.47377, grad norm 4.34679, param norm 88.63641
epoch 12, iter 10685, loss 4.51702, smoothed loss 4.47502, grad norm 4.42983, param norm 88.64970
epoch 12, iter 10690, loss 4.04234, smoothed loss 4.46754, grad norm 4.16578, param norm 88.66251
epoch 12, iter 10695, loss 4.49849, smoothed loss 4.47090, grad norm 4.46295, param norm 88.67445
epoch 12, iter 10700, loss 4.69835, smoothed loss 4.46643, grad norm 4.10245, param norm 88.68443
epoch 12, iter 10705, loss 4.43114, smoothed loss 4.45710, grad norm 4.28552, param norm 88.69566
epoch 12, iter 10710, loss 4.24183, smoothed loss 4.45441, grad norm 4.41109, param norm 88.70595
epoch 12, iter 10715, loss 5.00008, smoothed loss 4.45242, grad norm 4.28877, param norm 88.71754
epoch 12, iter 10720, loss 4.53501, smoothed loss 4.45316, grad norm 4.64317, param norm 88.73055
epoch 12, iter 10725, loss 4.73547, smoothed loss 4.46776, grad norm 3.48859, param norm 88.74293
epoch 12, iter 10730, loss 4.24724, smoothed loss 4.45935, grad norm 4.08000, param norm 88.75696
epoch 12, iter 10735, loss 4.26813, smoothed loss 4.45368, grad norm 3.79574, param norm 88.77127
epoch 12, iter 10740, loss 3.63005, smoothed loss 4.46065, grad norm 3.65213, param norm 88.78378
epoch 12, iter 10745, loss 4.42192, smoothed loss 4.46577, grad norm 4.42347, param norm 88.79462
epoch 12, iter 10750, loss 4.57811, smoothed loss 4.46834, grad norm 4.31455, param norm 88.80695
epoch 12, iter 10755, loss 3.58321, smoothed loss 4.45517, grad norm 4.12912, param norm 88.81896
epoch 12, iter 10760, loss 3.71883, smoothed loss 4.43996, grad norm 4.30239, param norm 88.83372
epoch 12, iter 10765, loss 3.62126, smoothed loss 4.43269, grad norm 4.05660, param norm 88.85143
epoch 12, iter 10770, loss 4.37060, smoothed loss 4.42926, grad norm 4.24314, param norm 88.86563
epoch 12, iter 10775, loss 4.93762, smoothed loss 4.43377, grad norm 4.81947, param norm 88.87613
epoch 12, iter 10780, loss 5.33556, smoothed loss 4.43944, grad norm 4.75743, param norm 88.88509
epoch 12, iter 10785, loss 5.08426, smoothed loss 4.46259, grad norm 3.98337, param norm 88.89512
epoch 12, iter 10790, loss 4.35769, smoothed loss 4.44292, grad norm 4.47797, param norm 88.90762
epoch 12, iter 10795, loss 4.92005, smoothed loss 4.44610, grad norm 5.36250, param norm 88.92376
epoch 12, iter 10800, loss 4.33817, smoothed loss 4.45479, grad norm 5.11266, param norm 88.93899
epoch 12, iter 10805, loss 4.44446, smoothed loss 4.46711, grad norm 4.04985, param norm 88.95245
epoch 12, iter 10810, loss 4.08374, smoothed loss 4.45005, grad norm 4.02581, param norm 88.96756
epoch 12, iter 10815, loss 4.07834, smoothed loss 4.45162, grad norm 4.77362, param norm 88.98258
epoch 12, iter 10820, loss 4.01123, smoothed loss 4.44685, grad norm 3.95283, param norm 88.99524
epoch 12, iter 10825, loss 4.56371, smoothed loss 4.43773, grad norm 4.15572, param norm 89.01110
epoch 12, iter 10830, loss 4.13595, smoothed loss 4.43202, grad norm 4.68956, param norm 89.02644
epoch 12, iter 10835, loss 4.08438, smoothed loss 4.41681, grad norm 4.43348, param norm 89.03822
epoch 12, iter 10840, loss 4.30177, smoothed loss 4.42102, grad norm 4.47234, param norm 89.05009
epoch 12, iter 10845, loss 4.21702, smoothed loss 4.42703, grad norm 4.19680, param norm 89.06078
epoch 12, iter 10850, loss 4.50918, smoothed loss 4.42637, grad norm 4.39325, param norm 89.07304
epoch 12, iter 10855, loss 4.16493, smoothed loss 4.42676, grad norm 4.05767, param norm 89.08505
epoch 12, iter 10860, loss 4.30035, smoothed loss 4.42273, grad norm 4.36921, param norm 89.09745
epoch 12, iter 10865, loss 3.81378, smoothed loss 4.40845, grad norm 3.66475, param norm 89.11055
epoch 12, iter 10870, loss 4.65300, smoothed loss 4.41025, grad norm 4.87437, param norm 89.12497
epoch 12, iter 10875, loss 3.85717, smoothed loss 4.41053, grad norm 3.96007, param norm 89.13844
epoch 12, iter 10880, loss 4.14783, smoothed loss 4.40118, grad norm 4.13483, param norm 89.15246
epoch 12, iter 10885, loss 4.07445, smoothed loss 4.40384, grad norm 4.45617, param norm 89.16563
epoch 12, iter 10890, loss 4.58828, smoothed loss 4.39421, grad norm 4.54214, param norm 89.17844
epoch 12, iter 10895, loss 4.20460, smoothed loss 4.38719, grad norm 4.19753, param norm 89.19131
epoch 12, iter 10900, loss 3.79051, smoothed loss 4.38510, grad norm 4.34064, param norm 89.20251
epoch 12, iter 10905, loss 3.82959, smoothed loss 4.37781, grad norm 4.06117, param norm 89.21335
epoch 12, iter 10910, loss 4.11042, smoothed loss 4.37454, grad norm 4.42342, param norm 89.22510
epoch 12, iter 10915, loss 3.78054, smoothed loss 4.37714, grad norm 3.61214, param norm 89.23637
epoch 12, iter 10920, loss 5.01513, smoothed loss 4.38237, grad norm 4.88121, param norm 89.24877
epoch 12, iter 10925, loss 4.37364, smoothed loss 4.37458, grad norm 4.45513, param norm 89.26067
epoch 12, iter 10930, loss 4.19957, smoothed loss 4.36997, grad norm 4.53860, param norm 89.27346
epoch 12, iter 10935, loss 4.14547, smoothed loss 4.37023, grad norm 4.85125, param norm 89.28264
epoch 12, iter 10940, loss 3.86732, smoothed loss 4.36873, grad norm 4.18307, param norm 89.29092
epoch 12, iter 10945, loss 4.69968, smoothed loss 4.37927, grad norm 4.69409, param norm 89.30272
epoch 12, iter 10950, loss 3.60030, smoothed loss 4.37723, grad norm 4.36132, param norm 89.31447
epoch 12, iter 10955, loss 4.78300, smoothed loss 4.38157, grad norm 4.63315, param norm 89.32527
epoch 12, iter 10960, loss 3.77465, smoothed loss 4.36235, grad norm 4.22858, param norm 89.33757
epoch 12, iter 10965, loss 3.42253, smoothed loss 4.35689, grad norm 4.68235, param norm 89.35164
epoch 12, iter 10970, loss 4.54474, smoothed loss 4.37876, grad norm 3.88315, param norm 89.36268
epoch 12, iter 10975, loss 4.40014, smoothed loss 4.37601, grad norm 5.02659, param norm 89.37303
epoch 12, iter 10980, loss 4.18800, smoothed loss 4.37820, grad norm 5.01457, param norm 89.38514
epoch 12, iter 10985, loss 4.07510, smoothed loss 4.37304, grad norm 4.92819, param norm 89.39699
epoch 12, iter 10990, loss 4.71068, smoothed loss 4.37788, grad norm 4.64017, param norm 89.40898
epoch 12, iter 10995, loss 4.66309, smoothed loss 4.38909, grad norm 4.89730, param norm 89.42174
epoch 12, iter 11000, loss 4.34369, smoothed loss 4.38366, grad norm 3.83903, param norm 89.43465
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 12, Iter 11000, dev loss: 4.909353
Calculating Train F1/EM...
F1 train: 1000 examples took 8.95292 seconds [Score: 0.48427]
Exact Match train: 1000 examples took 9.20117 seconds [Score: 0.40600]
Epoch 12, Iter 11000, Train F1 score: 0.484269, Train EM score: 0.406000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 57.93591 seconds [Score: 0.35593]
Exact Match dev: 7118 examples took 58.37212 seconds [Score: 0.25077]
Epoch 12, Iter 11000, Dev F1 score: 0.355931, Dev EM score: 0.250773
End of epoch 12
epoch 12, iter 11005, loss 4.52878, smoothed loss 4.39496, grad norm 4.14162, param norm 89.44709
epoch 12, iter 11010, loss 4.19201, smoothed loss 4.37612, grad norm 4.23231, param norm 89.46005
epoch 12, iter 11015, loss 3.89162, smoothed loss 4.38456, grad norm 4.24270, param norm 89.47333
epoch 12, iter 11020, loss 3.56389, smoothed loss 4.36705, grad norm 4.11204, param norm 89.48672
epoch 12, iter 11025, loss 4.52343, smoothed loss 4.36023, grad norm 4.81148, param norm 89.49992
epoch 12, iter 11030, loss 3.77902, smoothed loss 4.36126, grad norm 3.93429, param norm 89.51344
epoch 12, iter 11035, loss 4.26249, smoothed loss 4.36170, grad norm 4.37769, param norm 89.52694
epoch 12, iter 11040, loss 4.39252, smoothed loss 4.36969, grad norm 4.24313, param norm 89.54112
epoch 12, iter 11045, loss 4.55275, smoothed loss 4.37179, grad norm 4.62913, param norm 89.55417
epoch 12, iter 11050, loss 4.97217, smoothed loss 4.38334, grad norm 5.52184, param norm 89.56383
epoch 12, iter 11055, loss 5.10734, smoothed loss 4.37802, grad norm 4.86357, param norm 89.57609
epoch 12, iter 11060, loss 4.90889, smoothed loss 4.38574, grad norm 4.68625, param norm 89.58924
epoch 12, iter 11065, loss 4.69586, smoothed loss 4.38017, grad norm 4.34415, param norm 89.60242
epoch 12, iter 11070, loss 4.12227, smoothed loss 4.37910, grad norm 4.28639, param norm 89.61607
epoch 12, iter 11075, loss 4.21947, smoothed loss 4.37911, grad norm 4.26908, param norm 89.62765
epoch 12, iter 11080, loss 4.13697, smoothed loss 4.38702, grad norm 4.11984, param norm 89.63646
epoch 12, iter 11085, loss 4.52647, smoothed loss 4.38518, grad norm 4.75890, param norm 89.64521
epoch 12, iter 11090, loss 3.98007, smoothed loss 4.39189, grad norm 4.27263, param norm 89.65658
epoch 12, iter 11095, loss 5.12304, smoothed loss 4.40737, grad norm 4.80142, param norm 89.66772
epoch 12, iter 11100, loss 4.33242, smoothed loss 4.39788, grad norm 4.31374, param norm 89.67607
epoch 12, iter 11105, loss 3.94847, smoothed loss 4.40126, grad norm 4.78230, param norm 89.68688
epoch 12, iter 11110, loss 4.25849, smoothed loss 4.38809, grad norm 4.62121, param norm 89.70004
epoch 12, iter 11115, loss 3.95243, smoothed loss 4.38682, grad norm 4.46198, param norm 89.71439
epoch 12, iter 11120, loss 4.34475, smoothed loss 4.37400, grad norm 4.34242, param norm 89.72874
epoch 12, iter 11125, loss 4.78033, smoothed loss 4.37784, grad norm 3.86418, param norm 89.74053
epoch 12, iter 11130, loss 4.68859, smoothed loss 4.37423, grad norm 4.51430, param norm 89.75255
epoch 12, iter 11135, loss 4.03841, smoothed loss 4.37694, grad norm 3.98619, param norm 89.76395
epoch 12, iter 11140, loss 4.43499, smoothed loss 4.38126, grad norm 4.14162, param norm 89.77736
epoch 12, iter 11145, loss 4.74477, smoothed loss 4.38702, grad norm 3.82007, param norm 89.79198
epoch 12, iter 11150, loss 4.39022, smoothed loss 4.38308, grad norm 4.53898, param norm 89.80626
epoch 12, iter 11155, loss 3.53309, smoothed loss 4.38387, grad norm 4.06001, param norm 89.81971
epoch 12, iter 11160, loss 5.11196, smoothed loss 4.40383, grad norm 4.74388, param norm 89.83178
epoch 12, iter 11165, loss 4.65883, smoothed loss 4.40910, grad norm 4.09672, param norm 89.84161
epoch 12, iter 11170, loss 5.35121, smoothed loss 4.41738, grad norm 4.35024, param norm 89.85216
epoch 12, iter 11175, loss 4.38614, smoothed loss 4.41069, grad norm 3.85642, param norm 89.86242
epoch 12, iter 11180, loss 4.20243, smoothed loss 4.40733, grad norm 3.86782, param norm 89.87441
epoch 12, iter 11185, loss 4.41374, smoothed loss 4.42923, grad norm 4.56879, param norm 89.88424
epoch 12, iter 11190, loss 4.37277, smoothed loss 4.43476, grad norm 3.70858, param norm 89.89283
epoch 12, iter 11195, loss 4.76782, smoothed loss 4.45253, grad norm 3.94338, param norm 89.90262
epoch 12, iter 11200, loss 4.35274, smoothed loss 4.44358, grad norm 3.96399, param norm 89.91393
epoch 12, iter 11205, loss 4.54106, smoothed loss 4.44124, grad norm 4.62086, param norm 89.92540
epoch 12, iter 11210, loss 4.12419, smoothed loss 4.43779, grad norm 4.41089, param norm 89.93802
epoch 12, iter 11215, loss 4.85067, smoothed loss 4.44499, grad norm 4.73525, param norm 89.95065
epoch 12, iter 11220, loss 4.14427, smoothed loss 4.43359, grad norm 4.80191, param norm 89.96129
epoch 12, iter 11225, loss 4.22000, smoothed loss 4.43508, grad norm 4.06369, param norm 89.97454
epoch 12, iter 11230, loss 4.84918, smoothed loss 4.45768, grad norm 4.65048, param norm 89.98546
epoch 12, iter 11235, loss 4.45085, smoothed loss 4.45632, grad norm 4.11637, param norm 89.99548
epoch 12, iter 11240, loss 4.18589, smoothed loss 4.45287, grad norm 4.22304, param norm 90.00597
epoch 12, iter 11245, loss 4.33886, smoothed loss 4.43916, grad norm 4.34577, param norm 90.01926
epoch 12, iter 11250, loss 4.45125, smoothed loss 4.43125, grad norm 4.58624, param norm 90.03220
epoch 12, iter 11255, loss 3.72743, smoothed loss 4.43621, grad norm 3.87712, param norm 90.04555
epoch 12, iter 11260, loss 4.25018, smoothed loss 4.42483, grad norm 4.52870, param norm 90.06042
epoch 12, iter 11265, loss 4.78445, smoothed loss 4.42159, grad norm 4.53330, param norm 90.07633
epoch 12, iter 11270, loss 4.33613, smoothed loss 4.41267, grad norm 4.19815, param norm 90.09011
epoch 12, iter 11275, loss 4.35328, smoothed loss 4.40221, grad norm 4.55295, param norm 90.10309
epoch 12, iter 11280, loss 4.34963, smoothed loss 4.38655, grad norm 4.83178, param norm 90.11789
epoch 12, iter 11285, loss 4.64781, smoothed loss 4.38929, grad norm 4.10764, param norm 90.13187
epoch 12, iter 11290, loss 4.02979, smoothed loss 4.39959, grad norm 4.36924, param norm 90.14414
epoch 12, iter 11295, loss 5.03434, smoothed loss 4.39947, grad norm 5.57132, param norm 90.15604
epoch 12, iter 11300, loss 3.88304, smoothed loss 4.39806, grad norm 4.20281, param norm 90.16888
epoch 12, iter 11305, loss 4.23717, smoothed loss 4.39367, grad norm 3.96547, param norm 90.18182
epoch 12, iter 11310, loss 4.16802, smoothed loss 4.38307, grad norm 4.30524, param norm 90.19467
epoch 12, iter 11315, loss 4.22479, smoothed loss 4.37233, grad norm 3.92775, param norm 90.20776
epoch 12, iter 11320, loss 4.37987, smoothed loss 4.38348, grad norm 4.62071, param norm 90.21775
epoch 12, iter 11325, loss 4.57518, smoothed loss 4.38592, grad norm 4.19417, param norm 90.22769
epoch 13, iter 11330, loss 4.58450, smoothed loss 4.39320, grad norm 4.23908, param norm 90.23831
epoch 13, iter 11335, loss 4.68127, smoothed loss 4.39056, grad norm 4.71605, param norm 90.25031
epoch 13, iter 11340, loss 4.83734, smoothed loss 4.38693, grad norm 5.03962, param norm 90.26366
epoch 13, iter 11345, loss 4.47581, smoothed loss 4.40778, grad norm 4.65433, param norm 90.27519
epoch 13, iter 11350, loss 4.43541, smoothed loss 4.41552, grad norm 4.82503, param norm 90.28657
epoch 13, iter 11355, loss 4.52218, smoothed loss 4.41801, grad norm 4.87729, param norm 90.30083
epoch 13, iter 11360, loss 4.33820, smoothed loss 4.40314, grad norm 4.96095, param norm 90.31618
epoch 13, iter 11365, loss 4.33968, smoothed loss 4.39272, grad norm 4.34768, param norm 90.33107
epoch 13, iter 11370, loss 4.53903, smoothed loss 4.39047, grad norm 4.73150, param norm 90.34495
epoch 13, iter 11375, loss 4.34808, smoothed loss 4.39259, grad norm 4.72367, param norm 90.35857
epoch 13, iter 11380, loss 3.95596, smoothed loss 4.37394, grad norm 4.51573, param norm 90.37341
epoch 13, iter 11385, loss 4.47923, smoothed loss 4.38398, grad norm 5.32965, param norm 90.38834
epoch 13, iter 11390, loss 3.97320, smoothed loss 4.37467, grad norm 4.05664, param norm 90.40030
epoch 13, iter 11395, loss 5.18217, smoothed loss 4.40440, grad norm 4.22242, param norm 90.41006
epoch 13, iter 11400, loss 4.03633, smoothed loss 4.39404, grad norm 4.57381, param norm 90.42036
epoch 13, iter 11405, loss 4.89407, smoothed loss 4.38979, grad norm 3.88951, param norm 90.43060
epoch 13, iter 11410, loss 4.51958, smoothed loss 4.39503, grad norm 4.51345, param norm 90.44245
epoch 13, iter 11415, loss 4.34243, smoothed loss 4.39146, grad norm 4.14702, param norm 90.45609
epoch 13, iter 11420, loss 4.29396, smoothed loss 4.36930, grad norm 4.04350, param norm 90.47260
epoch 13, iter 11425, loss 4.09883, smoothed loss 4.36077, grad norm 4.43135, param norm 90.48757
epoch 13, iter 11430, loss 5.55833, smoothed loss 4.36990, grad norm 5.26807, param norm 90.50262
epoch 13, iter 11435, loss 3.87838, smoothed loss 4.36431, grad norm 4.14522, param norm 90.51312
epoch 13, iter 11440, loss 3.91593, smoothed loss 4.36441, grad norm 4.82791, param norm 90.52316
epoch 13, iter 11445, loss 4.93919, smoothed loss 4.38311, grad norm 3.91835, param norm 90.53423
epoch 13, iter 11450, loss 3.98417, smoothed loss 4.38096, grad norm 4.65978, param norm 90.54642
epoch 13, iter 11455, loss 4.12669, smoothed loss 4.37653, grad norm 3.93187, param norm 90.56124
epoch 13, iter 11460, loss 4.23959, smoothed loss 4.37285, grad norm 4.33074, param norm 90.57535
epoch 13, iter 11465, loss 5.52982, smoothed loss 4.38435, grad norm 5.39678, param norm 90.58890
epoch 13, iter 11470, loss 4.16012, smoothed loss 4.37464, grad norm 4.17663, param norm 90.59988
epoch 13, iter 11475, loss 4.25359, smoothed loss 4.35618, grad norm 4.66049, param norm 90.61428
epoch 13, iter 11480, loss 4.84859, smoothed loss 4.36186, grad norm 4.73350, param norm 90.62728
epoch 13, iter 11485, loss 4.35872, smoothed loss 4.35018, grad norm 4.47930, param norm 90.63692
epoch 13, iter 11490, loss 3.90745, smoothed loss 4.35149, grad norm 4.38442, param norm 90.64693
epoch 13, iter 11495, loss 3.91454, smoothed loss 4.35082, grad norm 4.58671, param norm 90.65877
epoch 13, iter 11500, loss 4.57639, smoothed loss 4.36305, grad norm 4.71967, param norm 90.67052
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 13, Iter 11500, dev loss: 4.915224
Calculating Train F1/EM...
F1 train: 1000 examples took 9.37172 seconds [Score: 0.55518]
Exact Match train: 1000 examples took 9.10241 seconds [Score: 0.45300]
Epoch 13, Iter 11500, Train F1 score: 0.555184, Train EM score: 0.453000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 59.02124 seconds [Score: 0.36166]
Exact Match dev: 7118 examples took 58.14690 seconds [Score: 0.25948]
Epoch 13, Iter 11500, Dev F1 score: 0.361662, Dev EM score: 0.259483
End of epoch 13
epoch 13, iter 11505, loss 4.82526, smoothed loss 4.36436, grad norm 4.39673, param norm 90.68172
epoch 13, iter 11510, loss 3.78835, smoothed loss 4.36061, grad norm 3.98266, param norm 90.69476
epoch 13, iter 11515, loss 3.74620, smoothed loss 4.35538, grad norm 4.05716, param norm 90.70865
epoch 13, iter 11520, loss 5.42204, smoothed loss 4.36513, grad norm 4.47361, param norm 90.72258
epoch 13, iter 11525, loss 4.91297, smoothed loss 4.37221, grad norm 4.15489, param norm 90.73278
epoch 13, iter 11530, loss 4.11912, smoothed loss 4.37001, grad norm 4.97178, param norm 90.74460
epoch 13, iter 11535, loss 4.81697, smoothed loss 4.38766, grad norm 5.25358, param norm 90.75735
epoch 13, iter 11540, loss 4.68976, smoothed loss 4.40346, grad norm 4.57945, param norm 90.76906
epoch 13, iter 11545, loss 4.36917, smoothed loss 4.40919, grad norm 4.23683, param norm 90.78073
epoch 13, iter 11550, loss 4.36844, smoothed loss 4.39341, grad norm 4.60486, param norm 90.79585
epoch 13, iter 11555, loss 4.74281, smoothed loss 4.40760, grad norm 4.87297, param norm 90.81001
epoch 13, iter 11560, loss 4.42142, smoothed loss 4.40432, grad norm 4.62112, param norm 90.82240
epoch 13, iter 11565, loss 4.04728, smoothed loss 4.41544, grad norm 4.35899, param norm 90.83423
epoch 13, iter 11570, loss 5.09893, smoothed loss 4.43593, grad norm 5.02152, param norm 90.84647
epoch 13, iter 11575, loss 4.19527, smoothed loss 4.42802, grad norm 4.19184, param norm 90.85930
epoch 13, iter 11580, loss 3.69418, smoothed loss 4.42312, grad norm 3.92002, param norm 90.87279
epoch 13, iter 11585, loss 4.37254, smoothed loss 4.41191, grad norm 4.84751, param norm 90.88889
epoch 13, iter 11590, loss 4.96374, smoothed loss 4.42116, grad norm 4.98955, param norm 90.90306
epoch 13, iter 11595, loss 4.09828, smoothed loss 4.41824, grad norm 4.05480, param norm 90.91438
epoch 13, iter 11600, loss 3.68170, smoothed loss 4.41581, grad norm 4.15610, param norm 90.92734
epoch 13, iter 11605, loss 4.44836, smoothed loss 4.41748, grad norm 4.26055, param norm 90.94246
epoch 13, iter 11610, loss 3.66201, smoothed loss 4.41392, grad norm 4.58389, param norm 90.95847
epoch 13, iter 11615, loss 4.10306, smoothed loss 4.41607, grad norm 4.09588, param norm 90.97314
epoch 13, iter 11620, loss 4.53311, smoothed loss 4.41287, grad norm 4.90557, param norm 90.98669
epoch 13, iter 11625, loss 5.16475, smoothed loss 4.42493, grad norm 5.00281, param norm 90.99919
epoch 13, iter 11630, loss 4.37728, smoothed loss 4.41898, grad norm 4.23409, param norm 91.01174
epoch 13, iter 11635, loss 4.14196, smoothed loss 4.41377, grad norm 4.12924, param norm 91.02488
epoch 13, iter 11640, loss 4.56980, smoothed loss 4.41890, grad norm 4.13348, param norm 91.03803
epoch 13, iter 11645, loss 4.41410, smoothed loss 4.41852, grad norm 4.43026, param norm 91.05107
epoch 13, iter 11650, loss 4.80973, smoothed loss 4.41846, grad norm 4.78848, param norm 91.06056
epoch 13, iter 11655, loss 4.30819, smoothed loss 4.40741, grad norm 4.03601, param norm 91.07139
epoch 13, iter 11660, loss 4.38120, smoothed loss 4.41224, grad norm 4.69484, param norm 91.08401
epoch 13, iter 11665, loss 4.99198, smoothed loss 4.41711, grad norm 4.05564, param norm 91.09562
epoch 13, iter 11670, loss 4.43758, smoothed loss 4.41485, grad norm 4.73583, param norm 91.10718
epoch 13, iter 11675, loss 4.54708, smoothed loss 4.40664, grad norm 4.39490, param norm 91.12110
epoch 13, iter 11680, loss 4.38366, smoothed loss 4.39367, grad norm 5.30445, param norm 91.13418
epoch 13, iter 11685, loss 4.98215, smoothed loss 4.38734, grad norm 4.47775, param norm 91.14853
epoch 13, iter 11690, loss 4.96210, smoothed loss 4.39869, grad norm 5.06977, param norm 91.16277
epoch 13, iter 11695, loss 4.23835, smoothed loss 4.39851, grad norm 4.71321, param norm 91.17772
epoch 13, iter 11700, loss 4.40897, smoothed loss 4.37755, grad norm 4.69856, param norm 91.19423
epoch 13, iter 11705, loss 4.37161, smoothed loss 4.37848, grad norm 5.13120, param norm 91.21122
epoch 13, iter 11710, loss 4.12063, smoothed loss 4.38262, grad norm 4.73739, param norm 91.22482
epoch 13, iter 11715, loss 4.28000, smoothed loss 4.39132, grad norm 3.89729, param norm 91.23692
epoch 13, iter 11720, loss 4.49673, smoothed loss 4.38124, grad norm 4.40414, param norm 91.24852
epoch 13, iter 11725, loss 4.21794, smoothed loss 4.39129, grad norm 5.39899, param norm 91.26122
epoch 13, iter 11730, loss 4.33848, smoothed loss 4.38104, grad norm 4.50598, param norm 91.27547
epoch 13, iter 11735, loss 4.03290, smoothed loss 4.37016, grad norm 3.99044, param norm 91.28867
epoch 13, iter 11740, loss 4.95607, smoothed loss 4.37423, grad norm 5.17100, param norm 91.30161
epoch 13, iter 11745, loss 4.43833, smoothed loss 4.39861, grad norm 4.54642, param norm 91.31184
epoch 13, iter 11750, loss 5.19360, smoothed loss 4.40601, grad norm 4.69976, param norm 91.32030
epoch 13, iter 11755, loss 4.49495, smoothed loss 4.40414, grad norm 4.13201, param norm 91.32988
epoch 13, iter 11760, loss 4.54755, smoothed loss 4.39987, grad norm 4.33273, param norm 91.34184
epoch 13, iter 11765, loss 4.38316, smoothed loss 4.39725, grad norm 4.00789, param norm 91.35509
epoch 13, iter 11770, loss 4.22368, smoothed loss 4.40774, grad norm 4.59142, param norm 91.36677
epoch 13, iter 11775, loss 4.60799, smoothed loss 4.40213, grad norm 4.56796, param norm 91.37634
epoch 13, iter 11780, loss 4.98214, smoothed loss 4.41739, grad norm 5.00704, param norm 91.38510
epoch 13, iter 11785, loss 3.75259, smoothed loss 4.40260, grad norm 3.88889, param norm 91.39394
epoch 13, iter 11790, loss 5.02153, smoothed loss 4.40728, grad norm 5.36871, param norm 91.40456
epoch 13, iter 11795, loss 4.08218, smoothed loss 4.40215, grad norm 4.60255, param norm 91.41589
epoch 13, iter 11800, loss 4.01050, smoothed loss 4.38923, grad norm 3.98327, param norm 91.43022
epoch 13, iter 11805, loss 3.83515, smoothed loss 4.38770, grad norm 4.36007, param norm 91.44247
epoch 13, iter 11810, loss 3.72917, smoothed loss 4.37708, grad norm 4.30045, param norm 91.45603
epoch 13, iter 11815, loss 4.48291, smoothed loss 4.37628, grad norm 5.18058, param norm 91.46935
epoch 13, iter 11820, loss 4.73702, smoothed loss 4.36904, grad norm 5.80589, param norm 91.48202
epoch 13, iter 11825, loss 4.37834, smoothed loss 4.36790, grad norm 4.62964, param norm 91.49527
epoch 13, iter 11830, loss 4.59400, smoothed loss 4.36523, grad norm 4.74040, param norm 91.50672
epoch 13, iter 11835, loss 5.39550, smoothed loss 4.36926, grad norm 4.29300, param norm 91.51835
epoch 13, iter 11840, loss 5.50758, smoothed loss 4.39041, grad norm 4.03548, param norm 91.52858
epoch 13, iter 11845, loss 3.84849, smoothed loss 4.39129, grad norm 4.26176, param norm 91.53912
epoch 13, iter 11850, loss 4.06068, smoothed loss 4.38945, grad norm 4.37021, param norm 91.55040
epoch 13, iter 11855, loss 4.25476, smoothed loss 4.38655, grad norm 4.49429, param norm 91.56320
epoch 13, iter 11860, loss 4.25224, smoothed loss 4.38175, grad norm 4.39250, param norm 91.57700
epoch 13, iter 11865, loss 4.13005, smoothed loss 4.37925, grad norm 4.24563, param norm 91.59079
epoch 13, iter 11870, loss 4.42502, smoothed loss 4.37176, grad norm 5.11498, param norm 91.60333
epoch 13, iter 11875, loss 4.15511, smoothed loss 4.36775, grad norm 4.24489, param norm 91.61710
epoch 13, iter 11880, loss 4.35349, smoothed loss 4.37181, grad norm 4.32406, param norm 91.63061
epoch 13, iter 11885, loss 4.19478, smoothed loss 4.35493, grad norm 4.37714, param norm 91.64297
epoch 13, iter 11890, loss 4.46254, smoothed loss 4.36005, grad norm 4.75102, param norm 91.65364
epoch 13, iter 11895, loss 3.99123, smoothed loss 4.35452, grad norm 3.95612, param norm 91.66302
epoch 13, iter 11900, loss 4.23725, smoothed loss 4.35253, grad norm 4.45540, param norm 91.67390
epoch 13, iter 11905, loss 4.15634, smoothed loss 4.33634, grad norm 3.94557, param norm 91.68580
epoch 13, iter 11910, loss 5.03350, smoothed loss 4.33542, grad norm 4.82744, param norm 91.69652
epoch 13, iter 11915, loss 4.60825, smoothed loss 4.34216, grad norm 4.42445, param norm 91.70681
epoch 13, iter 11920, loss 4.14597, smoothed loss 4.33694, grad norm 4.32068, param norm 91.71895
epoch 13, iter 11925, loss 4.08436, smoothed loss 4.32145, grad norm 4.27040, param norm 91.73151
epoch 13, iter 11930, loss 3.82662, smoothed loss 4.30983, grad norm 4.29659, param norm 91.74386
epoch 13, iter 11935, loss 3.57541, smoothed loss 4.30920, grad norm 3.89471, param norm 91.75499
epoch 13, iter 11940, loss 4.60060, smoothed loss 4.30553, grad norm 4.82873, param norm 91.76691
epoch 13, iter 11945, loss 4.34165, smoothed loss 4.29513, grad norm 4.54628, param norm 91.78053
epoch 13, iter 11950, loss 4.95293, smoothed loss 4.29857, grad norm 5.65390, param norm 91.79502
epoch 13, iter 11955, loss 4.61010, smoothed loss 4.29541, grad norm 4.56179, param norm 91.80593
epoch 13, iter 11960, loss 4.50204, smoothed loss 4.30868, grad norm 5.37618, param norm 91.81573
epoch 13, iter 11965, loss 4.30624, smoothed loss 4.29410, grad norm 4.30140, param norm 91.82585
epoch 13, iter 11970, loss 4.09494, smoothed loss 4.30282, grad norm 4.54213, param norm 91.83554
epoch 13, iter 11975, loss 3.40366, smoothed loss 4.29634, grad norm 3.92940, param norm 91.84511
epoch 13, iter 11980, loss 4.18940, smoothed loss 4.30549, grad norm 4.97603, param norm 91.85595
epoch 13, iter 11985, loss 4.71800, smoothed loss 4.30670, grad norm 4.44090, param norm 91.86970
epoch 13, iter 11990, loss 4.26402, smoothed loss 4.29798, grad norm 3.97721, param norm 91.88275
epoch 13, iter 11995, loss 4.69841, smoothed loss 4.29133, grad norm 4.75662, param norm 91.89499
epoch 13, iter 12000, loss 4.56028, smoothed loss 4.29592, grad norm 4.75481, param norm 91.90722
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 13, Iter 12000, dev loss: 4.917872
Calculating Train F1/EM...
F1 train: 1000 examples took 9.09579 seconds [Score: 0.49936]
Exact Match train: 1000 examples took 9.12010 seconds [Score: 0.41700]
Epoch 13, Iter 12000, Train F1 score: 0.499364, Train EM score: 0.417000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.63353 seconds [Score: 0.35859]
Exact Match dev: 7118 examples took 58.15585 seconds [Score: 0.25527]
Epoch 13, Iter 12000, Dev F1 score: 0.358590, Dev EM score: 0.255268
End of epoch 13
epoch 13, iter 12005, loss 3.93584, smoothed loss 4.28754, grad norm 3.97944, param norm 91.91911
epoch 13, iter 12010, loss 4.53736, smoothed loss 4.28936, grad norm 4.30510, param norm 91.93000
epoch 13, iter 12015, loss 4.58874, smoothed loss 4.29384, grad norm 5.23874, param norm 91.93989
epoch 13, iter 12020, loss 3.64454, smoothed loss 4.30464, grad norm 3.86480, param norm 91.94730
epoch 13, iter 12025, loss 3.94184, smoothed loss 4.29460, grad norm 4.20731, param norm 91.95589
epoch 13, iter 12030, loss 4.51043, smoothed loss 4.30991, grad norm 4.87223, param norm 91.96696
epoch 13, iter 12035, loss 4.16267, smoothed loss 4.30278, grad norm 4.37799, param norm 91.97878
epoch 13, iter 12040, loss 4.10273, smoothed loss 4.31284, grad norm 4.50980, param norm 91.99081
epoch 13, iter 12045, loss 4.56618, smoothed loss 4.32007, grad norm 5.09055, param norm 92.00353
epoch 13, iter 12050, loss 4.47994, smoothed loss 4.32780, grad norm 4.13596, param norm 92.01706
epoch 13, iter 12055, loss 4.00525, smoothed loss 4.32164, grad norm 4.33124, param norm 92.03050
epoch 13, iter 12060, loss 4.32536, smoothed loss 4.32076, grad norm 4.76335, param norm 92.04625
epoch 13, iter 12065, loss 4.88910, smoothed loss 4.31370, grad norm 5.15255, param norm 92.06035
epoch 13, iter 12070, loss 4.42143, smoothed loss 4.32716, grad norm 4.55298, param norm 92.07081
epoch 13, iter 12075, loss 4.67350, smoothed loss 4.32254, grad norm 4.21824, param norm 92.08130
epoch 13, iter 12080, loss 4.70433, smoothed loss 4.32486, grad norm 4.81571, param norm 92.09324
epoch 13, iter 12085, loss 4.45491, smoothed loss 4.33485, grad norm 4.66827, param norm 92.10435
epoch 13, iter 12090, loss 4.13273, smoothed loss 4.33926, grad norm 4.28595, param norm 92.11687
epoch 13, iter 12095, loss 3.79707, smoothed loss 4.33609, grad norm 4.07126, param norm 92.12984
epoch 13, iter 12100, loss 4.46909, smoothed loss 4.33589, grad norm 4.28562, param norm 92.14510
epoch 13, iter 12105, loss 4.87222, smoothed loss 4.34284, grad norm 4.62512, param norm 92.15963
epoch 13, iter 12110, loss 4.09104, smoothed loss 4.34149, grad norm 4.04484, param norm 92.17310
epoch 13, iter 12115, loss 4.57181, smoothed loss 4.34050, grad norm 4.14181, param norm 92.18755
epoch 13, iter 12120, loss 4.94375, smoothed loss 4.35582, grad norm 4.42448, param norm 92.20210
epoch 13, iter 12125, loss 3.93611, smoothed loss 4.35498, grad norm 4.07614, param norm 92.21350
epoch 13, iter 12130, loss 4.21823, smoothed loss 4.35410, grad norm 4.66316, param norm 92.22746
epoch 13, iter 12135, loss 4.19740, smoothed loss 4.34988, grad norm 4.25701, param norm 92.24240
epoch 13, iter 12140, loss 4.12580, smoothed loss 4.36248, grad norm 4.55691, param norm 92.25621
epoch 13, iter 12145, loss 3.91556, smoothed loss 4.35471, grad norm 4.22306, param norm 92.26955
epoch 13, iter 12150, loss 4.03997, smoothed loss 4.34320, grad norm 4.18367, param norm 92.28409
epoch 13, iter 12155, loss 4.19955, smoothed loss 4.34010, grad norm 4.19524, param norm 92.29922
epoch 13, iter 12160, loss 3.88322, smoothed loss 4.34883, grad norm 3.92007, param norm 92.31004
epoch 13, iter 12165, loss 4.21567, smoothed loss 4.35452, grad norm 4.44069, param norm 92.31927
epoch 13, iter 12170, loss 4.40499, smoothed loss 4.34906, grad norm 4.02994, param norm 92.33013
epoch 13, iter 12175, loss 4.79338, smoothed loss 4.36336, grad norm 4.47580, param norm 92.34115
epoch 13, iter 12180, loss 4.58132, smoothed loss 4.36266, grad norm 4.65531, param norm 92.35165
epoch 13, iter 12185, loss 4.65972, smoothed loss 4.37573, grad norm 4.44535, param norm 92.36317
epoch 13, iter 12190, loss 5.00264, smoothed loss 4.38475, grad norm 6.05940, param norm 92.37553
epoch 13, iter 12195, loss 4.46827, smoothed loss 4.38898, grad norm 4.65282, param norm 92.38867
epoch 13, iter 12200, loss 4.08686, smoothed loss 4.38389, grad norm 5.20815, param norm 92.40089
epoch 13, iter 12205, loss 4.84612, smoothed loss 4.37898, grad norm 4.37957, param norm 92.41408
epoch 13, iter 12210, loss 3.80031, smoothed loss 4.36802, grad norm 4.22535, param norm 92.42680
epoch 13, iter 12215, loss 3.97607, smoothed loss 4.36915, grad norm 4.15779, param norm 92.44048
epoch 13, iter 12220, loss 4.10055, smoothed loss 4.35542, grad norm 4.04427, param norm 92.45598
epoch 13, iter 12225, loss 4.62059, smoothed loss 4.37026, grad norm 5.24854, param norm 92.47089
epoch 13, iter 12230, loss 4.27173, smoothed loss 4.35892, grad norm 4.59127, param norm 92.48437
epoch 13, iter 12235, loss 4.76228, smoothed loss 4.35652, grad norm 4.52575, param norm 92.49715
epoch 13, iter 12240, loss 4.44873, smoothed loss 4.37340, grad norm 4.60970, param norm 92.50831
epoch 13, iter 12245, loss 4.58882, smoothed loss 4.37355, grad norm 4.13612, param norm 92.51763
epoch 13, iter 12250, loss 4.78736, smoothed loss 4.37365, grad norm 4.93616, param norm 92.52943
epoch 13, iter 12255, loss 4.35732, smoothed loss 4.36552, grad norm 4.46287, param norm 92.54212
epoch 13, iter 12260, loss 4.17820, smoothed loss 4.35686, grad norm 4.52605, param norm 92.55344
epoch 13, iter 12265, loss 5.14393, smoothed loss 4.35169, grad norm 3.89628, param norm 92.56526
epoch 13, iter 12270, loss 4.14812, smoothed loss 4.35126, grad norm 4.55971, param norm 92.57520
epoch 14, iter 12275, loss 4.16230, smoothed loss 4.35459, grad norm 4.39744, param norm 92.58432
epoch 14, iter 12280, loss 5.12448, smoothed loss 4.37416, grad norm 4.92194, param norm 92.59471
epoch 14, iter 12285, loss 4.16047, smoothed loss 4.37564, grad norm 4.35612, param norm 92.60480
epoch 14, iter 12290, loss 4.78699, smoothed loss 4.38576, grad norm 4.70291, param norm 92.61559
epoch 14, iter 12295, loss 4.09080, smoothed loss 4.37847, grad norm 4.76806, param norm 92.62566
epoch 14, iter 12300, loss 4.44264, smoothed loss 4.37677, grad norm 4.95338, param norm 92.64025
epoch 14, iter 12305, loss 4.31882, smoothed loss 4.37598, grad norm 4.14097, param norm 92.65681
epoch 14, iter 12310, loss 4.62677, smoothed loss 4.37321, grad norm 4.37017, param norm 92.67394
epoch 14, iter 12315, loss 4.13637, smoothed loss 4.36204, grad norm 4.55016, param norm 92.68962
epoch 14, iter 12320, loss 3.81269, smoothed loss 4.34239, grad norm 4.19307, param norm 92.70336
epoch 14, iter 12325, loss 4.18385, smoothed loss 4.33707, grad norm 4.43761, param norm 92.71678
epoch 14, iter 12330, loss 5.12377, smoothed loss 4.34057, grad norm 5.30660, param norm 92.72853
epoch 14, iter 12335, loss 3.82487, smoothed loss 4.33293, grad norm 4.64879, param norm 92.73725
epoch 14, iter 12340, loss 4.36464, smoothed loss 4.31699, grad norm 5.09939, param norm 92.74927
epoch 14, iter 12345, loss 3.74332, smoothed loss 4.31072, grad norm 4.21111, param norm 92.76255
epoch 14, iter 12350, loss 4.50983, smoothed loss 4.31551, grad norm 4.79669, param norm 92.77536
epoch 14, iter 12355, loss 4.20831, smoothed loss 4.31400, grad norm 3.95893, param norm 92.78476
epoch 14, iter 12360, loss 4.32466, smoothed loss 4.31271, grad norm 4.23080, param norm 92.79346
epoch 14, iter 12365, loss 3.75447, smoothed loss 4.31018, grad norm 3.91865, param norm 92.80338
epoch 14, iter 12370, loss 3.69201, smoothed loss 4.29016, grad norm 4.16157, param norm 92.81510
epoch 14, iter 12375, loss 3.91796, smoothed loss 4.29077, grad norm 4.22467, param norm 92.82632
epoch 14, iter 12380, loss 4.31122, smoothed loss 4.30915, grad norm 4.69702, param norm 92.83659
epoch 14, iter 12385, loss 3.84883, smoothed loss 4.29652, grad norm 4.52111, param norm 92.84630
epoch 14, iter 12390, loss 4.63849, smoothed loss 4.30226, grad norm 4.94336, param norm 92.85719
epoch 14, iter 12395, loss 3.57188, smoothed loss 4.28564, grad norm 4.17181, param norm 92.86816
epoch 14, iter 12400, loss 4.64205, smoothed loss 4.29241, grad norm 4.70586, param norm 92.88110
epoch 14, iter 12405, loss 4.86049, smoothed loss 4.32251, grad norm 6.03546, param norm 92.89328
epoch 14, iter 12410, loss 3.89938, smoothed loss 4.30776, grad norm 4.03607, param norm 92.90536
epoch 14, iter 12415, loss 4.91435, smoothed loss 4.30173, grad norm 4.06746, param norm 92.92068
epoch 14, iter 12420, loss 4.20994, smoothed loss 4.28932, grad norm 4.38615, param norm 92.93494
epoch 14, iter 12425, loss 4.43328, smoothed loss 4.27666, grad norm 4.63650, param norm 92.95038
epoch 14, iter 12430, loss 4.04805, smoothed loss 4.26579, grad norm 4.54263, param norm 92.96378
epoch 14, iter 12435, loss 4.24209, smoothed loss 4.29219, grad norm 4.37051, param norm 92.97489
epoch 14, iter 12440, loss 4.52823, smoothed loss 4.30308, grad norm 4.71678, param norm 92.98524
epoch 14, iter 12445, loss 4.59217, smoothed loss 4.29662, grad norm 4.55010, param norm 92.99746
epoch 14, iter 12450, loss 4.33938, smoothed loss 4.31376, grad norm 4.75959, param norm 93.00874
epoch 14, iter 12455, loss 4.86316, smoothed loss 4.31753, grad norm 4.04383, param norm 93.01889
epoch 14, iter 12460, loss 4.11134, smoothed loss 4.32746, grad norm 6.06264, param norm 93.02791
epoch 14, iter 12465, loss 5.22318, smoothed loss 4.32990, grad norm 5.28668, param norm 93.03876
epoch 14, iter 12470, loss 4.93003, smoothed loss 4.33900, grad norm 4.06610, param norm 93.05122
epoch 14, iter 12475, loss 4.44825, smoothed loss 4.33825, grad norm 4.46162, param norm 93.06144
epoch 14, iter 12480, loss 4.81913, smoothed loss 4.35196, grad norm 5.08231, param norm 93.07201
epoch 14, iter 12485, loss 3.89884, smoothed loss 4.33052, grad norm 4.11542, param norm 93.08449
epoch 14, iter 12490, loss 4.08400, smoothed loss 4.32676, grad norm 4.45771, param norm 93.09906
epoch 14, iter 12495, loss 4.97706, smoothed loss 4.34272, grad norm 4.81856, param norm 93.11211
epoch 14, iter 12500, loss 4.47473, smoothed loss 4.35578, grad norm 4.46411, param norm 93.12370
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 14, Iter 12500, dev loss: 4.927272
Calculating Train F1/EM...
F1 train: 1000 examples took 8.96529 seconds [Score: 0.52381]
Exact Match train: 1000 examples took 8.89400 seconds [Score: 0.45200]
Epoch 14, Iter 12500, Train F1 score: 0.523809, Train EM score: 0.452000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.39837 seconds [Score: 0.35911]
Exact Match dev: 7118 examples took 57.82672 seconds [Score: 0.25569]
Epoch 14, Iter 12500, Dev F1 score: 0.359112, Dev EM score: 0.255690
End of epoch 14
epoch 14, iter 12505, loss 4.56517, smoothed loss 4.35464, grad norm 4.19893, param norm 93.13497
epoch 14, iter 12510, loss 4.09357, smoothed loss 4.35175, grad norm 4.89637, param norm 93.14334
epoch 14, iter 12515, loss 4.37160, smoothed loss 4.34523, grad norm 4.03525, param norm 93.15208
epoch 14, iter 12520, loss 3.76451, smoothed loss 4.33932, grad norm 3.96539, param norm 93.16318
epoch 14, iter 12525, loss 4.61765, smoothed loss 4.32770, grad norm 4.73914, param norm 93.17546
epoch 14, iter 12530, loss 4.35843, smoothed loss 4.32485, grad norm 4.35773, param norm 93.18812
epoch 14, iter 12535, loss 3.97084, smoothed loss 4.31377, grad norm 4.27548, param norm 93.20114
epoch 14, iter 12540, loss 4.04478, smoothed loss 4.31363, grad norm 4.18417, param norm 93.21336
epoch 14, iter 12545, loss 4.84355, smoothed loss 4.33693, grad norm 4.81437, param norm 93.22517
epoch 14, iter 12550, loss 4.24346, smoothed loss 4.34610, grad norm 4.83343, param norm 93.23702
epoch 14, iter 12555, loss 4.28757, smoothed loss 4.33244, grad norm 4.61408, param norm 93.24901
epoch 14, iter 12560, loss 4.64549, smoothed loss 4.32988, grad norm 4.36859, param norm 93.26073
epoch 14, iter 12565, loss 4.22552, smoothed loss 4.32841, grad norm 3.95208, param norm 93.27328
epoch 14, iter 12570, loss 3.97047, smoothed loss 4.32963, grad norm 4.09293, param norm 93.28626
epoch 14, iter 12575, loss 5.30855, smoothed loss 4.32785, grad norm 4.01587, param norm 93.29987
epoch 14, iter 12580, loss 4.17076, smoothed loss 4.33406, grad norm 4.44898, param norm 93.31322
epoch 14, iter 12585, loss 4.23569, smoothed loss 4.34145, grad norm 4.10528, param norm 93.32713
epoch 14, iter 12590, loss 4.39239, smoothed loss 4.33954, grad norm 4.52751, param norm 93.33882
epoch 14, iter 12595, loss 4.01364, smoothed loss 4.33929, grad norm 4.76047, param norm 93.34958
epoch 14, iter 12600, loss 3.45959, smoothed loss 4.31844, grad norm 4.92727, param norm 93.36164
epoch 14, iter 12605, loss 4.44110, smoothed loss 4.32407, grad norm 4.47744, param norm 93.37419
epoch 14, iter 12610, loss 3.73896, smoothed loss 4.31893, grad norm 4.04114, param norm 93.38643
epoch 14, iter 12615, loss 4.00094, smoothed loss 4.31949, grad norm 4.17483, param norm 93.40084
epoch 14, iter 12620, loss 4.49094, smoothed loss 4.31772, grad norm 4.69720, param norm 93.41435
epoch 14, iter 12625, loss 3.93887, smoothed loss 4.31525, grad norm 4.42188, param norm 93.42554
epoch 14, iter 12630, loss 4.73600, smoothed loss 4.31576, grad norm 5.06540, param norm 93.43807
epoch 14, iter 12635, loss 4.64535, smoothed loss 4.33275, grad norm 3.93364, param norm 93.44821
epoch 14, iter 12640, loss 4.22931, smoothed loss 4.33360, grad norm 4.46577, param norm 93.45770
epoch 14, iter 12645, loss 4.02589, smoothed loss 4.32195, grad norm 4.31925, param norm 93.46863
epoch 14, iter 12650, loss 3.87120, smoothed loss 4.32641, grad norm 4.33720, param norm 93.47937
epoch 14, iter 12655, loss 3.95325, smoothed loss 4.34552, grad norm 4.94444, param norm 93.48789
epoch 14, iter 12660, loss 4.11423, smoothed loss 4.34988, grad norm 4.30876, param norm 93.49777
epoch 14, iter 12665, loss 4.49018, smoothed loss 4.35348, grad norm 4.47117, param norm 93.50901
epoch 14, iter 12670, loss 4.13302, smoothed loss 4.34682, grad norm 3.97732, param norm 93.52258
epoch 14, iter 12675, loss 4.39265, smoothed loss 4.33161, grad norm 4.64329, param norm 93.53859
epoch 14, iter 12680, loss 4.09650, smoothed loss 4.32722, grad norm 4.55574, param norm 93.55358
epoch 14, iter 12685, loss 4.65892, smoothed loss 4.33753, grad norm 4.62369, param norm 93.56611
epoch 14, iter 12690, loss 4.86160, smoothed loss 4.34004, grad norm 4.47507, param norm 93.57481
epoch 14, iter 12695, loss 4.92209, smoothed loss 4.35261, grad norm 4.89907, param norm 93.58533
epoch 14, iter 12700, loss 3.90238, smoothed loss 4.33879, grad norm 4.68872, param norm 93.59702
epoch 14, iter 12705, loss 4.46795, smoothed loss 4.34688, grad norm 4.87340, param norm 93.60811
epoch 14, iter 12710, loss 4.10559, smoothed loss 4.33925, grad norm 4.05243, param norm 93.62318
epoch 14, iter 12715, loss 4.40749, smoothed loss 4.33121, grad norm 4.55136, param norm 93.63960
epoch 14, iter 12720, loss 3.96258, smoothed loss 4.33487, grad norm 4.16065, param norm 93.65553
epoch 14, iter 12725, loss 4.30010, smoothed loss 4.33569, grad norm 4.87476, param norm 93.66923
epoch 14, iter 12730, loss 4.69538, smoothed loss 4.32988, grad norm 5.36026, param norm 93.68190
epoch 14, iter 12735, loss 3.92687, smoothed loss 4.33276, grad norm 4.09034, param norm 93.69247
epoch 14, iter 12740, loss 4.25503, smoothed loss 4.32155, grad norm 3.91188, param norm 93.70367
epoch 14, iter 12745, loss 4.59054, smoothed loss 4.31597, grad norm 5.00385, param norm 93.71599
epoch 14, iter 12750, loss 4.31394, smoothed loss 4.30700, grad norm 4.11453, param norm 93.72733
epoch 14, iter 12755, loss 4.82982, smoothed loss 4.31612, grad norm 4.31258, param norm 93.73939
epoch 14, iter 12760, loss 4.15549, smoothed loss 4.29876, grad norm 4.11856, param norm 93.75095
epoch 14, iter 12765, loss 3.88280, smoothed loss 4.30451, grad norm 4.66969, param norm 93.76395
epoch 14, iter 12770, loss 4.82095, smoothed loss 4.30911, grad norm 4.58208, param norm 93.77724
epoch 14, iter 12775, loss 4.58837, smoothed loss 4.30739, grad norm 3.67625, param norm 93.78729
epoch 14, iter 12780, loss 4.30962, smoothed loss 4.29971, grad norm 5.31447, param norm 93.79753
epoch 14, iter 12785, loss 4.28757, smoothed loss 4.29850, grad norm 3.84940, param norm 93.80738
epoch 14, iter 12790, loss 4.33004, smoothed loss 4.29448, grad norm 4.96864, param norm 93.81750
epoch 14, iter 12795, loss 3.88407, smoothed loss 4.28513, grad norm 4.23520, param norm 93.82910
epoch 14, iter 12800, loss 4.82712, smoothed loss 4.28928, grad norm 4.54805, param norm 93.84170
epoch 14, iter 12805, loss 4.35124, smoothed loss 4.29614, grad norm 4.38658, param norm 93.85195
epoch 14, iter 12810, loss 3.82153, smoothed loss 4.28387, grad norm 4.60832, param norm 93.86246
epoch 14, iter 12815, loss 4.01943, smoothed loss 4.27389, grad norm 4.38863, param norm 93.87442
epoch 14, iter 12820, loss 4.31675, smoothed loss 4.27804, grad norm 4.10526, param norm 93.88482
epoch 14, iter 12825, loss 3.48903, smoothed loss 4.26671, grad norm 3.98348, param norm 93.89421
epoch 14, iter 12830, loss 3.95232, smoothed loss 4.26953, grad norm 4.29207, param norm 93.90726
epoch 14, iter 12835, loss 4.02123, smoothed loss 4.25741, grad norm 4.20732, param norm 93.92098
epoch 14, iter 12840, loss 4.19426, smoothed loss 4.25941, grad norm 4.21276, param norm 93.93538
epoch 14, iter 12845, loss 4.06370, smoothed loss 4.25490, grad norm 5.10642, param norm 93.94997
epoch 14, iter 12850, loss 4.89812, smoothed loss 4.25840, grad norm 4.31820, param norm 93.96513
epoch 14, iter 12855, loss 4.03888, smoothed loss 4.25731, grad norm 4.32556, param norm 93.97588
epoch 14, iter 12860, loss 4.21410, smoothed loss 4.26366, grad norm 4.08485, param norm 93.98527
epoch 14, iter 12865, loss 4.31144, smoothed loss 4.26642, grad norm 3.91295, param norm 93.99554
epoch 14, iter 12870, loss 4.19411, smoothed loss 4.24977, grad norm 4.82816, param norm 94.00683
epoch 14, iter 12875, loss 4.26938, smoothed loss 4.24756, grad norm 4.06350, param norm 94.01847
epoch 14, iter 12880, loss 4.75409, smoothed loss 4.24817, grad norm 6.10883, param norm 94.02959
epoch 14, iter 12885, loss 4.81013, smoothed loss 4.24873, grad norm 4.73265, param norm 94.03999
epoch 14, iter 12890, loss 3.65333, smoothed loss 4.24799, grad norm 4.03681, param norm 94.04919
epoch 14, iter 12895, loss 3.84361, smoothed loss 4.23424, grad norm 4.83852, param norm 94.06086
epoch 14, iter 12900, loss 4.42631, smoothed loss 4.24553, grad norm 4.31584, param norm 94.07511
epoch 14, iter 12905, loss 4.95355, smoothed loss 4.25828, grad norm 4.29056, param norm 94.08728
epoch 14, iter 12910, loss 4.51359, smoothed loss 4.25203, grad norm 4.07353, param norm 94.09763
epoch 14, iter 12915, loss 4.73776, smoothed loss 4.26972, grad norm 4.52026, param norm 94.10740
epoch 14, iter 12920, loss 4.49233, smoothed loss 4.26487, grad norm 4.98696, param norm 94.11591
epoch 14, iter 12925, loss 4.40031, smoothed loss 4.26775, grad norm 4.03330, param norm 94.12684
epoch 14, iter 12930, loss 4.39543, smoothed loss 4.26180, grad norm 4.43790, param norm 94.13707
epoch 14, iter 12935, loss 4.62121, smoothed loss 4.25382, grad norm 4.73752, param norm 94.14785
epoch 14, iter 12940, loss 5.16758, smoothed loss 4.26219, grad norm 5.48613, param norm 94.15982
epoch 14, iter 12945, loss 3.58403, smoothed loss 4.27112, grad norm 4.25863, param norm 94.16936
epoch 14, iter 12950, loss 4.00435, smoothed loss 4.27532, grad norm 4.86263, param norm 94.17979
epoch 14, iter 12955, loss 3.72554, smoothed loss 4.27041, grad norm 4.33265, param norm 94.19189
epoch 14, iter 12960, loss 4.22935, smoothed loss 4.25923, grad norm 4.95784, param norm 94.20602
epoch 14, iter 12965, loss 4.73799, smoothed loss 4.26168, grad norm 4.55913, param norm 94.21972
epoch 14, iter 12970, loss 4.63396, smoothed loss 4.27057, grad norm 5.06605, param norm 94.23162
epoch 14, iter 12975, loss 4.78807, smoothed loss 4.28516, grad norm 4.57178, param norm 94.24288
epoch 14, iter 12980, loss 4.16942, smoothed loss 4.27939, grad norm 4.23162, param norm 94.25390
epoch 14, iter 12985, loss 4.61216, smoothed loss 4.27447, grad norm 4.69655, param norm 94.26700
epoch 14, iter 12990, loss 4.25073, smoothed loss 4.27400, grad norm 4.39115, param norm 94.28040
epoch 14, iter 12995, loss 4.26277, smoothed loss 4.27881, grad norm 4.78288, param norm 94.29388
epoch 14, iter 13000, loss 4.75516, smoothed loss 4.29168, grad norm 4.79150, param norm 94.30521
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 14, Iter 13000, dev loss: 4.906643
Calculating Train F1/EM...
F1 train: 1000 examples took 8.99804 seconds [Score: 0.51318]
Exact Match train: 1000 examples took 8.76030 seconds [Score: 0.38300]
Epoch 14, Iter 13000, Train F1 score: 0.513177, Train EM score: 0.383000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.38801 seconds [Score: 0.36200]
Exact Match dev: 7118 examples took 57.79420 seconds [Score: 0.25864]
Epoch 14, Iter 13000, Dev F1 score: 0.361998, Dev EM score: 0.258640
End of epoch 14
epoch 14, iter 13005, loss 4.83224, smoothed loss 4.30463, grad norm 5.08936, param norm 94.31784
epoch 14, iter 13010, loss 4.24137, smoothed loss 4.30462, grad norm 4.92567, param norm 94.32946
epoch 14, iter 13015, loss 4.14757, smoothed loss 4.31126, grad norm 4.38987, param norm 94.33869
epoch 14, iter 13020, loss 4.52519, smoothed loss 4.31022, grad norm 4.99308, param norm 94.34885
epoch 14, iter 13025, loss 4.08343, smoothed loss 4.30014, grad norm 4.52947, param norm 94.36008
epoch 14, iter 13030, loss 4.34429, smoothed loss 4.30369, grad norm 4.48293, param norm 94.37277
epoch 14, iter 13035, loss 4.79345, smoothed loss 4.31351, grad norm 4.97077, param norm 94.38471
epoch 14, iter 13040, loss 4.01882, smoothed loss 4.32008, grad norm 4.46889, param norm 94.39731
epoch 14, iter 13045, loss 4.66618, smoothed loss 4.32551, grad norm 4.81230, param norm 94.41000
epoch 14, iter 13050, loss 4.26116, smoothed loss 4.29512, grad norm 4.89790, param norm 94.42175
epoch 14, iter 13055, loss 3.88330, smoothed loss 4.28586, grad norm 4.21387, param norm 94.43205
epoch 14, iter 13060, loss 4.50401, smoothed loss 4.28333, grad norm 5.81820, param norm 94.44199
epoch 14, iter 13065, loss 3.79457, smoothed loss 4.26915, grad norm 4.16474, param norm 94.45124
epoch 14, iter 13070, loss 4.67545, smoothed loss 4.27873, grad norm 4.82965, param norm 94.46201
epoch 14, iter 13075, loss 3.95315, smoothed loss 4.26167, grad norm 4.40084, param norm 94.47286
epoch 14, iter 13080, loss 3.99817, smoothed loss 4.25258, grad norm 4.69956, param norm 94.48441
epoch 14, iter 13085, loss 4.22417, smoothed loss 4.26164, grad norm 4.98667, param norm 94.49519
epoch 14, iter 13090, loss 3.77136, smoothed loss 4.27168, grad norm 4.62876, param norm 94.50436
epoch 14, iter 13095, loss 4.08041, smoothed loss 4.26507, grad norm 4.36336, param norm 94.51664
epoch 14, iter 13100, loss 4.15982, smoothed loss 4.27511, grad norm 4.41391, param norm 94.52924
epoch 14, iter 13105, loss 4.07629, smoothed loss 4.27040, grad norm 4.87698, param norm 94.53941
epoch 14, iter 13110, loss 3.73514, smoothed loss 4.26681, grad norm 4.07354, param norm 94.55061
epoch 14, iter 13115, loss 3.44528, smoothed loss 4.24753, grad norm 3.84262, param norm 94.56469
epoch 14, iter 13120, loss 5.02067, smoothed loss 4.26129, grad norm 5.00932, param norm 94.57861
epoch 14, iter 13125, loss 4.07266, smoothed loss 4.26904, grad norm 4.17572, param norm 94.59038
epoch 14, iter 13130, loss 4.27929, smoothed loss 4.26474, grad norm 4.23293, param norm 94.60184
epoch 14, iter 13135, loss 4.78005, smoothed loss 4.27695, grad norm 4.29592, param norm 94.61138
epoch 14, iter 13140, loss 4.40820, smoothed loss 4.28047, grad norm 4.52864, param norm 94.61689
epoch 14, iter 13145, loss 4.36485, smoothed loss 4.27050, grad norm 4.62537, param norm 94.62387
epoch 14, iter 13150, loss 4.26151, smoothed loss 4.28462, grad norm 4.49564, param norm 94.63270
epoch 14, iter 13155, loss 4.15824, smoothed loss 4.30070, grad norm 4.22615, param norm 94.64099
epoch 14, iter 13160, loss 3.93084, smoothed loss 4.30535, grad norm 4.24450, param norm 94.65083
epoch 14, iter 13165, loss 4.07060, smoothed loss 4.29481, grad norm 4.17946, param norm 94.66109
epoch 14, iter 13170, loss 3.88770, smoothed loss 4.28935, grad norm 4.64011, param norm 94.67250
epoch 14, iter 13175, loss 3.96301, smoothed loss 4.28147, grad norm 4.25458, param norm 94.68509
epoch 14, iter 13180, loss 3.94210, smoothed loss 4.27205, grad norm 4.11306, param norm 94.69764
epoch 14, iter 13185, loss 4.47535, smoothed loss 4.27900, grad norm 4.32261, param norm 94.70876
epoch 14, iter 13190, loss 3.92205, smoothed loss 4.28383, grad norm 4.81876, param norm 94.71863
epoch 14, iter 13195, loss 4.43777, smoothed loss 4.28738, grad norm 4.50590, param norm 94.72922
epoch 14, iter 13200, loss 4.51285, smoothed loss 4.28636, grad norm 4.68743, param norm 94.73946
epoch 14, iter 13205, loss 4.11327, smoothed loss 4.28721, grad norm 4.54727, param norm 94.75061
epoch 14, iter 13210, loss 4.37986, smoothed loss 4.27811, grad norm 4.43593, param norm 94.76283
epoch 14, iter 13215, loss 4.21768, smoothed loss 4.28075, grad norm 4.11036, param norm 94.77623
epoch 15, iter 13220, loss 3.58192, smoothed loss 4.27791, grad norm 4.00515, param norm 94.78765
epoch 15, iter 13225, loss 3.95968, smoothed loss 4.28522, grad norm 4.57614, param norm 94.79876
epoch 15, iter 13230, loss 3.75901, smoothed loss 4.27126, grad norm 3.93633, param norm 94.81136
epoch 15, iter 13235, loss 4.54911, smoothed loss 4.27667, grad norm 5.22443, param norm 94.82610
epoch 15, iter 13240, loss 3.99431, smoothed loss 4.27127, grad norm 4.70705, param norm 94.83945
epoch 15, iter 13245, loss 4.36707, smoothed loss 4.27017, grad norm 4.40366, param norm 94.85258
epoch 15, iter 13250, loss 3.82881, smoothed loss 4.25733, grad norm 4.54570, param norm 94.86466
epoch 15, iter 13255, loss 4.21126, smoothed loss 4.25615, grad norm 4.82839, param norm 94.87700
epoch 15, iter 13260, loss 3.45459, smoothed loss 4.24379, grad norm 4.75887, param norm 94.88827
epoch 15, iter 13265, loss 3.62803, smoothed loss 4.23707, grad norm 4.48593, param norm 94.89983
epoch 15, iter 13270, loss 3.95918, smoothed loss 4.22856, grad norm 4.31064, param norm 94.91204
epoch 15, iter 13275, loss 3.59363, smoothed loss 4.22149, grad norm 4.42356, param norm 94.92347
epoch 15, iter 13280, loss 4.36545, smoothed loss 4.22593, grad norm 4.82040, param norm 94.93511
epoch 15, iter 13285, loss 4.35887, smoothed loss 4.21309, grad norm 5.12653, param norm 94.94681
epoch 15, iter 13290, loss 4.36667, smoothed loss 4.21949, grad norm 4.74575, param norm 94.95652
epoch 15, iter 13295, loss 4.23620, smoothed loss 4.20696, grad norm 4.82254, param norm 94.96867
epoch 15, iter 13300, loss 4.68837, smoothed loss 4.23761, grad norm 4.71594, param norm 94.97974
epoch 15, iter 13305, loss 4.77310, smoothed loss 4.25894, grad norm 4.44254, param norm 94.98789
epoch 15, iter 13310, loss 4.38590, smoothed loss 4.25874, grad norm 4.53788, param norm 94.99768
epoch 15, iter 13315, loss 4.01745, smoothed loss 4.27047, grad norm 4.37300, param norm 95.00893
epoch 15, iter 13320, loss 3.75962, smoothed loss 4.25351, grad norm 4.16764, param norm 95.02005
epoch 15, iter 13325, loss 5.19253, smoothed loss 4.25866, grad norm 4.94449, param norm 95.03192
epoch 15, iter 13330, loss 4.26329, smoothed loss 4.25549, grad norm 4.68032, param norm 95.04337
epoch 15, iter 13335, loss 4.51371, smoothed loss 4.26208, grad norm 4.75851, param norm 95.05504
epoch 15, iter 13340, loss 3.95798, smoothed loss 4.26136, grad norm 4.00105, param norm 95.06598
epoch 15, iter 13345, loss 4.78739, smoothed loss 4.25720, grad norm 4.87139, param norm 95.07697
epoch 15, iter 13350, loss 3.90292, smoothed loss 4.26438, grad norm 4.14491, param norm 95.08942
epoch 15, iter 13355, loss 4.05619, smoothed loss 4.25752, grad norm 4.30292, param norm 95.10320
epoch 15, iter 13360, loss 4.03450, smoothed loss 4.24610, grad norm 4.35389, param norm 95.11651
epoch 15, iter 13365, loss 4.34154, smoothed loss 4.25323, grad norm 4.11006, param norm 95.12727
epoch 15, iter 13370, loss 3.88655, smoothed loss 4.25258, grad norm 4.35218, param norm 95.13773
epoch 15, iter 13375, loss 3.79384, smoothed loss 4.24521, grad norm 3.84561, param norm 95.14909
epoch 15, iter 13380, loss 4.59189, smoothed loss 4.24564, grad norm 5.05475, param norm 95.16250
epoch 15, iter 13385, loss 3.98058, smoothed loss 4.23776, grad norm 4.46292, param norm 95.17650
epoch 15, iter 13390, loss 4.29695, smoothed loss 4.25279, grad norm 4.21513, param norm 95.18893
epoch 15, iter 13395, loss 5.41031, smoothed loss 4.26647, grad norm 4.16242, param norm 95.20062
epoch 15, iter 13400, loss 4.93000, smoothed loss 4.26815, grad norm 4.01603, param norm 95.21177
epoch 15, iter 13405, loss 4.23018, smoothed loss 4.26840, grad norm 4.59466, param norm 95.22420
epoch 15, iter 13410, loss 4.21854, smoothed loss 4.26264, grad norm 5.16672, param norm 95.23669
epoch 15, iter 13415, loss 4.28227, smoothed loss 4.27292, grad norm 4.82859, param norm 95.24631
epoch 15, iter 13420, loss 4.60172, smoothed loss 4.27461, grad norm 5.26107, param norm 95.25461
epoch 15, iter 13425, loss 3.80159, smoothed loss 4.27922, grad norm 4.67277, param norm 95.26580
epoch 15, iter 13430, loss 4.07787, smoothed loss 4.27922, grad norm 4.48780, param norm 95.27916
epoch 15, iter 13435, loss 4.37029, smoothed loss 4.28620, grad norm 5.30786, param norm 95.29050
epoch 15, iter 13440, loss 4.39399, smoothed loss 4.29528, grad norm 4.27151, param norm 95.29860
epoch 15, iter 13445, loss 4.36908, smoothed loss 4.29885, grad norm 4.20916, param norm 95.30409
epoch 15, iter 13450, loss 4.51123, smoothed loss 4.30582, grad norm 4.09277, param norm 95.31296
epoch 15, iter 13455, loss 4.27417, smoothed loss 4.29840, grad norm 4.41611, param norm 95.32388
epoch 15, iter 13460, loss 4.19552, smoothed loss 4.31148, grad norm 4.71902, param norm 95.33441
epoch 15, iter 13465, loss 4.37512, smoothed loss 4.29679, grad norm 4.97732, param norm 95.34612
epoch 15, iter 13470, loss 4.19329, smoothed loss 4.29419, grad norm 4.38274, param norm 95.35911
epoch 15, iter 13475, loss 4.38295, smoothed loss 4.29409, grad norm 4.35215, param norm 95.36906
epoch 15, iter 13480, loss 4.00730, smoothed loss 4.29328, grad norm 4.31292, param norm 95.37724
epoch 15, iter 13485, loss 4.59665, smoothed loss 4.29371, grad norm 4.33146, param norm 95.38441
epoch 15, iter 13490, loss 4.44692, smoothed loss 4.31757, grad norm 3.88613, param norm 95.39172
epoch 15, iter 13495, loss 4.18116, smoothed loss 4.31796, grad norm 3.91394, param norm 95.40097
epoch 15, iter 13500, loss 5.10756, smoothed loss 4.32952, grad norm 4.99997, param norm 95.41185
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 15, Iter 13500, dev loss: 4.889152
Calculating Train F1/EM...
F1 train: 1000 examples took 9.16323 seconds [Score: 0.57407]
Exact Match train: 1000 examples took 8.95840 seconds [Score: 0.43000]
Epoch 15, Iter 13500, Train F1 score: 0.574065, Train EM score: 0.430000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.66242 seconds [Score: 0.36475]
Exact Match dev: 7118 examples took 58.05691 seconds [Score: 0.25878]
Epoch 15, Iter 13500, Dev F1 score: 0.364747, Dev EM score: 0.258781
End of epoch 15
epoch 15, iter 13505, loss 4.18192, smoothed loss 4.33147, grad norm 4.06159, param norm 95.42075
epoch 15, iter 13510, loss 3.89223, smoothed loss 4.31407, grad norm 3.85364, param norm 95.43115
epoch 15, iter 13515, loss 4.03192, smoothed loss 4.31949, grad norm 4.19259, param norm 95.44194
epoch 15, iter 13520, loss 3.41092, smoothed loss 4.31138, grad norm 4.28806, param norm 95.45433
epoch 15, iter 13525, loss 4.26986, smoothed loss 4.31068, grad norm 5.05515, param norm 95.46799
epoch 15, iter 13530, loss 4.35670, smoothed loss 4.31575, grad norm 5.11798, param norm 95.47961
epoch 15, iter 13535, loss 4.46347, smoothed loss 4.31350, grad norm 5.06824, param norm 95.49125
epoch 15, iter 13540, loss 3.77551, smoothed loss 4.31229, grad norm 4.17521, param norm 95.50253
epoch 15, iter 13545, loss 4.06227, smoothed loss 4.31977, grad norm 4.11812, param norm 95.51376
epoch 15, iter 13550, loss 4.31283, smoothed loss 4.31072, grad norm 4.28447, param norm 95.52553
epoch 15, iter 13555, loss 4.11953, smoothed loss 4.30946, grad norm 4.19981, param norm 95.53786
epoch 15, iter 13560, loss 4.14351, smoothed loss 4.32721, grad norm 4.30493, param norm 95.54910
epoch 15, iter 13565, loss 3.64124, smoothed loss 4.32632, grad norm 3.99892, param norm 95.55898
epoch 15, iter 13570, loss 4.22583, smoothed loss 4.31119, grad norm 4.57108, param norm 95.56934
epoch 15, iter 13575, loss 4.70611, smoothed loss 4.30449, grad norm 5.22218, param norm 95.58076
epoch 15, iter 13580, loss 4.26104, smoothed loss 4.30497, grad norm 4.90697, param norm 95.59241
epoch 15, iter 13585, loss 3.69399, smoothed loss 4.28865, grad norm 4.56529, param norm 95.60558
epoch 15, iter 13590, loss 3.79280, smoothed loss 4.27374, grad norm 4.55989, param norm 95.61920
epoch 15, iter 13595, loss 4.28902, smoothed loss 4.28552, grad norm 4.34500, param norm 95.62920
epoch 15, iter 13600, loss 4.47181, smoothed loss 4.28423, grad norm 4.91464, param norm 95.63975
epoch 15, iter 13605, loss 4.25121, smoothed loss 4.28382, grad norm 4.29191, param norm 95.65049
epoch 15, iter 13610, loss 4.00574, smoothed loss 4.29123, grad norm 4.85067, param norm 95.66154
epoch 15, iter 13615, loss 3.67792, smoothed loss 4.28563, grad norm 4.20858, param norm 95.67215
epoch 15, iter 13620, loss 4.22885, smoothed loss 4.28456, grad norm 4.81189, param norm 95.68354
epoch 15, iter 13625, loss 3.78682, smoothed loss 4.29113, grad norm 4.48271, param norm 95.69382
epoch 15, iter 13630, loss 4.27950, smoothed loss 4.28887, grad norm 4.79068, param norm 95.70459
epoch 15, iter 13635, loss 4.12411, smoothed loss 4.27480, grad norm 5.34756, param norm 95.71532
epoch 15, iter 13640, loss 4.49718, smoothed loss 4.27243, grad norm 5.04679, param norm 95.72478
epoch 15, iter 13645, loss 4.62449, smoothed loss 4.27646, grad norm 4.92945, param norm 95.73395
epoch 15, iter 13650, loss 4.01454, smoothed loss 4.26672, grad norm 4.54743, param norm 95.74345
epoch 15, iter 13655, loss 4.42367, smoothed loss 4.26021, grad norm 5.51249, param norm 95.75508
epoch 15, iter 13660, loss 4.70949, smoothed loss 4.27029, grad norm 5.56175, param norm 95.76608
epoch 15, iter 13665, loss 4.29291, smoothed loss 4.27047, grad norm 4.51311, param norm 95.77538
epoch 15, iter 13670, loss 3.93173, smoothed loss 4.25709, grad norm 4.20331, param norm 95.78445
epoch 15, iter 13675, loss 4.66168, smoothed loss 4.26002, grad norm 4.74829, param norm 95.79263
epoch 15, iter 13680, loss 4.89148, smoothed loss 4.27221, grad norm 4.62113, param norm 95.80177
epoch 15, iter 13685, loss 3.56907, smoothed loss 4.27710, grad norm 4.85291, param norm 95.80869
epoch 15, iter 13690, loss 4.23184, smoothed loss 4.28948, grad norm 4.40476, param norm 95.81739
epoch 15, iter 13695, loss 4.39808, smoothed loss 4.28601, grad norm 4.63846, param norm 95.82955
epoch 15, iter 13700, loss 4.21259, smoothed loss 4.27805, grad norm 4.15217, param norm 95.84220
epoch 15, iter 13705, loss 4.29514, smoothed loss 4.27876, grad norm 3.81710, param norm 95.85499
epoch 15, iter 13710, loss 4.34440, smoothed loss 4.27954, grad norm 4.95897, param norm 95.86636
epoch 15, iter 13715, loss 4.10705, smoothed loss 4.28329, grad norm 4.31394, param norm 95.87642
epoch 15, iter 13720, loss 4.55034, smoothed loss 4.27922, grad norm 4.67520, param norm 95.88660
epoch 15, iter 13725, loss 4.17714, smoothed loss 4.28861, grad norm 4.81217, param norm 95.89704
epoch 15, iter 13730, loss 3.39789, smoothed loss 4.27608, grad norm 4.15707, param norm 95.90697
epoch 15, iter 13735, loss 4.24157, smoothed loss 4.27560, grad norm 4.14665, param norm 95.91716
epoch 15, iter 13740, loss 3.74846, smoothed loss 4.26877, grad norm 4.28216, param norm 95.92837
epoch 15, iter 13745, loss 4.20351, smoothed loss 4.26678, grad norm 4.57834, param norm 95.94079
epoch 15, iter 13750, loss 4.74460, smoothed loss 4.26741, grad norm 4.89565, param norm 95.95168
epoch 15, iter 13755, loss 4.47545, smoothed loss 4.25790, grad norm 5.63017, param norm 95.96134
epoch 15, iter 13760, loss 4.34242, smoothed loss 4.26331, grad norm 4.68562, param norm 95.97166
epoch 15, iter 13765, loss 4.16406, smoothed loss 4.27243, grad norm 4.52875, param norm 95.98179
epoch 15, iter 13770, loss 4.10879, smoothed loss 4.27945, grad norm 5.01004, param norm 95.99322
epoch 15, iter 13775, loss 4.44801, smoothed loss 4.27377, grad norm 4.83209, param norm 96.00477
epoch 15, iter 13780, loss 4.23658, smoothed loss 4.27068, grad norm 4.44166, param norm 96.01578
epoch 15, iter 13785, loss 3.73153, smoothed loss 4.26021, grad norm 4.46095, param norm 96.02813
epoch 15, iter 13790, loss 3.63127, smoothed loss 4.25914, grad norm 4.40804, param norm 96.04076
epoch 15, iter 13795, loss 4.04739, smoothed loss 4.25114, grad norm 4.36777, param norm 96.05317
epoch 15, iter 13800, loss 4.51660, smoothed loss 4.24286, grad norm 4.50525, param norm 96.06531
epoch 15, iter 13805, loss 4.31980, smoothed loss 4.24332, grad norm 4.52379, param norm 96.07556
epoch 15, iter 13810, loss 4.60217, smoothed loss 4.23691, grad norm 5.31563, param norm 96.08663
epoch 15, iter 13815, loss 4.49879, smoothed loss 4.24190, grad norm 5.37412, param norm 96.09844
epoch 15, iter 13820, loss 3.87617, smoothed loss 4.23799, grad norm 4.38826, param norm 96.10975
epoch 15, iter 13825, loss 4.07404, smoothed loss 4.23932, grad norm 4.21640, param norm 96.12170
epoch 15, iter 13830, loss 4.22783, smoothed loss 4.23643, grad norm 4.68734, param norm 96.13218
epoch 15, iter 13835, loss 4.22251, smoothed loss 4.23102, grad norm 4.75516, param norm 96.14255
epoch 15, iter 13840, loss 3.89218, smoothed loss 4.22321, grad norm 4.49794, param norm 96.15282
epoch 15, iter 13845, loss 4.49540, smoothed loss 4.22779, grad norm 4.22780, param norm 96.16330
epoch 15, iter 13850, loss 4.43303, smoothed loss 4.22345, grad norm 5.11634, param norm 96.17342
epoch 15, iter 13855, loss 3.71236, smoothed loss 4.21528, grad norm 4.93466, param norm 96.18458
epoch 15, iter 13860, loss 3.87528, smoothed loss 4.19898, grad norm 4.15813, param norm 96.19691
epoch 15, iter 13865, loss 3.87404, smoothed loss 4.19114, grad norm 4.91589, param norm 96.20773
epoch 15, iter 13870, loss 3.96193, smoothed loss 4.18079, grad norm 4.53495, param norm 96.21880
epoch 15, iter 13875, loss 4.59866, smoothed loss 4.19624, grad norm 4.60804, param norm 96.22821
epoch 15, iter 13880, loss 4.16488, smoothed loss 4.21234, grad norm 4.27600, param norm 96.23447
epoch 15, iter 13885, loss 2.79240, smoothed loss 4.20013, grad norm 3.82698, param norm 96.24127
epoch 15, iter 13890, loss 4.04900, smoothed loss 4.19635, grad norm 3.85635, param norm 96.25161
epoch 15, iter 13895, loss 4.37893, smoothed loss 4.18823, grad norm 4.61025, param norm 96.26381
epoch 15, iter 13900, loss 4.74890, smoothed loss 4.19180, grad norm 4.63415, param norm 96.27566
epoch 15, iter 13905, loss 4.25645, smoothed loss 4.18836, grad norm 4.58376, param norm 96.28607
epoch 15, iter 13910, loss 3.63951, smoothed loss 4.17040, grad norm 4.29882, param norm 96.29606
epoch 15, iter 13915, loss 4.91347, smoothed loss 4.18411, grad norm 4.73431, param norm 96.30889
epoch 15, iter 13920, loss 3.66637, smoothed loss 4.17176, grad norm 4.51675, param norm 96.31990
epoch 15, iter 13925, loss 4.35056, smoothed loss 4.16671, grad norm 3.96055, param norm 96.33099
epoch 15, iter 13930, loss 3.91807, smoothed loss 4.16371, grad norm 4.32722, param norm 96.34339
epoch 15, iter 13935, loss 4.00879, smoothed loss 4.16606, grad norm 4.68221, param norm 96.35420
epoch 15, iter 13940, loss 3.98447, smoothed loss 4.18743, grad norm 4.20126, param norm 96.36549
epoch 15, iter 13945, loss 3.88987, smoothed loss 4.18601, grad norm 4.03435, param norm 96.37645
epoch 15, iter 13950, loss 3.57427, smoothed loss 4.17961, grad norm 3.76330, param norm 96.38738
epoch 15, iter 13955, loss 4.18575, smoothed loss 4.18703, grad norm 4.32970, param norm 96.39901
epoch 15, iter 13960, loss 4.15364, smoothed loss 4.18508, grad norm 3.98666, param norm 96.40974
epoch 15, iter 13965, loss 3.96853, smoothed loss 4.18785, grad norm 4.21772, param norm 96.42099
epoch 15, iter 13970, loss 4.37233, smoothed loss 4.19684, grad norm 4.67172, param norm 96.43102
epoch 15, iter 13975, loss 4.42544, smoothed loss 4.21252, grad norm 4.48469, param norm 96.43970
epoch 15, iter 13980, loss 4.32005, smoothed loss 4.21238, grad norm 4.82550, param norm 96.44702
epoch 15, iter 13985, loss 4.55969, smoothed loss 4.22216, grad norm 4.65839, param norm 96.45533
epoch 15, iter 13990, loss 4.81338, smoothed loss 4.23376, grad norm 3.96770, param norm 96.46561
epoch 15, iter 13995, loss 4.40943, smoothed loss 4.23748, grad norm 4.62657, param norm 96.47724
epoch 15, iter 14000, loss 4.22789, smoothed loss 4.23044, grad norm 4.93715, param norm 96.48991
Saving to ./train/qa.ckpt...
Calculating dev loss...
Epoch 15, Iter 14000, dev loss: 4.920815
Calculating Train F1/EM...
F1 train: 1000 examples took 9.29194 seconds [Score: 0.49382]
Exact Match train: 1000 examples took 8.80901 seconds [Score: 0.40300]
Epoch 15, Iter 14000, Train F1 score: 0.493820, Train EM score: 0.403000
Calculating Dev F1/EM...
F1 dev: 7118 examples took 58.50085 seconds [Score: 0.36168]
Exact Match dev: 7118 examples took 58.02511 seconds [Score: 0.25386]
Epoch 15, Iter 14000, Dev F1 score: 0.361676, Dev EM score: 0.253863
End of epoch 15
epoch 15, iter 14005, loss 4.31160, smoothed loss 4.23576, grad norm 4.61214, param norm 96.50207
epoch 15, iter 14010, loss 4.49599, smoothed loss 4.23760, grad norm 5.06592, param norm 96.51189
epoch 15, iter 14015, loss 4.82426, smoothed loss 4.24032, grad norm 4.72895, param norm 96.52148
epoch 15, iter 14020, loss 4.50477, smoothed loss 4.23679, grad norm 4.84522, param norm 96.53155
epoch 15, iter 14025, loss 4.28259, smoothed loss 4.24884, grad norm 5.03983, param norm 96.54032
epoch 15, iter 14030, loss 5.01331, smoothed loss 4.24508, grad norm 4.95418, param norm 96.54716
epoch 15, iter 14035, loss 3.45898, smoothed loss 4.23401, grad norm 4.05185, param norm 96.55627
epoch 15, iter 14040, loss 3.69949, smoothed loss 4.22850, grad norm 4.42154, param norm 96.56898
epoch 15, iter 14045, loss 4.19967, smoothed loss 4.21772, grad norm 3.96258, param norm 96.58427
epoch 15, iter 14050, loss 4.30537, smoothed loss 4.22915, grad norm 5.15772, param norm 96.60008
epoch 15, iter 14055, loss 4.47125, smoothed loss 4.23586, grad norm 5.27642, param norm 96.61354
epoch 15, iter 14060, loss 4.86274, smoothed loss 4.23113, grad norm 4.12218, param norm 96.62284
epoch 15, iter 14065, loss 4.41629, smoothed loss 4.22297, grad norm 4.08811, param norm 96.63413
epoch 15, iter 14070, loss 4.13230, smoothed loss 4.20731, grad norm 5.12112, param norm 96.64729
epoch 15, iter 14075, loss 3.83357, smoothed loss 4.20675, grad norm 4.57297, param norm 96.66106
epoch 15, iter 14080, loss 4.26639, smoothed loss 4.21484, grad norm 4.39722, param norm 96.67392
epoch 15, iter 14085, loss 4.78193, smoothed loss 4.21629, grad norm 5.36802, param norm 96.68535
epoch 15, iter 14090, loss 4.11809, smoothed loss 4.22596, grad norm 4.98187, param norm 96.69368
epoch 15, iter 14095, loss 4.30974, smoothed loss 4.22124, grad norm 5.39132, param norm 96.70300
epoch 15, iter 14100, loss 3.50290, smoothed loss 4.21413, grad norm 3.98937, param norm 96.71320
epoch 15, iter 14105, loss 4.45681, smoothed loss 4.21078, grad norm 5.10842, param norm 96.72623
epoch 15, iter 14110, loss 3.76583, smoothed loss 4.20035, grad norm 4.88104, param norm 96.73941
epoch 15, iter 14115, loss 3.86390, smoothed loss 4.18266, grad norm 4.09645, param norm 96.75423
epoch 15, iter 14120, loss 5.20159, smoothed loss 4.19266, grad norm 4.90960, param norm 96.76960
epoch 15, iter 14125, loss 4.36675, smoothed loss 4.20632, grad norm 4.75667, param norm 96.77898
epoch 15, iter 14130, loss 4.56774, smoothed loss 4.20520, grad norm 6.25119, param norm 96.78717
epoch 15, iter 14135, loss 4.11020, smoothed loss 4.20940, grad norm 4.88317, param norm 96.79659
epoch 15, iter 14140, loss 4.09488, smoothed loss 4.21726, grad norm 4.75303, param norm 96.80662
epoch 15, iter 14145, loss 4.83561, smoothed loss 4.21955, grad norm 4.87149, param norm 96.81763
epoch 15, iter 14150, loss 4.44541, smoothed loss 4.22712, grad norm 4.67933, param norm 96.82635
epoch 15, iter 14155, loss 4.11150, smoothed loss 4.21960, grad norm 4.59018, param norm 96.83759
epoch 15, iter 14160, loss 3.92005, smoothed loss 4.21469, grad norm 4.56779, param norm 96.84935
