/content/Machine-Reading-Comprehension
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
Loading devset:
100% 48/48 [00:04<00:00, 10.78it/s]
Number of triples ignored due to token mapping problems:  3212
Number of triples ignored due to unalignment with tokenization problems:  240
Number of triples ignored due to span alignment problems:  0
Processed examples: 7118 out of 10570
Loading trainset:
100% 442/442 [00:37<00:00, 11.82it/s]
Number of triples ignored due to token mapping problems:  28669
Number of triples ignored due to unalignment with tokenization problems:  1760
Number of triples ignored due to span alignment problems:  7
Processed examples: 57163 out of 87599
100% 400000/400000 [00:12<00:00, 32949.56it/s]
Finished processing GloVe vectors
In Add Embed Layer
In RNN Encoder layer
In BiDAF Layer
In output layer
In loss function
Finished initialization of model
Training Network
2018-11-24 05:50:26.935194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-24 05:50:26.935687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-11-24 05:50:26.935728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-24 05:50:27.936353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-24 05:50:27.936415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-24 05:50:27.936442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-24 05:50:27.936793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Model finished setup
Adding batches start...
Added  160  batches
epoch 1, iter 5, loss 9.40177, smoothed loss 9.50806, grad norm 1.10909, param norm 57.01452
epoch 1, iter 10, loss 9.15639, smoothed loss 9.49456, grad norm 1.44249, param norm 57.11692
epoch 1, iter 15, loss 9.17795, smoothed loss 9.48333, grad norm 1.77361, param norm 57.22364
epoch 1, iter 20, loss 8.61287, smoothed loss 9.45856, grad norm 2.00675, param norm 57.35747
epoch 1, iter 25, loss 8.82313, smoothed loss 9.42898, grad norm 2.21208, param norm 57.51488
epoch 1, iter 30, loss 8.30195, smoothed loss 9.38450, grad norm 2.56424, param norm 57.69386
epoch 1, iter 35, loss 8.76934, smoothed loss 9.34136, grad norm 2.95855, param norm 57.86536
epoch 1, iter 40, loss 8.44012, smoothed loss 9.29603, grad norm 2.49832, param norm 58.01068
epoch 1, iter 45, loss 8.16163, smoothed loss 9.24880, grad norm 2.34419, param norm 58.15430
epoch 1, iter 50, loss 8.43230, smoothed loss 9.20951, grad norm 2.19766, param norm 58.29303
epoch 1, iter 55, loss 8.74641, smoothed loss 9.15341, grad norm 2.60912, param norm 58.42974
epoch 1, iter 60, loss 8.21699, smoothed loss 9.09926, grad norm 2.37645, param norm 58.58010
epoch 1, iter 65, loss 7.94417, smoothed loss 9.03614, grad norm 2.23076, param norm 58.71643
epoch 1, iter 70, loss 8.01742, smoothed loss 8.98759, grad norm 2.09800, param norm 58.83350
epoch 1, iter 75, loss 8.31926, smoothed loss 8.94993, grad norm 2.03753, param norm 58.94205
epoch 1, iter 80, loss 7.89902, smoothed loss 8.90132, grad norm 2.11677, param norm 59.04005
epoch 1, iter 85, loss 8.57347, smoothed loss 8.86444, grad norm 2.27248, param norm 59.15126
epoch 1, iter 90, loss 8.15581, smoothed loss 8.81591, grad norm 2.21228, param norm 59.28103
epoch 1, iter 95, loss 7.81428, smoothed loss 8.75883, grad norm 2.30014, param norm 59.42175
epoch 1, iter 100, loss 7.73258, smoothed loss 8.71131, grad norm 2.20573, param norm 59.53973
epoch 1, iter 105, loss 7.35945, smoothed loss 8.66368, grad norm 2.51768, param norm 59.66300
epoch 1, iter 110, loss 8.08073, smoothed loss 8.63022, grad norm 2.39010, param norm 59.80059
epoch 1, iter 115, loss 7.34560, smoothed loss 8.57719, grad norm 2.56943, param norm 59.92849
epoch 1, iter 120, loss 7.08419, smoothed loss 8.52534, grad norm 3.19216, param norm 60.06627
epoch 1, iter 125, loss 7.61672, smoothed loss 8.48082, grad norm 3.17710, param norm 60.21584
epoch 1, iter 130, loss 7.68043, smoothed loss 8.43752, grad norm 3.53358, param norm 60.36584
epoch 1, iter 135, loss 7.28999, smoothed loss 8.39568, grad norm 3.03412, param norm 60.54184
epoch 1, iter 140, loss 7.22161, smoothed loss 8.34567, grad norm 3.36235, param norm 60.70847
epoch 1, iter 145, loss 7.74759, smoothed loss 8.28935, grad norm 3.86181, param norm 60.89345
epoch 1, iter 150, loss 7.41402, smoothed loss 8.24968, grad norm 2.97021, param norm 61.04388
epoch 1, iter 155, loss 6.93087, smoothed loss 8.19946, grad norm 3.45387, param norm 61.20274
epoch 1, iter 160, loss 7.15251, smoothed loss 8.16295, grad norm 3.26720, param norm 61.36541
Adding batches start...
Added  160  batches
epoch 1, iter 165, loss 7.29114, smoothed loss 8.11168, grad norm 3.89747, param norm 61.52436
epoch 1, iter 170, loss 7.37754, smoothed loss 8.06494, grad norm 3.48728, param norm 61.67150
epoch 1, iter 175, loss 7.08207, smoothed loss 8.02209, grad norm 3.64454, param norm 61.80868
epoch 1, iter 180, loss 7.41116, smoothed loss 7.97181, grad norm 2.93649, param norm 61.96687
epoch 1, iter 185, loss 7.16338, smoothed loss 7.91394, grad norm 4.01475, param norm 62.13364
epoch 1, iter 190, loss 7.16542, smoothed loss 7.86357, grad norm 4.22729, param norm 62.28563
epoch 1, iter 195, loss 7.48940, smoothed loss 7.80798, grad norm 4.56094, param norm 62.42836
epoch 1, iter 200, loss 7.19442, smoothed loss 7.76446, grad norm 3.38536, param norm 62.57418
epoch 1, iter 205, loss 7.14935, smoothed loss 7.73520, grad norm 3.99252, param norm 62.71355
epoch 1, iter 210, loss 6.62206, smoothed loss 7.69263, grad norm 3.51144, param norm 62.85868
epoch 1, iter 215, loss 7.12403, smoothed loss 7.65813, grad norm 3.67909, param norm 63.01730
epoch 1, iter 220, loss 6.30208, smoothed loss 7.61515, grad norm 3.43365, param norm 63.16681
epoch 1, iter 225, loss 6.80989, smoothed loss 7.57128, grad norm 4.06840, param norm 63.31830
epoch 1, iter 230, loss 6.90219, smoothed loss 7.52302, grad norm 4.44598, param norm 63.47169
epoch 1, iter 235, loss 7.20782, smoothed loss 7.48474, grad norm 4.58794, param norm 63.61977
epoch 1, iter 240, loss 7.46240, smoothed loss 7.44083, grad norm 4.65689, param norm 63.77956
epoch 1, iter 245, loss 6.63628, smoothed loss 7.41384, grad norm 3.92774, param norm 63.90816
epoch 1, iter 250, loss 6.67558, smoothed loss 7.36979, grad norm 3.56386, param norm 64.05172
epoch 1, iter 255, loss 6.24433, smoothed loss 7.32585, grad norm 4.29543, param norm 64.19898
epoch 1, iter 260, loss 6.21611, smoothed loss 7.28657, grad norm 3.73323, param norm 64.33833
epoch 1, iter 265, loss 6.82258, smoothed loss 7.25125, grad norm 4.68608, param norm 64.46854
epoch 1, iter 270, loss 6.67266, smoothed loss 7.20479, grad norm 4.20452, param norm 64.60712
epoch 1, iter 275, loss 7.04971, smoothed loss 7.16227, grad norm 4.49389, param norm 64.74792
epoch 1, iter 280, loss 6.66156, smoothed loss 7.12871, grad norm 3.67620, param norm 64.87548
epoch 1, iter 285, loss 6.83686, smoothed loss 7.09977, grad norm 4.59124, param norm 65.00801
epoch 1, iter 290, loss 6.45191, smoothed loss 7.06878, grad norm 3.55604, param norm 65.14922
epoch 1, iter 295, loss 6.01681, smoothed loss 7.03576, grad norm 4.11314, param norm 65.28462
epoch 1, iter 300, loss 5.85631, smoothed loss 7.00492, grad norm 3.51520, param norm 65.39511
epoch 1, iter 305, loss 6.77628, smoothed loss 6.98651, grad norm 4.48989, param norm 65.51414
epoch 1, iter 310, loss 6.46879, smoothed loss 6.96749, grad norm 4.10829, param norm 65.65193
epoch 1, iter 315, loss 7.05202, smoothed loss 6.94435, grad norm 4.54542, param norm 65.79878
epoch 1, iter 320, loss 6.10388, smoothed loss 6.91393, grad norm 4.22296, param norm 65.93521
Adding batches start...
Added  160  batches
epoch 1, iter 325, loss 6.51638, smoothed loss 6.87867, grad norm 4.15583, param norm 66.05935
epoch 1, iter 330, loss 6.54230, smoothed loss 6.85420, grad norm 3.55879, param norm 66.18573
epoch 1, iter 335, loss 6.36881, smoothed loss 6.82743, grad norm 3.73550, param norm 66.32182
epoch 1, iter 340, loss 6.96889, smoothed loss 6.80816, grad norm 4.29713, param norm 66.44913
epoch 1, iter 345, loss 6.71769, smoothed loss 6.77344, grad norm 5.52638, param norm 66.57536
epoch 1, iter 350, loss 6.22713, smoothed loss 6.73404, grad norm 4.14707, param norm 66.71596
epoch 1, iter 355, loss 5.51496, smoothed loss 6.69676, grad norm 3.82278, param norm 66.84547
epoch 1, iter 360, loss 6.21394, smoothed loss 6.65930, grad norm 3.71237, param norm 66.97861
epoch 1, iter 365, loss 5.99535, smoothed loss 6.64173, grad norm 3.97708, param norm 67.10116
epoch 1, iter 370, loss 6.14488, smoothed loss 6.61533, grad norm 4.17257, param norm 67.22429
epoch 1, iter 375, loss 5.62632, smoothed loss 6.57683, grad norm 4.22478, param norm 67.36918
epoch 1, iter 380, loss 5.39799, smoothed loss 6.54487, grad norm 4.17681, param norm 67.52464
epoch 1, iter 385, loss 5.92503, smoothed loss 6.51835, grad norm 4.02890, param norm 67.66419
epoch 1, iter 390, loss 5.46757, smoothed loss 6.48842, grad norm 3.98158, param norm 67.79210
epoch 1, iter 395, loss 5.91552, smoothed loss 6.47361, grad norm 3.83082, param norm 67.90461
epoch 1, iter 400, loss 5.63580, smoothed loss 6.45378, grad norm 4.11363, param norm 68.01842
epoch 1, iter 405, loss 5.84841, smoothed loss 6.42235, grad norm 4.29543, param norm 68.13340
epoch 1, iter 410, loss 5.72520, smoothed loss 6.37857, grad norm 4.32362, param norm 68.26601
epoch 1, iter 415, loss 6.62100, smoothed loss 6.36381, grad norm 4.23030, param norm 68.39102
epoch 1, iter 420, loss 6.44382, smoothed loss 6.33373, grad norm 3.82329, param norm 68.50420
epoch 1, iter 425, loss 5.92026, smoothed loss 6.31241, grad norm 4.02023, param norm 68.62029
epoch 1, iter 430, loss 6.05733, smoothed loss 6.28356, grad norm 4.76856, param norm 68.72809
epoch 1, iter 435, loss 5.46087, smoothed loss 6.24280, grad norm 4.26040, param norm 68.85538
epoch 1, iter 440, loss 5.57991, smoothed loss 6.21498, grad norm 4.37685, param norm 68.97337
epoch 1, iter 445, loss 5.73339, smoothed loss 6.19783, grad norm 3.81692, param norm 69.09259
epoch 1, iter 450, loss 5.74034, smoothed loss 6.17685, grad norm 4.09146, param norm 69.22317
epoch 1, iter 455, loss 5.19222, smoothed loss 6.17699, grad norm 4.13726, param norm 69.33912
epoch 1, iter 460, loss 5.55041, smoothed loss 6.17472, grad norm 4.12307, param norm 69.45776
epoch 1, iter 465, loss 5.71408, smoothed loss 6.16400, grad norm 4.09243, param norm 69.58051
epoch 1, iter 470, loss 5.40230, smoothed loss 6.13939, grad norm 4.20914, param norm 69.70930
epoch 1, iter 475, loss 5.79558, smoothed loss 6.13346, grad norm 4.42242, param norm 69.84033
epoch 1, iter 480, loss 5.52064, smoothed loss 6.10960, grad norm 3.89953, param norm 69.96842
Adding batches start...
Added  160  batches
epoch 1, iter 485, loss 4.66488, smoothed loss 6.07115, grad norm 3.74491, param norm 70.09004
epoch 1, iter 490, loss 5.63473, smoothed loss 6.04552, grad norm 5.36236, param norm 70.22288
epoch 1, iter 495, loss 5.04974, smoothed loss 6.03044, grad norm 4.43704, param norm 70.34532
epoch 1, iter 500, loss 5.50209, smoothed loss 6.00961, grad norm 3.76401, param norm 70.47438
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 1, Iter 500, dev loss: 5.287797
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 19.79183 seconds [Score: 0.37457]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.20359 seconds [Score: 0.20800]
Epoch 1, Iter 500, Train F1 score: 0.374570, Train EM score: 0.208000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.83350 seconds [Score: 0.35129]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 118.44308 seconds [Score: 0.22450]
Epoch 1, Iter 500, Dev F1 score: 0.351294, Dev EM score: 0.224501
End of epoch 1
epoch 1, iter 505, loss 5.78081, smoothed loss 5.99442, grad norm 4.78885, param norm 70.60962
epoch 1, iter 510, loss 6.67604, smoothed loss 5.98792, grad norm 5.91502, param norm 70.74937
epoch 1, iter 515, loss 5.93923, smoothed loss 5.97581, grad norm 4.39824, param norm 70.85603
epoch 1, iter 520, loss 5.67717, smoothed loss 5.96195, grad norm 4.51369, param norm 70.98614
epoch 1, iter 525, loss 5.99553, smoothed loss 5.94202, grad norm 4.80295, param norm 71.13545
epoch 1, iter 530, loss 5.28894, smoothed loss 5.92488, grad norm 4.04075, param norm 71.27325
epoch 1, iter 535, loss 5.61717, smoothed loss 5.90760, grad norm 4.17945, param norm 71.39350
epoch 1, iter 540, loss 5.72058, smoothed loss 5.89130, grad norm 4.71162, param norm 71.51978
epoch 1, iter 545, loss 5.29034, smoothed loss 5.85106, grad norm 4.01300, param norm 71.65745
epoch 1, iter 550, loss 4.71405, smoothed loss 5.81141, grad norm 4.29953, param norm 71.79041
epoch 1, iter 555, loss 5.71753, smoothed loss 5.79392, grad norm 4.71261, param norm 71.92033
epoch 1, iter 560, loss 4.84072, smoothed loss 5.76916, grad norm 4.01702, param norm 72.04483
epoch 1, iter 565, loss 5.17550, smoothed loss 5.74004, grad norm 4.44303, param norm 72.17903
epoch 1, iter 570, loss 5.41417, smoothed loss 5.71921, grad norm 4.82078, param norm 72.31631
epoch 1, iter 575, loss 5.47352, smoothed loss 5.69615, grad norm 4.18867, param norm 72.44028
epoch 1, iter 580, loss 4.85171, smoothed loss 5.68669, grad norm 3.82907, param norm 72.55190
epoch 1, iter 585, loss 4.28742, smoothed loss 5.65264, grad norm 3.97511, param norm 72.67451
epoch 1, iter 590, loss 5.64386, smoothed loss 5.63166, grad norm 4.30504, param norm 72.81356
epoch 1, iter 595, loss 5.22588, smoothed loss 5.60921, grad norm 4.63548, param norm 72.93836
epoch 1, iter 600, loss 4.63209, smoothed loss 5.57116, grad norm 4.46034, param norm 73.06804
epoch 1, iter 605, loss 4.09728, smoothed loss 5.52932, grad norm 3.96080, param norm 73.18972
epoch 1, iter 610, loss 5.64604, smoothed loss 5.50527, grad norm 5.16519, param norm 73.30122
epoch 1, iter 615, loss 5.38832, smoothed loss 5.48009, grad norm 4.35833, param norm 73.42413
epoch 1, iter 620, loss 5.33677, smoothed loss 5.45152, grad norm 4.52928, param norm 73.54805
epoch 1, iter 625, loss 5.23417, smoothed loss 5.42464, grad norm 4.33097, param norm 73.67941
epoch 1, iter 630, loss 4.82127, smoothed loss 5.39092, grad norm 4.23565, param norm 73.80862
epoch 1, iter 635, loss 4.73376, smoothed loss 5.35883, grad norm 4.05435, param norm 73.91799
epoch 1, iter 640, loss 4.72241, smoothed loss 5.33513, grad norm 3.95454, param norm 74.03196
Adding batches start...
Added  160  batches
epoch 1, iter 645, loss 5.09116, smoothed loss 5.30505, grad norm 4.29666, param norm 74.15610
epoch 1, iter 650, loss 6.43803, smoothed loss 5.29654, grad norm 5.91232, param norm 74.27824
epoch 1, iter 655, loss 4.27710, smoothed loss 5.27432, grad norm 4.43471, param norm 74.39260
epoch 1, iter 660, loss 5.27697, smoothed loss 5.27706, grad norm 4.34972, param norm 74.50871
epoch 1, iter 665, loss 4.65000, smoothed loss 5.24793, grad norm 4.26896, param norm 74.61023
epoch 1, iter 670, loss 4.33157, smoothed loss 5.21785, grad norm 3.96467, param norm 74.71821
epoch 1, iter 675, loss 4.57665, smoothed loss 5.18604, grad norm 3.84480, param norm 74.84605
epoch 1, iter 680, loss 4.24630, smoothed loss 5.14206, grad norm 4.02219, param norm 74.95916
epoch 1, iter 685, loss 4.65417, smoothed loss 5.11519, grad norm 4.53524, param norm 75.06819
epoch 1, iter 690, loss 4.52513, smoothed loss 5.08961, grad norm 4.17509, param norm 75.17195
epoch 1, iter 695, loss 4.42228, smoothed loss 5.06021, grad norm 3.49832, param norm 75.28632
epoch 1, iter 700, loss 5.82258, smoothed loss 5.05004, grad norm 4.42144, param norm 75.39578
epoch 1, iter 705, loss 4.53385, smoothed loss 5.03903, grad norm 4.44513, param norm 75.49301
epoch 1, iter 710, loss 4.86115, smoothed loss 5.01921, grad norm 4.13620, param norm 75.60526
epoch 1, iter 715, loss 4.34577, smoothed loss 4.98815, grad norm 3.81654, param norm 75.72260
epoch 1, iter 720, loss 5.06273, smoothed loss 4.97097, grad norm 4.41445, param norm 75.83733
epoch 1, iter 725, loss 4.54528, smoothed loss 4.94924, grad norm 3.92048, param norm 75.93592
epoch 1, iter 730, loss 4.02980, smoothed loss 4.94513, grad norm 4.26900, param norm 76.02544
epoch 1, iter 735, loss 4.82811, smoothed loss 4.92702, grad norm 4.59563, param norm 76.12701
epoch 1, iter 740, loss 4.95061, smoothed loss 4.91195, grad norm 4.01446, param norm 76.23021
epoch 1, iter 745, loss 4.76988, smoothed loss 4.89433, grad norm 3.78423, param norm 76.33384
epoch 1, iter 750, loss 4.72757, smoothed loss 4.88470, grad norm 4.47114, param norm 76.42680
epoch 1, iter 755, loss 4.79505, smoothed loss 4.87981, grad norm 3.93646, param norm 76.51860
epoch 1, iter 760, loss 5.00108, smoothed loss 4.87737, grad norm 3.98695, param norm 76.61587
epoch 1, iter 765, loss 4.81154, smoothed loss 4.85959, grad norm 4.19469, param norm 76.71031
epoch 1, iter 770, loss 4.54457, smoothed loss 4.84010, grad norm 3.96731, param norm 76.81405
epoch 1, iter 775, loss 3.78811, smoothed loss 4.82329, grad norm 3.84074, param norm 76.91302
epoch 1, iter 780, loss 5.62965, smoothed loss 4.82073, grad norm 4.15754, param norm 77.01615
epoch 1, iter 785, loss 4.37924, smoothed loss 4.81331, grad norm 3.56311, param norm 77.10175
epoch 1, iter 790, loss 4.41853, smoothed loss 4.78602, grad norm 4.42429, param norm 77.20773
epoch 1, iter 795, loss 4.35902, smoothed loss 4.76759, grad norm 3.88815, param norm 77.31642
epoch 1, iter 800, loss 4.40430, smoothed loss 4.74447, grad norm 4.30338, param norm 77.40390
Adding batches start...
Added  144  batches
epoch 1, iter 805, loss 4.63551, smoothed loss 4.73007, grad norm 3.80829, param norm 77.48423
epoch 1, iter 810, loss 4.19545, smoothed loss 4.70146, grad norm 4.08292, param norm 77.57232
epoch 1, iter 815, loss 3.94615, smoothed loss 4.68996, grad norm 4.19542, param norm 77.66371
epoch 1, iter 820, loss 3.85833, smoothed loss 4.67373, grad norm 3.89414, param norm 77.73991
epoch 1, iter 825, loss 4.57306, smoothed loss 4.67224, grad norm 3.98649, param norm 77.83012
epoch 1, iter 830, loss 4.37359, smoothed loss 4.67133, grad norm 4.11580, param norm 77.92425
epoch 1, iter 835, loss 4.24640, smoothed loss 4.66700, grad norm 4.02877, param norm 78.01529
epoch 1, iter 840, loss 3.99405, smoothed loss 4.65415, grad norm 4.15799, param norm 78.10694
epoch 1, iter 845, loss 4.35065, smoothed loss 4.63990, grad norm 4.10517, param norm 78.19502
epoch 1, iter 850, loss 4.08543, smoothed loss 4.61295, grad norm 3.98513, param norm 78.27863
epoch 1, iter 855, loss 5.62028, smoothed loss 4.62586, grad norm 4.48809, param norm 78.36160
epoch 1, iter 860, loss 4.88842, smoothed loss 4.62189, grad norm 4.95800, param norm 78.44312
epoch 1, iter 865, loss 4.58143, smoothed loss 4.60891, grad norm 4.04039, param norm 78.53567
epoch 1, iter 870, loss 3.86967, smoothed loss 4.60372, grad norm 3.96532, param norm 78.61206
epoch 1, iter 875, loss 4.57803, smoothed loss 4.59567, grad norm 4.83664, param norm 78.68805
epoch 1, iter 880, loss 3.56238, smoothed loss 4.57202, grad norm 4.01497, param norm 78.75832
epoch 1, iter 885, loss 4.59317, smoothed loss 4.55663, grad norm 4.20388, param norm 78.84312
epoch 1, iter 890, loss 4.57953, smoothed loss 4.56003, grad norm 4.71479, param norm 78.92553
epoch 1, iter 895, loss 3.96368, smoothed loss 4.54073, grad norm 4.33397, param norm 79.01292
epoch 1, iter 900, loss 4.83437, smoothed loss 4.52259, grad norm 4.33733, param norm 79.09753
epoch 1, iter 905, loss 4.26347, smoothed loss 4.51448, grad norm 3.87479, param norm 79.17820
epoch 1, iter 910, loss 4.39806, smoothed loss 4.50360, grad norm 3.98854, param norm 79.26000
epoch 1, iter 915, loss 3.64307, smoothed loss 4.48817, grad norm 3.50440, param norm 79.33387
epoch 1, iter 920, loss 4.45387, smoothed loss 4.46472, grad norm 4.34157, param norm 79.41542
epoch 1, iter 925, loss 4.88650, smoothed loss 4.45969, grad norm 4.09893, param norm 79.50142
epoch 1, iter 930, loss 4.67089, smoothed loss 4.45952, grad norm 4.11188, param norm 79.58296
epoch 1, iter 935, loss 4.30214, smoothed loss 4.45009, grad norm 4.08243, param norm 79.65858
epoch 1, iter 940, loss 4.04198, smoothed loss 4.44848, grad norm 3.91640, param norm 79.72640
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 2, iter 945, loss 3.93238, smoothed loss 4.43049, grad norm 4.10813, param norm 79.80166
epoch 2, iter 950, loss 4.48133, smoothed loss 4.42641, grad norm 4.26419, param norm 79.88551
epoch 2, iter 955, loss 3.85358, smoothed loss 4.41493, grad norm 3.89994, param norm 79.96689
epoch 2, iter 960, loss 4.49363, smoothed loss 4.40634, grad norm 3.91336, param norm 80.04868
epoch 2, iter 965, loss 4.64069, smoothed loss 4.39322, grad norm 4.01116, param norm 80.12800
epoch 2, iter 970, loss 3.90773, smoothed loss 4.37928, grad norm 4.05901, param norm 80.20808
epoch 2, iter 975, loss 4.34945, smoothed loss 4.36398, grad norm 4.49982, param norm 80.28841
epoch 2, iter 980, loss 3.85842, smoothed loss 4.34999, grad norm 4.22878, param norm 80.36240
epoch 2, iter 985, loss 3.95251, smoothed loss 4.34703, grad norm 4.33200, param norm 80.44525
epoch 2, iter 990, loss 2.99616, smoothed loss 4.32872, grad norm 3.48569, param norm 80.51700
epoch 2, iter 995, loss 4.93998, smoothed loss 4.32967, grad norm 4.39815, param norm 80.59071
epoch 2, iter 1000, loss 3.73905, smoothed loss 4.31458, grad norm 4.01646, param norm 80.67563
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 2, Iter 1000, dev loss: 3.955745
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.30659 seconds [Score: 0.54676]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.29739 seconds [Score: 0.39800]
Epoch 2, Iter 1000, Train F1 score: 0.546764, Train EM score: 0.398000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.37673 seconds [Score: 0.51356]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.51709 seconds [Score: 0.36850]
Epoch 2, Iter 1000, Dev F1 score: 0.513558, Dev EM score: 0.368502
End of epoch 2
epoch 2, iter 1005, loss 3.87003, smoothed loss 4.29464, grad norm 4.01329, param norm 80.76611
epoch 2, iter 1010, loss 4.42629, smoothed loss 4.28382, grad norm 4.12717, param norm 80.85163
epoch 2, iter 1015, loss 5.05016, smoothed loss 4.28634, grad norm 4.79003, param norm 80.93194
epoch 2, iter 1020, loss 4.75206, smoothed loss 4.29177, grad norm 4.34823, param norm 81.00699
epoch 2, iter 1025, loss 3.84528, smoothed loss 4.28258, grad norm 3.74821, param norm 81.07335
epoch 2, iter 1030, loss 4.16468, smoothed loss 4.27961, grad norm 3.79327, param norm 81.14192
epoch 2, iter 1035, loss 4.38931, smoothed loss 4.28571, grad norm 4.15190, param norm 81.22210
epoch 2, iter 1040, loss 4.86248, smoothed loss 4.29872, grad norm 3.88667, param norm 81.29929
epoch 2, iter 1045, loss 3.81420, smoothed loss 4.28466, grad norm 3.66129, param norm 81.36785
epoch 2, iter 1050, loss 3.39886, smoothed loss 4.27276, grad norm 3.73697, param norm 81.44141
epoch 2, iter 1055, loss 5.16894, smoothed loss 4.29211, grad norm 4.31746, param norm 81.50697
epoch 2, iter 1060, loss 4.04623, smoothed loss 4.27859, grad norm 3.52513, param norm 81.57100
epoch 2, iter 1065, loss 4.66566, smoothed loss 4.28241, grad norm 4.31411, param norm 81.64523
epoch 2, iter 1070, loss 4.49885, smoothed loss 4.28845, grad norm 4.29253, param norm 81.72806
epoch 2, iter 1075, loss 3.83687, smoothed loss 4.27132, grad norm 3.77606, param norm 81.80918
epoch 2, iter 1080, loss 3.69257, smoothed loss 4.25569, grad norm 4.01246, param norm 81.88615
epoch 2, iter 1085, loss 4.14847, smoothed loss 4.24176, grad norm 4.35485, param norm 81.95658
epoch 2, iter 1090, loss 4.09051, smoothed loss 4.24766, grad norm 4.29437, param norm 82.02589
epoch 2, iter 1095, loss 4.86948, smoothed loss 4.24484, grad norm 4.01698, param norm 82.09581
epoch 2, iter 1100, loss 3.61770, smoothed loss 4.24118, grad norm 3.92094, param norm 82.16091
Adding batches start...
Added  160  batches
epoch 2, iter 1105, loss 4.26340, smoothed loss 4.24054, grad norm 4.18892, param norm 82.23272
epoch 2, iter 1110, loss 4.27914, smoothed loss 4.25103, grad norm 3.94622, param norm 82.30194
epoch 2, iter 1115, loss 4.22566, smoothed loss 4.24843, grad norm 4.07204, param norm 82.37409
epoch 2, iter 1120, loss 4.34186, smoothed loss 4.23106, grad norm 4.86213, param norm 82.44682
epoch 2, iter 1125, loss 3.85128, smoothed loss 4.21485, grad norm 3.91635, param norm 82.51517
epoch 2, iter 1130, loss 4.59248, smoothed loss 4.21112, grad norm 4.64659, param norm 82.57909
epoch 2, iter 1135, loss 3.58978, smoothed loss 4.20767, grad norm 3.96661, param norm 82.63369
epoch 2, iter 1140, loss 3.78062, smoothed loss 4.21028, grad norm 3.61620, param norm 82.69373
epoch 2, iter 1145, loss 3.74425, smoothed loss 4.19507, grad norm 4.16361, param norm 82.76476
epoch 2, iter 1150, loss 3.87137, smoothed loss 4.20834, grad norm 4.02563, param norm 82.83995
epoch 2, iter 1155, loss 3.73187, smoothed loss 4.19298, grad norm 3.81845, param norm 82.91477
epoch 2, iter 1160, loss 4.13562, smoothed loss 4.18632, grad norm 4.14066, param norm 82.99239
epoch 2, iter 1165, loss 3.93335, smoothed loss 4.17838, grad norm 3.93489, param norm 83.06758
epoch 2, iter 1170, loss 3.59445, smoothed loss 4.15971, grad norm 4.10853, param norm 83.14677
epoch 2, iter 1175, loss 4.12026, smoothed loss 4.15907, grad norm 5.21461, param norm 83.22929
epoch 2, iter 1180, loss 3.96517, smoothed loss 4.15605, grad norm 3.94066, param norm 83.29299
epoch 2, iter 1185, loss 4.39797, smoothed loss 4.14235, grad norm 4.44346, param norm 83.35928
epoch 2, iter 1190, loss 3.83168, smoothed loss 4.13855, grad norm 3.50637, param norm 83.43320
epoch 2, iter 1195, loss 3.42432, smoothed loss 4.13982, grad norm 3.49421, param norm 83.50421
epoch 2, iter 1200, loss 4.29668, smoothed loss 4.15168, grad norm 3.75087, param norm 83.56728
epoch 2, iter 1205, loss 3.56527, smoothed loss 4.13547, grad norm 3.60692, param norm 83.62430
epoch 2, iter 1210, loss 3.93470, smoothed loss 4.13373, grad norm 3.71958, param norm 83.67901
epoch 2, iter 1215, loss 4.15755, smoothed loss 4.14300, grad norm 4.43984, param norm 83.74406
epoch 2, iter 1220, loss 3.57555, smoothed loss 4.13065, grad norm 3.66433, param norm 83.80790
epoch 2, iter 1225, loss 4.50128, smoothed loss 4.12623, grad norm 4.30302, param norm 83.86668
epoch 2, iter 1230, loss 4.11195, smoothed loss 4.11976, grad norm 4.11804, param norm 83.92546
epoch 2, iter 1235, loss 4.79783, smoothed loss 4.11710, grad norm 4.29215, param norm 83.98966
epoch 2, iter 1240, loss 3.77066, smoothed loss 4.10249, grad norm 3.67291, param norm 84.05498
epoch 2, iter 1245, loss 3.87029, smoothed loss 4.09644, grad norm 4.28255, param norm 84.12432
epoch 2, iter 1250, loss 4.62916, smoothed loss 4.09367, grad norm 4.86034, param norm 84.19421
epoch 2, iter 1255, loss 3.77760, smoothed loss 4.08022, grad norm 4.08499, param norm 84.26102
epoch 2, iter 1260, loss 4.06302, smoothed loss 4.07603, grad norm 4.00819, param norm 84.31701
Adding batches start...
Added  160  batches
epoch 2, iter 1265, loss 3.62242, smoothed loss 4.07003, grad norm 3.73701, param norm 84.38103
epoch 2, iter 1270, loss 3.80059, smoothed loss 4.06874, grad norm 3.85786, param norm 84.45443
epoch 2, iter 1275, loss 4.26676, smoothed loss 4.06251, grad norm 4.24625, param norm 84.52451
epoch 2, iter 1280, loss 4.30479, smoothed loss 4.06872, grad norm 3.90176, param norm 84.58640
epoch 2, iter 1285, loss 3.65073, smoothed loss 4.06236, grad norm 3.54908, param norm 84.64259
epoch 2, iter 1290, loss 4.07618, smoothed loss 4.05733, grad norm 4.42081, param norm 84.70186
epoch 2, iter 1295, loss 3.31841, smoothed loss 4.03955, grad norm 3.65155, param norm 84.77247
epoch 2, iter 1300, loss 4.11790, smoothed loss 4.02771, grad norm 3.97852, param norm 84.84219
epoch 2, iter 1305, loss 3.85849, smoothed loss 4.01712, grad norm 4.17880, param norm 84.90452
epoch 2, iter 1310, loss 4.46693, smoothed loss 4.03087, grad norm 4.23754, param norm 84.95974
epoch 2, iter 1315, loss 3.64890, smoothed loss 4.01850, grad norm 3.79657, param norm 85.02242
epoch 2, iter 1320, loss 4.38772, smoothed loss 4.01681, grad norm 4.03105, param norm 85.09490
epoch 2, iter 1325, loss 4.04942, smoothed loss 3.99521, grad norm 4.26340, param norm 85.16416
epoch 2, iter 1330, loss 3.94367, smoothed loss 3.99244, grad norm 3.44752, param norm 85.23100
epoch 2, iter 1335, loss 3.81880, smoothed loss 3.98333, grad norm 3.91345, param norm 85.29282
epoch 2, iter 1340, loss 3.59625, smoothed loss 3.98460, grad norm 3.63065, param norm 85.35350
epoch 2, iter 1345, loss 3.28240, smoothed loss 3.97224, grad norm 4.19257, param norm 85.41727
epoch 2, iter 1350, loss 4.12817, smoothed loss 3.97476, grad norm 4.50399, param norm 85.48113
epoch 2, iter 1355, loss 3.57916, smoothed loss 3.96658, grad norm 3.57824, param norm 85.54023
epoch 2, iter 1360, loss 4.49902, smoothed loss 3.97124, grad norm 4.02887, param norm 85.60738
epoch 2, iter 1365, loss 4.42014, smoothed loss 3.98766, grad norm 4.11230, param norm 85.68113
epoch 2, iter 1370, loss 3.64645, smoothed loss 3.99857, grad norm 3.88184, param norm 85.74796
epoch 2, iter 1375, loss 4.10820, smoothed loss 3.98512, grad norm 3.70212, param norm 85.81283
epoch 2, iter 1380, loss 3.84756, smoothed loss 3.96215, grad norm 4.04411, param norm 85.88126
epoch 2, iter 1385, loss 4.78214, smoothed loss 3.97945, grad norm 4.31176, param norm 85.94757
epoch 2, iter 1390, loss 3.92632, smoothed loss 3.96842, grad norm 3.59444, param norm 86.01040
epoch 2, iter 1395, loss 4.59525, smoothed loss 3.98759, grad norm 4.14476, param norm 86.06258
epoch 2, iter 1400, loss 3.98922, smoothed loss 3.98323, grad norm 3.48913, param norm 86.10976
epoch 2, iter 1405, loss 4.20842, smoothed loss 3.98417, grad norm 3.82103, param norm 86.17097
epoch 2, iter 1410, loss 3.66702, smoothed loss 3.97197, grad norm 3.49522, param norm 86.23752
epoch 2, iter 1415, loss 2.97126, smoothed loss 3.95106, grad norm 3.51634, param norm 86.30489
epoch 2, iter 1420, loss 4.27577, smoothed loss 3.94632, grad norm 4.06269, param norm 86.36790
Adding batches start...
Added  160  batches
epoch 2, iter 1425, loss 3.80267, smoothed loss 3.95350, grad norm 3.89134, param norm 86.42493
epoch 2, iter 1430, loss 3.30723, smoothed loss 3.94152, grad norm 3.67077, param norm 86.48453
epoch 2, iter 1435, loss 3.37556, smoothed loss 3.93954, grad norm 3.83564, param norm 86.54623
epoch 2, iter 1440, loss 3.58230, smoothed loss 3.94378, grad norm 3.82873, param norm 86.61514
epoch 2, iter 1445, loss 2.90927, smoothed loss 3.93241, grad norm 3.43025, param norm 86.68121
epoch 2, iter 1450, loss 3.90378, smoothed loss 3.94253, grad norm 3.56644, param norm 86.74755
epoch 2, iter 1455, loss 3.99851, smoothed loss 3.93906, grad norm 3.79388, param norm 86.80997
epoch 2, iter 1460, loss 3.91257, smoothed loss 3.92975, grad norm 3.68952, param norm 86.87412
epoch 2, iter 1465, loss 3.60918, smoothed loss 3.92548, grad norm 4.24595, param norm 86.93156
epoch 2, iter 1470, loss 4.67927, smoothed loss 3.92240, grad norm 4.14660, param norm 86.98727
epoch 2, iter 1475, loss 3.50945, smoothed loss 3.92061, grad norm 3.52841, param norm 87.04896
epoch 2, iter 1480, loss 3.22560, smoothed loss 3.90088, grad norm 3.60632, param norm 87.11243
epoch 2, iter 1485, loss 4.03221, smoothed loss 3.89337, grad norm 4.10497, param norm 87.18238
epoch 2, iter 1490, loss 3.69085, smoothed loss 3.88713, grad norm 3.94717, param norm 87.23920
epoch 2, iter 1495, loss 3.14109, smoothed loss 3.88169, grad norm 3.77091, param norm 87.29642
epoch 2, iter 1500, loss 3.70406, smoothed loss 3.87505, grad norm 4.07223, param norm 87.36384
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 2, Iter 1500, dev loss: 3.546837
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.48652 seconds [Score: 0.62375]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.71202 seconds [Score: 0.45200]
Epoch 2, Iter 1500, Train F1 score: 0.623749, Train EM score: 0.452000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.07803 seconds [Score: 0.57365]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.42782 seconds [Score: 0.42203]
Epoch 2, Iter 1500, Dev F1 score: 0.573649, Dev EM score: 0.422029
End of epoch 2
epoch 2, iter 1505, loss 3.71975, smoothed loss 3.88301, grad norm 4.27226, param norm 87.43403
epoch 2, iter 1510, loss 4.10057, smoothed loss 3.87187, grad norm 3.97929, param norm 87.49905
epoch 2, iter 1515, loss 3.12602, smoothed loss 3.86048, grad norm 4.02504, param norm 87.56028
epoch 2, iter 1520, loss 4.79754, smoothed loss 3.87233, grad norm 5.18759, param norm 87.62112
epoch 2, iter 1525, loss 3.92675, smoothed loss 3.88601, grad norm 3.98175, param norm 87.68153
epoch 2, iter 1530, loss 4.18669, smoothed loss 3.89187, grad norm 3.67809, param norm 87.74014
epoch 2, iter 1535, loss 3.32018, smoothed loss 3.87456, grad norm 3.21989, param norm 87.79642
epoch 2, iter 1540, loss 4.26175, smoothed loss 3.87086, grad norm 4.15739, param norm 87.85813
epoch 2, iter 1545, loss 3.68059, smoothed loss 3.85799, grad norm 4.27679, param norm 87.91419
epoch 2, iter 1550, loss 3.37702, smoothed loss 3.85000, grad norm 3.64635, param norm 87.97134
epoch 2, iter 1555, loss 3.27811, smoothed loss 3.83344, grad norm 4.21710, param norm 88.02407
epoch 2, iter 1560, loss 2.98577, smoothed loss 3.83666, grad norm 3.65637, param norm 88.07898
epoch 2, iter 1565, loss 3.88392, smoothed loss 3.83828, grad norm 3.82420, param norm 88.13930
epoch 2, iter 1570, loss 3.42847, smoothed loss 3.82505, grad norm 3.93515, param norm 88.20850
epoch 2, iter 1575, loss 4.21137, smoothed loss 3.83813, grad norm 4.00482, param norm 88.27793
epoch 2, iter 1580, loss 3.78626, smoothed loss 3.83452, grad norm 3.60770, param norm 88.33438
Adding batches start...
Added  160  batches
epoch 2, iter 1585, loss 3.13951, smoothed loss 3.82099, grad norm 3.47464, param norm 88.38888
epoch 2, iter 1590, loss 3.59938, smoothed loss 3.81372, grad norm 3.75945, param norm 88.45182
epoch 2, iter 1595, loss 3.50007, smoothed loss 3.80907, grad norm 4.19311, param norm 88.51997
epoch 2, iter 1600, loss 3.60587, smoothed loss 3.78865, grad norm 3.89233, param norm 88.59164
epoch 2, iter 1605, loss 3.87924, smoothed loss 3.77520, grad norm 4.05299, param norm 88.66349
epoch 2, iter 1610, loss 3.20784, smoothed loss 3.77675, grad norm 4.41963, param norm 88.72847
epoch 2, iter 1615, loss 3.02416, smoothed loss 3.78095, grad norm 3.52133, param norm 88.78832
epoch 2, iter 1620, loss 4.06649, smoothed loss 3.80056, grad norm 3.86953, param norm 88.85280
epoch 2, iter 1625, loss 3.85730, smoothed loss 3.79859, grad norm 4.18078, param norm 88.91297
epoch 2, iter 1630, loss 3.41120, smoothed loss 3.80061, grad norm 3.39735, param norm 88.98087
epoch 2, iter 1635, loss 2.60139, smoothed loss 3.78023, grad norm 3.22481, param norm 89.04472
epoch 2, iter 1640, loss 4.34527, smoothed loss 3.77766, grad norm 4.84674, param norm 89.10755
epoch 2, iter 1645, loss 3.97488, smoothed loss 3.77845, grad norm 3.52150, param norm 89.16456
epoch 2, iter 1650, loss 3.32983, smoothed loss 3.77431, grad norm 3.85787, param norm 89.22195
epoch 2, iter 1655, loss 3.61015, smoothed loss 3.76908, grad norm 3.79772, param norm 89.27915
epoch 2, iter 1660, loss 3.95120, smoothed loss 3.76791, grad norm 3.73349, param norm 89.33869
epoch 2, iter 1665, loss 3.17926, smoothed loss 3.74237, grad norm 3.73386, param norm 89.40248
epoch 2, iter 1670, loss 3.95233, smoothed loss 3.74757, grad norm 4.46120, param norm 89.46905
epoch 2, iter 1675, loss 4.24917, smoothed loss 3.75173, grad norm 4.29800, param norm 89.53558
epoch 2, iter 1680, loss 3.58219, smoothed loss 3.73518, grad norm 4.34782, param norm 89.59921
epoch 2, iter 1685, loss 4.75190, smoothed loss 3.74052, grad norm 4.57201, param norm 89.66522
epoch 2, iter 1690, loss 3.14662, smoothed loss 3.73329, grad norm 3.75497, param norm 89.72904
epoch 2, iter 1695, loss 3.50989, smoothed loss 3.73281, grad norm 3.64143, param norm 89.78931
epoch 2, iter 1700, loss 3.65379, smoothed loss 3.72982, grad norm 4.26965, param norm 89.84394
epoch 2, iter 1705, loss 3.93465, smoothed loss 3.72445, grad norm 4.86358, param norm 89.89650
epoch 2, iter 1710, loss 4.86911, smoothed loss 3.71639, grad norm 4.83459, param norm 89.95588
epoch 2, iter 1715, loss 3.70179, smoothed loss 3.71826, grad norm 3.76344, param norm 90.01758
epoch 2, iter 1720, loss 3.75195, smoothed loss 3.71144, grad norm 4.36337, param norm 90.07737
epoch 2, iter 1725, loss 4.27950, smoothed loss 3.71710, grad norm 3.95458, param norm 90.13819
epoch 2, iter 1730, loss 3.30648, smoothed loss 3.71618, grad norm 3.60397, param norm 90.20370
epoch 2, iter 1735, loss 3.55638, smoothed loss 3.70878, grad norm 3.88440, param norm 90.26613
epoch 2, iter 1740, loss 3.84444, smoothed loss 3.72400, grad norm 3.95515, param norm 90.32119
Adding batches start...
Added  144  batches
epoch 2, iter 1745, loss 4.25611, smoothed loss 3.72845, grad norm 4.18188, param norm 90.38076
epoch 2, iter 1750, loss 4.14261, smoothed loss 3.73420, grad norm 4.56161, param norm 90.44521
epoch 2, iter 1755, loss 3.96686, smoothed loss 3.74378, grad norm 3.88606, param norm 90.50445
epoch 2, iter 1760, loss 4.14317, smoothed loss 3.75073, grad norm 3.79076, param norm 90.56634
epoch 2, iter 1765, loss 3.42412, smoothed loss 3.73556, grad norm 3.40316, param norm 90.63288
epoch 2, iter 1770, loss 4.39547, smoothed loss 3.73707, grad norm 5.04144, param norm 90.69095
epoch 2, iter 1775, loss 3.85531, smoothed loss 3.72027, grad norm 4.22317, param norm 90.75094
epoch 2, iter 1780, loss 3.92706, smoothed loss 3.72493, grad norm 4.14321, param norm 90.81119
epoch 2, iter 1785, loss 3.28453, smoothed loss 3.71438, grad norm 4.50364, param norm 90.86900
epoch 2, iter 1790, loss 4.22468, smoothed loss 3.72305, grad norm 4.02979, param norm 90.93388
epoch 2, iter 1795, loss 3.22182, smoothed loss 3.71191, grad norm 3.27942, param norm 90.99551
epoch 2, iter 1800, loss 3.30088, smoothed loss 3.72221, grad norm 3.22423, param norm 91.05375
epoch 2, iter 1805, loss 3.69391, smoothed loss 3.72269, grad norm 4.14197, param norm 91.10589
epoch 2, iter 1810, loss 4.06954, smoothed loss 3.72095, grad norm 4.13788, param norm 91.16548
epoch 2, iter 1815, loss 3.80803, smoothed loss 3.71897, grad norm 3.95581, param norm 91.22600
epoch 2, iter 1820, loss 3.35191, smoothed loss 3.71038, grad norm 3.91815, param norm 91.28482
epoch 2, iter 1825, loss 3.90369, smoothed loss 3.70761, grad norm 3.58265, param norm 91.34624
epoch 2, iter 1830, loss 3.07175, smoothed loss 3.69750, grad norm 3.53303, param norm 91.40990
epoch 2, iter 1835, loss 3.33067, smoothed loss 3.69230, grad norm 3.74899, param norm 91.46860
epoch 2, iter 1840, loss 3.74924, smoothed loss 3.69195, grad norm 3.63367, param norm 91.52102
epoch 2, iter 1845, loss 3.44000, smoothed loss 3.68224, grad norm 3.77948, param norm 91.57412
epoch 2, iter 1850, loss 3.10454, smoothed loss 3.67300, grad norm 3.79182, param norm 91.62794
epoch 2, iter 1855, loss 3.39169, smoothed loss 3.66614, grad norm 4.15998, param norm 91.68559
epoch 2, iter 1860, loss 4.22501, smoothed loss 3.66934, grad norm 4.41058, param norm 91.73468
epoch 2, iter 1865, loss 3.99061, smoothed loss 3.66883, grad norm 3.69224, param norm 91.77783
epoch 2, iter 1870, loss 3.70061, smoothed loss 3.65603, grad norm 4.08098, param norm 91.82869
epoch 2, iter 1875, loss 3.08024, smoothed loss 3.63809, grad norm 4.08872, param norm 91.88715
epoch 2, iter 1880, loss 3.28311, smoothed loss 3.62473, grad norm 4.26404, param norm 91.94864
epoch 2, iter 1885, loss 3.76394, smoothed loss 3.63575, grad norm 4.16456, param norm 92.01172
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 3, iter 1890, loss 2.95274, smoothed loss 3.62572, grad norm 3.53861, param norm 92.07812
epoch 3, iter 1895, loss 4.15646, smoothed loss 3.62655, grad norm 3.96508, param norm 92.14374
epoch 3, iter 1900, loss 3.40134, smoothed loss 3.61133, grad norm 4.76538, param norm 92.20069
epoch 3, iter 1905, loss 3.60842, smoothed loss 3.60850, grad norm 3.97660, param norm 92.25585
epoch 3, iter 1910, loss 3.68445, smoothed loss 3.62264, grad norm 4.06352, param norm 92.31001
epoch 3, iter 1915, loss 3.17030, smoothed loss 3.62680, grad norm 3.96980, param norm 92.36836
epoch 3, iter 1920, loss 3.03850, smoothed loss 3.61618, grad norm 3.59345, param norm 92.43243
epoch 3, iter 1925, loss 4.25317, smoothed loss 3.62122, grad norm 4.06401, param norm 92.48582
epoch 3, iter 1930, loss 3.32889, smoothed loss 3.61184, grad norm 3.66529, param norm 92.53159
epoch 3, iter 1935, loss 4.05341, smoothed loss 3.61885, grad norm 4.06354, param norm 92.59364
epoch 3, iter 1940, loss 3.41793, smoothed loss 3.62268, grad norm 3.81476, param norm 92.64697
epoch 3, iter 1945, loss 3.41262, smoothed loss 3.61623, grad norm 3.31381, param norm 92.69811
epoch 3, iter 1950, loss 2.95009, smoothed loss 3.64134, grad norm 3.94252, param norm 92.74726
epoch 3, iter 1955, loss 3.90075, smoothed loss 3.65058, grad norm 4.49716, param norm 92.80289
epoch 3, iter 1960, loss 3.34090, smoothed loss 3.64936, grad norm 3.76296, param norm 92.86681
epoch 3, iter 1965, loss 3.72672, smoothed loss 3.64890, grad norm 3.72382, param norm 92.92837
epoch 3, iter 1970, loss 3.88075, smoothed loss 3.65455, grad norm 3.89597, param norm 92.98241
epoch 3, iter 1975, loss 3.02630, smoothed loss 3.64113, grad norm 3.22574, param norm 93.03800
epoch 3, iter 1980, loss 3.67907, smoothed loss 3.64897, grad norm 3.56196, param norm 93.08923
epoch 3, iter 1985, loss 4.26623, smoothed loss 3.65207, grad norm 4.68200, param norm 93.14023
epoch 3, iter 1990, loss 3.90461, smoothed loss 3.64853, grad norm 3.84412, param norm 93.18574
epoch 3, iter 1995, loss 3.83626, smoothed loss 3.64650, grad norm 3.70865, param norm 93.23396
epoch 3, iter 2000, loss 4.60313, smoothed loss 3.66730, grad norm 3.92238, param norm 93.28421
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 3, Iter 2000, dev loss: 3.374414
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.48385 seconds [Score: 0.67863]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.24197 seconds [Score: 0.51800]
Epoch 3, Iter 2000, Train F1 score: 0.678631, Train EM score: 0.518000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.94847 seconds [Score: 0.59199]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.50031 seconds [Score: 0.43509]
Epoch 3, Iter 2000, Dev F1 score: 0.591988, Dev EM score: 0.435094
End of epoch 3
epoch 3, iter 2005, loss 4.12824, smoothed loss 3.66593, grad norm 4.08926, param norm 93.34166
epoch 3, iter 2010, loss 3.56525, smoothed loss 3.66941, grad norm 4.10090, param norm 93.40108
epoch 3, iter 2015, loss 3.60322, smoothed loss 3.67008, grad norm 3.93159, param norm 93.46049
epoch 3, iter 2020, loss 3.72899, smoothed loss 3.66255, grad norm 3.77530, param norm 93.51503
epoch 3, iter 2025, loss 2.92235, smoothed loss 3.64275, grad norm 3.79129, param norm 93.57356
epoch 3, iter 2030, loss 3.82238, smoothed loss 3.63308, grad norm 4.19604, param norm 93.63509
epoch 3, iter 2035, loss 4.02334, smoothed loss 3.64598, grad norm 3.86221, param norm 93.68757
epoch 3, iter 2040, loss 2.85177, smoothed loss 3.63793, grad norm 3.44118, param norm 93.74014
epoch 3, iter 2045, loss 3.04131, smoothed loss 3.64660, grad norm 3.44041, param norm 93.79987
Adding batches start...
Added  160  batches
epoch 3, iter 2050, loss 4.19982, smoothed loss 3.63111, grad norm 4.18256, param norm 93.85757
epoch 3, iter 2055, loss 3.04134, smoothed loss 3.61455, grad norm 3.63807, param norm 93.91225
epoch 3, iter 2060, loss 3.38869, smoothed loss 3.60635, grad norm 3.84327, param norm 93.96237
epoch 3, iter 2065, loss 3.44023, smoothed loss 3.60193, grad norm 4.05270, param norm 94.01221
epoch 3, iter 2070, loss 3.56437, smoothed loss 3.58632, grad norm 3.93136, param norm 94.07243
epoch 3, iter 2075, loss 3.18907, smoothed loss 3.58771, grad norm 3.67782, param norm 94.13001
epoch 3, iter 2080, loss 3.16429, smoothed loss 3.58424, grad norm 3.57725, param norm 94.18655
epoch 3, iter 2085, loss 3.05176, smoothed loss 3.59365, grad norm 3.52485, param norm 94.23939
epoch 3, iter 2090, loss 3.67060, smoothed loss 3.59663, grad norm 3.80185, param norm 94.29326
epoch 3, iter 2095, loss 3.07818, smoothed loss 3.59021, grad norm 3.95165, param norm 94.34892
epoch 3, iter 2100, loss 3.33077, smoothed loss 3.59187, grad norm 3.90061, param norm 94.40713
epoch 3, iter 2105, loss 3.16484, smoothed loss 3.58837, grad norm 3.60640, param norm 94.46628
epoch 3, iter 2110, loss 4.32577, smoothed loss 3.60052, grad norm 4.47412, param norm 94.52566
epoch 3, iter 2115, loss 3.65875, smoothed loss 3.59679, grad norm 3.56620, param norm 94.57687
epoch 3, iter 2120, loss 3.30278, smoothed loss 3.57246, grad norm 3.56291, param norm 94.63416
epoch 3, iter 2125, loss 3.70778, smoothed loss 3.57430, grad norm 4.27879, param norm 94.70232
epoch 3, iter 2130, loss 3.78594, smoothed loss 3.57136, grad norm 4.45981, param norm 94.76557
epoch 3, iter 2135, loss 3.37171, smoothed loss 3.57077, grad norm 3.31244, param norm 94.82808
epoch 3, iter 2140, loss 3.56378, smoothed loss 3.57397, grad norm 4.00103, param norm 94.89082
epoch 3, iter 2145, loss 3.69910, smoothed loss 3.58865, grad norm 4.20435, param norm 94.95261
epoch 3, iter 2150, loss 3.79670, smoothed loss 3.57345, grad norm 4.27638, param norm 95.01638
epoch 3, iter 2155, loss 3.61176, smoothed loss 3.56407, grad norm 3.66043, param norm 95.07537
epoch 3, iter 2160, loss 3.51535, smoothed loss 3.56964, grad norm 3.16724, param norm 95.13429
epoch 3, iter 2165, loss 3.91313, smoothed loss 3.56750, grad norm 3.83806, param norm 95.18810
epoch 3, iter 2170, loss 3.24194, smoothed loss 3.55756, grad norm 3.35275, param norm 95.23653
epoch 3, iter 2175, loss 3.31922, smoothed loss 3.54186, grad norm 3.65829, param norm 95.28775
epoch 3, iter 2180, loss 3.85068, smoothed loss 3.55685, grad norm 4.05505, param norm 95.33378
epoch 3, iter 2185, loss 3.28786, smoothed loss 3.55307, grad norm 3.76311, param norm 95.38577
epoch 3, iter 2190, loss 3.79570, smoothed loss 3.55330, grad norm 3.65800, param norm 95.44209
epoch 3, iter 2195, loss 4.24337, smoothed loss 3.56907, grad norm 4.74162, param norm 95.49197
epoch 3, iter 2200, loss 4.24649, smoothed loss 3.57882, grad norm 4.69826, param norm 95.54696
epoch 3, iter 2205, loss 4.20170, smoothed loss 3.57347, grad norm 4.16385, param norm 95.61008
Adding batches start...
Added  160  batches
epoch 3, iter 2210, loss 3.44637, smoothed loss 3.57127, grad norm 3.44281, param norm 95.67575
epoch 3, iter 2215, loss 4.15587, smoothed loss 3.57910, grad norm 4.39404, param norm 95.73064
epoch 3, iter 2220, loss 3.22877, smoothed loss 3.58573, grad norm 3.21248, param norm 95.78371
epoch 3, iter 2225, loss 3.86632, smoothed loss 3.58308, grad norm 3.98286, param norm 95.83621
epoch 3, iter 2230, loss 4.74905, smoothed loss 3.59598, grad norm 4.15240, param norm 95.89169
epoch 3, iter 2235, loss 3.00955, smoothed loss 3.58517, grad norm 3.42662, param norm 95.94437
epoch 3, iter 2240, loss 3.43057, smoothed loss 3.56797, grad norm 3.83249, param norm 95.99769
epoch 3, iter 2245, loss 4.16481, smoothed loss 3.57229, grad norm 4.74608, param norm 96.05395
epoch 3, iter 2250, loss 3.56405, smoothed loss 3.57116, grad norm 3.86873, param norm 96.10394
epoch 3, iter 2255, loss 3.82859, smoothed loss 3.56392, grad norm 4.19779, param norm 96.15354
epoch 3, iter 2260, loss 3.94544, smoothed loss 3.56803, grad norm 4.33430, param norm 96.20327
epoch 3, iter 2265, loss 4.16708, smoothed loss 3.57097, grad norm 3.93906, param norm 96.25114
epoch 3, iter 2270, loss 3.97758, smoothed loss 3.57312, grad norm 3.72208, param norm 96.29931
epoch 3, iter 2275, loss 3.18853, smoothed loss 3.57325, grad norm 3.50803, param norm 96.35854
epoch 3, iter 2280, loss 2.89969, smoothed loss 3.57493, grad norm 3.93305, param norm 96.41858
epoch 3, iter 2285, loss 3.64079, smoothed loss 3.55415, grad norm 3.70069, param norm 96.47225
epoch 3, iter 2290, loss 3.50274, smoothed loss 3.54271, grad norm 3.55353, param norm 96.52071
epoch 3, iter 2295, loss 3.66496, smoothed loss 3.55234, grad norm 3.68551, param norm 96.56441
epoch 3, iter 2300, loss 3.31636, smoothed loss 3.55191, grad norm 4.02785, param norm 96.60892
epoch 3, iter 2305, loss 2.68308, smoothed loss 3.53318, grad norm 3.37135, param norm 96.66868
epoch 3, iter 2310, loss 3.45026, smoothed loss 3.52250, grad norm 4.19284, param norm 96.73319
epoch 3, iter 2315, loss 2.76064, smoothed loss 3.51587, grad norm 3.67258, param norm 96.78425
epoch 3, iter 2320, loss 3.20421, smoothed loss 3.50487, grad norm 3.90009, param norm 96.83241
epoch 3, iter 2325, loss 3.07621, smoothed loss 3.51318, grad norm 3.78079, param norm 96.88119
epoch 3, iter 2330, loss 2.91610, smoothed loss 3.50254, grad norm 4.09641, param norm 96.93698
epoch 3, iter 2335, loss 2.84247, smoothed loss 3.49680, grad norm 3.27990, param norm 96.99003
epoch 3, iter 2340, loss 2.85586, smoothed loss 3.49351, grad norm 3.38827, param norm 97.04577
epoch 3, iter 2345, loss 2.92085, smoothed loss 3.50989, grad norm 3.62822, param norm 97.10048
epoch 3, iter 2350, loss 3.66926, smoothed loss 3.50705, grad norm 3.76788, param norm 97.15681
epoch 3, iter 2355, loss 3.86292, smoothed loss 3.50262, grad norm 4.03454, param norm 97.21775
epoch 3, iter 2360, loss 3.60729, smoothed loss 3.50262, grad norm 3.91045, param norm 97.28202
epoch 3, iter 2365, loss 3.79102, smoothed loss 3.50126, grad norm 3.71862, param norm 97.34375
Adding batches start...
Added  160  batches
epoch 3, iter 2370, loss 3.30025, smoothed loss 3.49261, grad norm 3.77032, param norm 97.40195
epoch 3, iter 2375, loss 2.64044, smoothed loss 3.46527, grad norm 3.18026, param norm 97.45957
epoch 3, iter 2380, loss 3.18274, smoothed loss 3.45592, grad norm 4.09646, param norm 97.50971
epoch 3, iter 2385, loss 2.98963, smoothed loss 3.43735, grad norm 3.95198, param norm 97.55629
epoch 3, iter 2390, loss 3.65666, smoothed loss 3.44626, grad norm 4.53222, param norm 97.60826
epoch 3, iter 2395, loss 3.93042, smoothed loss 3.44745, grad norm 3.78942, param norm 97.66019
epoch 3, iter 2400, loss 3.70788, smoothed loss 3.45434, grad norm 4.33088, param norm 97.70494
epoch 3, iter 2405, loss 2.94633, smoothed loss 3.44599, grad norm 3.34813, param norm 97.75309
epoch 3, iter 2410, loss 3.83196, smoothed loss 3.45634, grad norm 4.02581, param norm 97.80859
epoch 3, iter 2415, loss 3.22374, smoothed loss 3.44338, grad norm 3.53437, param norm 97.86619
epoch 3, iter 2420, loss 3.78119, smoothed loss 3.44112, grad norm 4.00393, param norm 97.92387
epoch 3, iter 2425, loss 3.03646, smoothed loss 3.44151, grad norm 3.88268, param norm 97.97816
epoch 3, iter 2430, loss 3.56669, smoothed loss 3.44048, grad norm 3.85995, param norm 98.02468
epoch 3, iter 2435, loss 3.54682, smoothed loss 3.43088, grad norm 4.20688, param norm 98.07442
epoch 3, iter 2440, loss 2.91584, smoothed loss 3.42818, grad norm 3.56935, param norm 98.13230
epoch 3, iter 2445, loss 4.07310, smoothed loss 3.43626, grad norm 4.24560, param norm 98.19227
epoch 3, iter 2450, loss 3.90231, smoothed loss 3.44442, grad norm 4.14088, param norm 98.24886
epoch 3, iter 2455, loss 3.42050, smoothed loss 3.45772, grad norm 3.44537, param norm 98.30037
epoch 3, iter 2460, loss 2.95262, smoothed loss 3.45782, grad norm 3.65035, param norm 98.35666
epoch 3, iter 2465, loss 2.79633, smoothed loss 3.44633, grad norm 3.48176, param norm 98.41740
epoch 3, iter 2470, loss 4.14218, smoothed loss 3.44150, grad norm 5.05131, param norm 98.47295
epoch 3, iter 2475, loss 3.28127, smoothed loss 3.44385, grad norm 4.22473, param norm 98.52208
epoch 3, iter 2480, loss 3.27074, smoothed loss 3.43714, grad norm 3.32062, param norm 98.57357
epoch 3, iter 2485, loss 3.47669, smoothed loss 3.44088, grad norm 3.66145, param norm 98.62519
epoch 3, iter 2490, loss 3.69769, smoothed loss 3.45057, grad norm 3.72985, param norm 98.67792
epoch 3, iter 2495, loss 2.80905, smoothed loss 3.43859, grad norm 3.28609, param norm 98.73351
epoch 3, iter 2500, loss 3.52083, smoothed loss 3.43649, grad norm 3.74147, param norm 98.78743
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 3, Iter 2500, dev loss: 3.292387
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.36164 seconds [Score: 0.67898]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.82209 seconds [Score: 0.49700]
Epoch 3, Iter 2500, Train F1 score: 0.678981, Train EM score: 0.497000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.64182 seconds [Score: 0.60062]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.54649 seconds [Score: 0.45209]
Epoch 3, Iter 2500, Dev F1 score: 0.600617, Dev EM score: 0.452093
End of epoch 3
epoch 3, iter 2505, loss 3.22280, smoothed loss 3.43312, grad norm 3.74245, param norm 98.84344
epoch 3, iter 2510, loss 3.87513, smoothed loss 3.44109, grad norm 4.28211, param norm 98.89540
epoch 3, iter 2515, loss 3.09039, smoothed loss 3.43044, grad norm 4.16414, param norm 98.94191
epoch 3, iter 2520, loss 3.36669, smoothed loss 3.41644, grad norm 4.22779, param norm 98.99242
epoch 3, iter 2525, loss 3.64606, smoothed loss 3.43022, grad norm 4.32141, param norm 99.04192
Adding batches start...
Added  160  batches
epoch 3, iter 2530, loss 3.93437, smoothed loss 3.44442, grad norm 4.02398, param norm 99.09589
epoch 3, iter 2535, loss 3.29912, smoothed loss 3.44379, grad norm 3.57499, param norm 99.14664
epoch 3, iter 2540, loss 3.66973, smoothed loss 3.45472, grad norm 4.11846, param norm 99.19595
epoch 3, iter 2545, loss 2.99842, smoothed loss 3.45505, grad norm 3.54066, param norm 99.25013
epoch 3, iter 2550, loss 4.04221, smoothed loss 3.45674, grad norm 4.65966, param norm 99.31031
epoch 3, iter 2555, loss 3.41899, smoothed loss 3.46713, grad norm 3.85688, param norm 99.36671
epoch 3, iter 2560, loss 3.48675, smoothed loss 3.46487, grad norm 3.64434, param norm 99.42198
epoch 3, iter 2565, loss 2.77355, smoothed loss 3.45773, grad norm 3.33834, param norm 99.47378
epoch 3, iter 2570, loss 3.28428, smoothed loss 3.43824, grad norm 3.83777, param norm 99.52917
epoch 3, iter 2575, loss 3.04856, smoothed loss 3.42394, grad norm 3.61670, param norm 99.58733
epoch 3, iter 2580, loss 3.10757, smoothed loss 3.40905, grad norm 3.90742, param norm 99.64275
epoch 3, iter 2585, loss 3.24269, smoothed loss 3.40426, grad norm 4.20423, param norm 99.69399
epoch 3, iter 2590, loss 3.03612, smoothed loss 3.39896, grad norm 3.72982, param norm 99.74004
epoch 3, iter 2595, loss 3.27098, smoothed loss 3.40569, grad norm 3.77884, param norm 99.78882
epoch 3, iter 2600, loss 4.31196, smoothed loss 3.41482, grad norm 4.33784, param norm 99.84838
epoch 3, iter 2605, loss 3.78328, smoothed loss 3.41044, grad norm 3.94991, param norm 99.90113
epoch 3, iter 2610, loss 2.35960, smoothed loss 3.38345, grad norm 3.02098, param norm 99.94868
epoch 3, iter 2615, loss 3.08020, smoothed loss 3.39540, grad norm 3.78881, param norm 99.98907
epoch 3, iter 2620, loss 3.60746, smoothed loss 3.39926, grad norm 4.25676, param norm 100.03091
epoch 3, iter 2625, loss 3.64628, smoothed loss 3.40387, grad norm 3.82947, param norm 100.07877
epoch 3, iter 2630, loss 4.20126, smoothed loss 3.41789, grad norm 4.15678, param norm 100.13810
epoch 3, iter 2635, loss 3.72966, smoothed loss 3.41650, grad norm 3.82674, param norm 100.20348
epoch 3, iter 2640, loss 3.17421, smoothed loss 3.42288, grad norm 3.73042, param norm 100.26355
epoch 3, iter 2645, loss 3.38204, smoothed loss 3.41900, grad norm 3.51378, param norm 100.31302
epoch 3, iter 2650, loss 3.51963, smoothed loss 3.40364, grad norm 3.66933, param norm 100.35980
epoch 3, iter 2655, loss 2.84410, smoothed loss 3.39015, grad norm 3.90048, param norm 100.41081
epoch 3, iter 2660, loss 2.57196, smoothed loss 3.36844, grad norm 3.58391, param norm 100.46450
epoch 3, iter 2665, loss 3.45571, smoothed loss 3.37140, grad norm 4.78601, param norm 100.51167
epoch 3, iter 2670, loss 2.72745, smoothed loss 3.36862, grad norm 4.20559, param norm 100.55399
epoch 3, iter 2675, loss 3.59005, smoothed loss 3.36146, grad norm 4.01630, param norm 100.60249
epoch 3, iter 2680, loss 2.59267, smoothed loss 3.36508, grad norm 3.40902, param norm 100.64606
epoch 3, iter 2685, loss 2.80332, smoothed loss 3.34879, grad norm 3.70626, param norm 100.69693
Adding batches start...
Added  144  batches
epoch 3, iter 2690, loss 4.03771, smoothed loss 3.34773, grad norm 4.99779, param norm 100.75220
epoch 3, iter 2695, loss 3.55868, smoothed loss 3.35224, grad norm 4.18416, param norm 100.80970
epoch 3, iter 2700, loss 3.32522, smoothed loss 3.35486, grad norm 3.57298, param norm 100.86704
epoch 3, iter 2705, loss 2.93836, smoothed loss 3.35715, grad norm 3.38921, param norm 100.92114
epoch 3, iter 2710, loss 3.80263, smoothed loss 3.36605, grad norm 3.95249, param norm 100.97012
epoch 3, iter 2715, loss 2.55766, smoothed loss 3.37066, grad norm 3.49844, param norm 101.02060
epoch 3, iter 2720, loss 3.85679, smoothed loss 3.37240, grad norm 4.56039, param norm 101.07533
epoch 3, iter 2725, loss 3.17762, smoothed loss 3.37759, grad norm 4.02257, param norm 101.12271
epoch 3, iter 2730, loss 3.49402, smoothed loss 3.36971, grad norm 4.11510, param norm 101.16876
epoch 3, iter 2735, loss 3.31604, smoothed loss 3.36377, grad norm 3.98361, param norm 101.22217
epoch 3, iter 2740, loss 3.26756, smoothed loss 3.35227, grad norm 3.84901, param norm 101.27457
epoch 3, iter 2745, loss 3.85862, smoothed loss 3.35997, grad norm 3.98806, param norm 101.32693
epoch 3, iter 2750, loss 3.05007, smoothed loss 3.35098, grad norm 3.72302, param norm 101.38040
epoch 3, iter 2755, loss 3.57202, smoothed loss 3.36853, grad norm 4.05024, param norm 101.43087
epoch 3, iter 2760, loss 3.84042, smoothed loss 3.37221, grad norm 3.82421, param norm 101.48209
epoch 3, iter 2765, loss 3.08072, smoothed loss 3.35481, grad norm 3.43475, param norm 101.53960
epoch 3, iter 2770, loss 2.86946, smoothed loss 3.36196, grad norm 3.65058, param norm 101.59188
epoch 3, iter 2775, loss 2.22867, smoothed loss 3.35646, grad norm 3.51878, param norm 101.64815
epoch 3, iter 2780, loss 3.16611, smoothed loss 3.34616, grad norm 3.72525, param norm 101.70622
epoch 3, iter 2785, loss 2.21400, smoothed loss 3.31665, grad norm 3.10447, param norm 101.76414
epoch 3, iter 2790, loss 3.64655, smoothed loss 3.31094, grad norm 4.02628, param norm 101.81480
epoch 3, iter 2795, loss 3.60259, smoothed loss 3.31258, grad norm 3.93344, param norm 101.85743
epoch 3, iter 2800, loss 3.36179, smoothed loss 3.31555, grad norm 4.01177, param norm 101.90482
epoch 3, iter 2805, loss 3.74243, smoothed loss 3.31901, grad norm 4.12727, param norm 101.95959
epoch 3, iter 2810, loss 3.22138, smoothed loss 3.31488, grad norm 4.09883, param norm 102.00878
epoch 3, iter 2815, loss 3.07856, smoothed loss 3.30782, grad norm 3.54144, param norm 102.05410
epoch 3, iter 2820, loss 3.24623, smoothed loss 3.31199, grad norm 3.43857, param norm 102.09552
epoch 3, iter 2825, loss 3.22727, smoothed loss 3.29524, grad norm 3.65784, param norm 102.14492
epoch 3, iter 2830, loss 3.70600, smoothed loss 3.30140, grad norm 4.05870, param norm 102.20139
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 4, iter 2835, loss 3.02061, smoothed loss 3.31895, grad norm 3.42922, param norm 102.26128
epoch 4, iter 2840, loss 3.15421, smoothed loss 3.32914, grad norm 3.88585, param norm 102.31680
epoch 4, iter 2845, loss 3.91909, smoothed loss 3.33371, grad norm 4.17383, param norm 102.37315
epoch 4, iter 2850, loss 3.90418, smoothed loss 3.33832, grad norm 3.75867, param norm 102.42254
epoch 4, iter 2855, loss 3.30080, smoothed loss 3.33748, grad norm 3.67436, param norm 102.47201
epoch 4, iter 2860, loss 3.40211, smoothed loss 3.35472, grad norm 3.79462, param norm 102.52187
epoch 4, iter 2865, loss 3.27853, smoothed loss 3.35650, grad norm 4.20191, param norm 102.56606
epoch 4, iter 2870, loss 3.60471, smoothed loss 3.34665, grad norm 4.33466, param norm 102.61736
epoch 4, iter 2875, loss 3.41087, smoothed loss 3.33742, grad norm 3.91676, param norm 102.66476
epoch 4, iter 2880, loss 3.08808, smoothed loss 3.33113, grad norm 3.51110, param norm 102.70957
epoch 4, iter 2885, loss 2.54063, smoothed loss 3.32243, grad norm 3.10405, param norm 102.76057
epoch 4, iter 2890, loss 3.42858, smoothed loss 3.32307, grad norm 3.85118, param norm 102.81183
epoch 4, iter 2895, loss 3.06537, smoothed loss 3.32076, grad norm 3.65206, param norm 102.86060
epoch 4, iter 2900, loss 2.63814, smoothed loss 3.31248, grad norm 3.38404, param norm 102.91209
epoch 4, iter 2905, loss 2.19612, smoothed loss 3.30689, grad norm 3.37685, param norm 102.96352
epoch 4, iter 2910, loss 3.57393, smoothed loss 3.31050, grad norm 4.07841, param norm 103.01140
epoch 4, iter 2915, loss 2.73597, smoothed loss 3.29320, grad norm 3.57842, param norm 103.06128
epoch 4, iter 2920, loss 3.49429, smoothed loss 3.29787, grad norm 4.17688, param norm 103.11518
epoch 4, iter 2925, loss 2.89859, smoothed loss 3.28487, grad norm 3.58543, param norm 103.17151
epoch 4, iter 2930, loss 3.14222, smoothed loss 3.28779, grad norm 3.69620, param norm 103.22213
epoch 4, iter 2935, loss 3.15416, smoothed loss 3.31373, grad norm 3.47900, param norm 103.27108
epoch 4, iter 2940, loss 3.21020, smoothed loss 3.30189, grad norm 3.63369, param norm 103.32079
epoch 4, iter 2945, loss 3.48876, smoothed loss 3.31635, grad norm 3.87460, param norm 103.37327
epoch 4, iter 2950, loss 3.18413, smoothed loss 3.30483, grad norm 3.58870, param norm 103.42490
epoch 4, iter 2955, loss 2.46701, smoothed loss 3.30135, grad norm 3.52870, param norm 103.47664
epoch 4, iter 2960, loss 2.32004, smoothed loss 3.27981, grad norm 3.21098, param norm 103.52946
epoch 4, iter 2965, loss 3.32405, smoothed loss 3.27001, grad norm 4.17730, param norm 103.57920
epoch 4, iter 2970, loss 3.72830, smoothed loss 3.28049, grad norm 3.92871, param norm 103.62142
epoch 4, iter 2975, loss 3.22263, smoothed loss 3.28159, grad norm 3.77391, param norm 103.66815
epoch 4, iter 2980, loss 3.57426, smoothed loss 3.29355, grad norm 3.46492, param norm 103.71722
epoch 4, iter 2985, loss 3.69396, smoothed loss 3.29882, grad norm 4.09523, param norm 103.76564
epoch 4, iter 2990, loss 3.25216, smoothed loss 3.29896, grad norm 3.55988, param norm 103.81176
Adding batches start...
Added  160  batches
epoch 4, iter 2995, loss 3.19566, smoothed loss 3.30502, grad norm 3.66299, param norm 103.85868
epoch 4, iter 3000, loss 3.82894, smoothed loss 3.30179, grad norm 3.98284, param norm 103.90352
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 4, Iter 3000, dev loss: 3.222714
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.43332 seconds [Score: 0.72739]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.40373 seconds [Score: 0.56400]
Epoch 4, Iter 3000, Train F1 score: 0.727387, Train EM score: 0.564000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.84422 seconds [Score: 0.60792]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.91695 seconds [Score: 0.45350]
Epoch 4, Iter 3000, Dev F1 score: 0.607917, Dev EM score: 0.453498
End of epoch 4
epoch 4, iter 3005, loss 3.35617, smoothed loss 3.30925, grad norm 3.94378, param norm 103.95347
epoch 4, iter 3010, loss 4.28590, smoothed loss 3.32745, grad norm 3.92744, param norm 104.00772
epoch 4, iter 3015, loss 3.49008, smoothed loss 3.31965, grad norm 4.30048, param norm 104.06279
epoch 4, iter 3020, loss 2.52501, smoothed loss 3.31367, grad norm 3.31271, param norm 104.11537
epoch 4, iter 3025, loss 3.53147, smoothed loss 3.31143, grad norm 3.86722, param norm 104.16541
epoch 4, iter 3030, loss 3.22017, smoothed loss 3.32160, grad norm 3.43732, param norm 104.21086
epoch 4, iter 3035, loss 3.57394, smoothed loss 3.31834, grad norm 4.06025, param norm 104.25519
epoch 4, iter 3040, loss 3.64758, smoothed loss 3.33098, grad norm 3.56270, param norm 104.30258
epoch 4, iter 3045, loss 3.79700, smoothed loss 3.33698, grad norm 4.30566, param norm 104.35128
epoch 4, iter 3050, loss 2.70944, smoothed loss 3.32286, grad norm 3.53334, param norm 104.40253
epoch 4, iter 3055, loss 4.15414, smoothed loss 3.33741, grad norm 4.44026, param norm 104.45496
epoch 4, iter 3060, loss 3.59175, smoothed loss 3.33226, grad norm 3.81122, param norm 104.50901
epoch 4, iter 3065, loss 3.06603, smoothed loss 3.32234, grad norm 3.79761, param norm 104.55866
epoch 4, iter 3070, loss 3.43234, smoothed loss 3.32419, grad norm 3.72108, param norm 104.60875
epoch 4, iter 3075, loss 3.53613, smoothed loss 3.33460, grad norm 3.80296, param norm 104.66227
epoch 4, iter 3080, loss 3.21548, smoothed loss 3.33083, grad norm 3.68967, param norm 104.71734
epoch 4, iter 3085, loss 3.24103, smoothed loss 3.31751, grad norm 3.98632, param norm 104.77282
epoch 4, iter 3090, loss 3.29001, smoothed loss 3.31679, grad norm 4.08734, param norm 104.82267
epoch 4, iter 3095, loss 2.88133, smoothed loss 3.32273, grad norm 4.07488, param norm 104.87273
epoch 4, iter 3100, loss 3.40836, smoothed loss 3.31543, grad norm 4.22354, param norm 104.92754
epoch 4, iter 3105, loss 3.70742, smoothed loss 3.29970, grad norm 4.77723, param norm 104.97543
epoch 4, iter 3110, loss 2.38337, smoothed loss 3.29164, grad norm 3.68903, param norm 105.01983
epoch 4, iter 3115, loss 2.99879, smoothed loss 3.27826, grad norm 3.33014, param norm 105.06528
epoch 4, iter 3120, loss 2.88965, smoothed loss 3.26908, grad norm 3.84573, param norm 105.10623
epoch 4, iter 3125, loss 2.76723, smoothed loss 3.26631, grad norm 3.55127, param norm 105.14890
epoch 4, iter 3130, loss 2.53933, smoothed loss 3.25742, grad norm 3.14619, param norm 105.19093
epoch 4, iter 3135, loss 3.32962, smoothed loss 3.25794, grad norm 4.15095, param norm 105.23866
epoch 4, iter 3140, loss 3.00311, smoothed loss 3.24965, grad norm 4.04476, param norm 105.28804
epoch 4, iter 3145, loss 3.23584, smoothed loss 3.24521, grad norm 3.98462, param norm 105.33566
epoch 4, iter 3150, loss 3.74954, smoothed loss 3.24511, grad norm 4.06845, param norm 105.38385
Adding batches start...
Added  160  batches
epoch 4, iter 3155, loss 3.15337, smoothed loss 3.24174, grad norm 3.76932, param norm 105.43504
epoch 4, iter 3160, loss 3.81657, smoothed loss 3.25106, grad norm 3.67481, param norm 105.48608
epoch 4, iter 3165, loss 3.19159, smoothed loss 3.24131, grad norm 3.55854, param norm 105.53780
epoch 4, iter 3170, loss 3.29768, smoothed loss 3.24130, grad norm 4.08883, param norm 105.58479
epoch 4, iter 3175, loss 3.11171, smoothed loss 3.25279, grad norm 3.81726, param norm 105.62543
epoch 4, iter 3180, loss 3.90020, smoothed loss 3.26454, grad norm 4.33822, param norm 105.66664
epoch 4, iter 3185, loss 3.24973, smoothed loss 3.26242, grad norm 4.08226, param norm 105.70894
epoch 4, iter 3190, loss 3.93453, smoothed loss 3.25880, grad norm 3.97367, param norm 105.75479
epoch 4, iter 3195, loss 2.88652, smoothed loss 3.26458, grad norm 3.16469, param norm 105.80453
epoch 4, iter 3200, loss 3.02709, smoothed loss 3.24899, grad norm 3.72014, param norm 105.85233
epoch 4, iter 3205, loss 2.95374, smoothed loss 3.24407, grad norm 3.40401, param norm 105.89213
epoch 4, iter 3210, loss 3.25987, smoothed loss 3.24781, grad norm 3.91893, param norm 105.93391
epoch 4, iter 3215, loss 2.90051, smoothed loss 3.24363, grad norm 3.37102, param norm 105.98215
epoch 4, iter 3220, loss 3.58307, smoothed loss 3.25598, grad norm 3.87361, param norm 106.03504
epoch 4, iter 3225, loss 3.37741, smoothed loss 3.25759, grad norm 4.19416, param norm 106.08636
epoch 4, iter 3230, loss 3.33000, smoothed loss 3.26346, grad norm 4.11167, param norm 106.13335
epoch 4, iter 3235, loss 3.24399, smoothed loss 3.26329, grad norm 3.91343, param norm 106.17787
epoch 4, iter 3240, loss 3.31553, smoothed loss 3.25935, grad norm 3.80644, param norm 106.22479
epoch 4, iter 3245, loss 4.10821, smoothed loss 3.26970, grad norm 4.29026, param norm 106.27487
epoch 4, iter 3250, loss 2.83659, smoothed loss 3.24927, grad norm 3.50296, param norm 106.32812
epoch 4, iter 3255, loss 3.45682, smoothed loss 3.24020, grad norm 3.66704, param norm 106.37987
epoch 4, iter 3260, loss 3.38661, smoothed loss 3.24878, grad norm 3.55172, param norm 106.42591
epoch 4, iter 3265, loss 3.70677, smoothed loss 3.24832, grad norm 4.25587, param norm 106.47084
epoch 4, iter 3270, loss 2.99892, smoothed loss 3.24565, grad norm 3.39595, param norm 106.51335
epoch 4, iter 3275, loss 2.93821, smoothed loss 3.23365, grad norm 3.60654, param norm 106.55859
epoch 4, iter 3280, loss 3.15083, smoothed loss 3.24621, grad norm 3.46539, param norm 106.61065
epoch 4, iter 3285, loss 2.93351, smoothed loss 3.22684, grad norm 3.44018, param norm 106.65919
epoch 4, iter 3290, loss 3.48849, smoothed loss 3.22830, grad norm 3.74400, param norm 106.70766
epoch 4, iter 3295, loss 3.42260, smoothed loss 3.22020, grad norm 4.18378, param norm 106.75804
epoch 4, iter 3300, loss 3.29443, smoothed loss 3.22052, grad norm 3.88671, param norm 106.80659
epoch 4, iter 3305, loss 2.93476, smoothed loss 3.21214, grad norm 3.69498, param norm 106.85085
epoch 4, iter 3310, loss 3.60777, smoothed loss 3.21133, grad norm 3.80079, param norm 106.89165
Adding batches start...
Added  160  batches
epoch 4, iter 3315, loss 3.39561, smoothed loss 3.22516, grad norm 4.07724, param norm 106.93739
epoch 4, iter 3320, loss 2.44430, smoothed loss 3.21382, grad norm 3.39928, param norm 106.98667
epoch 4, iter 3325, loss 3.37535, smoothed loss 3.20531, grad norm 4.16506, param norm 107.03757
epoch 4, iter 3330, loss 3.59465, smoothed loss 3.20212, grad norm 4.17031, param norm 107.08697
epoch 4, iter 3335, loss 2.66448, smoothed loss 3.18846, grad norm 3.57816, param norm 107.13530
epoch 4, iter 3340, loss 3.69952, smoothed loss 3.19169, grad norm 4.70528, param norm 107.18343
epoch 4, iter 3345, loss 2.80723, smoothed loss 3.18294, grad norm 3.29807, param norm 107.23381
epoch 4, iter 3350, loss 2.93434, smoothed loss 3.17873, grad norm 3.51383, param norm 107.28350
epoch 4, iter 3355, loss 2.52383, smoothed loss 3.18210, grad norm 3.58742, param norm 107.33166
epoch 4, iter 3360, loss 3.60685, smoothed loss 3.18770, grad norm 4.20915, param norm 107.38152
epoch 4, iter 3365, loss 3.85801, smoothed loss 3.19979, grad norm 4.71893, param norm 107.43135
epoch 4, iter 3370, loss 2.59624, smoothed loss 3.18812, grad norm 3.71473, param norm 107.47829
epoch 4, iter 3375, loss 2.72053, smoothed loss 3.18899, grad norm 3.47497, param norm 107.52980
epoch 4, iter 3380, loss 2.51516, smoothed loss 3.17630, grad norm 3.62720, param norm 107.57785
epoch 4, iter 3385, loss 2.79429, smoothed loss 3.18685, grad norm 3.87634, param norm 107.62629
epoch 4, iter 3390, loss 3.74135, smoothed loss 3.19462, grad norm 4.71086, param norm 107.67622
epoch 4, iter 3395, loss 2.56681, smoothed loss 3.18205, grad norm 3.67267, param norm 107.72397
epoch 4, iter 3400, loss 4.18887, smoothed loss 3.19542, grad norm 4.19632, param norm 107.76958
epoch 4, iter 3405, loss 3.09764, smoothed loss 3.18918, grad norm 3.82596, param norm 107.81245
epoch 4, iter 3410, loss 2.53210, smoothed loss 3.18189, grad norm 3.31653, param norm 107.85780
epoch 4, iter 3415, loss 3.29398, smoothed loss 3.19097, grad norm 3.88413, param norm 107.90211
epoch 4, iter 3420, loss 2.94561, smoothed loss 3.19423, grad norm 3.96876, param norm 107.94783
epoch 4, iter 3425, loss 3.02341, smoothed loss 3.20177, grad norm 3.82061, param norm 107.99151
epoch 4, iter 3430, loss 3.05022, smoothed loss 3.19624, grad norm 3.62756, param norm 108.03770
epoch 4, iter 3435, loss 3.11226, smoothed loss 3.20492, grad norm 3.66058, param norm 108.08131
epoch 4, iter 3440, loss 3.08364, smoothed loss 3.20641, grad norm 3.92573, param norm 108.12610
epoch 4, iter 3445, loss 3.01241, smoothed loss 3.20925, grad norm 4.19127, param norm 108.16911
epoch 4, iter 3450, loss 3.78312, smoothed loss 3.21279, grad norm 4.06587, param norm 108.21809
epoch 4, iter 3455, loss 2.95304, smoothed loss 3.21407, grad norm 3.78712, param norm 108.26692
epoch 4, iter 3460, loss 3.34033, smoothed loss 3.20147, grad norm 3.59367, param norm 108.31961
epoch 4, iter 3465, loss 3.36867, smoothed loss 3.20113, grad norm 4.19907, param norm 108.36966
epoch 4, iter 3470, loss 3.39538, smoothed loss 3.20734, grad norm 3.88222, param norm 108.41959
Adding batches start...
Added  160  batches
epoch 4, iter 3475, loss 2.68096, smoothed loss 3.19805, grad norm 3.56538, param norm 108.46656
epoch 4, iter 3480, loss 2.49559, smoothed loss 3.19021, grad norm 3.59669, param norm 108.51257
epoch 4, iter 3485, loss 2.09849, smoothed loss 3.17815, grad norm 3.09953, param norm 108.55701
epoch 4, iter 3490, loss 2.48135, smoothed loss 3.17391, grad norm 3.34584, param norm 108.60213
epoch 4, iter 3495, loss 3.57808, smoothed loss 3.18574, grad norm 4.24368, param norm 108.64395
epoch 4, iter 3500, loss 3.02496, smoothed loss 3.17283, grad norm 3.60558, param norm 108.68240
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 4, Iter 3500, dev loss: 3.176833
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.71993 seconds [Score: 0.68641]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.33007 seconds [Score: 0.53500]
Epoch 4, Iter 3500, Train F1 score: 0.686413, Train EM score: 0.535000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.83192 seconds [Score: 0.61914]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.52889 seconds [Score: 0.46038]
Epoch 4, Iter 3500, Dev F1 score: 0.619143, Dev EM score: 0.460382
End of epoch 4
epoch 4, iter 3505, loss 2.97426, smoothed loss 3.16964, grad norm 3.66365, param norm 108.72642
epoch 4, iter 3510, loss 3.09086, smoothed loss 3.16178, grad norm 3.89388, param norm 108.77513
epoch 4, iter 3515, loss 2.80921, smoothed loss 3.15416, grad norm 3.25298, param norm 108.82233
epoch 4, iter 3520, loss 3.60680, smoothed loss 3.15528, grad norm 4.13142, param norm 108.86861
epoch 4, iter 3525, loss 3.03706, smoothed loss 3.15212, grad norm 3.73158, param norm 108.90779
epoch 4, iter 3530, loss 3.01538, smoothed loss 3.15178, grad norm 4.15446, param norm 108.94755
epoch 4, iter 3535, loss 3.07713, smoothed loss 3.15778, grad norm 3.86859, param norm 108.99321
epoch 4, iter 3540, loss 3.05221, smoothed loss 3.15978, grad norm 3.69320, param norm 109.04615
epoch 4, iter 3545, loss 2.43378, smoothed loss 3.15562, grad norm 3.54996, param norm 109.09869
epoch 4, iter 3550, loss 2.76266, smoothed loss 3.15494, grad norm 3.58947, param norm 109.14909
epoch 4, iter 3555, loss 4.07896, smoothed loss 3.16981, grad norm 4.33380, param norm 109.19592
epoch 4, iter 3560, loss 2.26387, smoothed loss 3.16393, grad norm 3.22288, param norm 109.23843
epoch 4, iter 3565, loss 3.21863, smoothed loss 3.16323, grad norm 3.73572, param norm 109.28823
epoch 4, iter 3570, loss 3.09442, smoothed loss 3.14047, grad norm 3.51169, param norm 109.33497
epoch 4, iter 3575, loss 3.52521, smoothed loss 3.13820, grad norm 4.46405, param norm 109.38051
epoch 4, iter 3580, loss 3.00941, smoothed loss 3.16314, grad norm 3.52720, param norm 109.42792
epoch 4, iter 3585, loss 3.31122, smoothed loss 3.15764, grad norm 3.82789, param norm 109.47175
epoch 4, iter 3590, loss 3.20787, smoothed loss 3.15482, grad norm 4.08279, param norm 109.51781
epoch 4, iter 3595, loss 3.07634, smoothed loss 3.15111, grad norm 3.71255, param norm 109.56829
epoch 4, iter 3600, loss 2.90945, smoothed loss 3.14093, grad norm 3.35048, param norm 109.61566
epoch 4, iter 3605, loss 3.30463, smoothed loss 3.13799, grad norm 4.31678, param norm 109.65939
epoch 4, iter 3610, loss 3.35387, smoothed loss 3.12652, grad norm 4.38476, param norm 109.70450
epoch 4, iter 3615, loss 3.53350, smoothed loss 3.13797, grad norm 3.60549, param norm 109.74647
epoch 4, iter 3620, loss 3.13083, smoothed loss 3.14336, grad norm 4.51928, param norm 109.78805
epoch 4, iter 3625, loss 2.70219, smoothed loss 3.13775, grad norm 3.36075, param norm 109.83798
epoch 4, iter 3630, loss 3.19718, smoothed loss 3.14191, grad norm 3.42593, param norm 109.88605
Adding batches start...
Added  144  batches
epoch 4, iter 3635, loss 3.25844, smoothed loss 3.13941, grad norm 4.14361, param norm 109.92976
epoch 4, iter 3640, loss 3.08968, smoothed loss 3.14209, grad norm 3.59768, param norm 109.97633
epoch 4, iter 3645, loss 3.13620, smoothed loss 3.15508, grad norm 3.89706, param norm 110.02262
epoch 4, iter 3650, loss 2.95832, smoothed loss 3.14125, grad norm 3.35654, param norm 110.07157
epoch 4, iter 3655, loss 3.39067, smoothed loss 3.15127, grad norm 4.29283, param norm 110.11984
epoch 4, iter 3660, loss 3.30887, smoothed loss 3.14267, grad norm 3.85321, param norm 110.16638
epoch 4, iter 3665, loss 3.33055, smoothed loss 3.13640, grad norm 3.79031, param norm 110.21140
epoch 4, iter 3670, loss 4.21781, smoothed loss 3.15955, grad norm 4.16553, param norm 110.25026
epoch 4, iter 3675, loss 2.86905, smoothed loss 3.15842, grad norm 3.19329, param norm 110.29190
epoch 4, iter 3680, loss 2.56321, smoothed loss 3.15164, grad norm 3.48206, param norm 110.33566
epoch 4, iter 3685, loss 2.83508, smoothed loss 3.14168, grad norm 3.92771, param norm 110.38634
epoch 4, iter 3690, loss 3.14981, smoothed loss 3.15292, grad norm 3.59305, param norm 110.43502
epoch 4, iter 3695, loss 3.19053, smoothed loss 3.14645, grad norm 3.85324, param norm 110.48894
epoch 4, iter 3700, loss 3.35995, smoothed loss 3.15205, grad norm 4.73373, param norm 110.54184
epoch 4, iter 3705, loss 2.90903, smoothed loss 3.13852, grad norm 3.47572, param norm 110.58749
epoch 4, iter 3710, loss 2.78780, smoothed loss 3.12810, grad norm 3.64659, param norm 110.63007
epoch 4, iter 3715, loss 2.95655, smoothed loss 3.11392, grad norm 3.92936, param norm 110.67072
epoch 4, iter 3720, loss 3.23188, smoothed loss 3.11949, grad norm 3.94780, param norm 110.71463
epoch 4, iter 3725, loss 2.03965, smoothed loss 3.10628, grad norm 3.86748, param norm 110.76440
epoch 4, iter 3730, loss 3.32371, smoothed loss 3.10869, grad norm 4.58629, param norm 110.81470
epoch 4, iter 3735, loss 3.10844, smoothed loss 3.11155, grad norm 3.78187, param norm 110.86008
epoch 4, iter 3740, loss 3.53207, smoothed loss 3.12200, grad norm 4.34629, param norm 110.90482
epoch 4, iter 3745, loss 2.83085, smoothed loss 3.11747, grad norm 3.51303, param norm 110.95083
epoch 4, iter 3750, loss 2.54457, smoothed loss 3.11392, grad norm 3.77451, param norm 111.00134
epoch 4, iter 3755, loss 2.68119, smoothed loss 3.11124, grad norm 3.57789, param norm 111.05225
epoch 4, iter 3760, loss 2.23941, smoothed loss 3.10535, grad norm 3.17028, param norm 111.10170
epoch 4, iter 3765, loss 3.13949, smoothed loss 3.10831, grad norm 3.48695, param norm 111.14643
epoch 4, iter 3770, loss 3.27253, smoothed loss 3.10614, grad norm 3.97835, param norm 111.19024
epoch 4, iter 3775, loss 3.09746, smoothed loss 3.10119, grad norm 3.34229, param norm 111.23087
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 5, iter 3780, loss 3.50417, smoothed loss 3.09986, grad norm 3.78573, param norm 111.27171
epoch 5, iter 3785, loss 3.80245, smoothed loss 3.10117, grad norm 4.13228, param norm 111.32241
epoch 5, iter 3790, loss 2.74916, smoothed loss 3.10521, grad norm 3.90534, param norm 111.37226
epoch 5, iter 3795, loss 2.98865, smoothed loss 3.10118, grad norm 3.79353, param norm 111.41483
epoch 5, iter 3800, loss 3.08448, smoothed loss 3.09937, grad norm 3.51396, param norm 111.45248
epoch 5, iter 3805, loss 3.31381, smoothed loss 3.09934, grad norm 4.02457, param norm 111.49300
epoch 5, iter 3810, loss 3.68342, smoothed loss 3.09902, grad norm 4.33213, param norm 111.53708
epoch 5, iter 3815, loss 3.19438, smoothed loss 3.09850, grad norm 3.76106, param norm 111.58829
epoch 5, iter 3820, loss 2.96349, smoothed loss 3.09702, grad norm 3.74341, param norm 111.63961
epoch 5, iter 3825, loss 3.30550, smoothed loss 3.09219, grad norm 4.04693, param norm 111.68715
epoch 5, iter 3830, loss 2.96397, smoothed loss 3.10410, grad norm 3.66788, param norm 111.73250
epoch 5, iter 3835, loss 3.54462, smoothed loss 3.10401, grad norm 3.86873, param norm 111.77383
epoch 5, iter 3840, loss 3.13497, smoothed loss 3.11244, grad norm 3.77339, param norm 111.81715
epoch 5, iter 3845, loss 3.20220, smoothed loss 3.12706, grad norm 4.07198, param norm 111.86281
epoch 5, iter 3850, loss 2.82906, smoothed loss 3.11853, grad norm 3.45597, param norm 111.91125
epoch 5, iter 3855, loss 3.12651, smoothed loss 3.11743, grad norm 3.87184, param norm 111.95550
epoch 5, iter 3860, loss 2.89007, smoothed loss 3.12030, grad norm 3.53737, param norm 111.99860
epoch 5, iter 3865, loss 2.33042, smoothed loss 3.11278, grad norm 3.73383, param norm 112.04095
epoch 5, iter 3870, loss 2.96874, smoothed loss 3.09831, grad norm 3.88680, param norm 112.08282
epoch 5, iter 3875, loss 3.18116, smoothed loss 3.10442, grad norm 4.27174, param norm 112.12299
epoch 5, iter 3880, loss 3.02621, smoothed loss 3.09983, grad norm 3.65253, param norm 112.16028
epoch 5, iter 3885, loss 2.96443, smoothed loss 3.10476, grad norm 3.85224, param norm 112.19901
epoch 5, iter 3890, loss 2.05155, smoothed loss 3.08238, grad norm 3.03611, param norm 112.24305
epoch 5, iter 3895, loss 2.86552, smoothed loss 3.08232, grad norm 3.76609, param norm 112.29322
epoch 5, iter 3900, loss 3.67891, smoothed loss 3.08588, grad norm 3.98509, param norm 112.34283
epoch 5, iter 3905, loss 2.79435, smoothed loss 3.07094, grad norm 3.50434, param norm 112.39162
epoch 5, iter 3910, loss 3.35372, smoothed loss 3.07806, grad norm 4.49111, param norm 112.44104
epoch 5, iter 3915, loss 3.59202, smoothed loss 3.06855, grad norm 4.38380, param norm 112.48779
epoch 5, iter 3920, loss 3.25111, smoothed loss 3.07101, grad norm 4.21570, param norm 112.53505
epoch 5, iter 3925, loss 3.07392, smoothed loss 3.07717, grad norm 3.77223, param norm 112.58300
epoch 5, iter 3930, loss 3.39937, smoothed loss 3.08602, grad norm 3.76635, param norm 112.62536
epoch 5, iter 3935, loss 3.30743, smoothed loss 3.09461, grad norm 3.99501, param norm 112.66887
Adding batches start...
Added  160  batches
epoch 5, iter 3940, loss 2.73569, smoothed loss 3.09063, grad norm 3.66328, param norm 112.71706
epoch 5, iter 3945, loss 3.20835, smoothed loss 3.09414, grad norm 3.59836, param norm 112.76430
epoch 5, iter 3950, loss 3.40377, smoothed loss 3.10951, grad norm 3.43377, param norm 112.80737
epoch 5, iter 3955, loss 3.29608, smoothed loss 3.10240, grad norm 3.68549, param norm 112.85151
epoch 5, iter 3960, loss 2.66816, smoothed loss 3.08419, grad norm 3.57599, param norm 112.89578
epoch 5, iter 3965, loss 3.58125, smoothed loss 3.08482, grad norm 4.37570, param norm 112.93831
epoch 5, iter 3970, loss 3.02779, smoothed loss 3.09441, grad norm 3.60617, param norm 112.98566
epoch 5, iter 3975, loss 2.43084, smoothed loss 3.08883, grad norm 3.61873, param norm 113.03811
epoch 5, iter 3980, loss 4.05164, smoothed loss 3.08591, grad norm 4.66841, param norm 113.09290
epoch 5, iter 3985, loss 3.80965, smoothed loss 3.08948, grad norm 4.56058, param norm 113.14167
epoch 5, iter 3990, loss 3.45277, smoothed loss 3.09061, grad norm 4.39393, param norm 113.18677
epoch 5, iter 3995, loss 2.69975, smoothed loss 3.09140, grad norm 3.89287, param norm 113.22946
epoch 5, iter 4000, loss 2.73352, smoothed loss 3.07799, grad norm 4.07804, param norm 113.27177
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 5, Iter 4000, dev loss: 3.151730
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.15380 seconds [Score: 0.72124]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.59759 seconds [Score: 0.57200]
Epoch 5, Iter 4000, Train F1 score: 0.721238, Train EM score: 0.572000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.50233 seconds [Score: 0.61930]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.89233 seconds [Score: 0.46277]
Epoch 5, Iter 4000, Dev F1 score: 0.619304, Dev EM score: 0.462770
End of epoch 5
epoch 5, iter 4005, loss 3.76821, smoothed loss 3.06900, grad norm 4.39915, param norm 113.31813
epoch 5, iter 4010, loss 2.95081, smoothed loss 3.06130, grad norm 3.95083, param norm 113.36113
epoch 5, iter 4015, loss 3.53459, smoothed loss 3.06345, grad norm 4.70222, param norm 113.40691
epoch 5, iter 4020, loss 3.35680, smoothed loss 3.07015, grad norm 3.69053, param norm 113.45294
epoch 5, iter 4025, loss 2.51888, smoothed loss 3.07443, grad norm 3.00987, param norm 113.49615
epoch 5, iter 4030, loss 2.88364, smoothed loss 3.07460, grad norm 3.77449, param norm 113.53950
epoch 5, iter 4035, loss 2.76323, smoothed loss 3.06820, grad norm 3.50453, param norm 113.58107
epoch 5, iter 4040, loss 3.21908, smoothed loss 3.07634, grad norm 4.10552, param norm 113.62553
epoch 5, iter 4045, loss 3.25402, smoothed loss 3.06416, grad norm 3.85006, param norm 113.67637
epoch 5, iter 4050, loss 2.80920, smoothed loss 3.05813, grad norm 4.05671, param norm 113.72199
epoch 5, iter 4055, loss 2.74985, smoothed loss 3.05048, grad norm 3.87769, param norm 113.76712
epoch 5, iter 4060, loss 3.37820, smoothed loss 3.05092, grad norm 4.21750, param norm 113.81030
epoch 5, iter 4065, loss 2.88435, smoothed loss 3.05141, grad norm 4.03915, param norm 113.85024
epoch 5, iter 4070, loss 3.03390, smoothed loss 3.05463, grad norm 3.93608, param norm 113.89151
epoch 5, iter 4075, loss 2.73300, smoothed loss 3.05606, grad norm 3.81697, param norm 113.94135
epoch 5, iter 4080, loss 3.16648, smoothed loss 3.06996, grad norm 5.17786, param norm 113.99030
epoch 5, iter 4085, loss 2.84855, smoothed loss 3.06380, grad norm 3.57467, param norm 114.03987
epoch 5, iter 4090, loss 2.90744, smoothed loss 3.05636, grad norm 4.18397, param norm 114.08424
epoch 5, iter 4095, loss 2.82307, smoothed loss 3.05362, grad norm 4.15562, param norm 114.13108
Adding batches start...
Added  160  batches
epoch 5, iter 4100, loss 2.78902, smoothed loss 3.05599, grad norm 3.59564, param norm 114.17889
epoch 5, iter 4105, loss 2.47618, smoothed loss 3.06104, grad norm 3.36228, param norm 114.22260
epoch 5, iter 4110, loss 3.64591, smoothed loss 3.06613, grad norm 4.72882, param norm 114.26491
epoch 5, iter 4115, loss 2.77292, smoothed loss 3.05558, grad norm 3.85483, param norm 114.31760
epoch 5, iter 4120, loss 2.61725, smoothed loss 3.05608, grad norm 3.94086, param norm 114.36752
epoch 5, iter 4125, loss 3.44070, smoothed loss 3.06106, grad norm 3.76001, param norm 114.40714
epoch 5, iter 4130, loss 2.86046, smoothed loss 3.04627, grad norm 4.12078, param norm 114.44549
epoch 5, iter 4135, loss 3.08025, smoothed loss 3.05452, grad norm 3.70392, param norm 114.48965
epoch 5, iter 4140, loss 2.96420, smoothed loss 3.04559, grad norm 4.69499, param norm 114.53541
epoch 5, iter 4145, loss 2.69983, smoothed loss 3.04555, grad norm 4.20182, param norm 114.58255
epoch 5, iter 4150, loss 3.13319, smoothed loss 3.04009, grad norm 3.60520, param norm 114.63203
epoch 5, iter 4155, loss 2.84145, smoothed loss 3.04215, grad norm 4.11354, param norm 114.67625
epoch 5, iter 4160, loss 3.13232, smoothed loss 3.03012, grad norm 4.15632, param norm 114.71706
epoch 5, iter 4165, loss 2.75138, smoothed loss 3.03407, grad norm 4.17216, param norm 114.75468
epoch 5, iter 4170, loss 2.68761, smoothed loss 3.03458, grad norm 3.86628, param norm 114.79453
epoch 5, iter 4175, loss 3.06286, smoothed loss 3.02922, grad norm 4.11543, param norm 114.83817
epoch 5, iter 4180, loss 3.49338, smoothed loss 3.03042, grad norm 4.40758, param norm 114.88232
epoch 5, iter 4185, loss 2.64507, smoothed loss 3.02921, grad norm 3.38724, param norm 114.92451
epoch 5, iter 4190, loss 3.19835, smoothed loss 3.02088, grad norm 4.07690, param norm 114.96661
epoch 5, iter 4195, loss 3.77278, smoothed loss 3.02763, grad norm 4.67573, param norm 115.00414
epoch 5, iter 4200, loss 3.33186, smoothed loss 3.03169, grad norm 4.32507, param norm 115.04185
epoch 5, iter 4205, loss 3.03620, smoothed loss 3.02917, grad norm 3.61472, param norm 115.08698
epoch 5, iter 4210, loss 3.15913, smoothed loss 3.03762, grad norm 4.04101, param norm 115.12926
epoch 5, iter 4215, loss 3.04271, smoothed loss 3.03725, grad norm 4.00548, param norm 115.17133
epoch 5, iter 4220, loss 2.97692, smoothed loss 3.02362, grad norm 3.66761, param norm 115.21309
epoch 5, iter 4225, loss 2.65643, smoothed loss 3.02114, grad norm 3.68647, param norm 115.25316
epoch 5, iter 4230, loss 3.30443, smoothed loss 3.02029, grad norm 3.91637, param norm 115.29636
epoch 5, iter 4235, loss 2.03236, smoothed loss 3.00681, grad norm 3.20764, param norm 115.33807
epoch 5, iter 4240, loss 2.90152, smoothed loss 3.00319, grad norm 4.09428, param norm 115.37969
epoch 5, iter 4245, loss 3.91543, smoothed loss 3.01091, grad norm 4.60948, param norm 115.41936
epoch 5, iter 4250, loss 3.22990, smoothed loss 3.02039, grad norm 3.86709, param norm 115.45771
epoch 5, iter 4255, loss 3.27008, smoothed loss 3.01887, grad norm 4.25325, param norm 115.49533
Adding batches start...
Added  160  batches
epoch 5, iter 4260, loss 2.86948, smoothed loss 3.01378, grad norm 3.23652, param norm 115.53992
epoch 5, iter 4265, loss 2.80680, smoothed loss 3.01894, grad norm 3.71629, param norm 115.57794
epoch 5, iter 4270, loss 2.70682, smoothed loss 3.01208, grad norm 3.92353, param norm 115.61941
epoch 5, iter 4275, loss 2.32404, smoothed loss 3.01708, grad norm 3.85881, param norm 115.66531
epoch 5, iter 4280, loss 2.71481, smoothed loss 3.01022, grad norm 3.56809, param norm 115.71456
epoch 5, iter 4285, loss 4.05432, smoothed loss 3.02492, grad norm 4.51231, param norm 115.75793
epoch 5, iter 4290, loss 1.98139, smoothed loss 3.02105, grad norm 3.16420, param norm 115.79675
epoch 5, iter 4295, loss 3.10698, smoothed loss 3.01661, grad norm 3.15356, param norm 115.83858
epoch 5, iter 4300, loss 3.02843, smoothed loss 3.00870, grad norm 3.68159, param norm 115.88241
epoch 5, iter 4305, loss 3.20329, smoothed loss 3.00843, grad norm 4.11779, param norm 115.92608
epoch 5, iter 4310, loss 2.80240, smoothed loss 3.01007, grad norm 4.66827, param norm 115.96844
epoch 5, iter 4315, loss 2.97596, smoothed loss 3.00622, grad norm 3.70807, param norm 116.01329
epoch 5, iter 4320, loss 3.18363, smoothed loss 3.00063, grad norm 4.00145, param norm 116.05632
epoch 5, iter 4325, loss 2.83659, smoothed loss 2.98774, grad norm 3.75363, param norm 116.09685
epoch 5, iter 4330, loss 2.70295, smoothed loss 2.98195, grad norm 3.60346, param norm 116.13280
epoch 5, iter 4335, loss 3.63649, smoothed loss 3.00126, grad norm 4.18282, param norm 116.16816
epoch 5, iter 4340, loss 2.59266, smoothed loss 2.99317, grad norm 3.52727, param norm 116.20489
epoch 5, iter 4345, loss 4.02833, smoothed loss 3.01525, grad norm 4.58571, param norm 116.24604
epoch 5, iter 4350, loss 2.90188, smoothed loss 3.00644, grad norm 3.84359, param norm 116.29051
epoch 5, iter 4355, loss 2.77633, smoothed loss 3.00516, grad norm 4.09698, param norm 116.33639
epoch 5, iter 4360, loss 2.90139, smoothed loss 3.01577, grad norm 4.44555, param norm 116.38308
epoch 5, iter 4365, loss 3.08044, smoothed loss 3.02626, grad norm 4.14894, param norm 116.43381
epoch 5, iter 4370, loss 3.51510, smoothed loss 3.02080, grad norm 3.96913, param norm 116.48279
epoch 5, iter 4375, loss 2.74133, smoothed loss 3.00390, grad norm 4.08326, param norm 116.52740
epoch 5, iter 4380, loss 2.27299, smoothed loss 2.99612, grad norm 3.70012, param norm 116.56701
epoch 5, iter 4385, loss 3.33418, smoothed loss 2.99458, grad norm 4.38784, param norm 116.60713
epoch 5, iter 4390, loss 2.60404, smoothed loss 2.98204, grad norm 3.60314, param norm 116.64938
epoch 5, iter 4395, loss 2.18007, smoothed loss 2.96273, grad norm 3.64452, param norm 116.69447
epoch 5, iter 4400, loss 3.09881, smoothed loss 2.97746, grad norm 4.06549, param norm 116.73981
epoch 5, iter 4405, loss 2.62532, smoothed loss 2.97776, grad norm 3.56153, param norm 116.78272
epoch 5, iter 4410, loss 3.43471, smoothed loss 2.98895, grad norm 3.60813, param norm 116.82536
epoch 5, iter 4415, loss 2.56047, smoothed loss 2.97996, grad norm 3.53680, param norm 116.86812
Adding batches start...
Added  160  batches
epoch 5, iter 4420, loss 3.14339, smoothed loss 2.99580, grad norm 4.17409, param norm 116.91127
epoch 5, iter 4425, loss 3.26539, smoothed loss 3.00052, grad norm 3.95958, param norm 116.95251
epoch 5, iter 4430, loss 3.01478, smoothed loss 2.98965, grad norm 3.96058, param norm 116.98942
epoch 5, iter 4435, loss 2.74995, smoothed loss 2.99002, grad norm 3.42002, param norm 117.03025
epoch 5, iter 4440, loss 3.13433, smoothed loss 2.98995, grad norm 4.40914, param norm 117.07342
epoch 5, iter 4445, loss 4.00804, smoothed loss 2.99912, grad norm 4.09117, param norm 117.12227
epoch 5, iter 4450, loss 2.15073, smoothed loss 2.97223, grad norm 3.20216, param norm 117.17242
epoch 5, iter 4455, loss 3.33799, smoothed loss 2.99898, grad norm 4.28929, param norm 117.21468
epoch 5, iter 4460, loss 2.99257, smoothed loss 2.99812, grad norm 3.90917, param norm 117.25178
epoch 5, iter 4465, loss 3.21286, smoothed loss 3.00193, grad norm 3.68595, param norm 117.29346
epoch 5, iter 4470, loss 2.29494, smoothed loss 2.97426, grad norm 3.50793, param norm 117.33563
epoch 5, iter 4475, loss 3.49328, smoothed loss 2.96727, grad norm 4.47741, param norm 117.37739
epoch 5, iter 4480, loss 2.65375, smoothed loss 2.96421, grad norm 4.25512, param norm 117.41656
epoch 5, iter 4485, loss 3.39838, smoothed loss 2.95375, grad norm 4.79957, param norm 117.45834
epoch 5, iter 4490, loss 2.69677, smoothed loss 2.94983, grad norm 3.91451, param norm 117.50358
epoch 5, iter 4495, loss 2.88832, smoothed loss 2.96097, grad norm 3.80690, param norm 117.54630
epoch 5, iter 4500, loss 2.90340, smoothed loss 2.96665, grad norm 3.60713, param norm 117.58716
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 5, Iter 4500, dev loss: 3.134490
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.77661 seconds [Score: 0.71395]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.62482 seconds [Score: 0.59700]
Epoch 5, Iter 4500, Train F1 score: 0.713946, Train EM score: 0.597000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.78864 seconds [Score: 0.62624]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.49365 seconds [Score: 0.46305]
Epoch 5, Iter 4500, Dev F1 score: 0.626243, Dev EM score: 0.463051
End of epoch 5
epoch 5, iter 4505, loss 3.16861, smoothed loss 2.96949, grad norm 3.94447, param norm 117.63350
epoch 5, iter 4510, loss 2.79602, smoothed loss 2.97408, grad norm 3.51297, param norm 117.68494
epoch 5, iter 4515, loss 3.32784, smoothed loss 2.97126, grad norm 3.72607, param norm 117.73342
epoch 5, iter 4520, loss 2.60819, smoothed loss 2.95795, grad norm 3.44337, param norm 117.77821
epoch 5, iter 4525, loss 2.53079, smoothed loss 2.93707, grad norm 3.66141, param norm 117.82075
epoch 5, iter 4530, loss 3.27582, smoothed loss 2.94492, grad norm 3.96433, param norm 117.86507
epoch 5, iter 4535, loss 3.01350, smoothed loss 2.95698, grad norm 4.05633, param norm 117.90775
epoch 5, iter 4540, loss 2.43745, smoothed loss 2.94758, grad norm 3.62833, param norm 117.95152
epoch 5, iter 4545, loss 3.10120, smoothed loss 2.94586, grad norm 3.87957, param norm 118.00330
epoch 5, iter 4550, loss 2.94794, smoothed loss 2.94359, grad norm 4.31719, param norm 118.05411
epoch 5, iter 4555, loss 3.42087, smoothed loss 2.94093, grad norm 3.94118, param norm 118.09953
epoch 5, iter 4560, loss 3.61422, smoothed loss 2.94385, grad norm 4.37460, param norm 118.13966
epoch 5, iter 4565, loss 3.21203, smoothed loss 2.95356, grad norm 4.14086, param norm 118.17777
epoch 5, iter 4570, loss 3.61404, smoothed loss 2.95009, grad norm 4.02799, param norm 118.21304
epoch 5, iter 4575, loss 2.42873, smoothed loss 2.94736, grad norm 3.63578, param norm 118.25420
Adding batches start...
Added  144  batches
epoch 5, iter 4580, loss 3.97300, smoothed loss 2.96463, grad norm 4.89986, param norm 118.29999
epoch 5, iter 4585, loss 3.66811, smoothed loss 2.97393, grad norm 3.86665, param norm 118.34468
epoch 5, iter 4590, loss 3.01234, smoothed loss 2.97533, grad norm 4.40032, param norm 118.38684
epoch 5, iter 4595, loss 2.70069, smoothed loss 2.96902, grad norm 3.90402, param norm 118.42956
epoch 5, iter 4600, loss 2.57346, smoothed loss 2.97126, grad norm 3.77887, param norm 118.47224
epoch 5, iter 4605, loss 3.14513, smoothed loss 2.97297, grad norm 4.33087, param norm 118.51633
epoch 5, iter 4610, loss 2.98674, smoothed loss 2.96925, grad norm 3.92294, param norm 118.55889
epoch 5, iter 4615, loss 3.27510, smoothed loss 2.97205, grad norm 4.38478, param norm 118.59789
epoch 5, iter 4620, loss 2.67866, smoothed loss 2.97179, grad norm 4.11015, param norm 118.63248
epoch 5, iter 4625, loss 3.44070, smoothed loss 2.98669, grad norm 4.52383, param norm 118.67094
epoch 5, iter 4630, loss 3.35929, smoothed loss 2.99855, grad norm 4.02806, param norm 118.70882
epoch 5, iter 4635, loss 2.93148, smoothed loss 3.00510, grad norm 3.83095, param norm 118.74874
epoch 5, iter 4640, loss 3.13147, smoothed loss 3.00952, grad norm 3.77812, param norm 118.79395
epoch 5, iter 4645, loss 2.69444, smoothed loss 2.99994, grad norm 3.99268, param norm 118.83877
epoch 5, iter 4650, loss 2.78830, smoothed loss 2.99900, grad norm 3.85574, param norm 118.88454
epoch 5, iter 4655, loss 3.83629, smoothed loss 3.00712, grad norm 4.68219, param norm 118.93127
epoch 5, iter 4660, loss 3.10046, smoothed loss 3.00653, grad norm 4.55908, param norm 118.97410
epoch 5, iter 4665, loss 3.89117, smoothed loss 3.01766, grad norm 4.31928, param norm 119.01370
epoch 5, iter 4670, loss 2.59898, smoothed loss 3.01132, grad norm 3.49492, param norm 119.05463
epoch 5, iter 4675, loss 2.52935, smoothed loss 3.01759, grad norm 3.37706, param norm 119.09859
epoch 5, iter 4680, loss 3.04870, smoothed loss 3.00415, grad norm 3.81038, param norm 119.14247
epoch 5, iter 4685, loss 2.32428, smoothed loss 2.98535, grad norm 3.39622, param norm 119.18447
epoch 5, iter 4690, loss 3.00253, smoothed loss 2.98334, grad norm 3.60919, param norm 119.22618
epoch 5, iter 4695, loss 2.66591, smoothed loss 2.96915, grad norm 3.50480, param norm 119.26427
epoch 5, iter 4700, loss 2.77763, smoothed loss 2.95150, grad norm 3.87836, param norm 119.30178
epoch 5, iter 4705, loss 2.60571, smoothed loss 2.95190, grad norm 3.82540, param norm 119.34242
epoch 5, iter 4710, loss 3.04517, smoothed loss 2.93418, grad norm 4.26993, param norm 119.38560
epoch 5, iter 4715, loss 3.10035, smoothed loss 2.92092, grad norm 4.03123, param norm 119.42896
epoch 5, iter 4720, loss 3.20462, smoothed loss 2.92438, grad norm 4.44314, param norm 119.47022
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 6, iter 4725, loss 2.58990, smoothed loss 2.92673, grad norm 3.86536, param norm 119.51276
epoch 6, iter 4730, loss 2.95659, smoothed loss 2.91434, grad norm 3.83003, param norm 119.55588
epoch 6, iter 4735, loss 2.69184, smoothed loss 2.91183, grad norm 3.55223, param norm 119.60118
epoch 6, iter 4740, loss 3.03286, smoothed loss 2.90675, grad norm 4.13657, param norm 119.64569
epoch 6, iter 4745, loss 3.39964, smoothed loss 2.90108, grad norm 4.70965, param norm 119.69124
epoch 6, iter 4750, loss 3.41152, smoothed loss 2.91973, grad norm 4.72183, param norm 119.73665
epoch 6, iter 4755, loss 2.29840, smoothed loss 2.91365, grad norm 3.13605, param norm 119.78135
epoch 6, iter 4760, loss 3.25856, smoothed loss 2.90778, grad norm 3.79345, param norm 119.82690
epoch 6, iter 4765, loss 2.08315, smoothed loss 2.89764, grad norm 3.15365, param norm 119.86520
epoch 6, iter 4770, loss 2.74417, smoothed loss 2.89204, grad norm 3.88464, param norm 119.90379
epoch 6, iter 4775, loss 3.18063, smoothed loss 2.89755, grad norm 3.50325, param norm 119.94506
epoch 6, iter 4780, loss 2.72396, smoothed loss 2.91508, grad norm 3.92230, param norm 119.98774
epoch 6, iter 4785, loss 2.58186, smoothed loss 2.91897, grad norm 3.54952, param norm 120.02898
epoch 6, iter 4790, loss 2.65942, smoothed loss 2.92022, grad norm 3.58068, param norm 120.06931
epoch 6, iter 4795, loss 2.47886, smoothed loss 2.91182, grad norm 3.30469, param norm 120.11150
epoch 6, iter 4800, loss 3.18886, smoothed loss 2.92262, grad norm 4.46934, param norm 120.15166
epoch 6, iter 4805, loss 2.53964, smoothed loss 2.91969, grad norm 4.06409, param norm 120.19093
epoch 6, iter 4810, loss 2.79508, smoothed loss 2.92743, grad norm 3.94009, param norm 120.23211
epoch 6, iter 4815, loss 3.01549, smoothed loss 2.92893, grad norm 4.03757, param norm 120.27210
epoch 6, iter 4820, loss 1.93034, smoothed loss 2.91684, grad norm 2.95427, param norm 120.31420
epoch 6, iter 4825, loss 3.59313, smoothed loss 2.91624, grad norm 4.73350, param norm 120.35560
epoch 6, iter 4830, loss 2.16676, smoothed loss 2.91146, grad norm 3.43366, param norm 120.39843
epoch 6, iter 4835, loss 3.04707, smoothed loss 2.91928, grad norm 3.90056, param norm 120.44431
epoch 6, iter 4840, loss 2.46245, smoothed loss 2.91562, grad norm 3.75157, param norm 120.48903
epoch 6, iter 4845, loss 3.20526, smoothed loss 2.93111, grad norm 4.86474, param norm 120.53347
epoch 6, iter 4850, loss 2.90430, smoothed loss 2.95486, grad norm 4.03954, param norm 120.57102
epoch 6, iter 4855, loss 3.18018, smoothed loss 2.95705, grad norm 3.65053, param norm 120.61013
epoch 6, iter 4860, loss 2.52887, smoothed loss 2.95643, grad norm 3.72945, param norm 120.65134
epoch 6, iter 4865, loss 2.74343, smoothed loss 2.96647, grad norm 4.45293, param norm 120.69029
epoch 6, iter 4870, loss 3.08093, smoothed loss 2.98590, grad norm 3.79810, param norm 120.72762
epoch 6, iter 4875, loss 2.86116, smoothed loss 2.98541, grad norm 3.85300, param norm 120.76952
epoch 6, iter 4880, loss 2.49778, smoothed loss 2.98445, grad norm 3.67572, param norm 120.80899
Adding batches start...
Added  160  batches
epoch 6, iter 4885, loss 2.71392, smoothed loss 2.97875, grad norm 3.82154, param norm 120.84640
epoch 6, iter 4890, loss 2.55717, smoothed loss 2.98062, grad norm 4.00345, param norm 120.88259
epoch 6, iter 4895, loss 3.21364, smoothed loss 2.98013, grad norm 4.10562, param norm 120.91843
epoch 6, iter 4900, loss 2.58535, smoothed loss 2.96994, grad norm 4.07994, param norm 120.95939
epoch 6, iter 4905, loss 3.11109, smoothed loss 2.97930, grad norm 4.38139, param norm 121.00098
epoch 6, iter 4910, loss 2.76005, smoothed loss 2.96333, grad norm 3.74178, param norm 121.04532
epoch 6, iter 4915, loss 4.02755, smoothed loss 2.96469, grad norm 4.91881, param norm 121.09280
epoch 6, iter 4920, loss 3.03636, smoothed loss 2.97008, grad norm 4.10849, param norm 121.13889
epoch 6, iter 4925, loss 2.92989, smoothed loss 2.97261, grad norm 4.34976, param norm 121.17756
epoch 6, iter 4930, loss 3.26882, smoothed loss 2.96685, grad norm 3.82921, param norm 121.21935
epoch 6, iter 4935, loss 2.77305, smoothed loss 2.97031, grad norm 3.68897, param norm 121.25947
epoch 6, iter 4940, loss 3.17757, smoothed loss 2.96796, grad norm 4.14935, param norm 121.29716
epoch 6, iter 4945, loss 2.69333, smoothed loss 2.96162, grad norm 3.78990, param norm 121.34253
epoch 6, iter 4950, loss 2.91856, smoothed loss 2.94134, grad norm 4.21775, param norm 121.39108
epoch 6, iter 4955, loss 2.51089, smoothed loss 2.93882, grad norm 3.68167, param norm 121.43764
epoch 6, iter 4960, loss 2.91984, smoothed loss 2.92694, grad norm 4.42504, param norm 121.48029
epoch 6, iter 4965, loss 2.37688, smoothed loss 2.92146, grad norm 3.33112, param norm 121.51774
epoch 6, iter 4970, loss 2.67711, smoothed loss 2.92159, grad norm 3.51458, param norm 121.55365
epoch 6, iter 4975, loss 2.69724, smoothed loss 2.91551, grad norm 4.07967, param norm 121.59232
epoch 6, iter 4980, loss 3.19707, smoothed loss 2.92095, grad norm 3.66471, param norm 121.63460
epoch 6, iter 4985, loss 3.42254, smoothed loss 2.90924, grad norm 4.42102, param norm 121.67887
epoch 6, iter 4990, loss 2.57464, smoothed loss 2.91010, grad norm 3.99842, param norm 121.72086
epoch 6, iter 4995, loss 2.49294, smoothed loss 2.91967, grad norm 3.79250, param norm 121.76585
epoch 6, iter 5000, loss 2.54887, smoothed loss 2.90298, grad norm 3.96415, param norm 121.81472
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 6, Iter 5000, dev loss: 3.124577
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.08000 seconds [Score: 0.76381]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.64210 seconds [Score: 0.60900]
Epoch 6, Iter 5000, Train F1 score: 0.763812, Train EM score: 0.609000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.30220 seconds [Score: 0.62354]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.85605 seconds [Score: 0.46572]
Epoch 6, Iter 5000, Dev F1 score: 0.623538, Dev EM score: 0.465721
End of epoch 6
epoch 6, iter 5005, loss 3.27928, smoothed loss 2.90146, grad norm 4.25815, param norm 121.85910
epoch 6, iter 5010, loss 2.50680, smoothed loss 2.89924, grad norm 3.82577, param norm 121.89865
epoch 6, iter 5015, loss 2.55579, smoothed loss 2.89394, grad norm 4.09946, param norm 121.93752
epoch 6, iter 5020, loss 3.05661, smoothed loss 2.89938, grad norm 4.08372, param norm 121.97472
epoch 6, iter 5025, loss 2.97319, smoothed loss 2.89540, grad norm 4.08281, param norm 122.01337
epoch 6, iter 5030, loss 2.77221, smoothed loss 2.90756, grad norm 3.63503, param norm 122.05343
epoch 6, iter 5035, loss 2.88370, smoothed loss 2.90163, grad norm 3.64254, param norm 122.09589
epoch 6, iter 5040, loss 2.85666, smoothed loss 2.91707, grad norm 3.72403, param norm 122.13732
Adding batches start...
Added  160  batches
epoch 6, iter 5045, loss 2.02636, smoothed loss 2.90096, grad norm 3.18354, param norm 122.17532
epoch 6, iter 5050, loss 2.89126, smoothed loss 2.90292, grad norm 3.84238, param norm 122.21617
epoch 6, iter 5055, loss 2.68378, smoothed loss 2.89315, grad norm 3.84504, param norm 122.25554
epoch 6, iter 5060, loss 3.55134, smoothed loss 2.89846, grad norm 4.69757, param norm 122.29066
epoch 6, iter 5065, loss 2.51208, smoothed loss 2.89494, grad norm 3.38403, param norm 122.32368
epoch 6, iter 5070, loss 2.97536, smoothed loss 2.91429, grad norm 4.44922, param norm 122.35961
epoch 6, iter 5075, loss 2.89744, smoothed loss 2.91844, grad norm 3.88826, param norm 122.40266
epoch 6, iter 5080, loss 2.32883, smoothed loss 2.92147, grad norm 3.80144, param norm 122.44642
epoch 6, iter 5085, loss 2.64129, smoothed loss 2.92880, grad norm 3.81200, param norm 122.48956
epoch 6, iter 5090, loss 2.69054, smoothed loss 2.92448, grad norm 3.60051, param norm 122.53137
epoch 6, iter 5095, loss 2.41181, smoothed loss 2.91734, grad norm 3.92536, param norm 122.57037
epoch 6, iter 5100, loss 2.60119, smoothed loss 2.91351, grad norm 3.71573, param norm 122.60844
epoch 6, iter 5105, loss 2.58489, smoothed loss 2.90651, grad norm 3.73832, param norm 122.64292
epoch 6, iter 5110, loss 2.01827, smoothed loss 2.90283, grad norm 3.39459, param norm 122.67551
epoch 6, iter 5115, loss 2.93160, smoothed loss 2.90367, grad norm 3.67068, param norm 122.71295
epoch 6, iter 5120, loss 2.16854, smoothed loss 2.89487, grad norm 3.65739, param norm 122.75463
epoch 6, iter 5125, loss 2.94397, smoothed loss 2.90828, grad norm 4.37315, param norm 122.79391
epoch 6, iter 5130, loss 2.95261, smoothed loss 2.90902, grad norm 3.61950, param norm 122.82937
epoch 6, iter 5135, loss 3.47997, smoothed loss 2.91109, grad norm 4.00230, param norm 122.86719
epoch 6, iter 5140, loss 2.68782, smoothed loss 2.91050, grad norm 3.32285, param norm 122.90823
epoch 6, iter 5145, loss 3.11602, smoothed loss 2.91867, grad norm 3.64223, param norm 122.94668
epoch 6, iter 5150, loss 2.85664, smoothed loss 2.90529, grad norm 4.04853, param norm 122.98398
epoch 6, iter 5155, loss 3.46729, smoothed loss 2.92169, grad norm 3.85898, param norm 123.02189
epoch 6, iter 5160, loss 3.14523, smoothed loss 2.91393, grad norm 4.23448, param norm 123.06067
epoch 6, iter 5165, loss 3.04744, smoothed loss 2.91210, grad norm 3.95432, param norm 123.10121
epoch 6, iter 5170, loss 2.93043, smoothed loss 2.91096, grad norm 4.39183, param norm 123.14101
epoch 6, iter 5175, loss 3.06024, smoothed loss 2.91664, grad norm 3.83402, param norm 123.17960
epoch 6, iter 5180, loss 2.62917, smoothed loss 2.90389, grad norm 3.65157, param norm 123.21771
epoch 6, iter 5185, loss 2.69293, smoothed loss 2.90311, grad norm 4.05464, param norm 123.25359
epoch 6, iter 5190, loss 2.94461, smoothed loss 2.89778, grad norm 3.83797, param norm 123.29185
epoch 6, iter 5195, loss 2.17679, smoothed loss 2.89550, grad norm 3.64072, param norm 123.33074
epoch 6, iter 5200, loss 2.66983, smoothed loss 2.88704, grad norm 3.49692, param norm 123.37126
Adding batches start...
Added  160  batches
epoch 6, iter 5205, loss 2.39104, smoothed loss 2.88533, grad norm 3.63011, param norm 123.41313
epoch 6, iter 5210, loss 2.50132, smoothed loss 2.88848, grad norm 3.74726, param norm 123.45190
epoch 6, iter 5215, loss 2.67858, smoothed loss 2.88497, grad norm 4.24377, param norm 123.48783
epoch 6, iter 5220, loss 2.67906, smoothed loss 2.87185, grad norm 4.34105, param norm 123.52402
epoch 6, iter 5225, loss 2.78570, smoothed loss 2.88102, grad norm 3.78844, param norm 123.56097
epoch 6, iter 5230, loss 2.60987, smoothed loss 2.89031, grad norm 3.58902, param norm 123.59644
epoch 6, iter 5235, loss 2.51725, smoothed loss 2.88725, grad norm 3.71735, param norm 123.63482
epoch 6, iter 5240, loss 3.41909, smoothed loss 2.88740, grad norm 3.78683, param norm 123.67553
epoch 6, iter 5245, loss 3.31747, smoothed loss 2.89023, grad norm 4.19753, param norm 123.71007
epoch 6, iter 5250, loss 3.22632, smoothed loss 2.87940, grad norm 4.66165, param norm 123.74953
epoch 6, iter 5255, loss 2.92792, smoothed loss 2.87760, grad norm 4.76963, param norm 123.79081
epoch 6, iter 5260, loss 3.00017, smoothed loss 2.86295, grad norm 4.15469, param norm 123.83189
epoch 6, iter 5265, loss 2.98469, smoothed loss 2.85985, grad norm 3.85185, param norm 123.86930
epoch 6, iter 5270, loss 3.21315, smoothed loss 2.86244, grad norm 3.78109, param norm 123.90520
epoch 6, iter 5275, loss 3.03910, smoothed loss 2.86819, grad norm 4.04233, param norm 123.94245
epoch 6, iter 5280, loss 3.02558, smoothed loss 2.86216, grad norm 3.81719, param norm 123.98526
epoch 6, iter 5285, loss 2.50329, smoothed loss 2.85903, grad norm 3.61873, param norm 124.02513
epoch 6, iter 5290, loss 2.27647, smoothed loss 2.85359, grad norm 3.83638, param norm 124.06893
epoch 6, iter 5295, loss 3.19319, smoothed loss 2.86399, grad norm 4.30519, param norm 124.11298
epoch 6, iter 5300, loss 2.66704, smoothed loss 2.85047, grad norm 4.12805, param norm 124.15266
epoch 6, iter 5305, loss 2.83620, smoothed loss 2.83933, grad norm 3.93807, param norm 124.19283
epoch 6, iter 5310, loss 2.63411, smoothed loss 2.83850, grad norm 3.51534, param norm 124.23366
epoch 6, iter 5315, loss 3.56251, smoothed loss 2.86497, grad norm 4.18001, param norm 124.27022
epoch 6, iter 5320, loss 2.94520, smoothed loss 2.86239, grad norm 3.60641, param norm 124.30502
epoch 6, iter 5325, loss 2.63709, smoothed loss 2.86057, grad norm 3.76734, param norm 124.34331
epoch 6, iter 5330, loss 3.10542, smoothed loss 2.86609, grad norm 4.05059, param norm 124.38403
epoch 6, iter 5335, loss 3.29154, smoothed loss 2.86477, grad norm 4.50278, param norm 124.42309
epoch 6, iter 5340, loss 3.63205, smoothed loss 2.86477, grad norm 4.86447, param norm 124.46053
epoch 6, iter 5345, loss 3.47573, smoothed loss 2.86894, grad norm 4.38723, param norm 124.50123
epoch 6, iter 5350, loss 3.16779, smoothed loss 2.87999, grad norm 4.27410, param norm 124.54214
epoch 6, iter 5355, loss 3.57239, smoothed loss 2.88196, grad norm 4.35018, param norm 124.58393
epoch 6, iter 5360, loss 2.32577, smoothed loss 2.87804, grad norm 3.23899, param norm 124.62557
Adding batches start...
Added  160  batches
epoch 6, iter 5365, loss 2.70253, smoothed loss 2.88201, grad norm 3.85665, param norm 124.66730
epoch 6, iter 5370, loss 2.48097, smoothed loss 2.87158, grad norm 3.67638, param norm 124.70658
epoch 6, iter 5375, loss 3.21774, smoothed loss 2.86537, grad norm 4.19743, param norm 124.74787
epoch 6, iter 5380, loss 2.31954, smoothed loss 2.85791, grad norm 3.30839, param norm 124.78274
epoch 6, iter 5385, loss 2.40823, smoothed loss 2.86465, grad norm 4.08557, param norm 124.81496
epoch 6, iter 5390, loss 2.51681, smoothed loss 2.85892, grad norm 3.71082, param norm 124.84856
epoch 6, iter 5395, loss 2.40637, smoothed loss 2.85141, grad norm 3.41604, param norm 124.88431
epoch 6, iter 5400, loss 2.80230, smoothed loss 2.84283, grad norm 4.51575, param norm 124.92383
epoch 6, iter 5405, loss 2.72852, smoothed loss 2.83522, grad norm 3.69203, param norm 124.96511
epoch 6, iter 5410, loss 2.39420, smoothed loss 2.83190, grad norm 3.61685, param norm 125.00491
epoch 6, iter 5415, loss 3.11705, smoothed loss 2.82393, grad norm 4.45287, param norm 125.04218
epoch 6, iter 5420, loss 3.24851, smoothed loss 2.81864, grad norm 4.15794, param norm 125.08071
epoch 6, iter 5425, loss 2.91976, smoothed loss 2.82578, grad norm 4.15423, param norm 125.11906
epoch 6, iter 5430, loss 1.99378, smoothed loss 2.82235, grad norm 3.74049, param norm 125.15514
epoch 6, iter 5435, loss 3.23797, smoothed loss 2.82292, grad norm 4.16762, param norm 125.19638
epoch 6, iter 5440, loss 2.81926, smoothed loss 2.82322, grad norm 3.87848, param norm 125.23486
epoch 6, iter 5445, loss 2.17901, smoothed loss 2.82586, grad norm 3.69304, param norm 125.27526
epoch 6, iter 5450, loss 2.68849, smoothed loss 2.82566, grad norm 4.77860, param norm 125.31661
epoch 6, iter 5455, loss 2.57598, smoothed loss 2.81502, grad norm 3.70122, param norm 125.35601
epoch 6, iter 5460, loss 2.93596, smoothed loss 2.80734, grad norm 4.76891, param norm 125.39424
epoch 6, iter 5465, loss 1.60549, smoothed loss 2.78787, grad norm 3.00913, param norm 125.43385
epoch 6, iter 5470, loss 3.78886, smoothed loss 2.80216, grad norm 4.73032, param norm 125.47385
epoch 6, iter 5475, loss 2.99997, smoothed loss 2.80636, grad norm 4.53345, param norm 125.50999
epoch 6, iter 5480, loss 2.49700, smoothed loss 2.81932, grad norm 4.26733, param norm 125.54594
epoch 6, iter 5485, loss 3.09675, smoothed loss 2.83110, grad norm 3.99724, param norm 125.58169
epoch 6, iter 5490, loss 3.64648, smoothed loss 2.83375, grad norm 4.17705, param norm 125.62177
epoch 6, iter 5495, loss 2.83627, smoothed loss 2.82972, grad norm 3.85748, param norm 125.65972
epoch 6, iter 5500, loss 2.66791, smoothed loss 2.80920, grad norm 3.80453, param norm 125.69630
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 6, Iter 5500, dev loss: 3.089639
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.89008 seconds [Score: 0.73101]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.09634 seconds [Score: 0.61100]
Epoch 6, Iter 5500, Train F1 score: 0.731008, Train EM score: 0.611000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.40961 seconds [Score: 0.63408]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.34109 seconds [Score: 0.48609]
Epoch 6, Iter 5500, Dev F1 score: 0.634082, Dev EM score: 0.486092
End of epoch 6
epoch 6, iter 5505, loss 3.17350, smoothed loss 2.81155, grad norm 4.53266, param norm 125.73405
epoch 6, iter 5510, loss 2.87101, smoothed loss 2.81615, grad norm 3.95607, param norm 125.77181
epoch 6, iter 5515, loss 3.08717, smoothed loss 2.81938, grad norm 3.73786, param norm 125.80761
epoch 6, iter 5520, loss 2.70738, smoothed loss 2.82799, grad norm 4.05018, param norm 125.84383
Adding batches start...
Added  144  batches
epoch 6, iter 5525, loss 2.30594, smoothed loss 2.82001, grad norm 3.37745, param norm 125.88569
epoch 6, iter 5530, loss 2.21326, smoothed loss 2.81149, grad norm 3.23814, param norm 125.92725
epoch 6, iter 5535, loss 2.28978, smoothed loss 2.80381, grad norm 3.77863, param norm 125.96613
epoch 6, iter 5540, loss 3.41863, smoothed loss 2.80249, grad norm 4.41191, param norm 126.00582
epoch 6, iter 5545, loss 2.93288, smoothed loss 2.80291, grad norm 4.27015, param norm 126.04464
epoch 6, iter 5550, loss 3.41112, smoothed loss 2.81276, grad norm 4.51495, param norm 126.08125
epoch 6, iter 5555, loss 3.02997, smoothed loss 2.83176, grad norm 3.92841, param norm 126.11807
epoch 6, iter 5560, loss 3.04868, smoothed loss 2.82890, grad norm 3.90586, param norm 126.15865
epoch 6, iter 5565, loss 3.42906, smoothed loss 2.85063, grad norm 4.16537, param norm 126.19806
epoch 6, iter 5570, loss 3.57055, smoothed loss 2.86115, grad norm 4.28152, param norm 126.23874
epoch 6, iter 5575, loss 2.75651, smoothed loss 2.86418, grad norm 4.60005, param norm 126.27969
epoch 6, iter 5580, loss 2.92902, smoothed loss 2.85502, grad norm 3.93524, param norm 126.32076
epoch 6, iter 5585, loss 2.21855, smoothed loss 2.83979, grad norm 3.55637, param norm 126.36200
epoch 6, iter 5590, loss 2.62610, smoothed loss 2.83577, grad norm 4.41504, param norm 126.40092
epoch 6, iter 5595, loss 3.65288, smoothed loss 2.84152, grad norm 4.95549, param norm 126.44109
epoch 6, iter 5600, loss 1.96297, smoothed loss 2.83464, grad norm 3.50970, param norm 126.47993
epoch 6, iter 5605, loss 3.29582, smoothed loss 2.83223, grad norm 4.26777, param norm 126.51717
epoch 6, iter 5610, loss 3.33469, smoothed loss 2.83372, grad norm 4.44868, param norm 126.55772
epoch 6, iter 5615, loss 2.38155, smoothed loss 2.83860, grad norm 3.46120, param norm 126.59840
epoch 6, iter 5620, loss 3.26836, smoothed loss 2.84733, grad norm 4.16965, param norm 126.63908
epoch 6, iter 5625, loss 2.77672, smoothed loss 2.84070, grad norm 4.00513, param norm 126.68347
epoch 6, iter 5630, loss 2.49528, smoothed loss 2.82924, grad norm 4.07983, param norm 126.72793
epoch 6, iter 5635, loss 2.73564, smoothed loss 2.82786, grad norm 4.10647, param norm 126.77088
epoch 6, iter 5640, loss 2.75643, smoothed loss 2.82072, grad norm 4.62199, param norm 126.80985
epoch 6, iter 5645, loss 2.68412, smoothed loss 2.79885, grad norm 4.42493, param norm 126.84829
epoch 6, iter 5650, loss 2.40661, smoothed loss 2.80010, grad norm 4.41623, param norm 126.88591
epoch 6, iter 5655, loss 2.28003, smoothed loss 2.79787, grad norm 3.45864, param norm 126.92085
epoch 6, iter 5660, loss 3.20757, smoothed loss 2.80123, grad norm 4.38882, param norm 126.95821
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 7, iter 5665, loss 2.94493, smoothed loss 2.80051, grad norm 4.11450, param norm 127.00200
epoch 7, iter 5670, loss 3.00828, smoothed loss 2.80787, grad norm 4.27797, param norm 127.04402
epoch 7, iter 5675, loss 3.18218, smoothed loss 2.81236, grad norm 4.49647, param norm 127.08489
epoch 7, iter 5680, loss 2.51252, smoothed loss 2.81395, grad norm 3.82588, param norm 127.12575
epoch 7, iter 5685, loss 2.68307, smoothed loss 2.80328, grad norm 4.32854, param norm 127.16458
epoch 7, iter 5690, loss 2.60800, smoothed loss 2.79545, grad norm 3.81410, param norm 127.20103
epoch 7, iter 5695, loss 3.29592, smoothed loss 2.80075, grad norm 5.42582, param norm 127.23723
epoch 7, iter 5700, loss 3.52280, smoothed loss 2.80928, grad norm 4.61653, param norm 127.27379
epoch 7, iter 5705, loss 2.96934, smoothed loss 2.79360, grad norm 4.00226, param norm 127.31421
epoch 7, iter 5710, loss 2.22501, smoothed loss 2.79067, grad norm 3.41046, param norm 127.35332
epoch 7, iter 5715, loss 3.22161, smoothed loss 2.79074, grad norm 4.59111, param norm 127.39248
epoch 7, iter 5720, loss 2.63090, smoothed loss 2.77965, grad norm 4.14122, param norm 127.43207
epoch 7, iter 5725, loss 2.27294, smoothed loss 2.78677, grad norm 3.53958, param norm 127.47285
epoch 7, iter 5730, loss 3.24297, smoothed loss 2.78136, grad norm 4.14992, param norm 127.51256
epoch 7, iter 5735, loss 2.61812, smoothed loss 2.78165, grad norm 4.15466, param norm 127.54953
epoch 7, iter 5740, loss 3.01484, smoothed loss 2.79156, grad norm 3.90419, param norm 127.58411
epoch 7, iter 5745, loss 2.85634, smoothed loss 2.78454, grad norm 4.28962, param norm 127.62146
epoch 7, iter 5750, loss 2.90928, smoothed loss 2.78842, grad norm 3.63474, param norm 127.66211
epoch 7, iter 5755, loss 3.27315, smoothed loss 2.79855, grad norm 4.56396, param norm 127.70705
epoch 7, iter 5760, loss 3.34456, smoothed loss 2.80159, grad norm 4.11874, param norm 127.75269
epoch 7, iter 5765, loss 2.59086, smoothed loss 2.80424, grad norm 3.67562, param norm 127.79627
epoch 7, iter 5770, loss 3.08751, smoothed loss 2.81123, grad norm 4.25110, param norm 127.83470
epoch 7, iter 5775, loss 2.19096, smoothed loss 2.82332, grad norm 3.62801, param norm 127.87415
epoch 7, iter 5780, loss 3.06254, smoothed loss 2.80944, grad norm 4.28011, param norm 127.91696
epoch 7, iter 5785, loss 2.54340, smoothed loss 2.80522, grad norm 3.55999, param norm 127.96078
epoch 7, iter 5790, loss 2.86047, smoothed loss 2.79347, grad norm 4.23229, param norm 128.00496
epoch 7, iter 5795, loss 3.15795, smoothed loss 2.80559, grad norm 4.66670, param norm 128.04507
epoch 7, iter 5800, loss 3.55684, smoothed loss 2.81991, grad norm 4.53912, param norm 128.07965
epoch 7, iter 5805, loss 3.58800, smoothed loss 2.81718, grad norm 4.31797, param norm 128.11523
epoch 7, iter 5810, loss 3.23676, smoothed loss 2.82018, grad norm 4.36002, param norm 128.15300
epoch 7, iter 5815, loss 2.99598, smoothed loss 2.81348, grad norm 4.15689, param norm 128.19315
epoch 7, iter 5820, loss 2.50449, smoothed loss 2.81019, grad norm 3.94161, param norm 128.23285
Adding batches start...
Added  160  batches
epoch 7, iter 5825, loss 2.50201, smoothed loss 2.80799, grad norm 3.58367, param norm 128.27159
epoch 7, iter 5830, loss 2.73770, smoothed loss 2.81002, grad norm 4.49982, param norm 128.30981
epoch 7, iter 5835, loss 2.34177, smoothed loss 2.79348, grad norm 3.62338, param norm 128.35074
epoch 7, iter 5840, loss 2.68556, smoothed loss 2.78997, grad norm 4.04672, param norm 128.38721
epoch 7, iter 5845, loss 3.30069, smoothed loss 2.79334, grad norm 4.10463, param norm 128.42369
epoch 7, iter 5850, loss 2.62331, smoothed loss 2.78487, grad norm 4.39148, param norm 128.46161
epoch 7, iter 5855, loss 2.11188, smoothed loss 2.77194, grad norm 3.54368, param norm 128.50154
epoch 7, iter 5860, loss 2.40506, smoothed loss 2.76845, grad norm 3.90315, param norm 128.53816
epoch 7, iter 5865, loss 2.83048, smoothed loss 2.76684, grad norm 4.11343, param norm 128.57222
epoch 7, iter 5870, loss 2.82587, smoothed loss 2.76268, grad norm 3.55653, param norm 128.60669
epoch 7, iter 5875, loss 3.32245, smoothed loss 2.78643, grad norm 4.72279, param norm 128.64317
epoch 7, iter 5880, loss 3.13527, smoothed loss 2.78725, grad norm 3.96749, param norm 128.67995
epoch 7, iter 5885, loss 2.55830, smoothed loss 2.77345, grad norm 4.25395, param norm 128.71904
epoch 7, iter 5890, loss 2.49675, smoothed loss 2.78480, grad norm 3.66161, param norm 128.75777
epoch 7, iter 5895, loss 2.51903, smoothed loss 2.77789, grad norm 3.66998, param norm 128.79706
epoch 7, iter 5900, loss 1.99228, smoothed loss 2.76666, grad norm 3.51792, param norm 128.83569
epoch 7, iter 5905, loss 3.11869, smoothed loss 2.76759, grad norm 3.99855, param norm 128.87215
epoch 7, iter 5910, loss 2.52335, smoothed loss 2.77274, grad norm 3.67692, param norm 128.90762
epoch 7, iter 5915, loss 2.78081, smoothed loss 2.79297, grad norm 4.22395, param norm 128.94682
epoch 7, iter 5920, loss 3.33430, smoothed loss 2.81659, grad norm 4.73854, param norm 128.98712
epoch 7, iter 5925, loss 2.91238, smoothed loss 2.81065, grad norm 3.72576, param norm 129.02281
epoch 7, iter 5930, loss 3.00158, smoothed loss 2.82178, grad norm 3.87812, param norm 129.05630
epoch 7, iter 5935, loss 2.79024, smoothed loss 2.82067, grad norm 3.92336, param norm 129.09142
epoch 7, iter 5940, loss 2.89671, smoothed loss 2.81013, grad norm 4.44170, param norm 129.12555
epoch 7, iter 5945, loss 2.81844, smoothed loss 2.81166, grad norm 4.67987, param norm 129.16161
epoch 7, iter 5950, loss 2.43463, smoothed loss 2.80306, grad norm 3.83486, param norm 129.20146
epoch 7, iter 5955, loss 2.88244, smoothed loss 2.80410, grad norm 4.13026, param norm 129.24174
epoch 7, iter 5960, loss 3.14993, smoothed loss 2.80274, grad norm 3.97316, param norm 129.28090
epoch 7, iter 5965, loss 2.83290, smoothed loss 2.79826, grad norm 4.29554, param norm 129.31909
epoch 7, iter 5970, loss 3.08285, smoothed loss 2.80382, grad norm 4.36600, param norm 129.35452
epoch 7, iter 5975, loss 2.22510, smoothed loss 2.80357, grad norm 4.11412, param norm 129.39188
epoch 7, iter 5980, loss 2.86071, smoothed loss 2.80026, grad norm 4.31588, param norm 129.43169
Adding batches start...
Added  160  batches
epoch 7, iter 5985, loss 2.55901, smoothed loss 2.78790, grad norm 3.96668, param norm 129.47243
epoch 7, iter 5990, loss 2.79267, smoothed loss 2.79417, grad norm 4.33440, param norm 129.50890
epoch 7, iter 5995, loss 3.01855, smoothed loss 2.79552, grad norm 4.21567, param norm 129.54391
epoch 7, iter 6000, loss 2.67161, smoothed loss 2.80045, grad norm 3.86750, param norm 129.57831
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 7, Iter 6000, dev loss: 3.048771
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.00159 seconds [Score: 0.74584]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.95138 seconds [Score: 0.61500]
Epoch 7, Iter 6000, Train F1 score: 0.745841, Train EM score: 0.615000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.30852 seconds [Score: 0.64017]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.91140 seconds [Score: 0.48328]
Epoch 7, Iter 6000, Dev F1 score: 0.640171, Dev EM score: 0.483282
End of epoch 7
epoch 7, iter 6005, loss 2.67072, smoothed loss 2.78359, grad norm 3.84510, param norm 129.61555
epoch 7, iter 6010, loss 2.87063, smoothed loss 2.77591, grad norm 3.63000, param norm 129.65556
epoch 7, iter 6015, loss 2.64602, smoothed loss 2.76932, grad norm 4.02798, param norm 129.69283
epoch 7, iter 6020, loss 2.45911, smoothed loss 2.76658, grad norm 3.99513, param norm 129.72844
epoch 7, iter 6025, loss 3.01639, smoothed loss 2.76913, grad norm 4.58374, param norm 129.76303
epoch 7, iter 6030, loss 2.54540, smoothed loss 2.77529, grad norm 3.70572, param norm 129.79901
epoch 7, iter 6035, loss 2.03386, smoothed loss 2.75498, grad norm 3.73736, param norm 129.83574
epoch 7, iter 6040, loss 2.49892, smoothed loss 2.75179, grad norm 4.65994, param norm 129.87263
epoch 7, iter 6045, loss 2.12373, smoothed loss 2.76345, grad norm 3.43344, param norm 129.91330
epoch 7, iter 6050, loss 3.08528, smoothed loss 2.78051, grad norm 4.43650, param norm 129.95221
epoch 7, iter 6055, loss 2.16576, smoothed loss 2.76241, grad norm 3.82098, param norm 129.99335
epoch 7, iter 6060, loss 2.31430, smoothed loss 2.76340, grad norm 3.54567, param norm 130.03165
epoch 7, iter 6065, loss 2.96924, smoothed loss 2.75781, grad norm 3.83223, param norm 130.06697
epoch 7, iter 6070, loss 2.36555, smoothed loss 2.76188, grad norm 3.82159, param norm 130.09947
epoch 7, iter 6075, loss 2.57174, smoothed loss 2.76453, grad norm 4.04598, param norm 130.13361
epoch 7, iter 6080, loss 2.79350, smoothed loss 2.76289, grad norm 4.13035, param norm 130.16878
epoch 7, iter 6085, loss 2.86192, smoothed loss 2.76900, grad norm 4.22611, param norm 130.20680
epoch 7, iter 6090, loss 2.75243, smoothed loss 2.76331, grad norm 3.94681, param norm 130.24469
epoch 7, iter 6095, loss 2.35723, smoothed loss 2.74508, grad norm 3.94456, param norm 130.27997
epoch 7, iter 6100, loss 3.12285, smoothed loss 2.74250, grad norm 4.61792, param norm 130.31493
epoch 7, iter 6105, loss 2.47270, smoothed loss 2.73306, grad norm 3.58806, param norm 130.34688
epoch 7, iter 6110, loss 3.01547, smoothed loss 2.73636, grad norm 4.12711, param norm 130.38194
epoch 7, iter 6115, loss 2.82635, smoothed loss 2.74116, grad norm 4.24261, param norm 130.41962
epoch 7, iter 6120, loss 2.84871, smoothed loss 2.74974, grad norm 4.64312, param norm 130.45444
epoch 7, iter 6125, loss 2.67334, smoothed loss 2.76405, grad norm 4.13207, param norm 130.48923
epoch 7, iter 6130, loss 2.57066, smoothed loss 2.77668, grad norm 3.77684, param norm 130.52321
epoch 7, iter 6135, loss 3.88546, smoothed loss 2.79053, grad norm 4.50901, param norm 130.55722
epoch 7, iter 6140, loss 2.76607, smoothed loss 2.78466, grad norm 4.25331, param norm 130.59178
Adding batches start...
Added  160  batches
epoch 7, iter 6145, loss 2.46395, smoothed loss 2.78124, grad norm 3.54424, param norm 130.63214
epoch 7, iter 6150, loss 3.01746, smoothed loss 2.77579, grad norm 4.23723, param norm 130.67250
epoch 7, iter 6155, loss 2.68516, smoothed loss 2.76646, grad norm 4.33003, param norm 130.71010
epoch 7, iter 6160, loss 2.40926, smoothed loss 2.75487, grad norm 4.52297, param norm 130.75005
epoch 7, iter 6165, loss 2.77452, smoothed loss 2.76061, grad norm 4.41716, param norm 130.79018
epoch 7, iter 6170, loss 2.47325, smoothed loss 2.76553, grad norm 3.97526, param norm 130.82716
epoch 7, iter 6175, loss 3.55227, smoothed loss 2.78033, grad norm 4.72175, param norm 130.86423
epoch 7, iter 6180, loss 2.57979, smoothed loss 2.78434, grad norm 4.52047, param norm 130.89734
epoch 7, iter 6185, loss 3.50206, smoothed loss 2.79283, grad norm 4.45350, param norm 130.93254
epoch 7, iter 6190, loss 2.49082, smoothed loss 2.78993, grad norm 3.16371, param norm 130.96274
epoch 7, iter 6195, loss 2.61068, smoothed loss 2.79948, grad norm 3.44174, param norm 130.99510
epoch 7, iter 6200, loss 2.93472, smoothed loss 2.79888, grad norm 4.72850, param norm 131.02858
epoch 7, iter 6205, loss 2.72111, smoothed loss 2.79472, grad norm 4.16656, param norm 131.06248
epoch 7, iter 6210, loss 3.31355, smoothed loss 2.80226, grad norm 4.74252, param norm 131.09634
epoch 7, iter 6215, loss 3.37043, smoothed loss 2.80202, grad norm 4.42856, param norm 131.13362
epoch 7, iter 6220, loss 2.34013, smoothed loss 2.79171, grad norm 3.69891, param norm 131.17049
epoch 7, iter 6225, loss 2.75889, smoothed loss 2.79483, grad norm 4.21895, param norm 131.20811
epoch 7, iter 6230, loss 2.39325, smoothed loss 2.77551, grad norm 4.23362, param norm 131.24661
epoch 7, iter 6235, loss 2.51472, smoothed loss 2.77329, grad norm 4.36997, param norm 131.28528
epoch 7, iter 6240, loss 2.42496, smoothed loss 2.76035, grad norm 3.99320, param norm 131.32277
epoch 7, iter 6245, loss 2.93508, smoothed loss 2.75018, grad norm 4.15288, param norm 131.35854
epoch 7, iter 6250, loss 2.52173, smoothed loss 2.74273, grad norm 4.01304, param norm 131.39410
epoch 7, iter 6255, loss 1.76036, smoothed loss 2.72404, grad norm 3.44704, param norm 131.43625
epoch 7, iter 6260, loss 2.20561, smoothed loss 2.72729, grad norm 3.87657, param norm 131.47530
epoch 7, iter 6265, loss 2.41885, smoothed loss 2.72672, grad norm 4.01690, param norm 131.51306
epoch 7, iter 6270, loss 3.11814, smoothed loss 2.73551, grad norm 4.37786, param norm 131.55121
epoch 7, iter 6275, loss 3.11646, smoothed loss 2.75255, grad norm 3.88716, param norm 131.59013
epoch 7, iter 6280, loss 2.50679, smoothed loss 2.75139, grad norm 4.12451, param norm 131.63129
epoch 7, iter 6285, loss 2.06030, smoothed loss 2.73991, grad norm 4.05068, param norm 131.67120
epoch 7, iter 6290, loss 3.09025, smoothed loss 2.75575, grad norm 5.31531, param norm 131.71260
epoch 7, iter 6295, loss 2.59348, smoothed loss 2.76393, grad norm 3.76666, param norm 131.75192
epoch 7, iter 6300, loss 3.12823, smoothed loss 2.76776, grad norm 4.49271, param norm 131.78569
Adding batches start...
Added  160  batches
epoch 7, iter 6305, loss 2.99865, smoothed loss 2.75987, grad norm 4.03693, param norm 131.81886
epoch 7, iter 6310, loss 2.85158, smoothed loss 2.75482, grad norm 4.63253, param norm 131.85274
epoch 7, iter 6315, loss 2.57732, smoothed loss 2.75446, grad norm 3.84911, param norm 131.88640
epoch 7, iter 6320, loss 2.86263, smoothed loss 2.75694, grad norm 4.39453, param norm 131.92189
epoch 7, iter 6325, loss 2.33442, smoothed loss 2.74890, grad norm 4.08059, param norm 131.96049
epoch 7, iter 6330, loss 2.58344, smoothed loss 2.74478, grad norm 3.91754, param norm 132.00197
epoch 7, iter 6335, loss 2.85665, smoothed loss 2.74737, grad norm 4.19131, param norm 132.04359
epoch 7, iter 6340, loss 2.58719, smoothed loss 2.75180, grad norm 3.99459, param norm 132.08372
epoch 7, iter 6345, loss 2.18778, smoothed loss 2.75376, grad norm 3.99328, param norm 132.11923
epoch 7, iter 6350, loss 3.35567, smoothed loss 2.77336, grad norm 4.16601, param norm 132.15335
epoch 7, iter 6355, loss 2.73639, smoothed loss 2.76093, grad norm 4.20430, param norm 132.18677
epoch 7, iter 6360, loss 2.78612, smoothed loss 2.76599, grad norm 3.99495, param norm 132.22171
epoch 7, iter 6365, loss 3.04487, smoothed loss 2.76861, grad norm 4.38910, param norm 132.25725
epoch 7, iter 6370, loss 1.94952, smoothed loss 2.74761, grad norm 3.77401, param norm 132.29379
epoch 7, iter 6375, loss 2.53645, smoothed loss 2.74060, grad norm 4.04273, param norm 132.33299
epoch 7, iter 6380, loss 2.93284, smoothed loss 2.74019, grad norm 4.42913, param norm 132.37097
epoch 7, iter 6385, loss 2.09385, smoothed loss 2.73390, grad norm 3.57397, param norm 132.40294
epoch 7, iter 6390, loss 2.38262, smoothed loss 2.73373, grad norm 3.64840, param norm 132.43471
epoch 7, iter 6395, loss 2.43462, smoothed loss 2.73462, grad norm 3.67291, param norm 132.46936
epoch 7, iter 6400, loss 2.22330, smoothed loss 2.72085, grad norm 3.56796, param norm 132.50291
epoch 7, iter 6405, loss 3.22067, smoothed loss 2.72630, grad norm 4.23505, param norm 132.53522
epoch 7, iter 6410, loss 2.41323, smoothed loss 2.71779, grad norm 3.79182, param norm 132.56877
epoch 7, iter 6415, loss 2.62993, smoothed loss 2.70065, grad norm 4.50647, param norm 132.60028
epoch 7, iter 6420, loss 2.25194, smoothed loss 2.69058, grad norm 3.88774, param norm 132.63040
epoch 7, iter 6425, loss 2.77659, smoothed loss 2.69742, grad norm 4.63260, param norm 132.66356
epoch 7, iter 6430, loss 2.59902, smoothed loss 2.70082, grad norm 4.53576, param norm 132.70166
epoch 7, iter 6435, loss 2.74957, smoothed loss 2.69278, grad norm 5.00302, param norm 132.73874
epoch 7, iter 6440, loss 2.91937, smoothed loss 2.69953, grad norm 4.43763, param norm 132.77312
epoch 7, iter 6445, loss 2.64123, smoothed loss 2.69139, grad norm 4.19512, param norm 132.80655
epoch 7, iter 6450, loss 1.96546, smoothed loss 2.68977, grad norm 3.72780, param norm 132.84322
epoch 7, iter 6455, loss 3.11940, smoothed loss 2.69113, grad norm 4.04573, param norm 132.87825
epoch 7, iter 6460, loss 1.94998, smoothed loss 2.68527, grad norm 3.54559, param norm 132.91437
Adding batches start...
Added  144  batches
epoch 7, iter 6465, loss 2.35984, smoothed loss 2.69291, grad norm 3.66457, param norm 132.94681
epoch 7, iter 6470, loss 2.84781, smoothed loss 2.69507, grad norm 4.10808, param norm 132.98109
epoch 7, iter 6475, loss 2.61623, smoothed loss 2.68420, grad norm 3.69376, param norm 133.01567
epoch 7, iter 6480, loss 2.14800, smoothed loss 2.68249, grad norm 3.49424, param norm 133.04507
epoch 7, iter 6485, loss 2.68516, smoothed loss 2.68373, grad norm 4.13134, param norm 133.07562
epoch 7, iter 6490, loss 1.77917, smoothed loss 2.67106, grad norm 3.16892, param norm 133.10835
epoch 7, iter 6495, loss 3.23545, smoothed loss 2.67996, grad norm 4.53654, param norm 133.14163
epoch 7, iter 6500, loss 2.68219, smoothed loss 2.68724, grad norm 3.75683, param norm 133.17403
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 7, Iter 6500, dev loss: 3.092158
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.75026 seconds [Score: 0.72549]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.15991 seconds [Score: 0.61600]
Epoch 7, Iter 6500, Train F1 score: 0.725492, Train EM score: 0.616000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.83463 seconds [Score: 0.63705]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.47008 seconds [Score: 0.48019]
Epoch 7, Iter 6500, Dev F1 score: 0.637046, Dev EM score: 0.480191
End of epoch 7
epoch 7, iter 6505, loss 2.42286, smoothed loss 2.68455, grad norm 4.28312, param norm 133.20935
epoch 7, iter 6510, loss 2.50455, smoothed loss 2.68239, grad norm 3.63611, param norm 133.24301
epoch 7, iter 6515, loss 3.19490, smoothed loss 2.68598, grad norm 4.40738, param norm 133.27757
epoch 7, iter 6520, loss 2.03548, smoothed loss 2.68769, grad norm 3.81473, param norm 133.31184
epoch 7, iter 6525, loss 3.16878, smoothed loss 2.68657, grad norm 4.79698, param norm 133.34851
epoch 7, iter 6530, loss 2.52199, smoothed loss 2.67381, grad norm 4.45401, param norm 133.38509
epoch 7, iter 6535, loss 2.10388, smoothed loss 2.67075, grad norm 3.69488, param norm 133.42168
epoch 7, iter 6540, loss 2.70805, smoothed loss 2.65336, grad norm 4.09637, param norm 133.46280
epoch 7, iter 6545, loss 2.78169, smoothed loss 2.66073, grad norm 3.90555, param norm 133.49841
epoch 7, iter 6550, loss 2.95892, smoothed loss 2.67087, grad norm 4.25339, param norm 133.53075
epoch 7, iter 6555, loss 1.86699, smoothed loss 2.66327, grad norm 3.36342, param norm 133.56137
epoch 7, iter 6560, loss 2.93088, smoothed loss 2.66762, grad norm 4.28703, param norm 133.59375
epoch 7, iter 6565, loss 2.20172, smoothed loss 2.67045, grad norm 3.70144, param norm 133.62500
epoch 7, iter 6570, loss 2.94236, smoothed loss 2.67784, grad norm 4.12545, param norm 133.65930
epoch 7, iter 6575, loss 2.63837, smoothed loss 2.68107, grad norm 4.03248, param norm 133.69444
epoch 7, iter 6580, loss 3.35002, smoothed loss 2.68448, grad norm 4.17466, param norm 133.72867
epoch 7, iter 6585, loss 2.98864, smoothed loss 2.70828, grad norm 4.91714, param norm 133.76262
epoch 7, iter 6590, loss 2.91196, smoothed loss 2.71642, grad norm 4.37769, param norm 133.79712
epoch 7, iter 6595, loss 2.63532, smoothed loss 2.71344, grad norm 3.97053, param norm 133.83279
epoch 7, iter 6600, loss 2.17649, smoothed loss 2.71228, grad norm 3.29660, param norm 133.86868
epoch 7, iter 6605, loss 3.01849, smoothed loss 2.71855, grad norm 3.87091, param norm 133.90456
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 8, iter 6610, loss 2.92220, smoothed loss 2.72167, grad norm 4.34092, param norm 133.94009
epoch 8, iter 6615, loss 2.96473, smoothed loss 2.71517, grad norm 3.90195, param norm 133.97516
epoch 8, iter 6620, loss 2.12850, smoothed loss 2.70195, grad norm 3.66727, param norm 134.00848
epoch 8, iter 6625, loss 2.36348, smoothed loss 2.69286, grad norm 3.77064, param norm 134.04196
epoch 8, iter 6630, loss 3.76354, smoothed loss 2.70505, grad norm 4.73357, param norm 134.07619
epoch 8, iter 6635, loss 3.07850, smoothed loss 2.69792, grad norm 4.85769, param norm 134.11266
epoch 8, iter 6640, loss 2.30861, smoothed loss 2.70297, grad norm 3.87694, param norm 134.14905
epoch 8, iter 6645, loss 2.84317, smoothed loss 2.69674, grad norm 4.16526, param norm 134.18605
epoch 8, iter 6650, loss 2.33905, smoothed loss 2.68936, grad norm 3.65105, param norm 134.22327
epoch 8, iter 6655, loss 2.22497, smoothed loss 2.68774, grad norm 3.88770, param norm 134.26295
epoch 8, iter 6660, loss 2.46660, smoothed loss 2.68845, grad norm 4.49800, param norm 134.29999
epoch 8, iter 6665, loss 2.46166, smoothed loss 2.68760, grad norm 4.25537, param norm 134.33310
epoch 8, iter 6670, loss 2.25136, smoothed loss 2.67915, grad norm 3.72377, param norm 134.36499
epoch 8, iter 6675, loss 3.19251, smoothed loss 2.67700, grad norm 5.02019, param norm 134.39684
epoch 8, iter 6680, loss 2.00594, smoothed loss 2.66859, grad norm 3.60576, param norm 134.43123
epoch 8, iter 6685, loss 2.85370, smoothed loss 2.66772, grad norm 4.17630, param norm 134.46660
epoch 8, iter 6690, loss 3.10093, smoothed loss 2.67091, grad norm 4.25505, param norm 134.50269
epoch 8, iter 6695, loss 2.48783, smoothed loss 2.66725, grad norm 4.19348, param norm 134.54019
epoch 8, iter 6700, loss 2.38071, smoothed loss 2.66530, grad norm 4.06557, param norm 134.57819
epoch 8, iter 6705, loss 3.61700, smoothed loss 2.66591, grad norm 5.03255, param norm 134.61897
epoch 8, iter 6710, loss 2.73415, smoothed loss 2.67412, grad norm 4.05402, param norm 134.65504
epoch 8, iter 6715, loss 2.61993, smoothed loss 2.67105, grad norm 4.45420, param norm 134.69228
epoch 8, iter 6720, loss 2.37054, smoothed loss 2.67087, grad norm 4.22533, param norm 134.72549
epoch 8, iter 6725, loss 2.80224, smoothed loss 2.67498, grad norm 4.95233, param norm 134.75758
epoch 8, iter 6730, loss 2.61255, smoothed loss 2.68267, grad norm 4.48914, param norm 134.79227
epoch 8, iter 6735, loss 2.57335, smoothed loss 2.69684, grad norm 3.80785, param norm 134.82889
epoch 8, iter 6740, loss 3.06543, smoothed loss 2.69044, grad norm 4.40587, param norm 134.86693
epoch 8, iter 6745, loss 2.74268, smoothed loss 2.68574, grad norm 4.14541, param norm 134.90472
epoch 8, iter 6750, loss 2.94936, smoothed loss 2.71214, grad norm 3.87222, param norm 134.94075
epoch 8, iter 6755, loss 2.85451, smoothed loss 2.72311, grad norm 4.34615, param norm 134.97290
epoch 8, iter 6760, loss 2.10084, smoothed loss 2.73064, grad norm 3.50824, param norm 135.00398
epoch 8, iter 6765, loss 3.15489, smoothed loss 2.73322, grad norm 4.41762, param norm 135.03354
Adding batches start...
Added  160  batches
epoch 8, iter 6770, loss 2.32843, smoothed loss 2.72668, grad norm 4.05060, param norm 135.06033
epoch 8, iter 6775, loss 3.15762, smoothed loss 2.73539, grad norm 3.85436, param norm 135.09346
epoch 8, iter 6780, loss 3.13237, smoothed loss 2.74008, grad norm 4.11806, param norm 135.12965
epoch 8, iter 6785, loss 2.95044, smoothed loss 2.74419, grad norm 3.94496, param norm 135.16766
epoch 8, iter 6790, loss 2.40199, smoothed loss 2.73500, grad norm 4.28703, param norm 135.20587
epoch 8, iter 6795, loss 2.63783, smoothed loss 2.72914, grad norm 4.07608, param norm 135.24173
epoch 8, iter 6800, loss 3.44836, smoothed loss 2.72891, grad norm 5.41211, param norm 135.27452
epoch 8, iter 6805, loss 3.15311, smoothed loss 2.72244, grad norm 4.94970, param norm 135.30862
epoch 8, iter 6810, loss 2.53507, smoothed loss 2.72337, grad norm 3.91603, param norm 135.34149
epoch 8, iter 6815, loss 2.59131, smoothed loss 2.71941, grad norm 4.41269, param norm 135.37610
epoch 8, iter 6820, loss 2.70002, smoothed loss 2.73082, grad norm 4.28923, param norm 135.41199
epoch 8, iter 6825, loss 2.28754, smoothed loss 2.72027, grad norm 3.80624, param norm 135.44685
epoch 8, iter 6830, loss 2.64379, smoothed loss 2.71518, grad norm 4.14955, param norm 135.47940
epoch 8, iter 6835, loss 2.64368, smoothed loss 2.70152, grad norm 4.12074, param norm 135.51366
epoch 8, iter 6840, loss 2.24541, smoothed loss 2.69446, grad norm 3.76784, param norm 135.54970
epoch 8, iter 6845, loss 2.79684, smoothed loss 2.69591, grad norm 4.22945, param norm 135.58609
epoch 8, iter 6850, loss 2.93745, smoothed loss 2.69516, grad norm 4.22801, param norm 135.62094
epoch 8, iter 6855, loss 2.61306, smoothed loss 2.71478, grad norm 4.00614, param norm 135.65796
epoch 8, iter 6860, loss 2.46297, smoothed loss 2.72752, grad norm 3.55016, param norm 135.69559
epoch 8, iter 6865, loss 2.45419, smoothed loss 2.72116, grad norm 3.71510, param norm 135.73653
epoch 8, iter 6870, loss 2.55476, smoothed loss 2.71117, grad norm 3.94352, param norm 135.77356
epoch 8, iter 6875, loss 2.47274, smoothed loss 2.71432, grad norm 3.50919, param norm 135.80437
epoch 8, iter 6880, loss 2.58490, smoothed loss 2.70425, grad norm 3.61816, param norm 135.83331
epoch 8, iter 6885, loss 2.81214, smoothed loss 2.70467, grad norm 4.55020, param norm 135.86206
epoch 8, iter 6890, loss 3.01958, smoothed loss 2.69748, grad norm 3.61237, param norm 135.89423
epoch 8, iter 6895, loss 2.63138, smoothed loss 2.69711, grad norm 3.51063, param norm 135.92850
epoch 8, iter 6900, loss 2.56557, smoothed loss 2.70373, grad norm 4.42287, param norm 135.96548
epoch 8, iter 6905, loss 3.13695, smoothed loss 2.70213, grad norm 4.78135, param norm 136.00371
epoch 8, iter 6910, loss 2.52546, smoothed loss 2.69469, grad norm 3.91989, param norm 136.04016
epoch 8, iter 6915, loss 2.15945, smoothed loss 2.68699, grad norm 3.87311, param norm 136.07558
epoch 8, iter 6920, loss 3.64588, smoothed loss 2.69684, grad norm 5.07323, param norm 136.11038
epoch 8, iter 6925, loss 2.52628, smoothed loss 2.69768, grad norm 3.92153, param norm 136.14449
Adding batches start...
Added  160  batches
epoch 8, iter 6930, loss 2.67485, smoothed loss 2.69595, grad norm 3.94162, param norm 136.17860
epoch 8, iter 6935, loss 2.78742, smoothed loss 2.70182, grad norm 4.18117, param norm 136.20926
epoch 8, iter 6940, loss 2.41584, smoothed loss 2.68787, grad norm 4.14318, param norm 136.24388
epoch 8, iter 6945, loss 2.77956, smoothed loss 2.68872, grad norm 3.79435, param norm 136.27917
epoch 8, iter 6950, loss 2.50047, smoothed loss 2.68220, grad norm 4.17426, param norm 136.31435
epoch 8, iter 6955, loss 2.60123, smoothed loss 2.67761, grad norm 3.86185, param norm 136.34758
epoch 8, iter 6960, loss 2.27439, smoothed loss 2.68134, grad norm 3.58736, param norm 136.37999
epoch 8, iter 6965, loss 3.13161, smoothed loss 2.68673, grad norm 4.17675, param norm 136.41321
epoch 8, iter 6970, loss 2.44778, smoothed loss 2.67613, grad norm 3.64394, param norm 136.44817
epoch 8, iter 6975, loss 2.06319, smoothed loss 2.65942, grad norm 3.83524, param norm 136.48099
epoch 8, iter 6980, loss 3.69499, smoothed loss 2.66647, grad norm 4.93138, param norm 136.51331
epoch 8, iter 6985, loss 2.86346, smoothed loss 2.66878, grad norm 3.79100, param norm 136.54311
epoch 8, iter 6990, loss 2.58279, smoothed loss 2.65600, grad norm 4.44482, param norm 136.57448
epoch 8, iter 6995, loss 2.37945, smoothed loss 2.65295, grad norm 3.92220, param norm 136.61075
epoch 8, iter 7000, loss 2.56247, smoothed loss 2.65454, grad norm 3.95644, param norm 136.64690
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 8, Iter 7000, dev loss: 3.125774
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.20985 seconds [Score: 0.77184]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.22964 seconds [Score: 0.66600]
Epoch 8, Iter 7000, Train F1 score: 0.771836, Train EM score: 0.666000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.62658 seconds [Score: 0.63295]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.17150 seconds [Score: 0.48047]
Epoch 8, Iter 7000, Dev F1 score: 0.632948, Dev EM score: 0.480472
End of epoch 8
epoch 8, iter 7005, loss 2.60943, smoothed loss 2.65845, grad norm 4.02954, param norm 136.67870
epoch 8, iter 7010, loss 2.41970, smoothed loss 2.66151, grad norm 4.31535, param norm 136.70714
epoch 8, iter 7015, loss 2.74533, smoothed loss 2.65509, grad norm 4.73575, param norm 136.73744
epoch 8, iter 7020, loss 3.27843, smoothed loss 2.66392, grad norm 4.69855, param norm 136.76927
epoch 8, iter 7025, loss 2.45661, smoothed loss 2.66318, grad norm 3.82442, param norm 136.80272
epoch 8, iter 7030, loss 2.69183, smoothed loss 2.66519, grad norm 4.07750, param norm 136.83606
epoch 8, iter 7035, loss 3.09298, smoothed loss 2.65984, grad norm 4.33865, param norm 136.87321
epoch 8, iter 7040, loss 2.73985, smoothed loss 2.65846, grad norm 3.97592, param norm 136.91261
epoch 8, iter 7045, loss 2.92841, smoothed loss 2.66757, grad norm 4.01615, param norm 136.94797
epoch 8, iter 7050, loss 2.68503, smoothed loss 2.66645, grad norm 3.96255, param norm 136.97981
epoch 8, iter 7055, loss 2.32588, smoothed loss 2.65508, grad norm 3.47693, param norm 137.01157
epoch 8, iter 7060, loss 2.99861, smoothed loss 2.66742, grad norm 4.54446, param norm 137.04276
epoch 8, iter 7065, loss 2.80025, smoothed loss 2.67093, grad norm 4.71018, param norm 137.07440
epoch 8, iter 7070, loss 3.59531, smoothed loss 2.67894, grad norm 4.51958, param norm 137.10904
epoch 8, iter 7075, loss 3.17505, smoothed loss 2.68449, grad norm 4.49827, param norm 137.14281
epoch 8, iter 7080, loss 3.36025, smoothed loss 2.68625, grad norm 5.33798, param norm 137.17522
epoch 8, iter 7085, loss 2.11366, smoothed loss 2.68249, grad norm 3.42482, param norm 137.20816
Adding batches start...
Added  160  batches
epoch 8, iter 7090, loss 3.04278, smoothed loss 2.66950, grad norm 4.61785, param norm 137.24084
epoch 8, iter 7095, loss 2.50651, smoothed loss 2.65679, grad norm 4.13441, param norm 137.27646
epoch 8, iter 7100, loss 2.16125, smoothed loss 2.67181, grad norm 4.48645, param norm 137.31026
epoch 8, iter 7105, loss 2.83984, smoothed loss 2.65827, grad norm 5.01352, param norm 137.34799
epoch 8, iter 7110, loss 2.34192, smoothed loss 2.65688, grad norm 3.91179, param norm 137.38574
epoch 8, iter 7115, loss 2.92596, smoothed loss 2.67125, grad norm 4.63624, param norm 137.42082
epoch 8, iter 7120, loss 2.16800, smoothed loss 2.66913, grad norm 3.60605, param norm 137.45067
epoch 8, iter 7125, loss 2.25213, smoothed loss 2.65741, grad norm 3.30884, param norm 137.48550
epoch 8, iter 7130, loss 1.99664, smoothed loss 2.65809, grad norm 3.61792, param norm 137.52313
epoch 8, iter 7135, loss 2.50357, smoothed loss 2.65714, grad norm 4.01979, param norm 137.56149
epoch 8, iter 7140, loss 2.50431, smoothed loss 2.64187, grad norm 3.80855, param norm 137.60301
epoch 8, iter 7145, loss 3.02290, smoothed loss 2.63834, grad norm 4.57098, param norm 137.64041
epoch 8, iter 7150, loss 2.61274, smoothed loss 2.64084, grad norm 3.73442, param norm 137.67126
epoch 8, iter 7155, loss 2.83968, smoothed loss 2.64886, grad norm 4.38160, param norm 137.70230
epoch 8, iter 7160, loss 2.90532, smoothed loss 2.65385, grad norm 4.14826, param norm 137.73517
epoch 8, iter 7165, loss 2.51616, smoothed loss 2.65108, grad norm 3.69632, param norm 137.76988
epoch 8, iter 7170, loss 3.57308, smoothed loss 2.66235, grad norm 4.96117, param norm 137.80524
epoch 8, iter 7175, loss 2.16870, smoothed loss 2.67737, grad norm 4.01049, param norm 137.83575
epoch 8, iter 7180, loss 2.07745, smoothed loss 2.66212, grad norm 4.16617, param norm 137.86945
epoch 8, iter 7185, loss 2.88024, smoothed loss 2.65873, grad norm 4.01964, param norm 137.90796
epoch 8, iter 7190, loss 2.63436, smoothed loss 2.64713, grad norm 4.19627, param norm 137.94609
epoch 8, iter 7195, loss 2.88827, smoothed loss 2.64366, grad norm 5.24670, param norm 137.97894
epoch 8, iter 7200, loss 2.87299, smoothed loss 2.63765, grad norm 4.80487, param norm 138.00798
epoch 8, iter 7205, loss 2.89577, smoothed loss 2.63895, grad norm 4.76791, param norm 138.03729
epoch 8, iter 7210, loss 2.49956, smoothed loss 2.63595, grad norm 3.93600, param norm 138.06377
epoch 8, iter 7215, loss 2.73660, smoothed loss 2.64687, grad norm 3.77006, param norm 138.09119
epoch 8, iter 7220, loss 2.31845, smoothed loss 2.65005, grad norm 3.97131, param norm 138.12132
epoch 8, iter 7225, loss 3.12662, smoothed loss 2.65953, grad norm 4.54515, param norm 138.15134
epoch 8, iter 7230, loss 2.90611, smoothed loss 2.65489, grad norm 4.54244, param norm 138.18393
epoch 8, iter 7235, loss 2.89127, smoothed loss 2.64468, grad norm 5.00113, param norm 138.21716
epoch 8, iter 7240, loss 2.80165, smoothed loss 2.65025, grad norm 4.36623, param norm 138.24829
epoch 8, iter 7245, loss 2.96390, smoothed loss 2.65264, grad norm 4.18473, param norm 138.28122
Adding batches start...
Added  160  batches
epoch 8, iter 7250, loss 1.97978, smoothed loss 2.64643, grad norm 3.53111, param norm 138.31395
epoch 8, iter 7255, loss 2.31986, smoothed loss 2.64594, grad norm 3.63349, param norm 138.34894
epoch 8, iter 7260, loss 2.99227, smoothed loss 2.65092, grad norm 3.79368, param norm 138.38438
epoch 8, iter 7265, loss 2.65016, smoothed loss 2.64338, grad norm 4.34342, param norm 138.41766
epoch 8, iter 7270, loss 2.22598, smoothed loss 2.63737, grad norm 3.59788, param norm 138.44994
epoch 8, iter 7275, loss 1.84571, smoothed loss 2.63213, grad norm 3.78334, param norm 138.48518
epoch 8, iter 7280, loss 2.13544, smoothed loss 2.63065, grad norm 4.52263, param norm 138.52316
epoch 8, iter 7285, loss 3.03318, smoothed loss 2.63001, grad norm 4.70462, param norm 138.55733
epoch 8, iter 7290, loss 2.65126, smoothed loss 2.62973, grad norm 4.60260, param norm 138.58583
epoch 8, iter 7295, loss 2.21685, smoothed loss 2.62706, grad norm 3.88688, param norm 138.61469
epoch 8, iter 7300, loss 2.12571, smoothed loss 2.60726, grad norm 4.05813, param norm 138.64688
epoch 8, iter 7305, loss 2.40761, smoothed loss 2.61506, grad norm 4.05169, param norm 138.68062
epoch 8, iter 7310, loss 2.47079, smoothed loss 2.61846, grad norm 4.14710, param norm 138.71454
epoch 8, iter 7315, loss 2.55380, smoothed loss 2.60917, grad norm 3.72720, param norm 138.74947
epoch 8, iter 7320, loss 2.23278, smoothed loss 2.62458, grad norm 3.76761, param norm 138.78369
epoch 8, iter 7325, loss 3.25428, smoothed loss 2.61470, grad norm 4.33097, param norm 138.81900
epoch 8, iter 7330, loss 2.73163, smoothed loss 2.61852, grad norm 4.69719, param norm 138.85715
epoch 8, iter 7335, loss 2.95839, smoothed loss 2.61804, grad norm 4.79995, param norm 138.89221
epoch 8, iter 7340, loss 2.92104, smoothed loss 2.60023, grad norm 4.52652, param norm 138.92162
epoch 8, iter 7345, loss 2.11732, smoothed loss 2.59850, grad norm 3.74949, param norm 138.95041
epoch 8, iter 7350, loss 1.96440, smoothed loss 2.60023, grad norm 3.88415, param norm 138.98114
epoch 8, iter 7355, loss 2.74364, smoothed loss 2.60848, grad norm 4.67609, param norm 139.01337
epoch 8, iter 7360, loss 2.46155, smoothed loss 2.62173, grad norm 4.39920, param norm 139.04405
epoch 8, iter 7365, loss 2.40451, smoothed loss 2.62309, grad norm 3.89760, param norm 139.07831
epoch 8, iter 7370, loss 2.63420, smoothed loss 2.62644, grad norm 3.95558, param norm 139.11198
epoch 8, iter 7375, loss 2.80885, smoothed loss 2.61659, grad norm 4.26153, param norm 139.14684
epoch 8, iter 7380, loss 2.45360, smoothed loss 2.60896, grad norm 4.09690, param norm 139.17998
epoch 8, iter 7385, loss 2.69048, smoothed loss 2.60999, grad norm 4.43278, param norm 139.21095
epoch 8, iter 7390, loss 3.07116, smoothed loss 2.60312, grad norm 4.62668, param norm 139.24202
epoch 8, iter 7395, loss 2.15444, smoothed loss 2.60125, grad norm 3.54991, param norm 139.27344
epoch 8, iter 7400, loss 3.26968, smoothed loss 2.60741, grad norm 4.70201, param norm 139.30359
epoch 8, iter 7405, loss 2.93397, smoothed loss 2.60300, grad norm 4.02295, param norm 139.33546
Adding batches start...
Added  144  batches
epoch 8, iter 7410, loss 2.86283, smoothed loss 2.60429, grad norm 4.72830, param norm 139.37164
epoch 8, iter 7415, loss 2.62116, smoothed loss 2.59399, grad norm 4.13388, param norm 139.40913
epoch 8, iter 7420, loss 2.53259, smoothed loss 2.59360, grad norm 4.20458, param norm 139.44518
epoch 8, iter 7425, loss 2.82105, smoothed loss 2.59422, grad norm 4.26247, param norm 139.48053
epoch 8, iter 7430, loss 3.05925, smoothed loss 2.60153, grad norm 4.41637, param norm 139.51431
epoch 8, iter 7435, loss 2.60719, smoothed loss 2.58947, grad norm 4.49754, param norm 139.54851
epoch 8, iter 7440, loss 2.59574, smoothed loss 2.59547, grad norm 3.78437, param norm 139.58110
epoch 8, iter 7445, loss 2.17151, smoothed loss 2.59165, grad norm 4.24255, param norm 139.61136
epoch 8, iter 7450, loss 2.64023, smoothed loss 2.59631, grad norm 4.47948, param norm 139.64568
epoch 8, iter 7455, loss 3.07922, smoothed loss 2.59542, grad norm 4.52678, param norm 139.67851
epoch 8, iter 7460, loss 2.44144, smoothed loss 2.60934, grad norm 3.99185, param norm 139.70915
epoch 8, iter 7465, loss 2.84978, smoothed loss 2.62099, grad norm 4.50337, param norm 139.74078
epoch 8, iter 7470, loss 2.69590, smoothed loss 2.62566, grad norm 4.19788, param norm 139.77254
epoch 8, iter 7475, loss 3.16803, smoothed loss 2.62499, grad norm 4.27314, param norm 139.80635
epoch 8, iter 7480, loss 2.75659, smoothed loss 2.62352, grad norm 5.26250, param norm 139.84277
epoch 8, iter 7485, loss 2.32436, smoothed loss 2.62121, grad norm 4.34302, param norm 139.88010
epoch 8, iter 7490, loss 2.83176, smoothed loss 2.62016, grad norm 4.19743, param norm 139.91292
epoch 8, iter 7495, loss 2.42499, smoothed loss 2.62189, grad norm 3.54163, param norm 139.94249
epoch 8, iter 7500, loss 2.58067, smoothed loss 2.63039, grad norm 3.96450, param norm 139.97318
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 8, Iter 7500, dev loss: 3.069859
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.60858 seconds [Score: 0.75676]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.17761 seconds [Score: 0.63700]
Epoch 8, Iter 7500, Train F1 score: 0.756762, Train EM score: 0.637000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.37695 seconds [Score: 0.63992]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.38508 seconds [Score: 0.48469]
Epoch 8, Iter 7500, Dev F1 score: 0.639920, Dev EM score: 0.484687
End of epoch 8
epoch 8, iter 7505, loss 2.89072, smoothed loss 2.63436, grad norm 4.17329, param norm 140.00928
epoch 8, iter 7510, loss 1.90035, smoothed loss 2.61913, grad norm 3.21097, param norm 140.04991
epoch 8, iter 7515, loss 2.90956, smoothed loss 2.63384, grad norm 4.07706, param norm 140.08525
epoch 8, iter 7520, loss 2.45136, smoothed loss 2.62469, grad norm 3.88855, param norm 140.11832
epoch 8, iter 7525, loss 2.04554, smoothed loss 2.62372, grad norm 3.29206, param norm 140.14822
epoch 8, iter 7530, loss 2.49578, smoothed loss 2.60663, grad norm 4.26229, param norm 140.17796
epoch 8, iter 7535, loss 2.78807, smoothed loss 2.60594, grad norm 4.58347, param norm 140.20793
epoch 8, iter 7540, loss 3.04437, smoothed loss 2.61540, grad norm 4.99030, param norm 140.23944
epoch 8, iter 7545, loss 2.15869, smoothed loss 2.59810, grad norm 4.20850, param norm 140.27130
epoch 8, iter 7550, loss 2.80602, smoothed loss 2.59427, grad norm 4.05353, param norm 140.30646
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 9, iter 7555, loss 2.78279, smoothed loss 2.59374, grad norm 4.70148, param norm 140.34125
epoch 9, iter 7560, loss 2.39141, smoothed loss 2.59380, grad norm 3.58530, param norm 140.37134
epoch 9, iter 7565, loss 2.16536, smoothed loss 2.59677, grad norm 4.05658, param norm 140.39833
epoch 9, iter 7570, loss 2.91196, smoothed loss 2.60469, grad norm 4.25421, param norm 140.42676
epoch 9, iter 7575, loss 2.66656, smoothed loss 2.62671, grad norm 4.19033, param norm 140.45879
epoch 9, iter 7580, loss 2.74194, smoothed loss 2.62845, grad norm 4.26349, param norm 140.49211
epoch 9, iter 7585, loss 3.67639, smoothed loss 2.64442, grad norm 5.03724, param norm 140.52850
epoch 9, iter 7590, loss 2.88247, smoothed loss 2.64325, grad norm 3.99016, param norm 140.56493
epoch 9, iter 7595, loss 1.95438, smoothed loss 2.62841, grad norm 3.32273, param norm 140.60172
epoch 9, iter 7600, loss 2.47331, smoothed loss 2.61897, grad norm 4.05738, param norm 140.63876
epoch 9, iter 7605, loss 3.12182, smoothed loss 2.62434, grad norm 4.59600, param norm 140.67073
epoch 9, iter 7610, loss 2.45223, smoothed loss 2.61443, grad norm 4.67592, param norm 140.69994
epoch 9, iter 7615, loss 2.50477, smoothed loss 2.61324, grad norm 4.20366, param norm 140.73230
epoch 9, iter 7620, loss 2.31912, smoothed loss 2.60509, grad norm 4.02473, param norm 140.76811
epoch 9, iter 7625, loss 2.31947, smoothed loss 2.60465, grad norm 3.76691, param norm 140.80362
epoch 9, iter 7630, loss 3.32892, smoothed loss 2.61512, grad norm 5.05486, param norm 140.83826
epoch 9, iter 7635, loss 2.59569, smoothed loss 2.60719, grad norm 4.77804, param norm 140.87375
epoch 9, iter 7640, loss 3.37193, smoothed loss 2.62591, grad norm 4.56064, param norm 140.91002
epoch 9, iter 7645, loss 2.87686, smoothed loss 2.61778, grad norm 4.15699, param norm 140.94370
epoch 9, iter 7650, loss 2.42974, smoothed loss 2.61951, grad norm 3.74664, param norm 140.97404
epoch 9, iter 7655, loss 2.43876, smoothed loss 2.61967, grad norm 3.84267, param norm 141.00629
epoch 9, iter 7660, loss 2.94094, smoothed loss 2.60209, grad norm 4.04486, param norm 141.03758
epoch 9, iter 7665, loss 2.49191, smoothed loss 2.60922, grad norm 4.13057, param norm 141.06828
epoch 9, iter 7670, loss 2.92096, smoothed loss 2.61870, grad norm 4.47902, param norm 141.09721
epoch 9, iter 7675, loss 2.23514, smoothed loss 2.61766, grad norm 3.69055, param norm 141.12546
epoch 9, iter 7680, loss 2.58051, smoothed loss 2.61764, grad norm 3.77742, param norm 141.15242
epoch 9, iter 7685, loss 2.42231, smoothed loss 2.61222, grad norm 4.00801, param norm 141.18059
epoch 9, iter 7690, loss 1.63850, smoothed loss 2.60824, grad norm 2.97290, param norm 141.20985
epoch 9, iter 7695, loss 2.63446, smoothed loss 2.61564, grad norm 4.60924, param norm 141.24136
epoch 9, iter 7700, loss 2.57479, smoothed loss 2.62433, grad norm 4.03207, param norm 141.27541
epoch 9, iter 7705, loss 2.85462, smoothed loss 2.62116, grad norm 4.43264, param norm 141.31351
epoch 9, iter 7710, loss 1.68018, smoothed loss 2.61389, grad norm 2.98790, param norm 141.34877
Adding batches start...
Added  160  batches
epoch 9, iter 7715, loss 2.66446, smoothed loss 2.61840, grad norm 4.26999, param norm 141.38359
epoch 9, iter 7720, loss 2.11488, smoothed loss 2.61156, grad norm 3.77199, param norm 141.41946
epoch 9, iter 7725, loss 2.97696, smoothed loss 2.59883, grad norm 4.60448, param norm 141.45348
epoch 9, iter 7730, loss 2.81358, smoothed loss 2.59569, grad norm 4.79594, param norm 141.48442
epoch 9, iter 7735, loss 2.19062, smoothed loss 2.58534, grad norm 3.82218, param norm 141.51419
epoch 9, iter 7740, loss 2.57966, smoothed loss 2.58675, grad norm 4.03380, param norm 141.54410
epoch 9, iter 7745, loss 2.28757, smoothed loss 2.58675, grad norm 3.91449, param norm 141.57254
epoch 9, iter 7750, loss 2.56706, smoothed loss 2.58831, grad norm 4.29887, param norm 141.60490
epoch 9, iter 7755, loss 2.55810, smoothed loss 2.57863, grad norm 4.03885, param norm 141.64326
epoch 9, iter 7760, loss 2.54681, smoothed loss 2.57771, grad norm 4.31657, param norm 141.67694
epoch 9, iter 7765, loss 3.24772, smoothed loss 2.58495, grad norm 4.64334, param norm 141.70856
epoch 9, iter 7770, loss 2.87642, smoothed loss 2.58994, grad norm 4.69442, param norm 141.73994
epoch 9, iter 7775, loss 2.78808, smoothed loss 2.58413, grad norm 4.38655, param norm 141.77063
epoch 9, iter 7780, loss 2.70508, smoothed loss 2.58328, grad norm 4.33528, param norm 141.80185
epoch 9, iter 7785, loss 2.29885, smoothed loss 2.58241, grad norm 3.98352, param norm 141.83350
epoch 9, iter 7790, loss 2.30038, smoothed loss 2.58185, grad norm 3.78886, param norm 141.86290
epoch 9, iter 7795, loss 2.53465, smoothed loss 2.57801, grad norm 4.56156, param norm 141.89552
epoch 9, iter 7800, loss 2.46318, smoothed loss 2.57845, grad norm 4.23587, param norm 141.93446
epoch 9, iter 7805, loss 2.40278, smoothed loss 2.57821, grad norm 4.42265, param norm 141.96999
epoch 9, iter 7810, loss 3.22801, smoothed loss 2.59074, grad norm 4.19153, param norm 141.99988
epoch 9, iter 7815, loss 1.87548, smoothed loss 2.57821, grad norm 3.64760, param norm 142.02638
epoch 9, iter 7820, loss 2.24883, smoothed loss 2.57724, grad norm 3.43318, param norm 142.05457
epoch 9, iter 7825, loss 2.29269, smoothed loss 2.58002, grad norm 4.08883, param norm 142.08478
epoch 9, iter 7830, loss 2.04810, smoothed loss 2.57597, grad norm 3.63973, param norm 142.11642
epoch 9, iter 7835, loss 2.73481, smoothed loss 2.56977, grad norm 4.57340, param norm 142.14987
epoch 9, iter 7840, loss 2.22018, smoothed loss 2.56059, grad norm 3.73544, param norm 142.18274
epoch 9, iter 7845, loss 2.37276, smoothed loss 2.56097, grad norm 4.41459, param norm 142.21631
epoch 9, iter 7850, loss 2.51182, smoothed loss 2.55422, grad norm 4.60930, param norm 142.24850
epoch 9, iter 7855, loss 1.90492, smoothed loss 2.56567, grad norm 3.74662, param norm 142.27783
epoch 9, iter 7860, loss 3.00055, smoothed loss 2.58186, grad norm 4.91804, param norm 142.30956
epoch 9, iter 7865, loss 1.90664, smoothed loss 2.57684, grad norm 4.08901, param norm 142.34096
epoch 9, iter 7870, loss 2.35743, smoothed loss 2.58788, grad norm 3.80701, param norm 142.37373
Adding batches start...
Added  160  batches
epoch 9, iter 7875, loss 3.13491, smoothed loss 2.60836, grad norm 4.52122, param norm 142.40947
epoch 9, iter 7880, loss 2.19049, smoothed loss 2.60103, grad norm 3.70901, param norm 142.44447
epoch 9, iter 7885, loss 3.19798, smoothed loss 2.60640, grad norm 4.21822, param norm 142.47815
epoch 9, iter 7890, loss 2.82604, smoothed loss 2.60725, grad norm 4.78981, param norm 142.50804
epoch 9, iter 7895, loss 2.77532, smoothed loss 2.60019, grad norm 4.50054, param norm 142.53859
epoch 9, iter 7900, loss 2.72724, smoothed loss 2.59186, grad norm 4.37753, param norm 142.56844
epoch 9, iter 7905, loss 2.54682, smoothed loss 2.58674, grad norm 3.81816, param norm 142.59615
epoch 9, iter 7910, loss 3.04852, smoothed loss 2.59392, grad norm 4.44505, param norm 142.62631
epoch 9, iter 7915, loss 2.51444, smoothed loss 2.59442, grad norm 4.42036, param norm 142.65683
epoch 9, iter 7920, loss 2.68501, smoothed loss 2.59567, grad norm 3.92537, param norm 142.68622
epoch 9, iter 7925, loss 2.61116, smoothed loss 2.59928, grad norm 4.31419, param norm 142.71741
epoch 9, iter 7930, loss 2.75071, smoothed loss 2.61089, grad norm 4.07166, param norm 142.75215
epoch 9, iter 7935, loss 2.12499, smoothed loss 2.60747, grad norm 4.28446, param norm 142.78815
epoch 9, iter 7940, loss 3.14107, smoothed loss 2.59753, grad norm 4.39908, param norm 142.82285
epoch 9, iter 7945, loss 2.54360, smoothed loss 2.57763, grad norm 4.12760, param norm 142.85716
epoch 9, iter 7950, loss 2.72184, smoothed loss 2.57038, grad norm 4.43188, param norm 142.88971
epoch 9, iter 7955, loss 2.47978, smoothed loss 2.56743, grad norm 4.30163, param norm 142.92136
epoch 9, iter 7960, loss 2.72944, smoothed loss 2.57406, grad norm 4.81973, param norm 142.95383
epoch 9, iter 7965, loss 2.95099, smoothed loss 2.56746, grad norm 4.15305, param norm 142.98271
epoch 9, iter 7970, loss 3.25847, smoothed loss 2.58434, grad norm 4.42918, param norm 143.01074
epoch 9, iter 7975, loss 2.54141, smoothed loss 2.58140, grad norm 4.29673, param norm 143.03760
epoch 9, iter 7980, loss 3.36033, smoothed loss 2.58246, grad norm 4.75919, param norm 143.06699
epoch 9, iter 7985, loss 2.14476, smoothed loss 2.57108, grad norm 3.95660, param norm 143.09874
epoch 9, iter 7990, loss 3.26511, smoothed loss 2.57374, grad norm 4.35565, param norm 143.13199
epoch 9, iter 7995, loss 2.63863, smoothed loss 2.56405, grad norm 4.36889, param norm 143.16357
epoch 9, iter 8000, loss 3.53657, smoothed loss 2.55934, grad norm 4.63712, param norm 143.19205
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 9, Iter 8000, dev loss: 3.088636
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.75201 seconds [Score: 0.80193]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.77406 seconds [Score: 0.66500]
Epoch 9, Iter 8000, Train F1 score: 0.801934, Train EM score: 0.665000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.89398 seconds [Score: 0.64280]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.70729 seconds [Score: 0.49115]
Epoch 9, Iter 8000, Dev F1 score: 0.642801, Dev EM score: 0.491149
End of epoch 9
epoch 9, iter 8005, loss 2.45959, smoothed loss 2.54669, grad norm 4.31271, param norm 143.22031
epoch 9, iter 8010, loss 3.21320, smoothed loss 2.56270, grad norm 4.77851, param norm 143.24948
epoch 9, iter 8015, loss 2.28309, smoothed loss 2.55823, grad norm 4.68915, param norm 143.27870
epoch 9, iter 8020, loss 2.55609, smoothed loss 2.54366, grad norm 3.98043, param norm 143.31186
epoch 9, iter 8025, loss 2.72617, smoothed loss 2.55675, grad norm 4.46478, param norm 143.34489
epoch 9, iter 8030, loss 2.27711, smoothed loss 2.55328, grad norm 3.50895, param norm 143.37790
Adding batches start...
Added  160  batches
epoch 9, iter 8035, loss 2.35680, smoothed loss 2.56678, grad norm 4.30768, param norm 143.40781
epoch 9, iter 8040, loss 2.71382, smoothed loss 2.57207, grad norm 4.48169, param norm 143.43697
epoch 9, iter 8045, loss 2.44161, smoothed loss 2.56271, grad norm 4.17845, param norm 143.46928
epoch 9, iter 8050, loss 2.04550, smoothed loss 2.55015, grad norm 4.26548, param norm 143.50365
epoch 9, iter 8055, loss 2.46298, smoothed loss 2.53876, grad norm 4.68815, param norm 143.53766
epoch 9, iter 8060, loss 2.37827, smoothed loss 2.53387, grad norm 4.37158, param norm 143.56654
epoch 9, iter 8065, loss 2.50579, smoothed loss 2.54138, grad norm 4.47709, param norm 143.59213
epoch 9, iter 8070, loss 2.80030, smoothed loss 2.53883, grad norm 3.88175, param norm 143.62276
epoch 9, iter 8075, loss 2.25628, smoothed loss 2.53314, grad norm 4.04369, param norm 143.65494
epoch 9, iter 8080, loss 2.49209, smoothed loss 2.53908, grad norm 3.90075, param norm 143.68230
epoch 9, iter 8085, loss 2.26437, smoothed loss 2.53586, grad norm 4.02243, param norm 143.71140
epoch 9, iter 8090, loss 2.94922, smoothed loss 2.54641, grad norm 5.04016, param norm 143.74156
epoch 9, iter 8095, loss 2.61352, smoothed loss 2.54297, grad norm 3.96865, param norm 143.76949
epoch 9, iter 8100, loss 2.46719, smoothed loss 2.53359, grad norm 4.05125, param norm 143.80348
epoch 9, iter 8105, loss 2.58880, smoothed loss 2.53124, grad norm 4.51437, param norm 143.83652
epoch 9, iter 8110, loss 2.08873, smoothed loss 2.53024, grad norm 4.27012, param norm 143.87149
epoch 9, iter 8115, loss 2.45680, smoothed loss 2.52420, grad norm 3.79047, param norm 143.90227
epoch 9, iter 8120, loss 1.87075, smoothed loss 2.52343, grad norm 3.78591, param norm 143.93182
epoch 9, iter 8125, loss 2.82034, smoothed loss 2.52445, grad norm 4.49933, param norm 143.96219
epoch 9, iter 8130, loss 1.98363, smoothed loss 2.51974, grad norm 4.52017, param norm 143.99200
epoch 9, iter 8135, loss 2.48721, smoothed loss 2.52670, grad norm 4.26894, param norm 144.02185
epoch 9, iter 8140, loss 2.38288, smoothed loss 2.51145, grad norm 4.34436, param norm 144.05194
epoch 9, iter 8145, loss 2.71518, smoothed loss 2.52731, grad norm 4.65368, param norm 144.08505
epoch 9, iter 8150, loss 2.40916, smoothed loss 2.54476, grad norm 4.05727, param norm 144.11964
epoch 9, iter 8155, loss 2.57631, smoothed loss 2.55109, grad norm 4.14187, param norm 144.15509
epoch 9, iter 8160, loss 2.07739, smoothed loss 2.54940, grad norm 4.23534, param norm 144.19008
epoch 9, iter 8165, loss 2.32809, smoothed loss 2.54722, grad norm 4.23114, param norm 144.22316
epoch 9, iter 8170, loss 3.35743, smoothed loss 2.56275, grad norm 5.21419, param norm 144.25449
epoch 9, iter 8175, loss 2.95968, smoothed loss 2.57356, grad norm 4.77600, param norm 144.28549
epoch 9, iter 8180, loss 2.22855, smoothed loss 2.57740, grad norm 4.26179, param norm 144.31752
epoch 9, iter 8185, loss 2.49031, smoothed loss 2.57338, grad norm 3.73703, param norm 144.34987
epoch 9, iter 8190, loss 2.52979, smoothed loss 2.58104, grad norm 4.60326, param norm 144.37996
Adding batches start...
Added  160  batches
epoch 9, iter 8195, loss 2.54560, smoothed loss 2.57783, grad norm 4.11057, param norm 144.41341
epoch 9, iter 8200, loss 2.76131, smoothed loss 2.57197, grad norm 5.05868, param norm 144.44597
epoch 9, iter 8205, loss 2.57533, smoothed loss 2.56047, grad norm 5.00864, param norm 144.47777
epoch 9, iter 8210, loss 3.32332, smoothed loss 2.56869, grad norm 5.50950, param norm 144.50917
epoch 9, iter 8215, loss 2.54856, smoothed loss 2.57677, grad norm 4.33791, param norm 144.54520
epoch 9, iter 8220, loss 2.45635, smoothed loss 2.57795, grad norm 3.68515, param norm 144.58026
epoch 9, iter 8225, loss 2.44078, smoothed loss 2.58230, grad norm 4.43063, param norm 144.61343
epoch 9, iter 8230, loss 2.04349, smoothed loss 2.57799, grad norm 3.87881, param norm 144.64420
epoch 9, iter 8235, loss 2.04506, smoothed loss 2.57434, grad norm 3.89412, param norm 144.67349
epoch 9, iter 8240, loss 3.12353, smoothed loss 2.57450, grad norm 4.32908, param norm 144.70125
epoch 9, iter 8245, loss 2.11555, smoothed loss 2.55475, grad norm 3.85546, param norm 144.72836
epoch 9, iter 8250, loss 2.21309, smoothed loss 2.54181, grad norm 4.01311, param norm 144.75677
epoch 9, iter 8255, loss 2.01212, smoothed loss 2.53543, grad norm 4.00030, param norm 144.78610
epoch 9, iter 8260, loss 2.49603, smoothed loss 2.55154, grad norm 4.70540, param norm 144.81931
epoch 9, iter 8265, loss 2.92547, smoothed loss 2.56911, grad norm 4.53728, param norm 144.85129
epoch 9, iter 8270, loss 2.34237, smoothed loss 2.56419, grad norm 3.81080, param norm 144.88553
epoch 9, iter 8275, loss 2.38262, smoothed loss 2.56609, grad norm 3.81355, param norm 144.91928
epoch 9, iter 8280, loss 2.68452, smoothed loss 2.56254, grad norm 4.46789, param norm 144.94995
epoch 9, iter 8285, loss 2.60419, smoothed loss 2.55963, grad norm 4.18908, param norm 144.97687
epoch 9, iter 8290, loss 2.25529, smoothed loss 2.55444, grad norm 4.32048, param norm 145.00389
epoch 9, iter 8295, loss 2.31079, smoothed loss 2.55906, grad norm 4.41194, param norm 145.03459
epoch 9, iter 8300, loss 2.97853, smoothed loss 2.55813, grad norm 3.85913, param norm 145.07423
epoch 9, iter 8305, loss 2.28779, smoothed loss 2.54949, grad norm 4.21778, param norm 145.11012
epoch 9, iter 8310, loss 2.41785, smoothed loss 2.53773, grad norm 3.88404, param norm 145.14334
epoch 9, iter 8315, loss 2.86987, smoothed loss 2.53084, grad norm 4.36202, param norm 145.17383
epoch 9, iter 8320, loss 2.99576, smoothed loss 2.52583, grad norm 5.09303, param norm 145.20132
epoch 9, iter 8325, loss 2.61699, smoothed loss 2.51702, grad norm 4.03119, param norm 145.22639
epoch 9, iter 8330, loss 2.20900, smoothed loss 2.51318, grad norm 4.82276, param norm 145.25455
epoch 9, iter 8335, loss 2.78415, smoothed loss 2.52351, grad norm 4.44382, param norm 145.28691
epoch 9, iter 8340, loss 2.46538, smoothed loss 2.51773, grad norm 4.68706, param norm 145.31895
epoch 9, iter 8345, loss 2.05155, smoothed loss 2.50551, grad norm 4.08249, param norm 145.34972
epoch 9, iter 8350, loss 2.59528, smoothed loss 2.51389, grad norm 3.96440, param norm 145.38213
Adding batches start...
Added  144  batches
epoch 9, iter 8355, loss 2.30097, smoothed loss 2.52110, grad norm 4.42370, param norm 145.41472
epoch 9, iter 8360, loss 2.77749, smoothed loss 2.51995, grad norm 4.39585, param norm 145.44321
epoch 9, iter 8365, loss 3.01693, smoothed loss 2.52280, grad norm 4.73892, param norm 145.46996
epoch 9, iter 8370, loss 3.06250, smoothed loss 2.51973, grad norm 4.62358, param norm 145.49629
epoch 9, iter 8375, loss 2.67926, smoothed loss 2.52216, grad norm 3.90980, param norm 145.52650
epoch 9, iter 8380, loss 2.16172, smoothed loss 2.51299, grad norm 3.68429, param norm 145.56018
epoch 9, iter 8385, loss 3.17459, smoothed loss 2.52452, grad norm 4.41232, param norm 145.59239
epoch 9, iter 8390, loss 2.38883, smoothed loss 2.52097, grad norm 4.14536, param norm 145.62054
epoch 9, iter 8395, loss 2.43092, smoothed loss 2.52402, grad norm 4.17232, param norm 145.64670
epoch 9, iter 8400, loss 1.66559, smoothed loss 2.52657, grad norm 3.63347, param norm 145.67693
epoch 9, iter 8405, loss 3.05659, smoothed loss 2.53802, grad norm 4.93916, param norm 145.70798
epoch 9, iter 8410, loss 2.69727, smoothed loss 2.53732, grad norm 4.99603, param norm 145.73988
epoch 9, iter 8415, loss 2.10843, smoothed loss 2.52660, grad norm 3.95566, param norm 145.77327
epoch 9, iter 8420, loss 2.72853, smoothed loss 2.52854, grad norm 3.94451, param norm 145.80743
epoch 9, iter 8425, loss 2.40803, smoothed loss 2.52026, grad norm 4.40477, param norm 145.84274
epoch 9, iter 8430, loss 2.79735, smoothed loss 2.52533, grad norm 4.51026, param norm 145.87630
epoch 9, iter 8435, loss 2.54413, smoothed loss 2.53009, grad norm 4.21707, param norm 145.91019
epoch 9, iter 8440, loss 2.47242, smoothed loss 2.51981, grad norm 4.38990, param norm 145.94423
epoch 9, iter 8445, loss 2.33313, smoothed loss 2.51809, grad norm 3.88667, param norm 145.97557
epoch 9, iter 8450, loss 2.32119, smoothed loss 2.52070, grad norm 4.12708, param norm 146.00461
epoch 9, iter 8455, loss 1.82462, smoothed loss 2.51925, grad norm 3.45077, param norm 146.02888
epoch 9, iter 8460, loss 2.44242, smoothed loss 2.53514, grad norm 4.10421, param norm 146.05550
epoch 9, iter 8465, loss 2.83774, smoothed loss 2.53306, grad norm 4.35192, param norm 146.08334
epoch 9, iter 8470, loss 2.65534, smoothed loss 2.52268, grad norm 4.13412, param norm 146.11234
epoch 9, iter 8475, loss 2.35837, smoothed loss 2.52038, grad norm 3.84103, param norm 146.14001
epoch 9, iter 8480, loss 2.28502, smoothed loss 2.50639, grad norm 4.27140, param norm 146.16963
epoch 9, iter 8485, loss 2.93028, smoothed loss 2.51967, grad norm 4.42062, param norm 146.19878
epoch 9, iter 8490, loss 2.21642, smoothed loss 2.51289, grad norm 3.93972, param norm 146.22885
epoch 9, iter 8495, loss 2.17054, smoothed loss 2.51222, grad norm 3.86817, param norm 146.26169
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 10, iter 8500, loss 2.62550, smoothed loss 2.51003, grad norm 4.44498, param norm 146.29674
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 10, Iter 8500, dev loss: 3.154779
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 16.81268 seconds [Score: 0.77315]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.39772 seconds [Score: 0.63700]
Epoch 10, Iter 8500, Train F1 score: 0.773149, Train EM score: 0.637000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 114.75551 seconds [Score: 0.63921]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 114.92805 seconds [Score: 0.48736]
Epoch 10, Iter 8500, Dev F1 score: 0.639215, Dev EM score: 0.487356
End of epoch 10
epoch 10, iter 8505, loss 2.15883, smoothed loss 2.51843, grad norm 3.92267, param norm 146.32851
epoch 10, iter 8510, loss 2.82495, smoothed loss 2.51984, grad norm 4.35993, param norm 146.36067
epoch 10, iter 8515, loss 2.15441, smoothed loss 2.51967, grad norm 3.97546, param norm 146.39024
epoch 10, iter 8520, loss 2.91684, smoothed loss 2.52495, grad norm 4.94931, param norm 146.42067
epoch 10, iter 8525, loss 2.77458, smoothed loss 2.52525, grad norm 5.04966, param norm 146.45589
epoch 10, iter 8530, loss 2.69222, smoothed loss 2.53375, grad norm 3.85580, param norm 146.48929
epoch 10, iter 8535, loss 2.70710, smoothed loss 2.53005, grad norm 4.13777, param norm 146.52026
epoch 10, iter 8540, loss 2.55744, smoothed loss 2.53931, grad norm 4.24277, param norm 146.55014
epoch 10, iter 8545, loss 2.13279, smoothed loss 2.54296, grad norm 3.67871, param norm 146.58141
epoch 10, iter 8550, loss 2.29713, smoothed loss 2.54648, grad norm 3.88067, param norm 146.61409
epoch 10, iter 8555, loss 2.69852, smoothed loss 2.54760, grad norm 4.42301, param norm 146.64735
epoch 10, iter 8560, loss 2.99210, smoothed loss 2.55498, grad norm 4.78848, param norm 146.68214
epoch 10, iter 8565, loss 2.92791, smoothed loss 2.56141, grad norm 4.29317, param norm 146.71745
epoch 10, iter 8570, loss 2.34974, smoothed loss 2.55360, grad norm 4.10072, param norm 146.75299
epoch 10, iter 8575, loss 2.53762, smoothed loss 2.55921, grad norm 4.19507, param norm 146.78841
epoch 10, iter 8580, loss 2.18769, smoothed loss 2.55159, grad norm 4.05531, param norm 146.82436
epoch 10, iter 8585, loss 2.56658, smoothed loss 2.55123, grad norm 4.41708, param norm 146.85684
epoch 10, iter 8590, loss 1.87360, smoothed loss 2.55016, grad norm 3.65065, param norm 146.88612
epoch 10, iter 8595, loss 2.36067, smoothed loss 2.55276, grad norm 4.10799, param norm 146.91376
epoch 10, iter 8600, loss 2.73121, smoothed loss 2.54956, grad norm 4.18802, param norm 146.93996
epoch 10, iter 8605, loss 2.61266, smoothed loss 2.54525, grad norm 4.22617, param norm 146.96667
epoch 10, iter 8610, loss 2.85219, smoothed loss 2.55865, grad norm 4.68457, param norm 146.99442
epoch 10, iter 8615, loss 2.90842, smoothed loss 2.56619, grad norm 4.31928, param norm 147.02444
epoch 10, iter 8620, loss 2.85087, smoothed loss 2.56154, grad norm 5.08673, param norm 147.05379
epoch 10, iter 8625, loss 2.91745, smoothed loss 2.56598, grad norm 4.63045, param norm 147.08279
epoch 10, iter 8630, loss 2.46870, smoothed loss 2.56017, grad norm 4.26847, param norm 147.11320
epoch 10, iter 8635, loss 1.81294, smoothed loss 2.55627, grad norm 3.48700, param norm 147.14476
epoch 10, iter 8640, loss 3.36999, smoothed loss 2.54063, grad norm 5.06546, param norm 147.17688
epoch 10, iter 8645, loss 2.77163, smoothed loss 2.52777, grad norm 4.67560, param norm 147.20874
epoch 10, iter 8650, loss 2.20411, smoothed loss 2.51955, grad norm 4.12283, param norm 147.24211
epoch 10, iter 8655, loss 2.04197, smoothed loss 2.50809, grad norm 3.76554, param norm 147.27301
Adding batches start...
Added  160  batches
epoch 10, iter 8660, loss 2.08899, smoothed loss 2.50179, grad norm 3.59937, param norm 147.30026
epoch 10, iter 8665, loss 2.49850, smoothed loss 2.51440, grad norm 3.99035, param norm 147.32635
epoch 10, iter 8670, loss 3.11159, smoothed loss 2.51463, grad norm 4.78497, param norm 147.35399
epoch 10, iter 8675, loss 2.58543, smoothed loss 2.51308, grad norm 3.77510, param norm 147.38223
epoch 10, iter 8680, loss 2.08621, smoothed loss 2.49670, grad norm 4.35475, param norm 147.41287
epoch 10, iter 8685, loss 2.04623, smoothed loss 2.47576, grad norm 3.92228, param norm 147.44476
epoch 10, iter 8690, loss 2.80097, smoothed loss 2.47983, grad norm 4.75743, param norm 147.47343
epoch 10, iter 8695, loss 2.12775, smoothed loss 2.47525, grad norm 4.47312, param norm 147.50229
epoch 10, iter 8700, loss 2.97776, smoothed loss 2.47691, grad norm 4.26371, param norm 147.53578
epoch 10, iter 8705, loss 2.03995, smoothed loss 2.47097, grad norm 3.69053, param norm 147.57033
epoch 10, iter 8710, loss 2.52491, smoothed loss 2.46606, grad norm 4.89208, param norm 147.60683
epoch 10, iter 8715, loss 2.21269, smoothed loss 2.46715, grad norm 4.01190, param norm 147.64064
epoch 10, iter 8720, loss 2.40472, smoothed loss 2.47517, grad norm 4.47117, param norm 147.67351
epoch 10, iter 8725, loss 2.39810, smoothed loss 2.47104, grad norm 4.56062, param norm 147.70604
epoch 10, iter 8730, loss 2.52758, smoothed loss 2.45722, grad norm 5.15330, param norm 147.73837
epoch 10, iter 8735, loss 2.08441, smoothed loss 2.45150, grad norm 4.17877, param norm 147.76921
epoch 10, iter 8740, loss 2.50671, smoothed loss 2.44621, grad norm 4.39060, param norm 147.79662
epoch 10, iter 8745, loss 2.73971, smoothed loss 2.45988, grad norm 4.89644, param norm 147.82146
epoch 10, iter 8750, loss 2.22823, smoothed loss 2.45486, grad norm 4.30584, param norm 147.84647
epoch 10, iter 8755, loss 1.67771, smoothed loss 2.44467, grad norm 3.43497, param norm 147.87250
epoch 10, iter 8760, loss 1.98446, smoothed loss 2.46160, grad norm 3.64527, param norm 147.90028
epoch 10, iter 8765, loss 2.68825, smoothed loss 2.47744, grad norm 4.35148, param norm 147.92923
epoch 10, iter 8770, loss 2.68477, smoothed loss 2.47389, grad norm 4.42772, param norm 147.95993
epoch 10, iter 8775, loss 2.13128, smoothed loss 2.47640, grad norm 4.58296, param norm 147.99113
epoch 10, iter 8780, loss 3.75809, smoothed loss 2.48991, grad norm 5.19764, param norm 148.02177
epoch 10, iter 8785, loss 2.01915, smoothed loss 2.48976, grad norm 4.28837, param norm 148.05339
epoch 10, iter 8790, loss 2.37977, smoothed loss 2.49495, grad norm 4.18109, param norm 148.08940
epoch 10, iter 8795, loss 2.30602, smoothed loss 2.50738, grad norm 4.21894, param norm 148.12344
epoch 10, iter 8800, loss 2.44760, smoothed loss 2.50431, grad norm 4.36040, param norm 148.15540
epoch 10, iter 8805, loss 2.47323, smoothed loss 2.50431, grad norm 4.58728, param norm 148.18697
epoch 10, iter 8810, loss 2.03910, smoothed loss 2.49984, grad norm 3.71154, param norm 148.21883
epoch 10, iter 8815, loss 2.62490, smoothed loss 2.51173, grad norm 4.56302, param norm 148.24748
Adding batches start...
Added  160  batches
epoch 10, iter 8820, loss 1.82945, smoothed loss 2.51506, grad norm 3.66297, param norm 148.27406
epoch 10, iter 8825, loss 2.60754, smoothed loss 2.50899, grad norm 4.52631, param norm 148.30223
epoch 10, iter 8830, loss 2.22773, smoothed loss 2.51219, grad norm 4.07662, param norm 148.32935
epoch 10, iter 8835, loss 2.46667, smoothed loss 2.50905, grad norm 4.00573, param norm 148.35645
epoch 10, iter 8840, loss 2.99594, smoothed loss 2.51829, grad norm 4.50169, param norm 148.38457
epoch 10, iter 8845, loss 2.61094, smoothed loss 2.52717, grad norm 4.70499, param norm 148.41310
epoch 10, iter 8850, loss 1.92082, smoothed loss 2.52425, grad norm 3.55295, param norm 148.44661
epoch 10, iter 8855, loss 2.71461, smoothed loss 2.52625, grad norm 4.46944, param norm 148.48071
epoch 10, iter 8860, loss 3.13536, smoothed loss 2.52449, grad norm 5.02076, param norm 148.51247
epoch 10, iter 8865, loss 2.59107, smoothed loss 2.52711, grad norm 4.79806, param norm 148.54269
epoch 10, iter 8870, loss 2.64718, smoothed loss 2.51607, grad norm 4.05501, param norm 148.57086
epoch 10, iter 8875, loss 2.97431, smoothed loss 2.51615, grad norm 4.40198, param norm 148.59842
epoch 10, iter 8880, loss 3.12067, smoothed loss 2.51395, grad norm 4.93869, param norm 148.62856
epoch 10, iter 8885, loss 2.45815, smoothed loss 2.50492, grad norm 4.06890, param norm 148.65656
epoch 10, iter 8890, loss 1.67124, smoothed loss 2.49852, grad norm 4.11406, param norm 148.68346
epoch 10, iter 8895, loss 2.26531, smoothed loss 2.49523, grad norm 3.96038, param norm 148.71030
epoch 10, iter 8900, loss 1.80800, smoothed loss 2.49093, grad norm 3.76016, param norm 148.73854
epoch 10, iter 8905, loss 2.00706, smoothed loss 2.48603, grad norm 3.79303, param norm 148.76883
epoch 10, iter 8910, loss 2.63915, smoothed loss 2.48232, grad norm 4.43867, param norm 148.80066
epoch 10, iter 8915, loss 2.56702, smoothed loss 2.48214, grad norm 4.47155, param norm 148.82947
epoch 10, iter 8920, loss 2.06690, smoothed loss 2.47305, grad norm 4.20829, param norm 148.85880
epoch 10, iter 8925, loss 2.58951, smoothed loss 2.47307, grad norm 4.31971, param norm 148.88960
epoch 10, iter 8930, loss 2.76875, smoothed loss 2.47262, grad norm 4.44848, param norm 148.91876
epoch 10, iter 8935, loss 2.58475, smoothed loss 2.47263, grad norm 4.76836, param norm 148.94878
epoch 10, iter 8940, loss 2.28218, smoothed loss 2.47829, grad norm 4.37454, param norm 148.97769
epoch 10, iter 8945, loss 2.22027, smoothed loss 2.48033, grad norm 4.16413, param norm 149.00655
epoch 10, iter 8950, loss 2.08840, smoothed loss 2.48023, grad norm 4.30473, param norm 149.03172
epoch 10, iter 8955, loss 2.71462, smoothed loss 2.49213, grad norm 5.06316, param norm 149.05659
epoch 10, iter 8960, loss 2.34096, smoothed loss 2.48713, grad norm 3.78186, param norm 149.08806
epoch 10, iter 8965, loss 2.92788, smoothed loss 2.49126, grad norm 4.66627, param norm 149.11748
epoch 10, iter 8970, loss 2.20698, smoothed loss 2.48909, grad norm 4.26339, param norm 149.14755
epoch 10, iter 8975, loss 1.82665, smoothed loss 2.48193, grad norm 3.59724, param norm 149.17813
Adding batches start...
Added  160  batches
epoch 10, iter 8980, loss 2.23714, smoothed loss 2.48401, grad norm 3.91176, param norm 149.20593
epoch 10, iter 8985, loss 2.64568, smoothed loss 2.49356, grad norm 4.21817, param norm 149.23140
epoch 10, iter 8990, loss 2.64530, smoothed loss 2.50379, grad norm 4.52319, param norm 149.25670
epoch 10, iter 8995, loss 2.15318, smoothed loss 2.50018, grad norm 3.84693, param norm 149.28236
epoch 10, iter 9000, loss 2.59680, smoothed loss 2.49851, grad norm 4.10484, param norm 149.31052
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 10, Iter 9000, dev loss: 3.089977
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.73212 seconds [Score: 0.81065]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 18.21262 seconds [Score: 0.69100]
Epoch 10, Iter 9000, Train F1 score: 0.810655, Train EM score: 0.691000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.67326 seconds [Score: 0.63793]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 115.87273 seconds [Score: 0.48637]
Epoch 10, Iter 9000, Dev F1 score: 0.637928, Dev EM score: 0.486373
End of epoch 10
epoch 10, iter 9005, loss 2.30535, smoothed loss 2.49390, grad norm 4.06324, param norm 149.33926
epoch 10, iter 9010, loss 3.02781, smoothed loss 2.49258, grad norm 4.85845, param norm 149.36537
epoch 10, iter 9015, loss 2.65809, smoothed loss 2.50491, grad norm 4.48858, param norm 149.39316
epoch 10, iter 9020, loss 3.00057, smoothed loss 2.50362, grad norm 5.35295, param norm 149.42368
epoch 10, iter 9025, loss 2.93509, smoothed loss 2.50248, grad norm 4.80230, param norm 149.45473
epoch 10, iter 9030, loss 2.27017, smoothed loss 2.49611, grad norm 4.29804, param norm 149.48389
epoch 10, iter 9035, loss 2.25984, smoothed loss 2.50049, grad norm 3.95637, param norm 149.51263
epoch 10, iter 9040, loss 2.60065, smoothed loss 2.49232, grad norm 4.47265, param norm 149.54474
epoch 10, iter 9045, loss 2.79256, smoothed loss 2.50311, grad norm 5.00564, param norm 149.57518
epoch 10, iter 9050, loss 2.62794, smoothed loss 2.49228, grad norm 4.73962, param norm 149.60355
epoch 10, iter 9055, loss 2.04197, smoothed loss 2.50386, grad norm 4.58048, param norm 149.63091
epoch 10, iter 9060, loss 2.42742, smoothed loss 2.50434, grad norm 4.56514, param norm 149.65991
epoch 10, iter 9065, loss 2.08222, smoothed loss 2.50238, grad norm 3.56396, param norm 149.68703
epoch 10, iter 9070, loss 2.97935, smoothed loss 2.49824, grad norm 4.49386, param norm 149.71524
epoch 10, iter 9075, loss 2.98016, smoothed loss 2.50347, grad norm 4.33737, param norm 149.74405
epoch 10, iter 9080, loss 2.19169, smoothed loss 2.50336, grad norm 3.64482, param norm 149.77472
epoch 10, iter 9085, loss 2.84295, smoothed loss 2.50585, grad norm 4.99211, param norm 149.80736
epoch 10, iter 9090, loss 1.91391, smoothed loss 2.48800, grad norm 4.56940, param norm 149.83798
epoch 10, iter 9095, loss 2.37058, smoothed loss 2.47616, grad norm 3.80561, param norm 149.87125
epoch 10, iter 9100, loss 2.55778, smoothed loss 2.46997, grad norm 4.77666, param norm 149.90094
epoch 10, iter 9105, loss 2.33634, smoothed loss 2.45823, grad norm 4.37727, param norm 149.92970
epoch 10, iter 9110, loss 2.49426, smoothed loss 2.47040, grad norm 4.77185, param norm 149.95737
epoch 10, iter 9115, loss 2.86874, smoothed loss 2.48247, grad norm 4.40852, param norm 149.98537
epoch 10, iter 9120, loss 2.17255, smoothed loss 2.46370, grad norm 3.73877, param norm 150.01494
epoch 10, iter 9125, loss 2.68936, smoothed loss 2.46553, grad norm 5.00180, param norm 150.04527
epoch 10, iter 9130, loss 2.58359, smoothed loss 2.46380, grad norm 4.80754, param norm 150.07437
epoch 10, iter 9135, loss 2.27542, smoothed loss 2.45197, grad norm 4.64521, param norm 150.10574
Adding batches start...
Added  160  batches
epoch 10, iter 9140, loss 2.99148, smoothed loss 2.46169, grad norm 5.42819, param norm 150.13643
epoch 10, iter 9145, loss 2.44647, smoothed loss 2.47072, grad norm 4.19765, param norm 150.16525
epoch 10, iter 9150, loss 2.53876, smoothed loss 2.46584, grad norm 4.45142, param norm 150.19269
epoch 10, iter 9155, loss 2.90509, smoothed loss 2.46488, grad norm 4.98685, param norm 150.22084
epoch 10, iter 9160, loss 2.36643, smoothed loss 2.44988, grad norm 4.39660, param norm 150.24942
epoch 10, iter 9165, loss 2.56305, smoothed loss 2.44316, grad norm 4.13507, param norm 150.27780
epoch 10, iter 9170, loss 3.01837, smoothed loss 2.45623, grad norm 4.82188, param norm 150.30511
epoch 10, iter 9175, loss 3.34563, smoothed loss 2.46125, grad norm 5.22127, param norm 150.33218
epoch 10, iter 9180, loss 3.03631, smoothed loss 2.46327, grad norm 4.58797, param norm 150.35764
epoch 10, iter 9185, loss 2.37512, smoothed loss 2.45517, grad norm 4.11241, param norm 150.38673
epoch 10, iter 9190, loss 2.59491, smoothed loss 2.45370, grad norm 4.83600, param norm 150.41637
epoch 10, iter 9195, loss 2.55032, smoothed loss 2.44447, grad norm 4.62465, param norm 150.44353
epoch 10, iter 9200, loss 2.71685, smoothed loss 2.45340, grad norm 4.94779, param norm 150.47208
epoch 10, iter 9205, loss 2.44698, smoothed loss 2.46194, grad norm 3.85786, param norm 150.50148
epoch 10, iter 9210, loss 2.41408, smoothed loss 2.46462, grad norm 4.67175, param norm 150.53113
epoch 10, iter 9215, loss 2.27721, smoothed loss 2.46634, grad norm 4.09399, param norm 150.55927
epoch 10, iter 9220, loss 2.49525, smoothed loss 2.45948, grad norm 3.72703, param norm 150.58621
epoch 10, iter 9225, loss 2.76691, smoothed loss 2.46132, grad norm 4.07605, param norm 150.61421
epoch 10, iter 9230, loss 2.29083, smoothed loss 2.44848, grad norm 4.33467, param norm 150.64368
epoch 10, iter 9235, loss 2.54690, smoothed loss 2.45556, grad norm 4.32833, param norm 150.67174
epoch 10, iter 9240, loss 2.18107, smoothed loss 2.45895, grad norm 3.95770, param norm 150.69955
epoch 10, iter 9245, loss 2.28965, smoothed loss 2.46242, grad norm 3.68272, param norm 150.72739
epoch 10, iter 9250, loss 2.32532, smoothed loss 2.47381, grad norm 4.27701, param norm 150.75562
epoch 10, iter 9255, loss 2.58946, smoothed loss 2.47304, grad norm 4.29320, param norm 150.78238
epoch 10, iter 9260, loss 2.51172, smoothed loss 2.46596, grad norm 4.54521, param norm 150.80634
epoch 10, iter 9265, loss 2.84287, smoothed loss 2.46817, grad norm 4.29214, param norm 150.83421
epoch 10, iter 9270, loss 3.13917, smoothed loss 2.48230, grad norm 4.71681, param norm 150.86194
epoch 10, iter 9275, loss 2.55295, smoothed loss 2.48653, grad norm 3.96453, param norm 150.89027
epoch 10, iter 9280, loss 1.81405, smoothed loss 2.47953, grad norm 3.71849, param norm 150.91945
epoch 10, iter 9285, loss 2.32892, smoothed loss 2.46474, grad norm 3.66142, param norm 150.94826
epoch 10, iter 9290, loss 2.16091, smoothed loss 2.46202, grad norm 3.60128, param norm 150.97289
epoch 10, iter 9295, loss 2.22051, smoothed loss 2.45056, grad norm 5.07193, param norm 150.99995
Adding batches start...
Added  144  batches
epoch 10, iter 9300, loss 2.32103, smoothed loss 2.44889, grad norm 4.95076, param norm 151.02649
epoch 10, iter 9305, loss 2.32750, smoothed loss 2.44779, grad norm 4.16895, param norm 151.05322
epoch 10, iter 9310, loss 2.89709, smoothed loss 2.45492, grad norm 4.44074, param norm 151.08046
epoch 10, iter 9315, loss 2.35559, smoothed loss 2.45426, grad norm 5.22590, param norm 151.10864
epoch 10, iter 9320, loss 2.40004, smoothed loss 2.44825, grad norm 4.23670, param norm 151.13808
epoch 10, iter 9325, loss 3.28961, smoothed loss 2.46101, grad norm 4.88802, param norm 151.17018
epoch 10, iter 9330, loss 2.34439, smoothed loss 2.46247, grad norm 4.23901, param norm 151.20183
epoch 10, iter 9335, loss 2.89753, smoothed loss 2.47318, grad norm 4.65780, param norm 151.23390
epoch 10, iter 9340, loss 2.25384, smoothed loss 2.46396, grad norm 4.17189, param norm 151.26334
epoch 10, iter 9345, loss 2.29666, smoothed loss 2.45482, grad norm 3.93073, param norm 151.29395
epoch 10, iter 9350, loss 3.07809, smoothed loss 2.46634, grad norm 4.97481, param norm 151.32219
epoch 10, iter 9355, loss 2.18662, smoothed loss 2.46264, grad norm 5.28378, param norm 151.34848
epoch 10, iter 9360, loss 2.69372, smoothed loss 2.47064, grad norm 4.49271, param norm 151.37541
epoch 10, iter 9365, loss 2.30184, smoothed loss 2.45996, grad norm 4.07754, param norm 151.40285
epoch 10, iter 9370, loss 2.58651, smoothed loss 2.45339, grad norm 3.83390, param norm 151.42900
epoch 10, iter 9375, loss 2.37867, smoothed loss 2.45565, grad norm 4.38776, param norm 151.45700
epoch 10, iter 9380, loss 2.25394, smoothed loss 2.46292, grad norm 4.68423, param norm 151.48445
epoch 10, iter 9385, loss 2.51726, smoothed loss 2.45500, grad norm 4.72017, param norm 151.51163
epoch 10, iter 9390, loss 2.56504, smoothed loss 2.44258, grad norm 4.18025, param norm 151.53954
epoch 10, iter 9395, loss 2.04668, smoothed loss 2.43769, grad norm 4.42781, param norm 151.56871
epoch 10, iter 9400, loss 2.75789, smoothed loss 2.44558, grad norm 4.75018, param norm 151.59462
epoch 10, iter 9405, loss 2.58591, smoothed loss 2.44517, grad norm 4.41564, param norm 151.62000
epoch 10, iter 9410, loss 2.48577, smoothed loss 2.43822, grad norm 4.23965, param norm 151.64708
epoch 10, iter 9415, loss 2.35599, smoothed loss 2.43021, grad norm 4.79472, param norm 151.67781
epoch 10, iter 9420, loss 1.78744, smoothed loss 2.41583, grad norm 3.60929, param norm 151.71098
epoch 10, iter 9425, loss 2.55224, smoothed loss 2.42476, grad norm 4.52356, param norm 151.74231
epoch 10, iter 9430, loss 1.97585, smoothed loss 2.41128, grad norm 4.31083, param norm 151.77174
epoch 10, iter 9435, loss 2.87843, smoothed loss 2.41855, grad norm 4.55129, param norm 151.80072
epoch 10, iter 9440, loss 2.69221, smoothed loss 2.42377, grad norm 4.25942, param norm 151.82956
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 11, iter 9445, loss 2.68793, smoothed loss 2.41736, grad norm 4.94381, param norm 151.85971
epoch 11, iter 9450, loss 2.00269, smoothed loss 2.41745, grad norm 4.37150, param norm 151.88567
epoch 11, iter 9455, loss 2.75482, smoothed loss 2.41191, grad norm 4.63316, param norm 151.91292
epoch 11, iter 9460, loss 3.34948, smoothed loss 2.42163, grad norm 5.33445, param norm 151.94339
epoch 11, iter 9465, loss 2.40356, smoothed loss 2.42628, grad norm 4.51172, param norm 151.97217
epoch 11, iter 9470, loss 2.45996, smoothed loss 2.43916, grad norm 3.98899, param norm 152.00174
epoch 11, iter 9475, loss 1.69393, smoothed loss 2.42289, grad norm 3.35748, param norm 152.03183
epoch 11, iter 9480, loss 2.84374, smoothed loss 2.42727, grad norm 4.96957, param norm 152.06364
epoch 11, iter 9485, loss 2.25074, smoothed loss 2.44644, grad norm 4.20443, param norm 152.09303
epoch 11, iter 9490, loss 2.36976, smoothed loss 2.43620, grad norm 4.31963, param norm 152.12439
epoch 11, iter 9495, loss 3.22755, smoothed loss 2.44179, grad norm 4.73952, param norm 152.15636
epoch 11, iter 9500, loss 3.34346, smoothed loss 2.44721, grad norm 5.58222, param norm 152.18719
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 11, Iter 9500, dev loss: 3.156694
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.49721 seconds [Score: 0.79716]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.79196 seconds [Score: 0.66700]
Epoch 11, Iter 9500, Train F1 score: 0.797163, Train EM score: 0.667000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.95415 seconds [Score: 0.63844]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.05737 seconds [Score: 0.48679]
Epoch 11, Iter 9500, Dev F1 score: 0.638443, Dev EM score: 0.486794
End of epoch 11
epoch 11, iter 9505, loss 2.45448, smoothed loss 2.44166, grad norm 3.94561, param norm 152.21400
epoch 11, iter 9510, loss 2.25997, smoothed loss 2.44003, grad norm 4.22969, param norm 152.24069
epoch 11, iter 9515, loss 2.31466, smoothed loss 2.43890, grad norm 3.96878, param norm 152.26886
epoch 11, iter 9520, loss 3.05062, smoothed loss 2.45628, grad norm 5.05403, param norm 152.29849
epoch 11, iter 9525, loss 2.29853, smoothed loss 2.46332, grad norm 4.06947, param norm 152.32704
epoch 11, iter 9530, loss 2.65008, smoothed loss 2.46196, grad norm 3.95244, param norm 152.35602
epoch 11, iter 9535, loss 2.58932, smoothed loss 2.46632, grad norm 4.14446, param norm 152.38708
epoch 11, iter 9540, loss 2.77056, smoothed loss 2.46469, grad norm 4.52597, param norm 152.41782
epoch 11, iter 9545, loss 3.01947, smoothed loss 2.45976, grad norm 5.00045, param norm 152.44658
epoch 11, iter 9550, loss 2.59730, smoothed loss 2.45777, grad norm 4.78472, param norm 152.47610
epoch 11, iter 9555, loss 2.41288, smoothed loss 2.45228, grad norm 3.93041, param norm 152.50551
epoch 11, iter 9560, loss 2.54070, smoothed loss 2.44148, grad norm 4.06520, param norm 152.53229
epoch 11, iter 9565, loss 2.45949, smoothed loss 2.44763, grad norm 4.70526, param norm 152.55600
epoch 11, iter 9570, loss 2.83926, smoothed loss 2.45076, grad norm 4.87823, param norm 152.58087
epoch 11, iter 9575, loss 2.08329, smoothed loss 2.44596, grad norm 3.64039, param norm 152.60806
epoch 11, iter 9580, loss 2.11252, smoothed loss 2.43885, grad norm 4.60925, param norm 152.63808
epoch 11, iter 9585, loss 2.02395, smoothed loss 2.45365, grad norm 3.73144, param norm 152.66696
epoch 11, iter 9590, loss 2.70692, smoothed loss 2.45824, grad norm 4.37648, param norm 152.69347
epoch 11, iter 9595, loss 2.06610, smoothed loss 2.45749, grad norm 4.07835, param norm 152.72105
epoch 11, iter 9600, loss 1.99845, smoothed loss 2.44910, grad norm 3.79387, param norm 152.74860
Adding batches start...
Added  160  batches
epoch 11, iter 9605, loss 2.36359, smoothed loss 2.45187, grad norm 4.62452, param norm 152.77446
epoch 11, iter 9610, loss 2.86429, smoothed loss 2.45198, grad norm 5.10498, param norm 152.79880
epoch 11, iter 9615, loss 2.20129, smoothed loss 2.44707, grad norm 4.25976, param norm 152.82645
epoch 11, iter 9620, loss 2.53991, smoothed loss 2.45140, grad norm 4.39858, param norm 152.85551
epoch 11, iter 9625, loss 2.29370, smoothed loss 2.43646, grad norm 4.09040, param norm 152.88403
epoch 11, iter 9630, loss 2.35408, smoothed loss 2.43016, grad norm 4.90037, param norm 152.91248
epoch 11, iter 9635, loss 2.66997, smoothed loss 2.43355, grad norm 5.41616, param norm 152.93817
epoch 11, iter 9640, loss 2.67429, smoothed loss 2.43981, grad norm 5.82748, param norm 152.96674
epoch 11, iter 9645, loss 2.76166, smoothed loss 2.45292, grad norm 4.16292, param norm 152.99680
epoch 11, iter 9650, loss 2.82939, smoothed loss 2.45099, grad norm 4.27147, param norm 153.02585
epoch 11, iter 9655, loss 2.45908, smoothed loss 2.45297, grad norm 3.81107, param norm 153.05557
epoch 11, iter 9660, loss 2.66551, smoothed loss 2.45909, grad norm 4.75463, param norm 153.08234
epoch 11, iter 9665, loss 1.65249, smoothed loss 2.45275, grad norm 3.25818, param norm 153.10715
epoch 11, iter 9670, loss 2.26211, smoothed loss 2.45851, grad norm 3.82437, param norm 153.13403
epoch 11, iter 9675, loss 2.15381, smoothed loss 2.46544, grad norm 4.08109, param norm 153.16048
epoch 11, iter 9680, loss 2.86008, smoothed loss 2.46486, grad norm 4.58443, param norm 153.18851
epoch 11, iter 9685, loss 2.76534, smoothed loss 2.46146, grad norm 4.36803, param norm 153.21883
epoch 11, iter 9690, loss 3.17092, smoothed loss 2.46404, grad norm 5.16866, param norm 153.24812
epoch 11, iter 9695, loss 2.66508, smoothed loss 2.47285, grad norm 5.88946, param norm 153.27701
epoch 11, iter 9700, loss 2.08015, smoothed loss 2.46768, grad norm 4.18456, param norm 153.30400
epoch 11, iter 9705, loss 2.61329, smoothed loss 2.46793, grad norm 4.91351, param norm 153.33162
epoch 11, iter 9710, loss 1.90306, smoothed loss 2.46052, grad norm 4.41046, param norm 153.35907
epoch 11, iter 9715, loss 2.55783, smoothed loss 2.46094, grad norm 4.43625, param norm 153.38832
epoch 11, iter 9720, loss 3.75866, smoothed loss 2.48003, grad norm 5.04375, param norm 153.41882
epoch 11, iter 9725, loss 2.47954, smoothed loss 2.47236, grad norm 4.27748, param norm 153.44896
epoch 11, iter 9730, loss 2.30980, smoothed loss 2.46826, grad norm 4.60248, param norm 153.47740
epoch 11, iter 9735, loss 2.72978, smoothed loss 2.46645, grad norm 4.22217, param norm 153.50865
epoch 11, iter 9740, loss 2.40264, smoothed loss 2.45301, grad norm 4.85532, param norm 153.53563
epoch 11, iter 9745, loss 2.18638, smoothed loss 2.44417, grad norm 4.17771, param norm 153.56149
epoch 11, iter 9750, loss 2.18233, smoothed loss 2.45354, grad norm 3.90444, param norm 153.58777
epoch 11, iter 9755, loss 2.25155, smoothed loss 2.44641, grad norm 4.30065, param norm 153.61337
epoch 11, iter 9760, loss 2.19442, smoothed loss 2.43181, grad norm 3.92276, param norm 153.63951
Adding batches start...
Added  160  batches
epoch 11, iter 9765, loss 2.69138, smoothed loss 2.43447, grad norm 4.90298, param norm 153.66632
epoch 11, iter 9770, loss 2.63232, smoothed loss 2.43774, grad norm 4.30200, param norm 153.69345
epoch 11, iter 9775, loss 2.20031, smoothed loss 2.42519, grad norm 4.16043, param norm 153.72203
epoch 11, iter 9780, loss 1.94551, smoothed loss 2.41734, grad norm 4.33011, param norm 153.74968
epoch 11, iter 9785, loss 2.81481, smoothed loss 2.42903, grad norm 4.60604, param norm 153.77853
epoch 11, iter 9790, loss 2.62670, smoothed loss 2.42287, grad norm 4.52499, param norm 153.80774
epoch 11, iter 9795, loss 2.07197, smoothed loss 2.42796, grad norm 3.86676, param norm 153.83591
epoch 11, iter 9800, loss 2.39413, smoothed loss 2.42886, grad norm 3.99559, param norm 153.86560
epoch 11, iter 9805, loss 2.13423, smoothed loss 2.41887, grad norm 3.77513, param norm 153.89635
epoch 11, iter 9810, loss 2.90203, smoothed loss 2.42025, grad norm 4.27250, param norm 153.92694
epoch 11, iter 9815, loss 1.98163, smoothed loss 2.41690, grad norm 3.86275, param norm 153.95251
epoch 11, iter 9820, loss 2.13238, smoothed loss 2.41556, grad norm 4.43854, param norm 153.97472
epoch 11, iter 9825, loss 2.19295, smoothed loss 2.40228, grad norm 4.40732, param norm 154.00058
epoch 11, iter 9830, loss 2.69250, smoothed loss 2.40193, grad norm 5.16687, param norm 154.02826
epoch 11, iter 9835, loss 2.35911, smoothed loss 2.39745, grad norm 4.61137, param norm 154.05508
epoch 11, iter 9840, loss 2.18240, smoothed loss 2.39860, grad norm 4.07408, param norm 154.08264
epoch 11, iter 9845, loss 2.46447, smoothed loss 2.39871, grad norm 4.35154, param norm 154.11044
epoch 11, iter 9850, loss 1.95315, smoothed loss 2.40528, grad norm 4.11898, param norm 154.13873
epoch 11, iter 9855, loss 2.23507, smoothed loss 2.40846, grad norm 4.18850, param norm 154.16542
epoch 11, iter 9860, loss 2.02984, smoothed loss 2.41101, grad norm 3.74876, param norm 154.18839
epoch 11, iter 9865, loss 2.46312, smoothed loss 2.41289, grad norm 4.21230, param norm 154.21179
epoch 11, iter 9870, loss 2.53124, smoothed loss 2.41787, grad norm 4.67931, param norm 154.23524
epoch 11, iter 9875, loss 2.09442, smoothed loss 2.41211, grad norm 4.10301, param norm 154.25890
epoch 11, iter 9880, loss 2.19807, smoothed loss 2.41852, grad norm 4.69254, param norm 154.28468
epoch 11, iter 9885, loss 2.77698, smoothed loss 2.42186, grad norm 4.33673, param norm 154.31462
epoch 11, iter 9890, loss 2.35493, smoothed loss 2.42643, grad norm 4.78420, param norm 154.34669
epoch 11, iter 9895, loss 2.33913, smoothed loss 2.41408, grad norm 4.12090, param norm 154.37982
epoch 11, iter 9900, loss 2.90273, smoothed loss 2.41142, grad norm 4.91244, param norm 154.41286
epoch 11, iter 9905, loss 2.22492, smoothed loss 2.41876, grad norm 4.45522, param norm 154.44244
epoch 11, iter 9910, loss 2.63914, smoothed loss 2.43280, grad norm 4.66655, param norm 154.47061
epoch 11, iter 9915, loss 2.57642, smoothed loss 2.43494, grad norm 4.33245, param norm 154.49982
epoch 11, iter 9920, loss 3.00528, smoothed loss 2.43040, grad norm 4.96750, param norm 154.52849
Adding batches start...
Added  160  batches
epoch 11, iter 9925, loss 2.13086, smoothed loss 2.42733, grad norm 3.50455, param norm 154.55577
epoch 11, iter 9930, loss 2.20944, smoothed loss 2.43444, grad norm 4.01375, param norm 154.58298
epoch 11, iter 9935, loss 1.81694, smoothed loss 2.41561, grad norm 3.75838, param norm 154.61299
epoch 11, iter 9940, loss 2.19551, smoothed loss 2.41197, grad norm 4.79020, param norm 154.64369
epoch 11, iter 9945, loss 1.98878, smoothed loss 2.41479, grad norm 3.81946, param norm 154.67030
epoch 11, iter 9950, loss 3.08712, smoothed loss 2.42876, grad norm 5.26046, param norm 154.69572
epoch 11, iter 9955, loss 2.18325, smoothed loss 2.42336, grad norm 3.91784, param norm 154.72609
epoch 11, iter 9960, loss 2.46072, smoothed loss 2.42854, grad norm 4.40838, param norm 154.75804
epoch 11, iter 9965, loss 2.33399, smoothed loss 2.44420, grad norm 3.78703, param norm 154.78648
epoch 11, iter 9970, loss 2.57250, smoothed loss 2.44408, grad norm 4.43936, param norm 154.81410
epoch 11, iter 9975, loss 2.11582, smoothed loss 2.43031, grad norm 4.32571, param norm 154.84241
epoch 11, iter 9980, loss 2.31618, smoothed loss 2.42794, grad norm 4.13777, param norm 154.87022
epoch 11, iter 9985, loss 2.66863, smoothed loss 2.42792, grad norm 4.64275, param norm 154.89613
epoch 11, iter 9990, loss 2.39427, smoothed loss 2.42165, grad norm 4.01714, param norm 154.92368
epoch 11, iter 9995, loss 3.11970, smoothed loss 2.42987, grad norm 4.59714, param norm 154.95192
epoch 11, iter 10000, loss 2.46289, smoothed loss 2.42193, grad norm 4.32255, param norm 154.97928
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 11, Iter 10000, dev loss: 3.176952
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.50012 seconds [Score: 0.81278]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.67721 seconds [Score: 0.67100]
Epoch 11, Iter 10000, Train F1 score: 0.812779, Train EM score: 0.671000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.87567 seconds [Score: 0.63289]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.71239 seconds [Score: 0.47738]
Epoch 11, Iter 10000, Dev F1 score: 0.632894, Dev EM score: 0.477381
End of epoch 11
epoch 11, iter 10005, loss 2.55267, smoothed loss 2.40684, grad norm 4.74002, param norm 155.00763
epoch 11, iter 10010, loss 2.17090, smoothed loss 2.40470, grad norm 4.43086, param norm 155.03534
epoch 11, iter 10015, loss 2.19796, smoothed loss 2.40636, grad norm 4.14495, param norm 155.06201
epoch 11, iter 10020, loss 2.13170, smoothed loss 2.40773, grad norm 4.64813, param norm 155.08942
epoch 11, iter 10025, loss 2.09016, smoothed loss 2.40660, grad norm 4.14693, param norm 155.11615
epoch 11, iter 10030, loss 2.72563, smoothed loss 2.40231, grad norm 5.18198, param norm 155.14304
epoch 11, iter 10035, loss 2.38951, smoothed loss 2.39419, grad norm 4.34977, param norm 155.16623
epoch 11, iter 10040, loss 2.22147, smoothed loss 2.39059, grad norm 4.85225, param norm 155.19058
epoch 11, iter 10045, loss 2.83286, smoothed loss 2.39801, grad norm 4.54196, param norm 155.21759
epoch 11, iter 10050, loss 2.15057, smoothed loss 2.40061, grad norm 3.57232, param norm 155.24358
epoch 11, iter 10055, loss 1.88518, smoothed loss 2.39029, grad norm 3.93814, param norm 155.27110
epoch 11, iter 10060, loss 2.53356, smoothed loss 2.40776, grad norm 4.38666, param norm 155.29872
epoch 11, iter 10065, loss 2.19287, smoothed loss 2.40206, grad norm 4.48814, param norm 155.32295
epoch 11, iter 10070, loss 2.32175, smoothed loss 2.39624, grad norm 4.18379, param norm 155.34773
epoch 11, iter 10075, loss 2.22074, smoothed loss 2.38740, grad norm 4.02071, param norm 155.37302
epoch 11, iter 10080, loss 2.18257, smoothed loss 2.39457, grad norm 4.18117, param norm 155.40021
Adding batches start...
Added  160  batches
epoch 11, iter 10085, loss 1.75262, smoothed loss 2.38599, grad norm 4.39959, param norm 155.43057
epoch 11, iter 10090, loss 2.63086, smoothed loss 2.38939, grad norm 5.38616, param norm 155.46207
epoch 11, iter 10095, loss 1.99723, smoothed loss 2.37494, grad norm 4.51863, param norm 155.49416
epoch 11, iter 10100, loss 2.50682, smoothed loss 2.37932, grad norm 4.48231, param norm 155.52472
epoch 11, iter 10105, loss 2.46300, smoothed loss 2.38120, grad norm 4.24997, param norm 155.55269
epoch 11, iter 10110, loss 2.44982, smoothed loss 2.37420, grad norm 4.59537, param norm 155.57782
epoch 11, iter 10115, loss 1.98216, smoothed loss 2.37394, grad norm 3.76816, param norm 155.60388
epoch 11, iter 10120, loss 1.94978, smoothed loss 2.36642, grad norm 3.86668, param norm 155.63028
epoch 11, iter 10125, loss 2.26165, smoothed loss 2.36992, grad norm 3.97458, param norm 155.65488
epoch 11, iter 10130, loss 2.90094, smoothed loss 2.37581, grad norm 4.62340, param norm 155.67882
epoch 11, iter 10135, loss 2.02750, smoothed loss 2.38173, grad norm 3.89827, param norm 155.70578
epoch 11, iter 10140, loss 2.48064, smoothed loss 2.37694, grad norm 4.47696, param norm 155.73312
epoch 11, iter 10145, loss 2.62588, smoothed loss 2.38620, grad norm 4.55959, param norm 155.75975
epoch 11, iter 10150, loss 2.15587, smoothed loss 2.37731, grad norm 4.39052, param norm 155.78865
epoch 11, iter 10155, loss 2.49587, smoothed loss 2.36776, grad norm 4.55238, param norm 155.81749
epoch 11, iter 10160, loss 1.73342, smoothed loss 2.36256, grad norm 3.64061, param norm 155.84300
epoch 11, iter 10165, loss 2.38909, smoothed loss 2.36639, grad norm 4.53262, param norm 155.86812
epoch 11, iter 10170, loss 2.43737, smoothed loss 2.38025, grad norm 4.60102, param norm 155.89584
epoch 11, iter 10175, loss 1.87441, smoothed loss 2.36815, grad norm 3.97602, param norm 155.92555
epoch 11, iter 10180, loss 2.00630, smoothed loss 2.36782, grad norm 3.96783, param norm 155.95500
epoch 11, iter 10185, loss 2.56250, smoothed loss 2.36849, grad norm 4.28623, param norm 155.98528
epoch 11, iter 10190, loss 1.72180, smoothed loss 2.35610, grad norm 4.25610, param norm 156.01785
epoch 11, iter 10195, loss 2.63738, smoothed loss 2.34981, grad norm 5.00285, param norm 156.04922
epoch 11, iter 10200, loss 2.16103, smoothed loss 2.34384, grad norm 3.99806, param norm 156.07436
epoch 11, iter 10205, loss 1.87279, smoothed loss 2.33945, grad norm 4.23227, param norm 156.09865
epoch 11, iter 10210, loss 2.97635, smoothed loss 2.34725, grad norm 4.97192, param norm 156.12393
epoch 11, iter 10215, loss 2.82638, smoothed loss 2.35800, grad norm 4.54281, param norm 156.14951
epoch 11, iter 10220, loss 2.72150, smoothed loss 2.36405, grad norm 4.19039, param norm 156.17868
epoch 11, iter 10225, loss 2.96805, smoothed loss 2.38255, grad norm 4.43974, param norm 156.20862
epoch 11, iter 10230, loss 3.13391, smoothed loss 2.39292, grad norm 4.66461, param norm 156.23389
epoch 11, iter 10235, loss 2.78789, smoothed loss 2.39628, grad norm 4.87146, param norm 156.25719
epoch 11, iter 10240, loss 2.09796, smoothed loss 2.39451, grad norm 3.84949, param norm 156.27998
Adding batches start...
Added  144  batches
epoch 11, iter 10245, loss 1.89319, smoothed loss 2.38898, grad norm 3.94965, param norm 156.30588
epoch 11, iter 10250, loss 2.54919, smoothed loss 2.38605, grad norm 4.55216, param norm 156.33272
epoch 11, iter 10255, loss 2.48852, smoothed loss 2.39228, grad norm 4.80671, param norm 156.35837
epoch 11, iter 10260, loss 1.93146, smoothed loss 2.39039, grad norm 4.41853, param norm 156.38275
epoch 11, iter 10265, loss 2.69373, smoothed loss 2.39626, grad norm 4.41422, param norm 156.40768
epoch 11, iter 10270, loss 2.63510, smoothed loss 2.39967, grad norm 4.83376, param norm 156.43126
epoch 11, iter 10275, loss 2.11954, smoothed loss 2.40344, grad norm 4.68413, param norm 156.45702
epoch 11, iter 10280, loss 2.24943, smoothed loss 2.40529, grad norm 4.40675, param norm 156.48364
epoch 11, iter 10285, loss 2.14943, smoothed loss 2.40099, grad norm 4.11084, param norm 156.50938
epoch 11, iter 10290, loss 2.64826, smoothed loss 2.40389, grad norm 4.11527, param norm 156.53568
epoch 11, iter 10295, loss 2.22534, smoothed loss 2.40912, grad norm 3.57243, param norm 156.56367
epoch 11, iter 10300, loss 2.56975, smoothed loss 2.40219, grad norm 4.39381, param norm 156.59042
epoch 11, iter 10305, loss 2.38043, smoothed loss 2.40684, grad norm 4.49748, param norm 156.61522
epoch 11, iter 10310, loss 2.76056, smoothed loss 2.41943, grad norm 4.83947, param norm 156.64032
epoch 11, iter 10315, loss 2.29539, smoothed loss 2.41939, grad norm 5.06284, param norm 156.66682
epoch 11, iter 10320, loss 1.99639, smoothed loss 2.41477, grad norm 3.75164, param norm 156.69807
epoch 11, iter 10325, loss 2.58372, smoothed loss 2.41386, grad norm 3.95502, param norm 156.72899
epoch 11, iter 10330, loss 2.38192, smoothed loss 2.42236, grad norm 3.69177, param norm 156.75618
epoch 11, iter 10335, loss 1.68994, smoothed loss 2.41211, grad norm 3.73250, param norm 156.78067
epoch 11, iter 10340, loss 2.26428, smoothed loss 2.41647, grad norm 4.77461, param norm 156.80406
epoch 11, iter 10345, loss 2.25517, smoothed loss 2.40455, grad norm 4.42358, param norm 156.83278
epoch 11, iter 10350, loss 2.84530, smoothed loss 2.41186, grad norm 4.67740, param norm 156.86285
epoch 11, iter 10355, loss 2.62279, smoothed loss 2.40932, grad norm 4.79566, param norm 156.89056
epoch 11, iter 10360, loss 2.26512, smoothed loss 2.40780, grad norm 4.33444, param norm 156.91794
epoch 11, iter 10365, loss 2.21321, smoothed loss 2.39750, grad norm 4.40712, param norm 156.94603
epoch 11, iter 10370, loss 2.33170, smoothed loss 2.39583, grad norm 4.33545, param norm 156.97092
epoch 11, iter 10375, loss 2.35357, smoothed loss 2.39744, grad norm 5.11288, param norm 156.99390
epoch 11, iter 10380, loss 2.42792, smoothed loss 2.40131, grad norm 4.06431, param norm 157.01726
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 12, iter 10385, loss 2.64416, smoothed loss 2.38892, grad norm 4.78548, param norm 157.04350
epoch 12, iter 10390, loss 1.79919, smoothed loss 2.38642, grad norm 3.77147, param norm 157.07179
epoch 12, iter 10395, loss 2.95609, smoothed loss 2.37960, grad norm 5.19906, param norm 157.09883
epoch 12, iter 10400, loss 1.74481, smoothed loss 2.37962, grad norm 3.64969, param norm 157.12544
epoch 12, iter 10405, loss 1.80127, smoothed loss 2.38030, grad norm 3.23838, param norm 157.15276
epoch 12, iter 10410, loss 2.44089, smoothed loss 2.37421, grad norm 4.42864, param norm 157.18105
epoch 12, iter 10415, loss 2.42964, smoothed loss 2.37789, grad norm 4.52318, param norm 157.20563
epoch 12, iter 10420, loss 2.21942, smoothed loss 2.37183, grad norm 4.05214, param norm 157.23146
epoch 12, iter 10425, loss 2.59298, smoothed loss 2.38052, grad norm 4.09855, param norm 157.25699
epoch 12, iter 10430, loss 1.78417, smoothed loss 2.37368, grad norm 4.12127, param norm 157.28101
epoch 12, iter 10435, loss 2.23463, smoothed loss 2.37401, grad norm 4.78936, param norm 157.30533
epoch 12, iter 10440, loss 2.66533, smoothed loss 2.38331, grad norm 5.00859, param norm 157.32928
epoch 12, iter 10445, loss 2.51175, smoothed loss 2.39953, grad norm 4.31294, param norm 157.35385
epoch 12, iter 10450, loss 2.56738, smoothed loss 2.40231, grad norm 4.23895, param norm 157.38042
epoch 12, iter 10455, loss 2.38414, smoothed loss 2.38853, grad norm 4.65525, param norm 157.40768
epoch 12, iter 10460, loss 2.77537, smoothed loss 2.40234, grad norm 4.32321, param norm 157.43448
epoch 12, iter 10465, loss 2.28246, smoothed loss 2.40340, grad norm 4.59525, param norm 157.46031
epoch 12, iter 10470, loss 2.76264, smoothed loss 2.40867, grad norm 4.94568, param norm 157.48825
epoch 12, iter 10475, loss 2.55407, smoothed loss 2.41353, grad norm 4.05805, param norm 157.51625
epoch 12, iter 10480, loss 2.35471, smoothed loss 2.42017, grad norm 4.63235, param norm 157.54413
epoch 12, iter 10485, loss 2.02630, smoothed loss 2.40429, grad norm 4.01667, param norm 157.57225
epoch 12, iter 10490, loss 2.36194, smoothed loss 2.40099, grad norm 4.21576, param norm 157.59895
epoch 12, iter 10495, loss 2.15277, smoothed loss 2.39174, grad norm 4.38511, param norm 157.62730
epoch 12, iter 10500, loss 2.80557, smoothed loss 2.40473, grad norm 5.43812, param norm 157.65492
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 12, Iter 10500, dev loss: 3.211546
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.70514 seconds [Score: 0.81946]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.91945 seconds [Score: 0.71600]
Epoch 12, Iter 10500, Train F1 score: 0.819463, Train EM score: 0.716000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.01384 seconds [Score: 0.63531]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.91115 seconds [Score: 0.47794]
Epoch 12, Iter 10500, Dev F1 score: 0.635312, Dev EM score: 0.477943
End of epoch 12
epoch 12, iter 10505, loss 2.15233, smoothed loss 2.39175, grad norm 4.88700, param norm 157.68053
epoch 12, iter 10510, loss 2.91123, smoothed loss 2.40489, grad norm 5.48179, param norm 157.70734
epoch 12, iter 10515, loss 2.22267, smoothed loss 2.39423, grad norm 4.03717, param norm 157.73242
epoch 12, iter 10520, loss 2.60170, smoothed loss 2.39858, grad norm 4.41318, param norm 157.75569
epoch 12, iter 10525, loss 1.72406, smoothed loss 2.38477, grad norm 4.18384, param norm 157.78140
epoch 12, iter 10530, loss 2.70109, smoothed loss 2.38856, grad norm 4.75855, param norm 157.80690
epoch 12, iter 10535, loss 2.43435, smoothed loss 2.39642, grad norm 4.82092, param norm 157.83186
epoch 12, iter 10540, loss 2.23310, smoothed loss 2.39901, grad norm 4.85776, param norm 157.85725
Adding batches start...
Added  160  batches
epoch 12, iter 10545, loss 2.80349, smoothed loss 2.40352, grad norm 5.10300, param norm 157.88396
epoch 12, iter 10550, loss 2.95495, smoothed loss 2.40980, grad norm 5.28444, param norm 157.91118
epoch 12, iter 10555, loss 2.39171, smoothed loss 2.41686, grad norm 4.37760, param norm 157.93822
epoch 12, iter 10560, loss 2.46566, smoothed loss 2.40912, grad norm 4.94141, param norm 157.96294
epoch 12, iter 10565, loss 2.77545, smoothed loss 2.42360, grad norm 4.65772, param norm 157.98509
epoch 12, iter 10570, loss 2.57065, smoothed loss 2.42776, grad norm 4.47059, param norm 158.00685
epoch 12, iter 10575, loss 1.94638, smoothed loss 2.41679, grad norm 3.95015, param norm 158.03473
epoch 12, iter 10580, loss 2.00807, smoothed loss 2.40928, grad norm 4.07342, param norm 158.06290
epoch 12, iter 10585, loss 1.99877, smoothed loss 2.40941, grad norm 4.35106, param norm 158.09019
epoch 12, iter 10590, loss 1.85472, smoothed loss 2.39865, grad norm 3.78827, param norm 158.11562
epoch 12, iter 10595, loss 2.52457, smoothed loss 2.40171, grad norm 4.89427, param norm 158.14114
epoch 12, iter 10600, loss 2.17599, smoothed loss 2.39127, grad norm 4.07648, param norm 158.16856
epoch 12, iter 10605, loss 2.54658, smoothed loss 2.38669, grad norm 5.53069, param norm 158.19737
epoch 12, iter 10610, loss 2.48263, smoothed loss 2.37818, grad norm 4.15929, param norm 158.22562
epoch 12, iter 10615, loss 2.00257, smoothed loss 2.36650, grad norm 3.85556, param norm 158.25174
epoch 12, iter 10620, loss 2.69355, smoothed loss 2.36751, grad norm 4.71843, param norm 158.27785
epoch 12, iter 10625, loss 1.98346, smoothed loss 2.36993, grad norm 4.14478, param norm 158.30203
epoch 12, iter 10630, loss 2.66913, smoothed loss 2.36212, grad norm 4.53657, param norm 158.32719
epoch 12, iter 10635, loss 2.15180, smoothed loss 2.35960, grad norm 4.21954, param norm 158.35300
epoch 12, iter 10640, loss 2.34509, smoothed loss 2.35196, grad norm 4.55482, param norm 158.37932
epoch 12, iter 10645, loss 2.81323, smoothed loss 2.35526, grad norm 5.50275, param norm 158.40453
epoch 12, iter 10650, loss 2.27818, smoothed loss 2.35504, grad norm 4.32727, param norm 158.42903
epoch 12, iter 10655, loss 2.07059, smoothed loss 2.35689, grad norm 4.00266, param norm 158.45647
epoch 12, iter 10660, loss 2.41708, smoothed loss 2.34725, grad norm 5.14192, param norm 158.48280
epoch 12, iter 10665, loss 1.87136, smoothed loss 2.35366, grad norm 3.90635, param norm 158.50711
epoch 12, iter 10670, loss 1.91123, smoothed loss 2.34766, grad norm 4.23804, param norm 158.53160
epoch 12, iter 10675, loss 2.70804, smoothed loss 2.35795, grad norm 5.04625, param norm 158.55608
epoch 12, iter 10680, loss 1.89510, smoothed loss 2.34381, grad norm 4.08428, param norm 158.58223
epoch 12, iter 10685, loss 2.13547, smoothed loss 2.35546, grad norm 4.42756, param norm 158.60693
epoch 12, iter 10690, loss 2.30461, smoothed loss 2.36507, grad norm 4.34842, param norm 158.63167
epoch 12, iter 10695, loss 2.01443, smoothed loss 2.35729, grad norm 4.50222, param norm 158.65660
epoch 12, iter 10700, loss 2.35710, smoothed loss 2.36284, grad norm 4.28076, param norm 158.68274
Adding batches start...
Added  160  batches
epoch 12, iter 10705, loss 2.34777, smoothed loss 2.37378, grad norm 4.13221, param norm 158.70750
epoch 12, iter 10710, loss 2.59078, smoothed loss 2.37840, grad norm 4.22329, param norm 158.73326
epoch 12, iter 10715, loss 2.59938, smoothed loss 2.37550, grad norm 4.58481, param norm 158.75864
epoch 12, iter 10720, loss 1.98068, smoothed loss 2.37554, grad norm 5.00140, param norm 158.78162
epoch 12, iter 10725, loss 2.67694, smoothed loss 2.36642, grad norm 5.94621, param norm 158.80791
epoch 12, iter 10730, loss 2.14981, smoothed loss 2.37194, grad norm 4.70388, param norm 158.83278
epoch 12, iter 10735, loss 2.77557, smoothed loss 2.38375, grad norm 5.25083, param norm 158.85892
epoch 12, iter 10740, loss 2.39276, smoothed loss 2.37945, grad norm 4.38333, param norm 158.88771
epoch 12, iter 10745, loss 2.05685, smoothed loss 2.37340, grad norm 4.08221, param norm 158.92043
epoch 12, iter 10750, loss 2.58573, smoothed loss 2.38504, grad norm 4.74369, param norm 158.94714
epoch 12, iter 10755, loss 2.54807, smoothed loss 2.38136, grad norm 4.31966, param norm 158.97015
epoch 12, iter 10760, loss 2.09732, smoothed loss 2.37441, grad norm 4.32006, param norm 158.99197
epoch 12, iter 10765, loss 1.92951, smoothed loss 2.36112, grad norm 4.30813, param norm 159.01314
epoch 12, iter 10770, loss 1.76890, smoothed loss 2.36008, grad norm 4.26804, param norm 159.03860
epoch 12, iter 10775, loss 2.31406, smoothed loss 2.35494, grad norm 4.29949, param norm 159.06903
epoch 12, iter 10780, loss 2.39896, smoothed loss 2.35109, grad norm 4.63637, param norm 159.09767
epoch 12, iter 10785, loss 2.14678, smoothed loss 2.34522, grad norm 4.29707, param norm 159.12708
epoch 12, iter 10790, loss 2.35132, smoothed loss 2.33992, grad norm 5.16126, param norm 159.15445
epoch 12, iter 10795, loss 2.79427, smoothed loss 2.36045, grad norm 4.67723, param norm 159.18105
epoch 12, iter 10800, loss 2.78966, smoothed loss 2.36038, grad norm 4.42289, param norm 159.20793
epoch 12, iter 10805, loss 3.37203, smoothed loss 2.37166, grad norm 5.80436, param norm 159.23367
epoch 12, iter 10810, loss 2.70043, smoothed loss 2.38070, grad norm 4.94380, param norm 159.25906
epoch 12, iter 10815, loss 2.29295, smoothed loss 2.37889, grad norm 4.19235, param norm 159.28529
epoch 12, iter 10820, loss 2.49130, smoothed loss 2.37723, grad norm 4.03303, param norm 159.31100
epoch 12, iter 10825, loss 2.58355, smoothed loss 2.37124, grad norm 5.49174, param norm 159.33591
epoch 12, iter 10830, loss 2.37018, smoothed loss 2.37114, grad norm 4.55990, param norm 159.35924
epoch 12, iter 10835, loss 2.50283, smoothed loss 2.35904, grad norm 4.82194, param norm 159.38231
epoch 12, iter 10840, loss 2.37698, smoothed loss 2.36146, grad norm 4.43626, param norm 159.40431
epoch 12, iter 10845, loss 3.32006, smoothed loss 2.38209, grad norm 5.30315, param norm 159.42912
epoch 12, iter 10850, loss 2.49222, smoothed loss 2.38944, grad norm 4.70156, param norm 159.45438
epoch 12, iter 10855, loss 2.77909, smoothed loss 2.38386, grad norm 4.48782, param norm 159.48303
epoch 12, iter 10860, loss 1.93531, smoothed loss 2.37479, grad norm 3.98289, param norm 159.51068
Adding batches start...
Added  160  batches
epoch 12, iter 10865, loss 2.38414, smoothed loss 2.38272, grad norm 5.10995, param norm 159.53514
epoch 12, iter 10870, loss 1.56225, smoothed loss 2.36118, grad norm 3.36284, param norm 159.55882
epoch 12, iter 10875, loss 2.51034, smoothed loss 2.36755, grad norm 4.29379, param norm 159.58568
epoch 12, iter 10880, loss 2.26323, smoothed loss 2.35448, grad norm 4.68698, param norm 159.61407
epoch 12, iter 10885, loss 2.17153, smoothed loss 2.35680, grad norm 4.19248, param norm 159.64346
epoch 12, iter 10890, loss 2.78939, smoothed loss 2.35710, grad norm 5.17225, param norm 159.67264
epoch 12, iter 10895, loss 2.82214, smoothed loss 2.35404, grad norm 4.63266, param norm 159.70128
epoch 12, iter 10900, loss 2.03187, smoothed loss 2.33442, grad norm 4.16683, param norm 159.73048
epoch 12, iter 10905, loss 2.72588, smoothed loss 2.33538, grad norm 4.66893, param norm 159.75992
epoch 12, iter 10910, loss 3.29461, smoothed loss 2.33973, grad norm 5.24057, param norm 159.78633
epoch 12, iter 10915, loss 2.27361, smoothed loss 2.34191, grad norm 4.70526, param norm 159.81129
epoch 12, iter 10920, loss 2.49871, smoothed loss 2.35109, grad norm 5.05852, param norm 159.83611
epoch 12, iter 10925, loss 2.73487, smoothed loss 2.35499, grad norm 5.23715, param norm 159.86118
epoch 12, iter 10930, loss 3.07584, smoothed loss 2.35631, grad norm 5.19382, param norm 159.89130
epoch 12, iter 10935, loss 2.34454, smoothed loss 2.35620, grad norm 4.40410, param norm 159.92137
epoch 12, iter 10940, loss 2.18671, smoothed loss 2.35379, grad norm 4.88512, param norm 159.95335
epoch 12, iter 10945, loss 2.31074, smoothed loss 2.36616, grad norm 4.20609, param norm 159.98236
epoch 12, iter 10950, loss 2.76081, smoothed loss 2.36629, grad norm 4.86275, param norm 160.00845
epoch 12, iter 10955, loss 2.87446, smoothed loss 2.37106, grad norm 4.75487, param norm 160.03322
epoch 12, iter 10960, loss 2.64435, smoothed loss 2.36989, grad norm 4.89274, param norm 160.05716
epoch 12, iter 10965, loss 3.05515, smoothed loss 2.38038, grad norm 4.80572, param norm 160.08055
epoch 12, iter 10970, loss 2.08117, smoothed loss 2.37447, grad norm 4.23439, param norm 160.10373
epoch 12, iter 10975, loss 2.86140, smoothed loss 2.37916, grad norm 4.63546, param norm 160.12996
epoch 12, iter 10980, loss 2.44802, smoothed loss 2.36490, grad norm 4.30991, param norm 160.15451
epoch 12, iter 10985, loss 2.30361, smoothed loss 2.37387, grad norm 4.78780, param norm 160.17850
epoch 12, iter 10990, loss 1.96619, smoothed loss 2.38370, grad norm 3.40792, param norm 160.20325
epoch 12, iter 10995, loss 2.69506, smoothed loss 2.38141, grad norm 4.69132, param norm 160.22948
epoch 12, iter 11000, loss 1.58493, smoothed loss 2.36554, grad norm 3.45374, param norm 160.25792
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 12, Iter 11000, dev loss: 3.163278
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.80911 seconds [Score: 0.80240]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.75124 seconds [Score: 0.69000]
Epoch 12, Iter 11000, Train F1 score: 0.802402, Train EM score: 0.690000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.77767 seconds [Score: 0.63941]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.67251 seconds [Score: 0.48117]
Epoch 12, Iter 11000, Dev F1 score: 0.639414, Dev EM score: 0.481174
End of epoch 12
epoch 12, iter 11005, loss 2.47831, smoothed loss 2.37099, grad norm 4.60808, param norm 160.28551
epoch 12, iter 11010, loss 2.55391, smoothed loss 2.37544, grad norm 4.90730, param norm 160.31219
epoch 12, iter 11015, loss 2.44524, smoothed loss 2.36351, grad norm 5.16504, param norm 160.34003
epoch 12, iter 11020, loss 2.59908, smoothed loss 2.36528, grad norm 4.53111, param norm 160.36699
Adding batches start...
Added  160  batches
epoch 12, iter 11025, loss 2.38207, smoothed loss 2.36028, grad norm 4.71287, param norm 160.39214
epoch 12, iter 11030, loss 2.53907, smoothed loss 2.35822, grad norm 4.76417, param norm 160.41759
epoch 12, iter 11035, loss 3.03761, smoothed loss 2.35928, grad norm 5.27910, param norm 160.44374
epoch 12, iter 11040, loss 2.93921, smoothed loss 2.36271, grad norm 6.11977, param norm 160.46913
epoch 12, iter 11045, loss 2.92934, smoothed loss 2.36709, grad norm 4.75217, param norm 160.49461
epoch 12, iter 11050, loss 2.18436, smoothed loss 2.35475, grad norm 4.08119, param norm 160.52074
epoch 12, iter 11055, loss 1.99094, smoothed loss 2.35604, grad norm 3.75539, param norm 160.54802
epoch 12, iter 11060, loss 1.76204, smoothed loss 2.35528, grad norm 4.36155, param norm 160.57242
epoch 12, iter 11065, loss 2.12319, smoothed loss 2.35228, grad norm 5.03128, param norm 160.59645
epoch 12, iter 11070, loss 2.70303, smoothed loss 2.35901, grad norm 4.82913, param norm 160.62440
epoch 12, iter 11075, loss 2.56504, smoothed loss 2.36328, grad norm 5.38171, param norm 160.64980
epoch 12, iter 11080, loss 2.44151, smoothed loss 2.37771, grad norm 4.46934, param norm 160.67325
epoch 12, iter 11085, loss 2.40235, smoothed loss 2.37501, grad norm 3.94249, param norm 160.69772
epoch 12, iter 11090, loss 2.43049, smoothed loss 2.38015, grad norm 4.35121, param norm 160.72081
epoch 12, iter 11095, loss 2.15558, smoothed loss 2.39366, grad norm 5.02711, param norm 160.74696
epoch 12, iter 11100, loss 3.10441, smoothed loss 2.39340, grad norm 5.66060, param norm 160.77266
epoch 12, iter 11105, loss 2.80243, smoothed loss 2.38276, grad norm 4.24077, param norm 160.79753
epoch 12, iter 11110, loss 2.29155, smoothed loss 2.36748, grad norm 4.02706, param norm 160.82173
epoch 12, iter 11115, loss 2.32178, smoothed loss 2.35968, grad norm 4.86460, param norm 160.84740
epoch 12, iter 11120, loss 2.94148, smoothed loss 2.35750, grad norm 5.05572, param norm 160.87358
epoch 12, iter 11125, loss 2.23957, smoothed loss 2.36408, grad norm 5.45619, param norm 160.89748
epoch 12, iter 11130, loss 1.99893, smoothed loss 2.35697, grad norm 4.00367, param norm 160.92337
epoch 12, iter 11135, loss 2.23995, smoothed loss 2.36444, grad norm 4.62957, param norm 160.94968
epoch 12, iter 11140, loss 2.14634, smoothed loss 2.36668, grad norm 4.49086, param norm 160.97514
epoch 12, iter 11145, loss 3.60822, smoothed loss 2.35570, grad norm 5.36650, param norm 161.00116
epoch 12, iter 11150, loss 2.27241, smoothed loss 2.35258, grad norm 4.58552, param norm 161.02554
epoch 12, iter 11155, loss 2.17194, smoothed loss 2.36180, grad norm 4.87274, param norm 161.05086
epoch 12, iter 11160, loss 2.67512, smoothed loss 2.35832, grad norm 4.87874, param norm 161.07478
epoch 12, iter 11165, loss 1.92601, smoothed loss 2.34856, grad norm 4.13135, param norm 161.10001
epoch 12, iter 11170, loss 2.25008, smoothed loss 2.34431, grad norm 5.08087, param norm 161.12442
epoch 12, iter 11175, loss 2.75017, smoothed loss 2.33534, grad norm 5.46781, param norm 161.14854
epoch 12, iter 11180, loss 2.81479, smoothed loss 2.32611, grad norm 4.91102, param norm 161.17409
Adding batches start...
Added  144  batches
epoch 12, iter 11185, loss 2.20647, smoothed loss 2.31985, grad norm 4.66884, param norm 161.20003
epoch 12, iter 11190, loss 2.12455, smoothed loss 2.32309, grad norm 4.36197, param norm 161.22395
epoch 12, iter 11195, loss 2.58563, smoothed loss 2.32611, grad norm 4.80799, param norm 161.24838
epoch 12, iter 11200, loss 3.05719, smoothed loss 2.32991, grad norm 5.52314, param norm 161.27478
epoch 12, iter 11205, loss 2.60491, smoothed loss 2.32630, grad norm 4.48010, param norm 161.30315
epoch 12, iter 11210, loss 2.33751, smoothed loss 2.32032, grad norm 4.59922, param norm 161.33202
epoch 12, iter 11215, loss 1.84047, smoothed loss 2.31755, grad norm 4.12600, param norm 161.35919
epoch 12, iter 11220, loss 2.36479, smoothed loss 2.31731, grad norm 5.23659, param norm 161.38486
epoch 12, iter 11225, loss 2.81499, smoothed loss 2.32733, grad norm 4.65631, param norm 161.41307
epoch 12, iter 11230, loss 2.49277, smoothed loss 2.32442, grad norm 4.68979, param norm 161.44128
epoch 12, iter 11235, loss 2.34259, smoothed loss 2.32969, grad norm 4.52502, param norm 161.46931
epoch 12, iter 11240, loss 2.51823, smoothed loss 2.32859, grad norm 4.87859, param norm 161.49617
epoch 12, iter 11245, loss 2.91049, smoothed loss 2.33096, grad norm 4.68984, param norm 161.52229
epoch 12, iter 11250, loss 2.02023, smoothed loss 2.33490, grad norm 4.16585, param norm 161.54373
epoch 12, iter 11255, loss 2.02046, smoothed loss 2.33310, grad norm 3.63168, param norm 161.56812
epoch 12, iter 11260, loss 2.61803, smoothed loss 2.33328, grad norm 4.78093, param norm 161.59273
epoch 12, iter 11265, loss 1.36577, smoothed loss 2.31322, grad norm 3.78016, param norm 161.61949
epoch 12, iter 11270, loss 2.18848, smoothed loss 2.30286, grad norm 4.57117, param norm 161.64536
epoch 12, iter 11275, loss 2.52849, smoothed loss 2.30425, grad norm 5.23912, param norm 161.66827
epoch 12, iter 11280, loss 2.20517, smoothed loss 2.29831, grad norm 4.50111, param norm 161.69211
epoch 12, iter 11285, loss 2.36005, smoothed loss 2.30313, grad norm 4.55999, param norm 161.71594
epoch 12, iter 11290, loss 2.12119, smoothed loss 2.30904, grad norm 4.51211, param norm 161.74318
epoch 12, iter 11295, loss 2.55363, smoothed loss 2.30803, grad norm 4.26539, param norm 161.76999
epoch 12, iter 11300, loss 2.02785, smoothed loss 2.29406, grad norm 4.01499, param norm 161.79568
epoch 12, iter 11305, loss 2.53246, smoothed loss 2.28983, grad norm 4.58400, param norm 161.82431
epoch 12, iter 11310, loss 2.56684, smoothed loss 2.30620, grad norm 4.42901, param norm 161.84813
epoch 12, iter 11315, loss 2.71029, smoothed loss 2.31811, grad norm 4.86461, param norm 161.87148
epoch 12, iter 11320, loss 2.89875, smoothed loss 2.31660, grad norm 5.03962, param norm 161.89545
epoch 12, iter 11325, loss 2.10876, smoothed loss 2.32517, grad norm 4.50548, param norm 161.92018
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 13, iter 11330, loss 2.16437, smoothed loss 2.33065, grad norm 4.97980, param norm 161.94705
epoch 13, iter 11335, loss 2.56709, smoothed loss 2.34788, grad norm 4.67270, param norm 161.97340
epoch 13, iter 11340, loss 2.33742, smoothed loss 2.34968, grad norm 4.58357, param norm 162.00002
epoch 13, iter 11345, loss 2.64532, smoothed loss 2.35068, grad norm 4.76452, param norm 162.02902
epoch 13, iter 11350, loss 1.96667, smoothed loss 2.34644, grad norm 4.18842, param norm 162.05597
epoch 13, iter 11355, loss 2.01367, smoothed loss 2.33448, grad norm 4.98757, param norm 162.08279
epoch 13, iter 11360, loss 2.37339, smoothed loss 2.34606, grad norm 4.47583, param norm 162.10962
epoch 13, iter 11365, loss 2.21458, smoothed loss 2.34678, grad norm 4.11656, param norm 162.13417
epoch 13, iter 11370, loss 2.56181, smoothed loss 2.36114, grad norm 4.06352, param norm 162.15865
epoch 13, iter 11375, loss 2.22219, smoothed loss 2.35964, grad norm 4.32441, param norm 162.18314
epoch 13, iter 11380, loss 2.49297, smoothed loss 2.34972, grad norm 4.47488, param norm 162.20651
epoch 13, iter 11385, loss 1.65321, smoothed loss 2.32099, grad norm 3.30195, param norm 162.22992
epoch 13, iter 11390, loss 2.52672, smoothed loss 2.33685, grad norm 5.23149, param norm 162.25500
epoch 13, iter 11395, loss 2.11316, smoothed loss 2.33392, grad norm 3.80477, param norm 162.27966
epoch 13, iter 11400, loss 2.70922, smoothed loss 2.33620, grad norm 4.70607, param norm 162.30681
epoch 13, iter 11405, loss 2.80610, smoothed loss 2.34298, grad norm 4.76463, param norm 162.33429
epoch 13, iter 11410, loss 2.18837, smoothed loss 2.33688, grad norm 4.15117, param norm 162.35889
epoch 13, iter 11415, loss 2.77067, smoothed loss 2.34139, grad norm 5.41573, param norm 162.38132
epoch 13, iter 11420, loss 2.10222, smoothed loss 2.33950, grad norm 4.30243, param norm 162.40477
epoch 13, iter 11425, loss 2.69686, smoothed loss 2.34574, grad norm 4.68544, param norm 162.42938
epoch 13, iter 11430, loss 2.11019, smoothed loss 2.34383, grad norm 3.90545, param norm 162.45303
epoch 13, iter 11435, loss 3.05574, smoothed loss 2.34179, grad norm 5.68253, param norm 162.47566
epoch 13, iter 11440, loss 1.99320, smoothed loss 2.33654, grad norm 4.75893, param norm 162.50032
epoch 13, iter 11445, loss 2.24120, smoothed loss 2.32771, grad norm 4.07974, param norm 162.52461
epoch 13, iter 11450, loss 1.66985, smoothed loss 2.33218, grad norm 4.20385, param norm 162.55029
epoch 13, iter 11455, loss 2.08766, smoothed loss 2.32293, grad norm 4.56431, param norm 162.57722
epoch 13, iter 11460, loss 2.57802, smoothed loss 2.32498, grad norm 5.14295, param norm 162.60521
epoch 13, iter 11465, loss 2.03000, smoothed loss 2.32760, grad norm 4.05580, param norm 162.63130
epoch 13, iter 11470, loss 1.68764, smoothed loss 2.31546, grad norm 3.42230, param norm 162.65709
epoch 13, iter 11475, loss 2.81546, smoothed loss 2.32675, grad norm 5.51884, param norm 162.68063
epoch 13, iter 11480, loss 2.51619, smoothed loss 2.33139, grad norm 4.21168, param norm 162.70389
epoch 13, iter 11485, loss 2.64157, smoothed loss 2.34562, grad norm 4.40183, param norm 162.72667
Adding batches start...
Added  160  batches
epoch 13, iter 11490, loss 1.83418, smoothed loss 2.34778, grad norm 3.67323, param norm 162.75117
epoch 13, iter 11495, loss 2.00465, smoothed loss 2.33917, grad norm 3.98294, param norm 162.77856
epoch 13, iter 11500, loss 2.31857, smoothed loss 2.34131, grad norm 4.01137, param norm 162.80418
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 13, Iter 11500, dev loss: 3.160855
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.01266 seconds [Score: 0.84834]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.79901 seconds [Score: 0.71400]
Epoch 13, Iter 11500, Train F1 score: 0.848339, Train EM score: 0.714000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 118.71621 seconds [Score: 0.63534]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 118.34082 seconds [Score: 0.47851]
Epoch 13, Iter 11500, Dev F1 score: 0.635341, Dev EM score: 0.478505
End of epoch 13
epoch 13, iter 11505, loss 1.87254, smoothed loss 2.32720, grad norm 3.43903, param norm 162.82710
epoch 13, iter 11510, loss 2.05961, smoothed loss 2.33472, grad norm 4.37775, param norm 162.85059
epoch 13, iter 11515, loss 2.18823, smoothed loss 2.32851, grad norm 4.33519, param norm 162.87531
epoch 13, iter 11520, loss 2.91638, smoothed loss 2.32622, grad norm 5.29245, param norm 162.89941
epoch 13, iter 11525, loss 2.23305, smoothed loss 2.32306, grad norm 4.36579, param norm 162.92392
epoch 13, iter 11530, loss 2.52957, smoothed loss 2.33433, grad norm 5.32692, param norm 162.94786
epoch 13, iter 11535, loss 2.57233, smoothed loss 2.34334, grad norm 4.86782, param norm 162.97392
epoch 13, iter 11540, loss 2.18247, smoothed loss 2.34242, grad norm 4.35830, param norm 162.99782
epoch 13, iter 11545, loss 2.60000, smoothed loss 2.34547, grad norm 4.60521, param norm 163.02031
epoch 13, iter 11550, loss 2.07107, smoothed loss 2.33905, grad norm 4.55665, param norm 163.04300
epoch 13, iter 11555, loss 1.87438, smoothed loss 2.32701, grad norm 3.80767, param norm 163.06621
epoch 13, iter 11560, loss 2.07962, smoothed loss 2.31850, grad norm 4.12901, param norm 163.09013
epoch 13, iter 11565, loss 2.33322, smoothed loss 2.31020, grad norm 4.76019, param norm 163.11575
epoch 13, iter 11570, loss 1.92854, smoothed loss 2.30208, grad norm 3.72651, param norm 163.14256
epoch 13, iter 11575, loss 2.93632, smoothed loss 2.30343, grad norm 4.75449, param norm 163.16573
epoch 13, iter 11580, loss 2.10973, smoothed loss 2.31044, grad norm 4.42290, param norm 163.18719
epoch 13, iter 11585, loss 2.75154, smoothed loss 2.32440, grad norm 5.30289, param norm 163.21278
epoch 13, iter 11590, loss 2.01292, smoothed loss 2.31383, grad norm 4.30845, param norm 163.23991
epoch 13, iter 11595, loss 2.08724, smoothed loss 2.31319, grad norm 4.28389, param norm 163.26801
epoch 13, iter 11600, loss 3.20325, smoothed loss 2.31917, grad norm 5.63902, param norm 163.29715
epoch 13, iter 11605, loss 2.45050, smoothed loss 2.30938, grad norm 5.12314, param norm 163.32512
epoch 13, iter 11610, loss 2.77222, smoothed loss 2.32772, grad norm 5.03185, param norm 163.34996
epoch 13, iter 11615, loss 2.19009, smoothed loss 2.33930, grad norm 5.39009, param norm 163.37471
epoch 13, iter 11620, loss 2.08351, smoothed loss 2.34034, grad norm 4.84180, param norm 163.40157
epoch 13, iter 11625, loss 2.24432, smoothed loss 2.34123, grad norm 4.40566, param norm 163.42883
epoch 13, iter 11630, loss 2.29931, smoothed loss 2.33513, grad norm 4.81839, param norm 163.45728
epoch 13, iter 11635, loss 2.76267, smoothed loss 2.33679, grad norm 4.19236, param norm 163.48695
epoch 13, iter 11640, loss 2.35511, smoothed loss 2.33190, grad norm 4.90702, param norm 163.51375
epoch 13, iter 11645, loss 2.68006, smoothed loss 2.33335, grad norm 4.94229, param norm 163.53955
Adding batches start...
Added  160  batches
epoch 13, iter 11650, loss 3.15452, smoothed loss 2.33745, grad norm 5.21656, param norm 163.56596
epoch 13, iter 11655, loss 3.04707, smoothed loss 2.34944, grad norm 4.71706, param norm 163.59172
epoch 13, iter 11660, loss 1.87405, smoothed loss 2.33925, grad norm 3.54573, param norm 163.61494
epoch 13, iter 11665, loss 2.71564, smoothed loss 2.34616, grad norm 4.56899, param norm 163.63663
epoch 13, iter 11670, loss 2.39100, smoothed loss 2.35513, grad norm 4.63293, param norm 163.66006
epoch 13, iter 11675, loss 2.82096, smoothed loss 2.35997, grad norm 4.58083, param norm 163.68492
epoch 13, iter 11680, loss 2.63424, smoothed loss 2.35552, grad norm 4.02939, param norm 163.70946
epoch 13, iter 11685, loss 2.83562, smoothed loss 2.35814, grad norm 5.16404, param norm 163.73405
epoch 13, iter 11690, loss 3.35070, smoothed loss 2.36401, grad norm 6.24543, param norm 163.75688
epoch 13, iter 11695, loss 2.42371, smoothed loss 2.35894, grad norm 3.95197, param norm 163.78253
epoch 13, iter 11700, loss 2.78124, smoothed loss 2.36947, grad norm 4.40978, param norm 163.80934
epoch 13, iter 11705, loss 2.32146, smoothed loss 2.36609, grad norm 4.49359, param norm 163.83498
epoch 13, iter 11710, loss 1.96821, smoothed loss 2.35923, grad norm 4.20562, param norm 163.85924
epoch 13, iter 11715, loss 2.13215, smoothed loss 2.35791, grad norm 4.10748, param norm 163.88068
epoch 13, iter 11720, loss 2.24786, smoothed loss 2.36478, grad norm 4.08471, param norm 163.90393
epoch 13, iter 11725, loss 1.97225, smoothed loss 2.35543, grad norm 3.83063, param norm 163.92763
epoch 13, iter 11730, loss 2.42690, smoothed loss 2.35626, grad norm 4.37709, param norm 163.95166
epoch 13, iter 11735, loss 2.83503, smoothed loss 2.35804, grad norm 4.49797, param norm 163.97647
epoch 13, iter 11740, loss 1.89246, smoothed loss 2.34150, grad norm 4.45096, param norm 164.00252
epoch 13, iter 11745, loss 2.07578, smoothed loss 2.33325, grad norm 4.27671, param norm 164.02672
epoch 13, iter 11750, loss 2.10059, smoothed loss 2.31827, grad norm 4.22752, param norm 164.04915
epoch 13, iter 11755, loss 2.92708, smoothed loss 2.32420, grad norm 4.90574, param norm 164.07057
epoch 13, iter 11760, loss 2.18969, smoothed loss 2.32862, grad norm 4.36422, param norm 164.09166
epoch 13, iter 11765, loss 2.62667, smoothed loss 2.33500, grad norm 5.16352, param norm 164.11267
epoch 13, iter 11770, loss 2.39489, smoothed loss 2.33582, grad norm 4.59576, param norm 164.13423
epoch 13, iter 11775, loss 2.15033, smoothed loss 2.34315, grad norm 4.46532, param norm 164.15765
epoch 13, iter 11780, loss 2.44249, smoothed loss 2.34436, grad norm 4.89654, param norm 164.18196
epoch 13, iter 11785, loss 2.34962, smoothed loss 2.34691, grad norm 5.06483, param norm 164.20386
epoch 13, iter 11790, loss 1.93113, smoothed loss 2.34516, grad norm 4.15188, param norm 164.22397
epoch 13, iter 11795, loss 2.73846, smoothed loss 2.34649, grad norm 5.16252, param norm 164.24753
epoch 13, iter 11800, loss 2.40670, smoothed loss 2.34812, grad norm 4.60045, param norm 164.27058
epoch 13, iter 11805, loss 2.67390, smoothed loss 2.34139, grad norm 4.83076, param norm 164.29469
Adding batches start...
Added  160  batches
epoch 13, iter 11810, loss 2.68367, smoothed loss 2.33758, grad norm 4.73047, param norm 164.31856
epoch 13, iter 11815, loss 1.87500, smoothed loss 2.32648, grad norm 3.92764, param norm 164.33968
epoch 13, iter 11820, loss 1.68821, smoothed loss 2.32256, grad norm 3.73770, param norm 164.36348
epoch 13, iter 11825, loss 2.01751, smoothed loss 2.31966, grad norm 4.10633, param norm 164.38834
epoch 13, iter 11830, loss 3.25216, smoothed loss 2.31523, grad norm 4.94607, param norm 164.41217
epoch 13, iter 11835, loss 2.27113, smoothed loss 2.30887, grad norm 4.65384, param norm 164.43571
epoch 13, iter 11840, loss 2.50706, smoothed loss 2.30920, grad norm 5.02941, param norm 164.45929
epoch 13, iter 11845, loss 2.69581, smoothed loss 2.31487, grad norm 5.13939, param norm 164.48143
epoch 13, iter 11850, loss 1.60155, smoothed loss 2.30845, grad norm 3.87674, param norm 164.50681
epoch 13, iter 11855, loss 2.67460, smoothed loss 2.30919, grad norm 4.84122, param norm 164.53270
epoch 13, iter 11860, loss 1.64451, smoothed loss 2.30502, grad norm 4.10298, param norm 164.55589
epoch 13, iter 11865, loss 1.81157, smoothed loss 2.30001, grad norm 4.72207, param norm 164.58101
epoch 13, iter 11870, loss 2.46940, smoothed loss 2.29226, grad norm 4.57293, param norm 164.60837
epoch 13, iter 11875, loss 2.27127, smoothed loss 2.29389, grad norm 4.70166, param norm 164.63675
epoch 13, iter 11880, loss 1.97846, smoothed loss 2.29718, grad norm 4.33288, param norm 164.66322
epoch 13, iter 11885, loss 2.38674, smoothed loss 2.30118, grad norm 4.86077, param norm 164.69064
epoch 13, iter 11890, loss 1.71785, smoothed loss 2.30649, grad norm 3.91430, param norm 164.71613
epoch 13, iter 11895, loss 2.47755, smoothed loss 2.30964, grad norm 4.32585, param norm 164.73958
epoch 13, iter 11900, loss 2.28322, smoothed loss 2.30122, grad norm 4.13242, param norm 164.76361
epoch 13, iter 11905, loss 2.28435, smoothed loss 2.30668, grad norm 4.43648, param norm 164.78954
epoch 13, iter 11910, loss 2.66207, smoothed loss 2.32019, grad norm 5.61854, param norm 164.81299
epoch 13, iter 11915, loss 1.46526, smoothed loss 2.30836, grad norm 3.37885, param norm 164.83838
epoch 13, iter 11920, loss 1.77709, smoothed loss 2.30022, grad norm 4.49983, param norm 164.86449
epoch 13, iter 11925, loss 2.67862, smoothed loss 2.32271, grad norm 4.93348, param norm 164.88864
epoch 13, iter 11930, loss 2.28567, smoothed loss 2.32739, grad norm 5.39679, param norm 164.91086
epoch 13, iter 11935, loss 2.18571, smoothed loss 2.32407, grad norm 4.56267, param norm 164.93233
epoch 13, iter 11940, loss 2.00482, smoothed loss 2.32141, grad norm 4.17302, param norm 164.95457
epoch 13, iter 11945, loss 2.02606, smoothed loss 2.32432, grad norm 4.41345, param norm 164.97806
epoch 13, iter 11950, loss 2.47353, smoothed loss 2.32138, grad norm 4.69098, param norm 165.00244
epoch 13, iter 11955, loss 2.46192, smoothed loss 2.32412, grad norm 4.85456, param norm 165.02628
epoch 13, iter 11960, loss 2.62552, smoothed loss 2.32217, grad norm 4.87709, param norm 165.05179
epoch 13, iter 11965, loss 1.96589, smoothed loss 2.30625, grad norm 4.33674, param norm 165.08147
Adding batches start...
Added  160  batches
epoch 13, iter 11970, loss 2.11779, smoothed loss 2.30386, grad norm 4.43048, param norm 165.10941
epoch 13, iter 11975, loss 1.65888, smoothed loss 2.29883, grad norm 3.89076, param norm 165.13831
epoch 13, iter 11980, loss 2.36130, smoothed loss 2.30889, grad norm 5.62919, param norm 165.16667
epoch 13, iter 11985, loss 1.86031, smoothed loss 2.29100, grad norm 4.54073, param norm 165.19376
epoch 13, iter 11990, loss 1.59766, smoothed loss 2.28458, grad norm 4.29112, param norm 165.21730
epoch 13, iter 11995, loss 2.08697, smoothed loss 2.26540, grad norm 4.37231, param norm 165.24057
epoch 13, iter 12000, loss 2.53995, smoothed loss 2.27586, grad norm 5.07468, param norm 165.26460
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 13, Iter 12000, dev loss: 3.219453
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.74872 seconds [Score: 0.81351]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.24907 seconds [Score: 0.68300]
Epoch 13, Iter 12000, Train F1 score: 0.813514, Train EM score: 0.683000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.89826 seconds [Score: 0.63772]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.81816 seconds [Score: 0.48384]
Epoch 13, Iter 12000, Dev F1 score: 0.637719, Dev EM score: 0.483844
End of epoch 13
epoch 13, iter 12005, loss 2.46050, smoothed loss 2.27296, grad norm 5.26212, param norm 165.28865
epoch 13, iter 12010, loss 2.85052, smoothed loss 2.28061, grad norm 5.37202, param norm 165.31065
epoch 13, iter 12015, loss 2.55728, smoothed loss 2.29774, grad norm 4.69860, param norm 165.33269
epoch 13, iter 12020, loss 2.60298, smoothed loss 2.29773, grad norm 4.69089, param norm 165.35544
epoch 13, iter 12025, loss 2.34065, smoothed loss 2.28612, grad norm 3.89253, param norm 165.37871
epoch 13, iter 12030, loss 1.74538, smoothed loss 2.28450, grad norm 4.54567, param norm 165.40239
epoch 13, iter 12035, loss 1.92375, smoothed loss 2.29320, grad norm 4.15915, param norm 165.42674
epoch 13, iter 12040, loss 2.18918, smoothed loss 2.28840, grad norm 4.15416, param norm 165.45105
epoch 13, iter 12045, loss 2.35722, smoothed loss 2.29210, grad norm 4.40774, param norm 165.47644
epoch 13, iter 12050, loss 1.89741, smoothed loss 2.27981, grad norm 4.21651, param norm 165.50446
epoch 13, iter 12055, loss 2.42995, smoothed loss 2.28673, grad norm 4.23374, param norm 165.53175
epoch 13, iter 12060, loss 1.94877, smoothed loss 2.28633, grad norm 4.41926, param norm 165.55540
epoch 13, iter 12065, loss 2.14614, smoothed loss 2.28677, grad norm 4.15545, param norm 165.57739
epoch 13, iter 12070, loss 2.48952, smoothed loss 2.28901, grad norm 5.80911, param norm 165.60056
epoch 13, iter 12075, loss 1.74688, smoothed loss 2.29358, grad norm 4.35647, param norm 165.62457
epoch 13, iter 12080, loss 2.39178, smoothed loss 2.28614, grad norm 4.83739, param norm 165.64757
epoch 13, iter 12085, loss 2.19267, smoothed loss 2.29429, grad norm 5.22001, param norm 165.66937
epoch 13, iter 12090, loss 3.43876, smoothed loss 2.30683, grad norm 5.09884, param norm 165.69153
epoch 13, iter 12095, loss 1.79352, smoothed loss 2.30417, grad norm 3.87566, param norm 165.71483
epoch 13, iter 12100, loss 2.05557, smoothed loss 2.30194, grad norm 4.69288, param norm 165.74126
epoch 13, iter 12105, loss 2.29996, smoothed loss 2.29686, grad norm 4.00493, param norm 165.76756
epoch 13, iter 12110, loss 2.88286, smoothed loss 2.29155, grad norm 5.76232, param norm 165.79489
epoch 13, iter 12115, loss 2.73420, smoothed loss 2.29912, grad norm 5.14896, param norm 165.82047
epoch 13, iter 12120, loss 1.78550, smoothed loss 2.28961, grad norm 4.50956, param norm 165.84445
epoch 13, iter 12125, loss 2.42409, smoothed loss 2.29631, grad norm 4.40601, param norm 165.86761
Adding batches start...
Added  144  batches
epoch 13, iter 12130, loss 2.15672, smoothed loss 2.30341, grad norm 4.74711, param norm 165.88895
epoch 13, iter 12135, loss 2.70095, smoothed loss 2.30772, grad norm 5.46965, param norm 165.90770
epoch 13, iter 12140, loss 2.76013, smoothed loss 2.29830, grad norm 5.85382, param norm 165.92709
epoch 13, iter 12145, loss 2.55550, smoothed loss 2.30311, grad norm 4.98985, param norm 165.94926
epoch 13, iter 12150, loss 3.05492, smoothed loss 2.30822, grad norm 5.10640, param norm 165.97133
epoch 13, iter 12155, loss 1.90889, smoothed loss 2.30085, grad norm 3.93926, param norm 165.99203
epoch 13, iter 12160, loss 2.39842, smoothed loss 2.31859, grad norm 4.49022, param norm 166.01421
epoch 13, iter 12165, loss 2.45607, smoothed loss 2.32846, grad norm 4.74750, param norm 166.03860
epoch 13, iter 12170, loss 1.71856, smoothed loss 2.31703, grad norm 4.50753, param norm 166.06265
epoch 13, iter 12175, loss 2.39083, smoothed loss 2.31290, grad norm 5.23486, param norm 166.08736
epoch 13, iter 12180, loss 2.18389, smoothed loss 2.31237, grad norm 4.45417, param norm 166.11189
epoch 13, iter 12185, loss 2.71741, smoothed loss 2.31033, grad norm 5.24560, param norm 166.13733
epoch 13, iter 12190, loss 1.68045, smoothed loss 2.30479, grad norm 3.95940, param norm 166.16219
epoch 13, iter 12195, loss 2.35897, smoothed loss 2.30792, grad norm 5.15001, param norm 166.18765
epoch 13, iter 12200, loss 1.73763, smoothed loss 2.29687, grad norm 3.69599, param norm 166.21169
epoch 13, iter 12205, loss 2.03603, smoothed loss 2.28724, grad norm 3.86965, param norm 166.23697
epoch 13, iter 12210, loss 2.27542, smoothed loss 2.28351, grad norm 4.39578, param norm 166.26404
epoch 13, iter 12215, loss 2.33851, smoothed loss 2.29496, grad norm 4.73968, param norm 166.28813
epoch 13, iter 12220, loss 2.42649, smoothed loss 2.30048, grad norm 4.29242, param norm 166.31410
epoch 13, iter 12225, loss 1.69021, smoothed loss 2.29531, grad norm 4.35818, param norm 166.34061
epoch 13, iter 12230, loss 2.10219, smoothed loss 2.28734, grad norm 4.93523, param norm 166.36557
epoch 13, iter 12235, loss 2.23577, smoothed loss 2.28780, grad norm 4.31852, param norm 166.39098
epoch 13, iter 12240, loss 2.54551, smoothed loss 2.29065, grad norm 5.44408, param norm 166.41501
epoch 13, iter 12245, loss 2.05488, smoothed loss 2.30079, grad norm 4.86156, param norm 166.43689
epoch 13, iter 12250, loss 2.21519, smoothed loss 2.28862, grad norm 5.03231, param norm 166.46005
epoch 13, iter 12255, loss 2.69827, smoothed loss 2.29671, grad norm 5.53156, param norm 166.48448
epoch 13, iter 12260, loss 2.46790, smoothed loss 2.30540, grad norm 4.88077, param norm 166.50810
epoch 13, iter 12265, loss 2.26939, smoothed loss 2.29890, grad norm 4.58524, param norm 166.52960
epoch 13, iter 12270, loss 2.03126, smoothed loss 2.29679, grad norm 4.39016, param norm 166.55182
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 14, iter 12275, loss 2.33349, smoothed loss 2.29261, grad norm 4.94552, param norm 166.57513
epoch 14, iter 12280, loss 2.65305, smoothed loss 2.28570, grad norm 5.58327, param norm 166.59935
epoch 14, iter 12285, loss 2.13219, smoothed loss 2.28315, grad norm 4.11674, param norm 166.62341
epoch 14, iter 12290, loss 2.77405, smoothed loss 2.28494, grad norm 4.81229, param norm 166.64610
epoch 14, iter 12295, loss 2.08895, smoothed loss 2.28404, grad norm 4.31665, param norm 166.66687
epoch 14, iter 12300, loss 2.28087, smoothed loss 2.27869, grad norm 4.91466, param norm 166.68910
epoch 14, iter 12305, loss 2.22592, smoothed loss 2.29251, grad norm 4.96459, param norm 166.71484
epoch 14, iter 12310, loss 2.46224, smoothed loss 2.29490, grad norm 4.63707, param norm 166.74069
epoch 14, iter 12315, loss 1.91673, smoothed loss 2.28486, grad norm 4.36421, param norm 166.76483
epoch 14, iter 12320, loss 2.42342, smoothed loss 2.27154, grad norm 4.68571, param norm 166.78755
epoch 14, iter 12325, loss 1.91805, smoothed loss 2.26544, grad norm 4.35653, param norm 166.80774
epoch 14, iter 12330, loss 2.12789, smoothed loss 2.25710, grad norm 4.52600, param norm 166.83093
epoch 14, iter 12335, loss 2.39049, smoothed loss 2.26492, grad norm 4.93682, param norm 166.85661
epoch 14, iter 12340, loss 2.47798, smoothed loss 2.26653, grad norm 5.08830, param norm 166.88287
epoch 14, iter 12345, loss 3.09249, smoothed loss 2.27411, grad norm 5.01472, param norm 166.91022
epoch 14, iter 12350, loss 1.91375, smoothed loss 2.27105, grad norm 4.43263, param norm 166.93532
epoch 14, iter 12355, loss 2.07083, smoothed loss 2.27580, grad norm 4.08949, param norm 166.95779
epoch 14, iter 12360, loss 2.68806, smoothed loss 2.29314, grad norm 5.33072, param norm 166.97731
epoch 14, iter 12365, loss 1.85507, smoothed loss 2.28656, grad norm 4.25191, param norm 166.99846
epoch 14, iter 12370, loss 2.02974, smoothed loss 2.28627, grad norm 4.23368, param norm 167.02130
epoch 14, iter 12375, loss 1.76691, smoothed loss 2.27687, grad norm 3.70913, param norm 167.04489
epoch 14, iter 12380, loss 3.02926, smoothed loss 2.27398, grad norm 5.17375, param norm 167.07025
epoch 14, iter 12385, loss 1.88626, smoothed loss 2.27071, grad norm 3.78822, param norm 167.09610
epoch 14, iter 12390, loss 1.86450, smoothed loss 2.26810, grad norm 4.43319, param norm 167.12146
epoch 14, iter 12395, loss 1.30569, smoothed loss 2.27070, grad norm 4.02421, param norm 167.14389
epoch 14, iter 12400, loss 1.82686, smoothed loss 2.27576, grad norm 4.13103, param norm 167.16713
epoch 14, iter 12405, loss 2.15954, smoothed loss 2.28557, grad norm 4.09001, param norm 167.18968
epoch 14, iter 12410, loss 2.39255, smoothed loss 2.29716, grad norm 4.63266, param norm 167.21402
epoch 14, iter 12415, loss 2.54619, smoothed loss 2.29237, grad norm 4.19175, param norm 167.23969
epoch 14, iter 12420, loss 2.15472, smoothed loss 2.29439, grad norm 4.66966, param norm 167.26440
epoch 14, iter 12425, loss 2.60276, smoothed loss 2.29025, grad norm 4.96441, param norm 167.28888
epoch 14, iter 12430, loss 2.80658, smoothed loss 2.30230, grad norm 5.59679, param norm 167.30974
Adding batches start...
Added  160  batches
epoch 14, iter 12435, loss 2.00954, smoothed loss 2.30199, grad norm 4.43056, param norm 167.33162
epoch 14, iter 12440, loss 2.25221, smoothed loss 2.29773, grad norm 4.88983, param norm 167.35597
epoch 14, iter 12445, loss 2.10911, smoothed loss 2.28823, grad norm 4.71574, param norm 167.38025
epoch 14, iter 12450, loss 1.73672, smoothed loss 2.27742, grad norm 4.22897, param norm 167.40425
epoch 14, iter 12455, loss 1.96213, smoothed loss 2.27745, grad norm 4.08065, param norm 167.43010
epoch 14, iter 12460, loss 2.24528, smoothed loss 2.27173, grad norm 4.75988, param norm 167.45375
epoch 14, iter 12465, loss 2.55729, smoothed loss 2.27332, grad norm 4.77516, param norm 167.47653
epoch 14, iter 12470, loss 2.26839, smoothed loss 2.27009, grad norm 4.77722, param norm 167.49817
epoch 14, iter 12475, loss 2.00025, smoothed loss 2.26518, grad norm 4.72270, param norm 167.52081
epoch 14, iter 12480, loss 2.88663, smoothed loss 2.26363, grad norm 5.32629, param norm 167.54364
epoch 14, iter 12485, loss 1.86557, smoothed loss 2.26402, grad norm 4.49420, param norm 167.56764
epoch 14, iter 12490, loss 2.41844, smoothed loss 2.26355, grad norm 4.81330, param norm 167.59291
epoch 14, iter 12495, loss 2.40132, smoothed loss 2.26322, grad norm 5.13228, param norm 167.61684
epoch 14, iter 12500, loss 1.93716, smoothed loss 2.27146, grad norm 4.72062, param norm 167.64235
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 14, Iter 12500, dev loss: 3.248585
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.13941 seconds [Score: 0.85013]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.54992 seconds [Score: 0.71600]
Epoch 14, Iter 12500, Train F1 score: 0.850129, Train EM score: 0.716000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.06467 seconds [Score: 0.63624]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.01473 seconds [Score: 0.47780]
Epoch 14, Iter 12500, Dev F1 score: 0.636242, Dev EM score: 0.477803
End of epoch 14
epoch 14, iter 12505, loss 2.00407, smoothed loss 2.26935, grad norm 4.57688, param norm 167.66812
epoch 14, iter 12510, loss 3.27954, smoothed loss 2.27515, grad norm 6.23991, param norm 167.69232
epoch 14, iter 12515, loss 1.97248, smoothed loss 2.26259, grad norm 4.23338, param norm 167.71764
epoch 14, iter 12520, loss 1.45337, smoothed loss 2.25927, grad norm 3.92485, param norm 167.74144
epoch 14, iter 12525, loss 2.64036, smoothed loss 2.26711, grad norm 5.94539, param norm 167.76508
epoch 14, iter 12530, loss 2.83674, smoothed loss 2.27703, grad norm 4.98143, param norm 167.78835
epoch 14, iter 12535, loss 3.06369, smoothed loss 2.28837, grad norm 5.57199, param norm 167.81239
epoch 14, iter 12540, loss 2.21268, smoothed loss 2.28287, grad norm 4.14693, param norm 167.83681
epoch 14, iter 12545, loss 1.79024, smoothed loss 2.26931, grad norm 4.26377, param norm 167.86206
epoch 14, iter 12550, loss 1.84656, smoothed loss 2.25971, grad norm 3.95867, param norm 167.88669
epoch 14, iter 12555, loss 2.06435, smoothed loss 2.26750, grad norm 4.77375, param norm 167.91005
epoch 14, iter 12560, loss 2.82553, smoothed loss 2.27708, grad norm 4.90552, param norm 167.93562
epoch 14, iter 12565, loss 2.65005, smoothed loss 2.27914, grad norm 4.73825, param norm 167.96236
epoch 14, iter 12570, loss 2.89728, smoothed loss 2.28741, grad norm 5.61582, param norm 167.98592
epoch 14, iter 12575, loss 2.77215, smoothed loss 2.28720, grad norm 4.90574, param norm 168.00815
epoch 14, iter 12580, loss 2.23995, smoothed loss 2.28599, grad norm 4.67227, param norm 168.02814
epoch 14, iter 12585, loss 1.71736, smoothed loss 2.28323, grad norm 4.29045, param norm 168.04854
epoch 14, iter 12590, loss 2.30637, smoothed loss 2.29909, grad norm 5.04751, param norm 168.07047
Adding batches start...
Added  160  batches
epoch 14, iter 12595, loss 3.01167, smoothed loss 2.30108, grad norm 5.53899, param norm 168.09264
epoch 14, iter 12600, loss 2.33215, smoothed loss 2.30022, grad norm 4.25204, param norm 168.11279
epoch 14, iter 12605, loss 2.01798, smoothed loss 2.29620, grad norm 3.73109, param norm 168.13388
epoch 14, iter 12610, loss 2.29273, smoothed loss 2.29588, grad norm 4.62883, param norm 168.15556
epoch 14, iter 12615, loss 2.47924, smoothed loss 2.29766, grad norm 4.46675, param norm 168.18044
epoch 14, iter 12620, loss 2.36630, smoothed loss 2.29108, grad norm 5.09330, param norm 168.20531
epoch 14, iter 12625, loss 2.69813, smoothed loss 2.30022, grad norm 4.76236, param norm 168.22974
epoch 14, iter 12630, loss 2.00578, smoothed loss 2.29771, grad norm 4.24386, param norm 168.25250
epoch 14, iter 12635, loss 2.46500, smoothed loss 2.28574, grad norm 4.88618, param norm 168.27615
epoch 14, iter 12640, loss 1.96791, smoothed loss 2.29024, grad norm 4.26965, param norm 168.30008
epoch 14, iter 12645, loss 2.38734, smoothed loss 2.28256, grad norm 5.51106, param norm 168.32175
epoch 14, iter 12650, loss 1.64890, smoothed loss 2.27768, grad norm 4.11755, param norm 168.34378
epoch 14, iter 12655, loss 2.31226, smoothed loss 2.26326, grad norm 4.96387, param norm 168.36722
epoch 14, iter 12660, loss 2.47056, smoothed loss 2.27070, grad norm 4.86647, param norm 168.39070
epoch 14, iter 12665, loss 2.45225, smoothed loss 2.27874, grad norm 5.04569, param norm 168.41272
epoch 14, iter 12670, loss 1.73096, smoothed loss 2.28635, grad norm 3.80788, param norm 168.43298
epoch 14, iter 12675, loss 2.58889, smoothed loss 2.28567, grad norm 4.83586, param norm 168.45709
epoch 14, iter 12680, loss 2.30113, smoothed loss 2.28623, grad norm 4.29443, param norm 168.48003
epoch 14, iter 12685, loss 2.37890, smoothed loss 2.27963, grad norm 5.32655, param norm 168.50319
epoch 14, iter 12690, loss 1.82052, smoothed loss 2.26187, grad norm 4.37583, param norm 168.52644
epoch 14, iter 12695, loss 2.27020, smoothed loss 2.27142, grad norm 5.17153, param norm 168.54929
epoch 14, iter 12700, loss 2.07484, smoothed loss 2.27569, grad norm 4.62139, param norm 168.57088
epoch 14, iter 12705, loss 2.19830, smoothed loss 2.25850, grad norm 4.23765, param norm 168.59396
epoch 14, iter 12710, loss 3.00939, smoothed loss 2.26735, grad norm 5.32706, param norm 168.61533
epoch 14, iter 12715, loss 2.05562, smoothed loss 2.25294, grad norm 4.73523, param norm 168.63737
epoch 14, iter 12720, loss 2.12526, smoothed loss 2.25885, grad norm 4.47470, param norm 168.65874
epoch 14, iter 12725, loss 2.81800, smoothed loss 2.26982, grad norm 5.01381, param norm 168.68164
epoch 14, iter 12730, loss 1.96580, smoothed loss 2.27261, grad norm 4.13681, param norm 168.70447
epoch 14, iter 12735, loss 2.85334, smoothed loss 2.28096, grad norm 5.45365, param norm 168.72977
epoch 14, iter 12740, loss 2.05792, smoothed loss 2.27961, grad norm 3.99263, param norm 168.75607
epoch 14, iter 12745, loss 2.21897, smoothed loss 2.27695, grad norm 5.77026, param norm 168.78050
epoch 14, iter 12750, loss 2.58380, smoothed loss 2.28195, grad norm 5.41019, param norm 168.80573
Adding batches start...
Added  160  batches
epoch 14, iter 12755, loss 2.29065, smoothed loss 2.28396, grad norm 4.41212, param norm 168.82857
epoch 14, iter 12760, loss 2.18373, smoothed loss 2.27274, grad norm 4.48504, param norm 168.85110
epoch 14, iter 12765, loss 1.73088, smoothed loss 2.26807, grad norm 5.18598, param norm 168.87283
epoch 14, iter 12770, loss 2.01164, smoothed loss 2.26944, grad norm 6.09677, param norm 168.89528
epoch 14, iter 12775, loss 1.94465, smoothed loss 2.27749, grad norm 4.08162, param norm 168.92052
epoch 14, iter 12780, loss 2.61154, smoothed loss 2.27582, grad norm 4.71917, param norm 168.94533
epoch 14, iter 12785, loss 1.90781, smoothed loss 2.27981, grad norm 3.99678, param norm 168.97008
epoch 14, iter 12790, loss 1.94104, smoothed loss 2.27365, grad norm 4.27527, param norm 168.99382
epoch 14, iter 12795, loss 2.33307, smoothed loss 2.26294, grad norm 5.10289, param norm 169.01729
epoch 14, iter 12800, loss 2.55201, smoothed loss 2.26779, grad norm 5.29399, param norm 169.03929
epoch 14, iter 12805, loss 2.39232, smoothed loss 2.27094, grad norm 4.64287, param norm 169.06352
epoch 14, iter 12810, loss 2.46543, smoothed loss 2.27574, grad norm 4.92773, param norm 169.08679
epoch 14, iter 12815, loss 1.62331, smoothed loss 2.26195, grad norm 4.16687, param norm 169.11179
epoch 14, iter 12820, loss 2.56685, smoothed loss 2.26642, grad norm 4.39564, param norm 169.13837
epoch 14, iter 12825, loss 1.56537, smoothed loss 2.26068, grad norm 4.04748, param norm 169.16614
epoch 14, iter 12830, loss 2.47745, smoothed loss 2.27376, grad norm 4.94600, param norm 169.19144
epoch 14, iter 12835, loss 2.22221, smoothed loss 2.27755, grad norm 4.15855, param norm 169.21504
epoch 14, iter 12840, loss 2.25461, smoothed loss 2.27665, grad norm 4.40566, param norm 169.23892
epoch 14, iter 12845, loss 2.36827, smoothed loss 2.27816, grad norm 4.57899, param norm 169.26105
epoch 14, iter 12850, loss 1.63966, smoothed loss 2.26205, grad norm 3.56933, param norm 169.28568
epoch 14, iter 12855, loss 1.81940, smoothed loss 2.25368, grad norm 4.03177, param norm 169.30983
epoch 14, iter 12860, loss 2.01957, smoothed loss 2.25575, grad norm 4.49210, param norm 169.33278
epoch 14, iter 12865, loss 2.20783, smoothed loss 2.25048, grad norm 4.60752, param norm 169.35541
epoch 14, iter 12870, loss 2.06357, smoothed loss 2.25078, grad norm 5.57783, param norm 169.37932
epoch 14, iter 12875, loss 2.19227, smoothed loss 2.24691, grad norm 4.88614, param norm 169.40720
epoch 14, iter 12880, loss 1.80554, smoothed loss 2.23797, grad norm 4.86235, param norm 169.43202
epoch 14, iter 12885, loss 2.41514, smoothed loss 2.25723, grad norm 5.82696, param norm 169.45299
epoch 14, iter 12890, loss 3.29134, smoothed loss 2.26538, grad norm 5.30643, param norm 169.47354
epoch 14, iter 12895, loss 2.43317, smoothed loss 2.27453, grad norm 4.91078, param norm 169.49634
epoch 14, iter 12900, loss 2.20578, smoothed loss 2.28195, grad norm 4.72343, param norm 169.51941
epoch 14, iter 12905, loss 1.91386, smoothed loss 2.26531, grad norm 4.14759, param norm 169.54439
epoch 14, iter 12910, loss 2.03931, smoothed loss 2.25253, grad norm 4.71078, param norm 169.56889
Adding batches start...
Added  160  batches
epoch 14, iter 12915, loss 1.94108, smoothed loss 2.26366, grad norm 4.63198, param norm 169.59248
epoch 14, iter 12920, loss 2.71490, smoothed loss 2.27130, grad norm 5.61104, param norm 169.61458
epoch 14, iter 12925, loss 2.79978, smoothed loss 2.28185, grad norm 5.37371, param norm 169.63364
epoch 14, iter 12930, loss 2.28018, smoothed loss 2.27239, grad norm 5.06440, param norm 169.65291
epoch 14, iter 12935, loss 2.52416, smoothed loss 2.28198, grad norm 4.50567, param norm 169.67480
epoch 14, iter 12940, loss 1.84652, smoothed loss 2.26934, grad norm 4.50312, param norm 169.69765
epoch 14, iter 12945, loss 1.93515, smoothed loss 2.25564, grad norm 4.81981, param norm 169.72014
epoch 14, iter 12950, loss 2.16125, smoothed loss 2.26454, grad norm 4.19157, param norm 169.74461
epoch 14, iter 12955, loss 1.87660, smoothed loss 2.25353, grad norm 4.45575, param norm 169.76891
epoch 14, iter 12960, loss 2.23761, smoothed loss 2.24851, grad norm 4.64038, param norm 169.79483
epoch 14, iter 12965, loss 1.74846, smoothed loss 2.25106, grad norm 4.19026, param norm 169.81824
epoch 14, iter 12970, loss 1.84614, smoothed loss 2.24355, grad norm 4.77940, param norm 169.84015
epoch 14, iter 12975, loss 1.40324, smoothed loss 2.24403, grad norm 3.99345, param norm 169.86139
epoch 14, iter 12980, loss 2.65680, smoothed loss 2.25951, grad norm 5.38276, param norm 169.88258
epoch 14, iter 12985, loss 3.00497, smoothed loss 2.27634, grad norm 5.59249, param norm 169.90242
epoch 14, iter 12990, loss 2.32195, smoothed loss 2.28812, grad norm 4.70049, param norm 169.92354
epoch 14, iter 12995, loss 1.86574, smoothed loss 2.27609, grad norm 4.08351, param norm 169.94635
epoch 14, iter 13000, loss 2.44045, smoothed loss 2.27955, grad norm 4.40514, param norm 169.96799
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 14, Iter 13000, dev loss: 3.191687
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 18.01089 seconds [Score: 0.81994]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.05887 seconds [Score: 0.68100]
Epoch 14, Iter 13000, Train F1 score: 0.819938, Train EM score: 0.681000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.26647 seconds [Score: 0.63251]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 115.29768 seconds [Score: 0.48075]
Epoch 14, Iter 13000, Dev F1 score: 0.632514, Dev EM score: 0.480753
End of epoch 14
epoch 14, iter 13005, loss 1.85971, smoothed loss 2.26259, grad norm 3.87927, param norm 169.98859
epoch 14, iter 13010, loss 1.89090, smoothed loss 2.26434, grad norm 4.26021, param norm 170.00906
epoch 14, iter 13015, loss 2.00157, smoothed loss 2.25000, grad norm 4.64658, param norm 170.02905
epoch 14, iter 13020, loss 1.72001, smoothed loss 2.24061, grad norm 3.43779, param norm 170.04935
epoch 14, iter 13025, loss 2.04429, smoothed loss 2.23096, grad norm 4.28683, param norm 170.06953
epoch 14, iter 13030, loss 1.92739, smoothed loss 2.23132, grad norm 4.52425, param norm 170.09079
epoch 14, iter 13035, loss 2.09267, smoothed loss 2.23978, grad norm 4.59877, param norm 170.11176
epoch 14, iter 13040, loss 2.27571, smoothed loss 2.23927, grad norm 5.08112, param norm 170.13159
epoch 14, iter 13045, loss 2.27194, smoothed loss 2.24333, grad norm 4.06775, param norm 170.15283
epoch 14, iter 13050, loss 2.75030, smoothed loss 2.26335, grad norm 4.88176, param norm 170.17497
epoch 14, iter 13055, loss 2.01180, smoothed loss 2.26584, grad norm 4.87413, param norm 170.19753
epoch 14, iter 13060, loss 2.72971, smoothed loss 2.26683, grad norm 5.54957, param norm 170.22031
epoch 14, iter 13065, loss 2.55080, smoothed loss 2.27211, grad norm 5.13747, param norm 170.24242
epoch 14, iter 13070, loss 2.33928, smoothed loss 2.26908, grad norm 4.59030, param norm 170.26440
Adding batches start...
Added  144  batches
epoch 14, iter 13075, loss 2.31037, smoothed loss 2.26752, grad norm 4.47574, param norm 170.28798
epoch 14, iter 13080, loss 1.94567, smoothed loss 2.26019, grad norm 4.15290, param norm 170.31178
epoch 14, iter 13085, loss 2.43576, smoothed loss 2.26004, grad norm 5.08911, param norm 170.33525
epoch 14, iter 13090, loss 2.47700, smoothed loss 2.25339, grad norm 5.58096, param norm 170.35733
epoch 14, iter 13095, loss 2.53713, smoothed loss 2.25268, grad norm 4.60445, param norm 170.38208
epoch 14, iter 13100, loss 2.00141, smoothed loss 2.24845, grad norm 4.52668, param norm 170.40309
epoch 14, iter 13105, loss 2.26590, smoothed loss 2.24771, grad norm 4.31007, param norm 170.42346
epoch 14, iter 13110, loss 2.59422, smoothed loss 2.24849, grad norm 5.21948, param norm 170.44652
epoch 14, iter 13115, loss 1.82499, smoothed loss 2.23607, grad norm 4.25484, param norm 170.46912
epoch 14, iter 13120, loss 2.41656, smoothed loss 2.23583, grad norm 5.26287, param norm 170.49054
epoch 14, iter 13125, loss 1.55577, smoothed loss 2.23945, grad norm 4.24590, param norm 170.50902
epoch 14, iter 13130, loss 2.28006, smoothed loss 2.23896, grad norm 5.69194, param norm 170.52902
epoch 14, iter 13135, loss 1.71976, smoothed loss 2.23549, grad norm 4.27279, param norm 170.54839
epoch 14, iter 13140, loss 1.73755, smoothed loss 2.22699, grad norm 4.15638, param norm 170.56888
epoch 14, iter 13145, loss 1.84563, smoothed loss 2.23321, grad norm 4.62753, param norm 170.59064
epoch 14, iter 13150, loss 2.37142, smoothed loss 2.23738, grad norm 4.46636, param norm 170.61348
epoch 14, iter 13155, loss 2.62520, smoothed loss 2.24965, grad norm 5.06245, param norm 170.63794
epoch 14, iter 13160, loss 1.91517, smoothed loss 2.25434, grad norm 4.30174, param norm 170.66158
epoch 14, iter 13165, loss 2.24348, smoothed loss 2.24919, grad norm 3.52661, param norm 170.68394
epoch 14, iter 13170, loss 2.61500, smoothed loss 2.25751, grad norm 5.00047, param norm 170.70602
epoch 14, iter 13175, loss 1.98182, smoothed loss 2.26015, grad norm 4.06486, param norm 170.72701
epoch 14, iter 13180, loss 2.13318, smoothed loss 2.24857, grad norm 5.43753, param norm 170.74817
epoch 14, iter 13185, loss 1.86655, smoothed loss 2.26487, grad norm 4.10664, param norm 170.76949
epoch 14, iter 13190, loss 2.43135, smoothed loss 2.26368, grad norm 4.86490, param norm 170.79283
epoch 14, iter 13195, loss 2.23065, smoothed loss 2.27291, grad norm 5.58802, param norm 170.81604
epoch 14, iter 13200, loss 2.94503, smoothed loss 2.28211, grad norm 5.69186, param norm 170.83867
epoch 14, iter 13205, loss 2.34745, smoothed loss 2.28713, grad norm 4.47721, param norm 170.86148
epoch 14, iter 13210, loss 2.38084, smoothed loss 2.28473, grad norm 4.65521, param norm 170.88574
epoch 14, iter 13215, loss 2.22730, smoothed loss 2.27557, grad norm 4.37726, param norm 170.91098
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
epoch 15, iter 13220, loss 1.83801, smoothed loss 2.27182, grad norm 4.36115, param norm 170.93399
epoch 15, iter 13225, loss 3.07459, smoothed loss 2.28354, grad norm 5.62485, param norm 170.95412
epoch 15, iter 13230, loss 3.06921, smoothed loss 2.29359, grad norm 5.69100, param norm 170.97469
epoch 15, iter 13235, loss 2.37235, smoothed loss 2.29300, grad norm 4.89659, param norm 170.99620
epoch 15, iter 13240, loss 2.43179, smoothed loss 2.29966, grad norm 4.65454, param norm 171.01845
epoch 15, iter 13245, loss 2.22446, smoothed loss 2.31179, grad norm 4.67915, param norm 171.04179
epoch 15, iter 13250, loss 3.18739, smoothed loss 2.32554, grad norm 5.09269, param norm 171.06499
epoch 15, iter 13255, loss 1.88152, smoothed loss 2.31225, grad norm 4.41255, param norm 171.09081
epoch 15, iter 13260, loss 2.21480, smoothed loss 2.30387, grad norm 4.62816, param norm 171.11827
epoch 15, iter 13265, loss 2.33301, smoothed loss 2.30097, grad norm 5.16660, param norm 171.13995
epoch 15, iter 13270, loss 2.08052, smoothed loss 2.30759, grad norm 3.94559, param norm 171.16069
epoch 15, iter 13275, loss 2.33714, smoothed loss 2.29412, grad norm 4.14104, param norm 171.18417
epoch 15, iter 13280, loss 1.84207, smoothed loss 2.28995, grad norm 3.92210, param norm 171.20755
epoch 15, iter 13285, loss 2.31127, smoothed loss 2.29472, grad norm 5.15390, param norm 171.23007
epoch 15, iter 13290, loss 2.84321, smoothed loss 2.30178, grad norm 4.86272, param norm 171.25183
epoch 15, iter 13295, loss 2.75134, smoothed loss 2.31344, grad norm 5.39799, param norm 171.27599
epoch 15, iter 13300, loss 2.62171, smoothed loss 2.30629, grad norm 5.03164, param norm 171.30023
epoch 15, iter 13305, loss 2.40786, smoothed loss 2.31103, grad norm 4.41232, param norm 171.32472
epoch 15, iter 13310, loss 1.96484, smoothed loss 2.30834, grad norm 3.98264, param norm 171.35019
epoch 15, iter 13315, loss 2.11523, smoothed loss 2.29776, grad norm 4.56010, param norm 171.37653
epoch 15, iter 13320, loss 1.66449, smoothed loss 2.28879, grad norm 3.74494, param norm 171.40019
epoch 15, iter 13325, loss 2.33156, smoothed loss 2.27900, grad norm 4.93974, param norm 171.42297
epoch 15, iter 13330, loss 2.29435, smoothed loss 2.28930, grad norm 5.07223, param norm 171.44687
epoch 15, iter 13335, loss 2.84702, smoothed loss 2.29159, grad norm 5.47641, param norm 171.47124
epoch 15, iter 13340, loss 1.77158, smoothed loss 2.27483, grad norm 4.89033, param norm 171.49629
epoch 15, iter 13345, loss 2.42672, smoothed loss 2.26493, grad norm 5.36249, param norm 171.52208
epoch 15, iter 13350, loss 1.92985, smoothed loss 2.26106, grad norm 5.37547, param norm 171.54485
epoch 15, iter 13355, loss 1.91400, smoothed loss 2.24320, grad norm 4.40901, param norm 171.56725
epoch 15, iter 13360, loss 1.62024, smoothed loss 2.23910, grad norm 3.77578, param norm 171.58794
epoch 15, iter 13365, loss 2.32867, smoothed loss 2.24392, grad norm 4.58522, param norm 171.60954
epoch 15, iter 13370, loss 2.69325, smoothed loss 2.24232, grad norm 5.65897, param norm 171.63141
epoch 15, iter 13375, loss 2.22810, smoothed loss 2.24468, grad norm 4.18656, param norm 171.65446
Adding batches start...
Added  160  batches
epoch 15, iter 13380, loss 2.26785, smoothed loss 2.24850, grad norm 4.41313, param norm 171.67851
epoch 15, iter 13385, loss 1.96111, smoothed loss 2.24100, grad norm 4.11925, param norm 171.70404
epoch 15, iter 13390, loss 2.45946, smoothed loss 2.25131, grad norm 4.94950, param norm 171.73148
epoch 15, iter 13395, loss 2.47973, smoothed loss 2.24518, grad norm 5.31806, param norm 171.75647
epoch 15, iter 13400, loss 2.70236, smoothed loss 2.24333, grad norm 4.42170, param norm 171.77919
epoch 15, iter 13405, loss 1.69216, smoothed loss 2.23599, grad norm 3.97482, param norm 171.79810
epoch 15, iter 13410, loss 1.89770, smoothed loss 2.22532, grad norm 4.52439, param norm 171.81532
epoch 15, iter 13415, loss 1.93802, smoothed loss 2.21775, grad norm 4.72588, param norm 171.83403
epoch 15, iter 13420, loss 2.25943, smoothed loss 2.21304, grad norm 4.60362, param norm 171.85744
epoch 15, iter 13425, loss 1.75648, smoothed loss 2.21907, grad norm 4.79072, param norm 171.88069
epoch 15, iter 13430, loss 2.47443, smoothed loss 2.23010, grad norm 5.03433, param norm 171.90428
epoch 15, iter 13435, loss 2.28632, smoothed loss 2.23276, grad norm 4.53374, param norm 171.92500
epoch 15, iter 13440, loss 2.55781, smoothed loss 2.24003, grad norm 4.59494, param norm 171.94650
epoch 15, iter 13445, loss 1.87406, smoothed loss 2.24232, grad norm 4.13164, param norm 171.97012
epoch 15, iter 13450, loss 2.13382, smoothed loss 2.24478, grad norm 4.46457, param norm 171.99283
epoch 15, iter 13455, loss 2.02402, smoothed loss 2.25435, grad norm 4.60926, param norm 172.01608
epoch 15, iter 13460, loss 1.95096, smoothed loss 2.24502, grad norm 4.47229, param norm 172.04120
epoch 15, iter 13465, loss 1.80658, smoothed loss 2.24317, grad norm 4.77391, param norm 172.06538
epoch 15, iter 13470, loss 2.46077, smoothed loss 2.24874, grad norm 4.55115, param norm 172.08853
epoch 15, iter 13475, loss 2.37490, smoothed loss 2.25567, grad norm 4.89301, param norm 172.11305
epoch 15, iter 13480, loss 1.62090, smoothed loss 2.25398, grad norm 4.00933, param norm 172.13695
epoch 15, iter 13485, loss 2.33778, smoothed loss 2.24945, grad norm 4.71626, param norm 172.15976
epoch 15, iter 13490, loss 2.11720, smoothed loss 2.24296, grad norm 4.69902, param norm 172.18071
epoch 15, iter 13495, loss 2.42925, smoothed loss 2.24733, grad norm 5.40239, param norm 172.19989
epoch 15, iter 13500, loss 2.50328, smoothed loss 2.24968, grad norm 5.07599, param norm 172.22035
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 15, Iter 13500, dev loss: 3.180254
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.74143 seconds [Score: 0.84806]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.33021 seconds [Score: 0.72700]
Epoch 15, Iter 13500, Train F1 score: 0.848059, Train EM score: 0.727000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 116.99779 seconds [Score: 0.64135]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 117.07608 seconds [Score: 0.48665]
Epoch 15, Iter 13500, Dev F1 score: 0.641348, Dev EM score: 0.486654
End of epoch 15
epoch 15, iter 13505, loss 2.69150, smoothed loss 2.25217, grad norm 4.58680, param norm 172.24249
epoch 15, iter 13510, loss 2.28971, smoothed loss 2.25155, grad norm 4.07732, param norm 172.26492
epoch 15, iter 13515, loss 2.30131, smoothed loss 2.24737, grad norm 4.88376, param norm 172.28687
epoch 15, iter 13520, loss 2.55651, smoothed loss 2.25195, grad norm 5.01819, param norm 172.30661
epoch 15, iter 13525, loss 2.78552, smoothed loss 2.25298, grad norm 4.77020, param norm 172.32709
epoch 15, iter 13530, loss 2.27866, smoothed loss 2.24416, grad norm 4.30465, param norm 172.34875
epoch 15, iter 13535, loss 1.67656, smoothed loss 2.23439, grad norm 3.59645, param norm 172.37172
Adding batches start...
Added  160  batches
epoch 15, iter 13540, loss 2.62375, smoothed loss 2.24154, grad norm 5.29446, param norm 172.39458
epoch 15, iter 13545, loss 2.15148, smoothed loss 2.25397, grad norm 4.76123, param norm 172.41629
epoch 15, iter 13550, loss 2.40847, smoothed loss 2.25557, grad norm 5.20103, param norm 172.43774
epoch 15, iter 13555, loss 2.18631, smoothed loss 2.24442, grad norm 4.86299, param norm 172.46130
epoch 15, iter 13560, loss 2.39305, smoothed loss 2.25007, grad norm 4.25328, param norm 172.48543
epoch 15, iter 13565, loss 1.92417, smoothed loss 2.23914, grad norm 4.87297, param norm 172.50897
epoch 15, iter 13570, loss 1.59037, smoothed loss 2.23480, grad norm 3.73090, param norm 172.53314
epoch 15, iter 13575, loss 2.34029, smoothed loss 2.23297, grad norm 4.92626, param norm 172.55626
epoch 15, iter 13580, loss 2.43857, smoothed loss 2.24396, grad norm 5.50545, param norm 172.57726
epoch 15, iter 13585, loss 2.73516, smoothed loss 2.24289, grad norm 4.96280, param norm 172.59776
epoch 15, iter 13590, loss 2.61213, smoothed loss 2.23239, grad norm 4.65066, param norm 172.61816
epoch 15, iter 13595, loss 2.91073, smoothed loss 2.25535, grad norm 5.19035, param norm 172.63806
epoch 15, iter 13600, loss 2.06271, smoothed loss 2.26905, grad norm 3.98398, param norm 172.65834
epoch 15, iter 13605, loss 2.24723, smoothed loss 2.26859, grad norm 4.05772, param norm 172.67818
epoch 15, iter 13610, loss 2.24604, smoothed loss 2.26052, grad norm 4.90005, param norm 172.69862
epoch 15, iter 13615, loss 1.97974, smoothed loss 2.26062, grad norm 4.63575, param norm 172.72055
epoch 15, iter 13620, loss 1.91784, smoothed loss 2.25792, grad norm 3.83068, param norm 172.74222
epoch 15, iter 13625, loss 2.24903, smoothed loss 2.25864, grad norm 4.66441, param norm 172.76340
epoch 15, iter 13630, loss 2.24673, smoothed loss 2.26174, grad norm 4.37124, param norm 172.78401
epoch 15, iter 13635, loss 2.33517, smoothed loss 2.26426, grad norm 4.34413, param norm 172.80544
epoch 15, iter 13640, loss 2.24699, smoothed loss 2.25912, grad norm 5.00433, param norm 172.82716
epoch 15, iter 13645, loss 2.20195, smoothed loss 2.25541, grad norm 4.23981, param norm 172.85059
epoch 15, iter 13650, loss 2.37971, smoothed loss 2.26830, grad norm 4.74115, param norm 172.87192
epoch 15, iter 13655, loss 2.63356, smoothed loss 2.26704, grad norm 4.98345, param norm 172.89217
epoch 15, iter 13660, loss 2.31070, smoothed loss 2.26638, grad norm 4.39558, param norm 172.91374
epoch 15, iter 13665, loss 1.71449, smoothed loss 2.25952, grad norm 3.53712, param norm 172.93474
epoch 15, iter 13670, loss 1.99002, smoothed loss 2.26445, grad norm 4.77090, param norm 172.95433
epoch 15, iter 13675, loss 2.04106, smoothed loss 2.25916, grad norm 4.42939, param norm 172.97414
epoch 15, iter 13680, loss 1.91094, smoothed loss 2.27208, grad norm 3.88903, param norm 172.99754
epoch 15, iter 13685, loss 1.72435, smoothed loss 2.26451, grad norm 3.76516, param norm 173.01912
epoch 15, iter 13690, loss 2.15952, smoothed loss 2.25764, grad norm 4.35831, param norm 173.04082
epoch 15, iter 13695, loss 2.95099, smoothed loss 2.25638, grad norm 5.31500, param norm 173.06046
Adding batches start...
Added  160  batches
epoch 15, iter 13700, loss 2.33254, smoothed loss 2.24966, grad norm 4.93714, param norm 173.07840
epoch 15, iter 13705, loss 1.79903, smoothed loss 2.24090, grad norm 4.79657, param norm 173.09747
epoch 15, iter 13710, loss 2.17731, smoothed loss 2.24967, grad norm 4.84081, param norm 173.12111
epoch 15, iter 13715, loss 2.50085, smoothed loss 2.25014, grad norm 4.79597, param norm 173.14668
epoch 15, iter 13720, loss 2.18431, smoothed loss 2.25430, grad norm 5.30497, param norm 173.17177
epoch 15, iter 13725, loss 2.22916, smoothed loss 2.25836, grad norm 4.22243, param norm 173.19356
epoch 15, iter 13730, loss 2.03248, smoothed loss 2.25581, grad norm 4.50284, param norm 173.21434
epoch 15, iter 13735, loss 2.50880, smoothed loss 2.26490, grad norm 4.99667, param norm 173.23442
epoch 15, iter 13740, loss 1.88060, smoothed loss 2.25606, grad norm 4.00591, param norm 173.25414
epoch 15, iter 13745, loss 2.44937, smoothed loss 2.26947, grad norm 5.14094, param norm 173.27330
epoch 15, iter 13750, loss 2.48746, smoothed loss 2.26936, grad norm 4.59291, param norm 173.29347
epoch 15, iter 13755, loss 1.94931, smoothed loss 2.26406, grad norm 4.42837, param norm 173.31694
epoch 15, iter 13760, loss 2.26165, smoothed loss 2.26708, grad norm 4.13247, param norm 173.33934
epoch 15, iter 13765, loss 2.08977, smoothed loss 2.25420, grad norm 4.15220, param norm 173.36172
epoch 15, iter 13770, loss 2.39747, smoothed loss 2.25663, grad norm 4.74967, param norm 173.38268
epoch 15, iter 13775, loss 2.11448, smoothed loss 2.24858, grad norm 4.03402, param norm 173.40547
epoch 15, iter 13780, loss 2.21875, smoothed loss 2.24730, grad norm 4.63041, param norm 173.43143
epoch 15, iter 13785, loss 1.63596, smoothed loss 2.24346, grad norm 3.80846, param norm 173.45461
epoch 15, iter 13790, loss 2.07919, smoothed loss 2.23837, grad norm 4.23299, param norm 173.47583
epoch 15, iter 13795, loss 2.66992, smoothed loss 2.24689, grad norm 5.65109, param norm 173.49655
epoch 15, iter 13800, loss 2.26172, smoothed loss 2.24023, grad norm 5.18432, param norm 173.51666
epoch 15, iter 13805, loss 1.71423, smoothed loss 2.23506, grad norm 3.97407, param norm 173.53630
epoch 15, iter 13810, loss 2.65162, smoothed loss 2.24627, grad norm 5.57081, param norm 173.55804
epoch 15, iter 13815, loss 2.65515, smoothed loss 2.23775, grad norm 4.99748, param norm 173.58255
epoch 15, iter 13820, loss 2.78082, smoothed loss 2.22770, grad norm 4.70910, param norm 173.60696
epoch 15, iter 13825, loss 2.87039, smoothed loss 2.24662, grad norm 4.57246, param norm 173.63016
epoch 15, iter 13830, loss 2.17455, smoothed loss 2.24370, grad norm 4.19023, param norm 173.65118
epoch 15, iter 13835, loss 2.60495, smoothed loss 2.24216, grad norm 4.45468, param norm 173.67104
epoch 15, iter 13840, loss 2.41831, smoothed loss 2.24976, grad norm 4.77190, param norm 173.69034
epoch 15, iter 13845, loss 2.39252, smoothed loss 2.24646, grad norm 5.39148, param norm 173.71355
epoch 15, iter 13850, loss 2.49807, smoothed loss 2.25211, grad norm 5.51993, param norm 173.73703
epoch 15, iter 13855, loss 2.17382, smoothed loss 2.24549, grad norm 3.98214, param norm 173.76247
Adding batches start...
Added  160  batches
epoch 15, iter 13860, loss 1.62100, smoothed loss 2.24103, grad norm 4.47857, param norm 173.78555
epoch 15, iter 13865, loss 2.00462, smoothed loss 2.21988, grad norm 5.18674, param norm 173.80698
epoch 15, iter 13870, loss 2.27949, smoothed loss 2.21984, grad norm 4.74459, param norm 173.82851
epoch 15, iter 13875, loss 2.37307, smoothed loss 2.22092, grad norm 5.08883, param norm 173.85014
epoch 15, iter 13880, loss 2.59886, smoothed loss 2.23987, grad norm 4.47716, param norm 173.86856
epoch 15, iter 13885, loss 2.90093, smoothed loss 2.24833, grad norm 5.22595, param norm 173.88770
epoch 15, iter 13890, loss 2.40108, smoothed loss 2.25445, grad norm 4.99916, param norm 173.90611
epoch 15, iter 13895, loss 2.23488, smoothed loss 2.25067, grad norm 4.62528, param norm 173.92729
epoch 15, iter 13900, loss 2.11812, smoothed loss 2.24147, grad norm 4.60090, param norm 173.94983
epoch 15, iter 13905, loss 2.19785, smoothed loss 2.25545, grad norm 5.15321, param norm 173.97076
epoch 15, iter 13910, loss 2.01952, smoothed loss 2.25425, grad norm 4.61579, param norm 173.99239
epoch 15, iter 13915, loss 1.89453, smoothed loss 2.23510, grad norm 4.07254, param norm 174.01567
epoch 15, iter 13920, loss 1.92526, smoothed loss 2.24490, grad norm 4.05900, param norm 174.03708
epoch 15, iter 13925, loss 2.75392, smoothed loss 2.26318, grad norm 5.33614, param norm 174.05846
epoch 15, iter 13930, loss 2.21293, smoothed loss 2.25829, grad norm 5.26707, param norm 174.07880
epoch 15, iter 13935, loss 2.38177, smoothed loss 2.25978, grad norm 4.32066, param norm 174.09941
epoch 15, iter 13940, loss 2.16637, smoothed loss 2.25479, grad norm 4.37855, param norm 174.11870
epoch 15, iter 13945, loss 2.65723, smoothed loss 2.25198, grad norm 6.06610, param norm 174.13939
epoch 15, iter 13950, loss 2.43608, smoothed loss 2.25701, grad norm 4.48882, param norm 174.16209
epoch 15, iter 13955, loss 2.23529, smoothed loss 2.25045, grad norm 4.23470, param norm 174.18451
epoch 15, iter 13960, loss 1.74732, smoothed loss 2.24806, grad norm 3.80183, param norm 174.20586
epoch 15, iter 13965, loss 2.59100, smoothed loss 2.23155, grad norm 4.84412, param norm 174.22581
epoch 15, iter 13970, loss 2.12720, smoothed loss 2.21963, grad norm 4.69618, param norm 174.24545
epoch 15, iter 13975, loss 2.66859, smoothed loss 2.22221, grad norm 4.83607, param norm 174.26636
epoch 15, iter 13980, loss 1.96749, smoothed loss 2.21421, grad norm 4.66353, param norm 174.28661
epoch 15, iter 13985, loss 2.50726, smoothed loss 2.22159, grad norm 5.81410, param norm 174.30603
epoch 15, iter 13990, loss 1.83586, smoothed loss 2.22114, grad norm 4.37006, param norm 174.32646
epoch 15, iter 13995, loss 2.17718, smoothed loss 2.22347, grad norm 4.73606, param norm 174.34833
epoch 15, iter 14000, loss 2.25630, smoothed loss 2.22797, grad norm 4.82437, param norm 174.36998
Saving to ./train/qa.ckpt...
Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
Epoch 15, Iter 14000, dev loss: 3.224262
Calculating Train F1/EM...
Adding batches start...
Added  160  batches
F1 train: 1000 examples took 17.59172 seconds [Score: 0.81078]
Adding batches start...
Added  160  batches
Exact Match train: 1000 examples took 17.63821 seconds [Score: 0.69900]
Epoch 15, Iter 14000, Train F1 score: 0.810776, Train EM score: 0.699000
Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
F1 dev: 7118 examples took 117.45203 seconds [Score: 0.63316]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
Exact Match dev: 7118 examples took 116.62592 seconds [Score: 0.48061]
Epoch 15, Iter 14000, Dev F1 score: 0.633156, Dev EM score: 0.480613
End of epoch 15
epoch 15, iter 14005, loss 2.10253, smoothed loss 2.23597, grad norm 4.85987, param norm 174.39267
epoch 15, iter 14010, loss 1.87476, smoothed loss 2.22374, grad norm 3.77109, param norm 174.41553
epoch 15, iter 14015, loss 1.57378, smoothed loss 2.21766, grad norm 4.00593, param norm 174.43593
Adding batches start...
Added  144  batches
epoch 15, iter 14020, loss 2.47371, smoothed loss 2.22430, grad norm 4.93502, param norm 174.45566
epoch 15, iter 14025, loss 2.35913, smoothed loss 2.23310, grad norm 5.33613, param norm 174.47502
epoch 15, iter 14030, loss 2.71578, smoothed loss 2.23873, grad norm 5.15247, param norm 174.49745
epoch 15, iter 14035, loss 2.48659, smoothed loss 2.23272, grad norm 5.31481, param norm 174.52094
epoch 15, iter 14040, loss 1.99136, smoothed loss 2.22461, grad norm 4.52673, param norm 174.54530
epoch 15, iter 14045, loss 2.41954, smoothed loss 2.21616, grad norm 4.74072, param norm 174.57150
epoch 15, iter 14050, loss 2.71298, smoothed loss 2.22763, grad norm 5.59284, param norm 174.59460