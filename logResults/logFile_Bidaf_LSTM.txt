[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
Loading devset:
100% 48/48 [00:04<00:00, 10.88it/s]
Number of triples ignored due to token mapping problems:  3212
Number of triples ignored due to unalignment with tokenization problems:  240
Number of triples ignored due to span alignment problems:  0
Processed examples: 7118 out of 10570
Loading trainset:
100% 442/442 [00:37<00:00, 11.76it/s]
Number of triples ignored due to token mapping problems:  28669
Number of triples ignored due to unalignment with tokenization problems:  1760
Number of triples ignored due to span alignment problems:  7
Processed examples: 57163 out of 87599
100% 400000/400000 [00:12<00:00, 32474.50it/s]
Finished processing GloVe vectors
In Add Embed Layer
In RNN Encoder layer
In BiDAF Layer
In output layer
In loss function
Finished initialization of model
Training Network
2018-11-24 19:26:26.553250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-24 19:26:26.553724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-11-24 19:26:26.553791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-24 19:26:27.439563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-24 19:26:27.439626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-24 19:26:27.439650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-24 19:26:27.440049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Model finished setup
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 5, loss 9.54700, smoothed loss 9.40842, grad norm 0.99101, param norm 60.64235
INFO:root:epoch 1, iter 10, loss 9.40992, smoothed loss 9.41409, grad norm 1.19084, param norm 60.76308
INFO:root:epoch 1, iter 15, loss 9.58593, smoothed loss 9.41495, grad norm 1.60552, param norm 60.88892
INFO:root:epoch 1, iter 20, loss 9.23911, smoothed loss 9.41649, grad norm 1.20674, param norm 61.01384
INFO:root:epoch 1, iter 25, loss 8.96315, smoothed loss 9.40630, grad norm 1.46133, param norm 61.14592
INFO:root:epoch 1, iter 30, loss 9.10584, smoothed loss 9.39478, grad norm 1.32658, param norm 61.28719
INFO:root:epoch 1, iter 35, loss 8.79721, smoothed loss 9.37982, grad norm 1.64746, param norm 61.44530
INFO:root:epoch 1, iter 40, loss 8.89701, smoothed loss 9.36500, grad norm 1.65424, param norm 61.61404
INFO:root:epoch 1, iter 45, loss 8.41806, smoothed loss 9.32983, grad norm 1.68587, param norm 61.79456
INFO:root:epoch 1, iter 50, loss 8.63854, smoothed loss 9.29401, grad norm 1.70158, param norm 61.97939
INFO:root:epoch 1, iter 55, loss 8.83168, smoothed loss 9.25220, grad norm 1.86729, param norm 62.16894
INFO:root:epoch 1, iter 60, loss 8.51438, smoothed loss 9.20822, grad norm 1.53928, param norm 62.34380
INFO:root:epoch 1, iter 65, loss 8.18368, smoothed loss 9.15414, grad norm 1.69890, param norm 62.52694
INFO:root:epoch 1, iter 70, loss 8.63017, smoothed loss 9.11875, grad norm 1.93519, param norm 62.71046
INFO:root:epoch 1, iter 75, loss 7.89442, smoothed loss 9.07975, grad norm 1.95835, param norm 62.88054
INFO:root:epoch 1, iter 80, loss 8.17729, smoothed loss 9.03962, grad norm 1.79942, param norm 63.05401
INFO:root:epoch 1, iter 85, loss 8.27012, smoothed loss 8.99699, grad norm 1.37681, param norm 63.18820
INFO:root:epoch 1, iter 90, loss 8.11224, smoothed loss 8.95038, grad norm 1.51299, param norm 63.31576
INFO:root:epoch 1, iter 95, loss 7.94218, smoothed loss 8.89963, grad norm 1.53444, param norm 63.45671
INFO:root:epoch 1, iter 100, loss 7.76420, smoothed loss 8.85710, grad norm 1.61106, param norm 63.58113
INFO:root:epoch 1, iter 105, loss 7.87697, smoothed loss 8.81424, grad norm 1.48140, param norm 63.71371
INFO:root:epoch 1, iter 110, loss 7.57686, smoothed loss 8.76636, grad norm 1.49126, param norm 63.84880
INFO:root:epoch 1, iter 115, loss 7.65547, smoothed loss 8.71681, grad norm 1.65277, param norm 63.97181
INFO:root:epoch 1, iter 120, loss 8.03337, smoothed loss 8.67658, grad norm 1.53182, param norm 64.07302
INFO:root:epoch 1, iter 125, loss 7.94840, smoothed loss 8.64212, grad norm 1.56073, param norm 64.16985
INFO:root:epoch 1, iter 130, loss 7.60245, smoothed loss 8.59722, grad norm 1.67224, param norm 64.30382
INFO:root:epoch 1, iter 135, loss 7.99690, smoothed loss 8.55814, grad norm 1.51969, param norm 64.43590
INFO:root:epoch 1, iter 140, loss 8.18829, smoothed loss 8.52476, grad norm 1.36743, param norm 64.55080
INFO:root:epoch 1, iter 145, loss 8.30267, smoothed loss 8.49295, grad norm 2.23790, param norm 64.67074
INFO:root:epoch 1, iter 150, loss 8.29886, smoothed loss 8.46073, grad norm 1.90087, param norm 64.78625
INFO:root:epoch 1, iter 155, loss 7.81768, smoothed loss 8.42875, grad norm 1.81992, param norm 64.91033
INFO:root:epoch 1, iter 160, loss 8.10119, smoothed loss 8.40253, grad norm 2.01146, param norm 65.03295
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 165, loss 7.62610, smoothed loss 8.36643, grad norm 1.86013, param norm 65.14100
INFO:root:epoch 1, iter 170, loss 7.63617, smoothed loss 8.33463, grad norm 1.62226, param norm 65.26736
INFO:root:epoch 1, iter 175, loss 8.17748, smoothed loss 8.31013, grad norm 1.66388, param norm 65.39024
INFO:root:epoch 1, iter 180, loss 7.90515, smoothed loss 8.28180, grad norm 1.33301, param norm 65.50116
INFO:root:epoch 1, iter 185, loss 8.03977, smoothed loss 8.25436, grad norm 1.62990, param norm 65.60477
INFO:root:epoch 1, iter 190, loss 7.19488, smoothed loss 8.21955, grad norm 1.39399, param norm 65.70725
INFO:root:epoch 1, iter 195, loss 8.06858, smoothed loss 8.18613, grad norm 2.21945, param norm 65.82505
INFO:root:epoch 1, iter 200, loss 7.47910, smoothed loss 8.15967, grad norm 1.77766, param norm 65.92266
INFO:root:epoch 1, iter 205, loss 7.75532, smoothed loss 8.12878, grad norm 1.44820, param norm 66.02303
INFO:root:epoch 1, iter 210, loss 7.97849, smoothed loss 8.09730, grad norm 2.04849, param norm 66.14439
INFO:root:epoch 1, iter 215, loss 7.21346, smoothed loss 8.07005, grad norm 1.39872, param norm 66.26914
INFO:root:epoch 1, iter 220, loss 7.31298, smoothed loss 8.04014, grad norm 1.82701, param norm 66.40981
INFO:root:epoch 1, iter 225, loss 7.53161, smoothed loss 8.01478, grad norm 2.06986, param norm 66.54577
INFO:root:epoch 1, iter 230, loss 7.62989, smoothed loss 7.99435, grad norm 1.53182, param norm 66.67638
INFO:root:epoch 1, iter 235, loss 7.23295, smoothed loss 7.96298, grad norm 1.62686, param norm 66.80955
INFO:root:epoch 1, iter 240, loss 7.32164, smoothed loss 7.94266, grad norm 1.54810, param norm 66.93823
INFO:root:epoch 1, iter 245, loss 7.17592, smoothed loss 7.92000, grad norm 1.65389, param norm 67.06235
INFO:root:epoch 1, iter 250, loss 7.61982, smoothed loss 7.90016, grad norm 1.55185, param norm 67.19471
INFO:root:epoch 1, iter 255, loss 7.25183, smoothed loss 7.87051, grad norm 1.94288, param norm 67.33892
INFO:root:epoch 1, iter 260, loss 7.25381, smoothed loss 7.84589, grad norm 1.95929, param norm 67.48048
INFO:root:epoch 1, iter 265, loss 7.79243, smoothed loss 7.83206, grad norm 1.63306, param norm 67.62540
INFO:root:epoch 1, iter 270, loss 7.90691, smoothed loss 7.81772, grad norm 1.81876, param norm 67.77798
INFO:root:epoch 1, iter 275, loss 6.97394, smoothed loss 7.78804, grad norm 1.81672, param norm 67.93518
INFO:root:epoch 1, iter 280, loss 7.29888, smoothed loss 7.76700, grad norm 1.71698, param norm 68.08862
INFO:root:epoch 1, iter 285, loss 7.46387, smoothed loss 7.75318, grad norm 1.76299, param norm 68.25253
INFO:root:epoch 1, iter 290, loss 7.40531, smoothed loss 7.73791, grad norm 1.96358, param norm 68.40578
INFO:root:epoch 1, iter 295, loss 6.70727, smoothed loss 7.71160, grad norm 1.79803, param norm 68.56606
INFO:root:epoch 1, iter 300, loss 7.24442, smoothed loss 7.69290, grad norm 1.79971, param norm 68.74174
INFO:root:epoch 1, iter 305, loss 7.72783, smoothed loss 7.68173, grad norm 2.30455, param norm 68.91533
INFO:root:epoch 1, iter 310, loss 7.44284, smoothed loss 7.66446, grad norm 1.85716, param norm 69.05536
INFO:root:epoch 1, iter 315, loss 6.90426, smoothed loss 7.63543, grad norm 1.68148, param norm 69.20130
INFO:root:epoch 1, iter 320, loss 7.09651, smoothed loss 7.61447, grad norm 2.09479, param norm 69.37180
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 325, loss 7.62628, smoothed loss 7.59294, grad norm 2.99964, param norm 69.53569
INFO:root:epoch 1, iter 330, loss 6.68705, smoothed loss 7.56376, grad norm 2.53833, param norm 69.70432
INFO:root:epoch 1, iter 335, loss 7.25135, smoothed loss 7.54081, grad norm 2.29035, param norm 69.88270
INFO:root:epoch 1, iter 340, loss 7.18243, smoothed loss 7.51592, grad norm 1.83400, param norm 70.04024
INFO:root:epoch 1, iter 345, loss 6.86678, smoothed loss 7.49411, grad norm 1.99189, param norm 70.18025
INFO:root:epoch 1, iter 350, loss 7.39336, smoothed loss 7.46825, grad norm 2.21237, param norm 70.34352
INFO:root:epoch 1, iter 355, loss 7.01545, smoothed loss 7.43910, grad norm 2.75187, param norm 70.52094
INFO:root:epoch 1, iter 360, loss 7.15303, smoothed loss 7.41912, grad norm 2.01070, param norm 70.68616
INFO:root:epoch 1, iter 365, loss 6.74604, smoothed loss 7.38548, grad norm 2.04254, param norm 70.85425
INFO:root:epoch 1, iter 370, loss 6.97944, smoothed loss 7.35716, grad norm 2.05606, param norm 71.02828
INFO:root:epoch 1, iter 375, loss 6.73555, smoothed loss 7.33073, grad norm 1.79967, param norm 71.20281
INFO:root:epoch 1, iter 380, loss 6.75486, smoothed loss 7.31584, grad norm 1.99548, param norm 71.36295
INFO:root:epoch 1, iter 385, loss 7.15011, smoothed loss 7.29806, grad norm 2.43743, param norm 71.53109
INFO:root:epoch 1, iter 390, loss 6.44808, smoothed loss 7.28172, grad norm 1.96151, param norm 71.69028
INFO:root:epoch 1, iter 395, loss 7.17675, smoothed loss 7.25911, grad norm 1.99855, param norm 71.86800
INFO:root:epoch 1, iter 400, loss 7.05734, smoothed loss 7.24197, grad norm 2.15899, param norm 72.03259
INFO:root:epoch 1, iter 405, loss 7.09366, smoothed loss 7.23266, grad norm 2.18168, param norm 72.15951
INFO:root:epoch 1, iter 410, loss 6.32871, smoothed loss 7.21061, grad norm 2.32621, param norm 72.31400
INFO:root:epoch 1, iter 415, loss 6.77205, smoothed loss 7.20074, grad norm 1.69814, param norm 72.48336
INFO:root:epoch 1, iter 420, loss 6.51040, smoothed loss 7.19385, grad norm 2.16509, param norm 72.63960
INFO:root:epoch 1, iter 425, loss 7.60474, smoothed loss 7.19529, grad norm 2.32088, param norm 72.79288
INFO:root:epoch 1, iter 430, loss 5.77143, smoothed loss 7.17250, grad norm 2.55089, param norm 72.92723
INFO:root:epoch 1, iter 435, loss 6.57323, smoothed loss 7.15379, grad norm 2.14172, param norm 73.08948
INFO:root:epoch 1, iter 440, loss 6.39717, smoothed loss 7.13374, grad norm 2.01728, param norm 73.23235
INFO:root:epoch 1, iter 445, loss 6.60311, smoothed loss 7.10612, grad norm 2.30626, param norm 73.37106
INFO:root:epoch 1, iter 450, loss 6.82864, smoothed loss 7.09148, grad norm 2.24156, param norm 73.51785
INFO:root:epoch 1, iter 455, loss 7.45630, smoothed loss 7.07727, grad norm 2.52094, param norm 73.66585
INFO:root:epoch 1, iter 460, loss 6.89639, smoothed loss 7.06713, grad norm 2.64266, param norm 73.82411
INFO:root:epoch 1, iter 465, loss 6.91144, smoothed loss 7.05716, grad norm 1.95180, param norm 73.99654
INFO:root:epoch 1, iter 470, loss 6.31476, smoothed loss 7.04738, grad norm 2.01579, param norm 74.15289
INFO:root:epoch 1, iter 475, loss 6.71809, smoothed loss 7.03195, grad norm 1.79909, param norm 74.29606
INFO:root:epoch 1, iter 480, loss 6.69248, smoothed loss 7.01877, grad norm 2.55093, param norm 74.43484
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 485, loss 6.60119, smoothed loss 7.00312, grad norm 2.06839, param norm 74.57270
INFO:root:epoch 1, iter 490, loss 7.04014, smoothed loss 6.99963, grad norm 2.07269, param norm 74.71423
INFO:root:epoch 1, iter 495, loss 6.78518, smoothed loss 6.98358, grad norm 1.82007, param norm 74.85881
INFO:root:epoch 1, iter 500, loss 6.85173, smoothed loss 6.97057, grad norm 1.88322, param norm 75.02175
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 1, Iter 500, dev loss: 6.574493
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 19.48249 seconds [Score: 0.18860]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.41430 seconds [Score: 0.10800]
INFO:root:Epoch 1, Iter 500, Train F1 score: 0.188597, Train EM score: 0.108000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.36710 seconds [Score: 0.17668]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.70885 seconds [Score: 0.11127]
INFO:root:Epoch 1, Iter 500, Dev F1 score: 0.176685, Dev EM score: 0.111267
INFO:root:End of epoch 1
INFO:root:epoch 1, iter 505, loss 6.64236, smoothed loss 6.95049, grad norm 3.05693, param norm 75.16964
INFO:root:epoch 1, iter 510, loss 6.25505, smoothed loss 6.91621, grad norm 2.35871, param norm 75.30863
INFO:root:epoch 1, iter 515, loss 6.40710, smoothed loss 6.90150, grad norm 2.57999, param norm 75.45324
INFO:root:epoch 1, iter 520, loss 6.45840, smoothed loss 6.87582, grad norm 2.18747, param norm 75.59911
INFO:root:epoch 1, iter 525, loss 6.50180, smoothed loss 6.87122, grad norm 2.18183, param norm 75.74896
INFO:root:epoch 1, iter 530, loss 5.57749, smoothed loss 6.84545, grad norm 1.87433, param norm 75.89001
INFO:root:epoch 1, iter 535, loss 6.90849, smoothed loss 6.82797, grad norm 2.02200, param norm 76.04272
INFO:root:epoch 1, iter 540, loss 6.44700, smoothed loss 6.80136, grad norm 2.10824, param norm 76.19183
INFO:root:epoch 1, iter 545, loss 6.83429, smoothed loss 6.77082, grad norm 2.50955, param norm 76.33408
INFO:root:epoch 1, iter 550, loss 5.66325, smoothed loss 6.74725, grad norm 2.02500, param norm 76.47924
INFO:root:epoch 1, iter 555, loss 5.36518, smoothed loss 6.71457, grad norm 2.19350, param norm 76.61560
INFO:root:epoch 1, iter 560, loss 5.75051, smoothed loss 6.69302, grad norm 2.26638, param norm 76.75509
INFO:root:epoch 1, iter 565, loss 6.31252, smoothed loss 6.68939, grad norm 2.50746, param norm 76.90459
INFO:root:epoch 1, iter 570, loss 6.47367, smoothed loss 6.66849, grad norm 2.02874, param norm 77.05524
INFO:root:epoch 1, iter 575, loss 6.41049, smoothed loss 6.64874, grad norm 2.18287, param norm 77.19716
INFO:root:epoch 1, iter 580, loss 5.81564, smoothed loss 6.63147, grad norm 2.23738, param norm 77.33122
INFO:root:epoch 1, iter 585, loss 6.69891, smoothed loss 6.62393, grad norm 2.25698, param norm 77.47444
INFO:root:epoch 1, iter 590, loss 5.56953, smoothed loss 6.60771, grad norm 2.38416, param norm 77.61319
INFO:root:epoch 1, iter 595, loss 6.09783, smoothed loss 6.60184, grad norm 1.97741, param norm 77.73970
INFO:root:epoch 1, iter 600, loss 6.96116, smoothed loss 6.58845, grad norm 2.38288, param norm 77.85685
INFO:root:epoch 1, iter 605, loss 6.80865, smoothed loss 6.59478, grad norm 2.65743, param norm 77.97977
INFO:root:epoch 1, iter 610, loss 6.87578, smoothed loss 6.58490, grad norm 2.49771, param norm 78.11616
INFO:root:epoch 1, iter 615, loss 6.62911, smoothed loss 6.57157, grad norm 2.17871, param norm 78.24718
INFO:root:epoch 1, iter 620, loss 5.89707, smoothed loss 6.55805, grad norm 2.37653, param norm 78.38641
INFO:root:epoch 1, iter 625, loss 6.34533, smoothed loss 6.54619, grad norm 3.48890, param norm 78.52513
INFO:root:epoch 1, iter 630, loss 7.01729, smoothed loss 6.54516, grad norm 2.82905, param norm 78.66670
INFO:root:epoch 1, iter 635, loss 6.40034, smoothed loss 6.55083, grad norm 2.09393, param norm 78.81940
INFO:root:epoch 1, iter 640, loss 5.92495, smoothed loss 6.54518, grad norm 2.14981, param norm 78.96700
Adding batches start...
Added  160  batches
INFO:root:epoch 1, iter 645, loss 5.97346, smoothed loss 6.53423, grad norm 2.02992, param norm 79.09921
INFO:root:epoch 1, iter 650, loss 6.02637, smoothed loss 6.52208, grad norm 2.40223, param norm 79.22258
INFO:root:epoch 1, iter 655, loss 6.48275, smoothed loss 6.51262, grad norm 2.39888, param norm 79.34177
INFO:root:epoch 1, iter 660, loss 5.87963, smoothed loss 6.48327, grad norm 2.53092, param norm 79.46371
INFO:root:epoch 1, iter 665, loss 6.22222, smoothed loss 6.47978, grad norm 2.65158, param norm 79.58987
INFO:root:epoch 1, iter 670, loss 5.57117, smoothed loss 6.46992, grad norm 2.08283, param norm 79.72624
INFO:root:epoch 1, iter 675, loss 6.22123, smoothed loss 6.46458, grad norm 2.23985, param norm 79.86277
INFO:root:epoch 1, iter 680, loss 5.62612, smoothed loss 6.44472, grad norm 2.63184, param norm 80.00483
INFO:root:epoch 1, iter 685, loss 6.34557, smoothed loss 6.43316, grad norm 2.44665, param norm 80.14538
INFO:root:epoch 1, iter 690, loss 6.55206, smoothed loss 6.41488, grad norm 2.34477, param norm 80.28440
INFO:root:epoch 1, iter 695, loss 5.99067, smoothed loss 6.39753, grad norm 2.88778, param norm 80.41837
INFO:root:epoch 1, iter 700, loss 6.78537, smoothed loss 6.38882, grad norm 2.37543, param norm 80.55234
INFO:root:epoch 1, iter 705, loss 6.34216, smoothed loss 6.39079, grad norm 2.04399, param norm 80.69308
INFO:root:epoch 1, iter 710, loss 6.74145, smoothed loss 6.39448, grad norm 2.43046, param norm 80.82853
INFO:root:epoch 1, iter 715, loss 5.85527, smoothed loss 6.39463, grad norm 2.29451, param norm 80.96076
INFO:root:epoch 1, iter 720, loss 6.27082, smoothed loss 6.38880, grad norm 2.41631, param norm 81.08970
INFO:root:epoch 1, iter 725, loss 6.33773, smoothed loss 6.38390, grad norm 2.23623, param norm 81.21018
INFO:root:epoch 1, iter 730, loss 6.83288, smoothed loss 6.37532, grad norm 2.46113, param norm 81.33578
INFO:root:epoch 1, iter 735, loss 6.28537, smoothed loss 6.36429, grad norm 2.31830, param norm 81.46744
INFO:root:epoch 1, iter 740, loss 5.83643, smoothed loss 6.34309, grad norm 2.35621, param norm 81.58699
INFO:root:epoch 1, iter 745, loss 6.36176, smoothed loss 6.33935, grad norm 2.56576, param norm 81.71367
INFO:root:epoch 1, iter 750, loss 6.42716, smoothed loss 6.35909, grad norm 2.57288, param norm 81.83754
INFO:root:epoch 1, iter 755, loss 6.66463, smoothed loss 6.35612, grad norm 2.10967, param norm 81.96535
INFO:root:epoch 1, iter 760, loss 6.11360, smoothed loss 6.33826, grad norm 2.13280, param norm 82.10378
INFO:root:epoch 1, iter 765, loss 6.29850, smoothed loss 6.31610, grad norm 2.46590, param norm 82.23652
INFO:root:epoch 1, iter 770, loss 6.63834, smoothed loss 6.30655, grad norm 2.79751, param norm 82.35329
INFO:root:epoch 1, iter 775, loss 7.13638, smoothed loss 6.29052, grad norm 2.95263, param norm 82.45944
INFO:root:epoch 1, iter 780, loss 6.50974, smoothed loss 6.27422, grad norm 2.45709, param norm 82.56389
INFO:root:epoch 1, iter 785, loss 6.32136, smoothed loss 6.25535, grad norm 2.72530, param norm 82.67341
INFO:root:epoch 1, iter 790, loss 5.65053, smoothed loss 6.24873, grad norm 2.34453, param norm 82.79844
INFO:root:epoch 1, iter 795, loss 6.56385, smoothed loss 6.25080, grad norm 2.27058, param norm 82.92895
INFO:root:epoch 1, iter 800, loss 6.96821, smoothed loss 6.25078, grad norm 2.99776, param norm 83.05936
Adding batches start...
Added  144  batches
INFO:root:epoch 1, iter 805, loss 6.11948, smoothed loss 6.24623, grad norm 2.52665, param norm 83.18597
INFO:root:epoch 1, iter 810, loss 6.21355, smoothed loss 6.22290, grad norm 2.65950, param norm 83.32860
INFO:root:epoch 1, iter 815, loss 5.77169, smoothed loss 6.20639, grad norm 2.48698, param norm 83.46137
INFO:root:epoch 1, iter 820, loss 5.90219, smoothed loss 6.19906, grad norm 2.68786, param norm 83.59602
INFO:root:epoch 1, iter 825, loss 5.93417, smoothed loss 6.19886, grad norm 2.58819, param norm 83.72751
INFO:root:epoch 1, iter 830, loss 5.68456, smoothed loss 6.19143, grad norm 2.28076, param norm 83.85081
INFO:root:epoch 1, iter 835, loss 6.78791, smoothed loss 6.18372, grad norm 2.96431, param norm 83.98155
INFO:root:epoch 1, iter 840, loss 6.18738, smoothed loss 6.17773, grad norm 2.48236, param norm 84.10952
INFO:root:epoch 1, iter 845, loss 6.60085, smoothed loss 6.18403, grad norm 2.54618, param norm 84.22855
INFO:root:epoch 1, iter 850, loss 5.82376, smoothed loss 6.18140, grad norm 2.11325, param norm 84.35082
INFO:root:epoch 1, iter 855, loss 6.09770, smoothed loss 6.16227, grad norm 2.26932, param norm 84.47541
INFO:root:epoch 1, iter 860, loss 5.74343, smoothed loss 6.15586, grad norm 2.49554, param norm 84.59187
INFO:root:epoch 1, iter 865, loss 5.88802, smoothed loss 6.14314, grad norm 2.68031, param norm 84.71267
INFO:root:epoch 1, iter 870, loss 6.30695, smoothed loss 6.14393, grad norm 2.76586, param norm 84.83646
INFO:root:epoch 1, iter 875, loss 6.34364, smoothed loss 6.14809, grad norm 2.45103, param norm 84.97206
INFO:root:epoch 1, iter 880, loss 6.20224, smoothed loss 6.14968, grad norm 2.57029, param norm 85.10040
INFO:root:epoch 1, iter 885, loss 6.42177, smoothed loss 6.13858, grad norm 2.49680, param norm 85.22883
INFO:root:epoch 1, iter 890, loss 6.25926, smoothed loss 6.13265, grad norm 2.61689, param norm 85.35841
INFO:root:epoch 1, iter 895, loss 6.20812, smoothed loss 6.12144, grad norm 2.47967, param norm 85.49012
INFO:root:epoch 1, iter 900, loss 5.64462, smoothed loss 6.10853, grad norm 2.11997, param norm 85.61088
INFO:root:epoch 1, iter 905, loss 5.65830, smoothed loss 6.10008, grad norm 2.46546, param norm 85.71737
INFO:root:epoch 1, iter 910, loss 5.25026, smoothed loss 6.08282, grad norm 2.47469, param norm 85.83487
INFO:root:epoch 1, iter 915, loss 5.56313, smoothed loss 6.05913, grad norm 2.74430, param norm 85.96816
INFO:root:epoch 1, iter 920, loss 5.83557, smoothed loss 6.06050, grad norm 3.02544, param norm 86.09870
INFO:root:epoch 1, iter 925, loss 5.98390, smoothed loss 6.04628, grad norm 2.34304, param norm 86.22946
INFO:root:epoch 1, iter 930, loss 6.13589, smoothed loss 6.03427, grad norm 2.39849, param norm 86.34788
INFO:root:epoch 1, iter 935, loss 6.70451, smoothed loss 6.03321, grad norm 2.60945, param norm 86.45601
INFO:root:epoch 1, iter 940, loss 6.88123, smoothed loss 6.03984, grad norm 2.84502, param norm 86.56115
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 945, loss 5.89760, smoothed loss 6.03311, grad norm 2.48975, param norm 86.66234
INFO:root:epoch 2, iter 950, loss 6.65689, smoothed loss 6.04916, grad norm 2.24731, param norm 86.76979
INFO:root:epoch 2, iter 955, loss 6.02660, smoothed loss 6.04094, grad norm 2.46430, param norm 86.88538
INFO:root:epoch 2, iter 960, loss 6.19984, smoothed loss 6.02757, grad norm 2.73738, param norm 87.00993
INFO:root:epoch 2, iter 965, loss 5.50985, smoothed loss 6.02847, grad norm 2.33499, param norm 87.11779
INFO:root:epoch 2, iter 970, loss 6.36349, smoothed loss 6.02534, grad norm 2.60342, param norm 87.22005
INFO:root:epoch 2, iter 975, loss 6.65821, smoothed loss 6.03452, grad norm 2.66088, param norm 87.32532
INFO:root:epoch 2, iter 980, loss 5.62535, smoothed loss 6.04343, grad norm 2.39468, param norm 87.43536
INFO:root:epoch 2, iter 985, loss 6.00671, smoothed loss 6.02695, grad norm 2.22826, param norm 87.54594
INFO:root:epoch 2, iter 990, loss 5.50808, smoothed loss 6.00662, grad norm 2.50609, param norm 87.66171
INFO:root:epoch 2, iter 995, loss 6.07310, smoothed loss 6.00756, grad norm 2.94771, param norm 87.77283
INFO:root:epoch 2, iter 1000, loss 6.18592, smoothed loss 6.01277, grad norm 2.49906, param norm 87.86883
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 2, Iter 1000, dev loss: 5.756270
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.81459 seconds [Score: 0.29116]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.55619 seconds [Score: 0.20300]
INFO:root:Epoch 2, Iter 1000, Train F1 score: 0.291163, Train EM score: 0.203000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.88440 seconds [Score: 0.27393]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.64854 seconds [Score: 0.18502]
INFO:root:Epoch 2, Iter 1000, Dev F1 score: 0.273926, Dev EM score: 0.185024
INFO:root:End of epoch 2
INFO:root:epoch 2, iter 1005, loss 6.03793, smoothed loss 6.00259, grad norm 2.46213, param norm 87.97215
INFO:root:epoch 2, iter 1010, loss 6.12591, smoothed loss 6.00018, grad norm 2.66152, param norm 88.08862
INFO:root:epoch 2, iter 1015, loss 6.43259, smoothed loss 5.99778, grad norm 2.64079, param norm 88.19417
INFO:root:epoch 2, iter 1020, loss 5.74907, smoothed loss 6.00178, grad norm 2.19886, param norm 88.29328
INFO:root:epoch 2, iter 1025, loss 5.77129, smoothed loss 5.98199, grad norm 2.55560, param norm 88.39158
INFO:root:epoch 2, iter 1030, loss 5.90497, smoothed loss 5.97416, grad norm 3.23440, param norm 88.50180
INFO:root:epoch 2, iter 1035, loss 5.23635, smoothed loss 5.94050, grad norm 2.55375, param norm 88.62495
INFO:root:epoch 2, iter 1040, loss 5.72214, smoothed loss 5.93410, grad norm 2.63078, param norm 88.74915
INFO:root:epoch 2, iter 1045, loss 5.53085, smoothed loss 5.92817, grad norm 2.26057, param norm 88.87038
INFO:root:epoch 2, iter 1050, loss 5.54068, smoothed loss 5.92449, grad norm 2.47166, param norm 88.97424
INFO:root:epoch 2, iter 1055, loss 5.31965, smoothed loss 5.90448, grad norm 2.44756, param norm 89.07352
INFO:root:epoch 2, iter 1060, loss 5.56711, smoothed loss 5.90639, grad norm 3.05295, param norm 89.18376
INFO:root:epoch 2, iter 1065, loss 6.58258, smoothed loss 5.91211, grad norm 2.93134, param norm 89.29818
INFO:root:epoch 2, iter 1070, loss 5.29451, smoothed loss 5.89014, grad norm 2.29753, param norm 89.41038
INFO:root:epoch 2, iter 1075, loss 5.44959, smoothed loss 5.88130, grad norm 2.21929, param norm 89.51791
INFO:root:epoch 2, iter 1080, loss 5.51305, smoothed loss 5.86380, grad norm 3.10701, param norm 89.61517
INFO:root:epoch 2, iter 1085, loss 5.99488, smoothed loss 5.86702, grad norm 2.84255, param norm 89.70493
INFO:root:epoch 2, iter 1090, loss 5.70424, smoothed loss 5.86542, grad norm 2.43287, param norm 89.80210
INFO:root:epoch 2, iter 1095, loss 5.61819, smoothed loss 5.85470, grad norm 2.24147, param norm 89.91301
INFO:root:epoch 2, iter 1100, loss 6.40067, smoothed loss 5.86009, grad norm 2.56491, param norm 90.02944
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1105, loss 5.68008, smoothed loss 5.86155, grad norm 2.37926, param norm 90.14618
INFO:root:epoch 2, iter 1110, loss 5.83864, smoothed loss 5.86648, grad norm 2.83498, param norm 90.24409
INFO:root:epoch 2, iter 1115, loss 6.08087, smoothed loss 5.86473, grad norm 2.88897, param norm 90.34174
INFO:root:epoch 2, iter 1120, loss 6.31925, smoothed loss 5.85102, grad norm 2.95418, param norm 90.43691
INFO:root:epoch 2, iter 1125, loss 6.40327, smoothed loss 5.84214, grad norm 2.78943, param norm 90.53632
INFO:root:epoch 2, iter 1130, loss 6.77675, smoothed loss 5.85761, grad norm 2.68143, param norm 90.63763
INFO:root:epoch 2, iter 1135, loss 6.28033, smoothed loss 5.85137, grad norm 2.56315, param norm 90.73550
INFO:root:epoch 2, iter 1140, loss 5.76728, smoothed loss 5.83777, grad norm 2.35951, param norm 90.83493
INFO:root:epoch 2, iter 1145, loss 5.85205, smoothed loss 5.83641, grad norm 2.54666, param norm 90.93874
INFO:root:epoch 2, iter 1150, loss 5.64010, smoothed loss 5.82494, grad norm 2.38139, param norm 91.05058
INFO:root:epoch 2, iter 1155, loss 5.50634, smoothed loss 5.81577, grad norm 2.53707, param norm 91.16519
INFO:root:epoch 2, iter 1160, loss 6.43521, smoothed loss 5.81377, grad norm 2.60638, param norm 91.27223
INFO:root:epoch 2, iter 1165, loss 5.50960, smoothed loss 5.80087, grad norm 2.35309, param norm 91.37398
INFO:root:epoch 2, iter 1170, loss 5.38297, smoothed loss 5.79019, grad norm 2.58437, param norm 91.47931
INFO:root:epoch 2, iter 1175, loss 6.01764, smoothed loss 5.80423, grad norm 2.82989, param norm 91.58272
INFO:root:epoch 2, iter 1180, loss 5.06246, smoothed loss 5.78654, grad norm 2.53889, param norm 91.66721
INFO:root:epoch 2, iter 1185, loss 5.62797, smoothed loss 5.77336, grad norm 2.42627, param norm 91.76463
INFO:root:epoch 2, iter 1190, loss 5.86414, smoothed loss 5.77865, grad norm 2.67795, param norm 91.87520
INFO:root:epoch 2, iter 1195, loss 5.70401, smoothed loss 5.76600, grad norm 2.53987, param norm 91.98398
INFO:root:epoch 2, iter 1200, loss 5.27193, smoothed loss 5.75267, grad norm 2.90890, param norm 92.09524
INFO:root:epoch 2, iter 1205, loss 5.61589, smoothed loss 5.74816, grad norm 3.03520, param norm 92.21251
INFO:root:epoch 2, iter 1210, loss 6.09046, smoothed loss 5.74240, grad norm 2.73157, param norm 92.31747
INFO:root:epoch 2, iter 1215, loss 4.88353, smoothed loss 5.72083, grad norm 2.41192, param norm 92.42540
INFO:root:epoch 2, iter 1220, loss 6.30529, smoothed loss 5.73790, grad norm 3.15350, param norm 92.53388
INFO:root:epoch 2, iter 1225, loss 5.79509, smoothed loss 5.72531, grad norm 2.43399, param norm 92.63527
INFO:root:epoch 2, iter 1230, loss 4.99041, smoothed loss 5.70645, grad norm 2.65516, param norm 92.73562
INFO:root:epoch 2, iter 1235, loss 5.94638, smoothed loss 5.69813, grad norm 2.89202, param norm 92.83623
INFO:root:epoch 2, iter 1240, loss 5.67938, smoothed loss 5.70122, grad norm 2.73886, param norm 92.93179
INFO:root:epoch 2, iter 1245, loss 5.36878, smoothed loss 5.68471, grad norm 2.36413, param norm 93.03366
INFO:root:epoch 2, iter 1250, loss 5.58157, smoothed loss 5.69456, grad norm 2.66701, param norm 93.14165
INFO:root:epoch 2, iter 1255, loss 5.59921, smoothed loss 5.68203, grad norm 2.52126, param norm 93.24732
INFO:root:epoch 2, iter 1260, loss 5.56494, smoothed loss 5.67661, grad norm 3.29459, param norm 93.35465
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1265, loss 5.32177, smoothed loss 5.68086, grad norm 2.73619, param norm 93.46128
INFO:root:epoch 2, iter 1270, loss 5.83175, smoothed loss 5.68051, grad norm 2.61668, param norm 93.56908
INFO:root:epoch 2, iter 1275, loss 5.68807, smoothed loss 5.67631, grad norm 2.66168, param norm 93.67542
INFO:root:epoch 2, iter 1280, loss 5.18024, smoothed loss 5.65459, grad norm 2.61539, param norm 93.78803
INFO:root:epoch 2, iter 1285, loss 5.63812, smoothed loss 5.65884, grad norm 2.73305, param norm 93.89322
INFO:root:epoch 2, iter 1290, loss 5.15506, smoothed loss 5.65348, grad norm 3.16213, param norm 93.98035
INFO:root:epoch 2, iter 1295, loss 6.09294, smoothed loss 5.65421, grad norm 3.61243, param norm 94.08204
INFO:root:epoch 2, iter 1300, loss 5.72309, smoothed loss 5.64466, grad norm 2.59872, param norm 94.19106
INFO:root:epoch 2, iter 1305, loss 5.93661, smoothed loss 5.64771, grad norm 2.88988, param norm 94.28999
INFO:root:epoch 2, iter 1310, loss 6.19158, smoothed loss 5.64247, grad norm 3.11774, param norm 94.40188
INFO:root:epoch 2, iter 1315, loss 5.45778, smoothed loss 5.63653, grad norm 2.88440, param norm 94.52556
INFO:root:epoch 2, iter 1320, loss 5.16653, smoothed loss 5.62444, grad norm 2.66892, param norm 94.63602
INFO:root:epoch 2, iter 1325, loss 5.38426, smoothed loss 5.61279, grad norm 3.01786, param norm 94.73123
INFO:root:epoch 2, iter 1330, loss 5.48966, smoothed loss 5.61962, grad norm 2.63205, param norm 94.82024
INFO:root:epoch 2, iter 1335, loss 4.91064, smoothed loss 5.61800, grad norm 2.76450, param norm 94.91787
INFO:root:epoch 2, iter 1340, loss 4.44218, smoothed loss 5.60895, grad norm 2.62748, param norm 95.01907
INFO:root:epoch 2, iter 1345, loss 5.80678, smoothed loss 5.59339, grad norm 3.33716, param norm 95.12428
INFO:root:epoch 2, iter 1350, loss 5.05281, smoothed loss 5.58163, grad norm 2.58344, param norm 95.23042
INFO:root:epoch 2, iter 1355, loss 5.44863, smoothed loss 5.58096, grad norm 3.02120, param norm 95.34207
INFO:root:epoch 2, iter 1360, loss 5.39941, smoothed loss 5.56713, grad norm 2.66289, param norm 95.43956
INFO:root:epoch 2, iter 1365, loss 5.93144, smoothed loss 5.56547, grad norm 2.95722, param norm 95.53354
INFO:root:epoch 2, iter 1370, loss 6.07855, smoothed loss 5.57848, grad norm 3.12704, param norm 95.63850
INFO:root:epoch 2, iter 1375, loss 5.36997, smoothed loss 5.57378, grad norm 2.64739, param norm 95.75408
INFO:root:epoch 2, iter 1380, loss 5.38492, smoothed loss 5.57661, grad norm 2.80575, param norm 95.86549
INFO:root:epoch 2, iter 1385, loss 6.16649, smoothed loss 5.57017, grad norm 2.87515, param norm 95.96926
INFO:root:epoch 2, iter 1390, loss 5.44103, smoothed loss 5.56688, grad norm 2.48771, param norm 96.06750
INFO:root:epoch 2, iter 1395, loss 5.25403, smoothed loss 5.58065, grad norm 2.70237, param norm 96.15840
INFO:root:epoch 2, iter 1400, loss 5.63792, smoothed loss 5.57374, grad norm 2.52690, param norm 96.24300
INFO:root:epoch 2, iter 1405, loss 5.80764, smoothed loss 5.58129, grad norm 2.33016, param norm 96.32423
INFO:root:epoch 2, iter 1410, loss 5.48181, smoothed loss 5.58636, grad norm 2.46494, param norm 96.41544
INFO:root:epoch 2, iter 1415, loss 5.47950, smoothed loss 5.57274, grad norm 2.48266, param norm 96.51477
INFO:root:epoch 2, iter 1420, loss 5.44165, smoothed loss 5.55996, grad norm 2.95435, param norm 96.61423
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1425, loss 5.17621, smoothed loss 5.56517, grad norm 2.66247, param norm 96.72538
INFO:root:epoch 2, iter 1430, loss 4.38744, smoothed loss 5.54789, grad norm 2.29188, param norm 96.83749
INFO:root:epoch 2, iter 1435, loss 5.13343, smoothed loss 5.54629, grad norm 2.52158, param norm 96.94750
INFO:root:epoch 2, iter 1440, loss 5.06758, smoothed loss 5.53193, grad norm 2.73283, param norm 97.05099
INFO:root:epoch 2, iter 1445, loss 5.28897, smoothed loss 5.53169, grad norm 2.72562, param norm 97.14846
INFO:root:epoch 2, iter 1450, loss 4.88175, smoothed loss 5.51566, grad norm 2.49717, param norm 97.23785
INFO:root:epoch 2, iter 1455, loss 4.96105, smoothed loss 5.51386, grad norm 2.18435, param norm 97.33022
INFO:root:epoch 2, iter 1460, loss 5.94817, smoothed loss 5.52155, grad norm 3.00815, param norm 97.42901
INFO:root:epoch 2, iter 1465, loss 5.63536, smoothed loss 5.51228, grad norm 2.55591, param norm 97.53312
INFO:root:epoch 2, iter 1470, loss 4.72879, smoothed loss 5.50149, grad norm 2.86538, param norm 97.63346
INFO:root:epoch 2, iter 1475, loss 5.35192, smoothed loss 5.50182, grad norm 2.50607, param norm 97.73180
INFO:root:epoch 2, iter 1480, loss 5.68980, smoothed loss 5.49565, grad norm 3.21615, param norm 97.82668
INFO:root:epoch 2, iter 1485, loss 6.03871, smoothed loss 5.49013, grad norm 2.93880, param norm 97.92463
INFO:root:epoch 2, iter 1490, loss 5.56436, smoothed loss 5.47426, grad norm 2.81651, param norm 98.02108
INFO:root:epoch 2, iter 1495, loss 5.51244, smoothed loss 5.46772, grad norm 2.79160, param norm 98.12263
INFO:root:epoch 2, iter 1500, loss 4.81033, smoothed loss 5.46949, grad norm 2.83473, param norm 98.21806
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 2, Iter 1500, dev loss: 5.261852
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.39405 seconds [Score: 0.38160]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.21783 seconds [Score: 0.25600]
INFO:root:Epoch 2, Iter 1500, Train F1 score: 0.381600, Train EM score: 0.256000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 115.96235 seconds [Score: 0.33858]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 115.86191 seconds [Score: 0.23925]
INFO:root:Epoch 2, Iter 1500, Dev F1 score: 0.338581, Dev EM score: 0.239253
INFO:root:End of epoch 2
INFO:root:epoch 2, iter 1505, loss 5.74129, smoothed loss 5.46456, grad norm 3.28566, param norm 98.31407
INFO:root:epoch 2, iter 1510, loss 5.21644, smoothed loss 5.44039, grad norm 2.63778, param norm 98.41782
INFO:root:epoch 2, iter 1515, loss 4.60984, smoothed loss 5.44005, grad norm 2.94569, param norm 98.50950
INFO:root:epoch 2, iter 1520, loss 4.78156, smoothed loss 5.42741, grad norm 2.70418, param norm 98.60547
INFO:root:epoch 2, iter 1525, loss 5.88786, smoothed loss 5.43499, grad norm 3.24877, param norm 98.70151
INFO:root:epoch 2, iter 1530, loss 5.99577, smoothed loss 5.43534, grad norm 3.00649, param norm 98.78854
INFO:root:epoch 2, iter 1535, loss 5.39627, smoothed loss 5.42545, grad norm 3.10358, param norm 98.87891
INFO:root:epoch 2, iter 1540, loss 4.69372, smoothed loss 5.41156, grad norm 3.07300, param norm 98.97818
INFO:root:epoch 2, iter 1545, loss 5.94226, smoothed loss 5.41397, grad norm 3.46477, param norm 99.07699
INFO:root:epoch 2, iter 1550, loss 5.47638, smoothed loss 5.41761, grad norm 2.88002, param norm 99.16877
INFO:root:epoch 2, iter 1555, loss 5.67554, smoothed loss 5.41770, grad norm 2.40473, param norm 99.25786
INFO:root:epoch 2, iter 1560, loss 5.43837, smoothed loss 5.40669, grad norm 2.71186, param norm 99.36247
INFO:root:epoch 2, iter 1565, loss 5.23545, smoothed loss 5.39455, grad norm 2.91404, param norm 99.47150
INFO:root:epoch 2, iter 1570, loss 5.32332, smoothed loss 5.39175, grad norm 3.19146, param norm 99.56882
INFO:root:epoch 2, iter 1575, loss 5.04339, smoothed loss 5.37954, grad norm 2.82312, param norm 99.66058
INFO:root:epoch 2, iter 1580, loss 5.26796, smoothed loss 5.36771, grad norm 2.84579, param norm 99.75245
Adding batches start...
Added  160  batches
INFO:root:epoch 2, iter 1585, loss 4.55380, smoothed loss 5.35031, grad norm 2.77208, param norm 99.85099
INFO:root:epoch 2, iter 1590, loss 4.70969, smoothed loss 5.33603, grad norm 2.64866, param norm 99.95047
INFO:root:epoch 2, iter 1595, loss 5.51227, smoothed loss 5.34181, grad norm 3.01416, param norm 100.03887
INFO:root:epoch 2, iter 1600, loss 5.15850, smoothed loss 5.31893, grad norm 2.78090, param norm 100.13105
INFO:root:epoch 2, iter 1605, loss 4.25480, smoothed loss 5.30348, grad norm 2.69266, param norm 100.21851
INFO:root:epoch 2, iter 1610, loss 5.05386, smoothed loss 5.31041, grad norm 2.85678, param norm 100.31456
INFO:root:epoch 2, iter 1615, loss 5.13138, smoothed loss 5.30863, grad norm 2.71814, param norm 100.41803
INFO:root:epoch 2, iter 1620, loss 5.75111, smoothed loss 5.31750, grad norm 2.99432, param norm 100.52319
INFO:root:epoch 2, iter 1625, loss 5.77211, smoothed loss 5.31661, grad norm 2.84966, param norm 100.62685
INFO:root:epoch 2, iter 1630, loss 5.41094, smoothed loss 5.30626, grad norm 2.57624, param norm 100.72200
INFO:root:epoch 2, iter 1635, loss 6.03472, smoothed loss 5.30889, grad norm 3.09403, param norm 100.81859
INFO:root:epoch 2, iter 1640, loss 5.15385, smoothed loss 5.30928, grad norm 2.94772, param norm 100.91290
INFO:root:epoch 2, iter 1645, loss 5.20863, smoothed loss 5.31186, grad norm 2.91804, param norm 100.99957
INFO:root:epoch 2, iter 1650, loss 4.47576, smoothed loss 5.30447, grad norm 2.52524, param norm 101.08062
INFO:root:epoch 2, iter 1655, loss 5.31256, smoothed loss 5.28781, grad norm 3.00708, param norm 101.16431
INFO:root:epoch 2, iter 1660, loss 5.00708, smoothed loss 5.28940, grad norm 2.96018, param norm 101.25643
INFO:root:epoch 2, iter 1665, loss 4.54821, smoothed loss 5.28163, grad norm 2.63016, param norm 101.34977
INFO:root:epoch 2, iter 1670, loss 4.98529, smoothed loss 5.27691, grad norm 2.51457, param norm 101.44543
INFO:root:epoch 2, iter 1675, loss 5.53472, smoothed loss 5.27286, grad norm 3.50280, param norm 101.54009
INFO:root:epoch 2, iter 1680, loss 5.16695, smoothed loss 5.27277, grad norm 4.25575, param norm 101.62450
INFO:root:epoch 2, iter 1685, loss 5.76494, smoothed loss 5.29014, grad norm 3.50020, param norm 101.71846
INFO:root:epoch 2, iter 1690, loss 5.31570, smoothed loss 5.27140, grad norm 3.15396, param norm 101.82375
INFO:root:epoch 2, iter 1695, loss 5.98079, smoothed loss 5.27812, grad norm 3.00311, param norm 101.93636
INFO:root:epoch 2, iter 1700, loss 5.12743, smoothed loss 5.26304, grad norm 2.98591, param norm 102.04090
INFO:root:epoch 2, iter 1705, loss 5.86528, smoothed loss 5.26887, grad norm 3.61477, param norm 102.13662
INFO:root:epoch 2, iter 1710, loss 5.72948, smoothed loss 5.27102, grad norm 3.30928, param norm 102.23354
INFO:root:epoch 2, iter 1715, loss 5.07740, smoothed loss 5.25525, grad norm 2.85079, param norm 102.33354
INFO:root:epoch 2, iter 1720, loss 4.98002, smoothed loss 5.25920, grad norm 2.71094, param norm 102.44041
INFO:root:epoch 2, iter 1725, loss 5.47281, smoothed loss 5.25834, grad norm 2.71768, param norm 102.54922
INFO:root:epoch 2, iter 1730, loss 5.15397, smoothed loss 5.23817, grad norm 2.58181, param norm 102.65949
INFO:root:epoch 2, iter 1735, loss 5.78196, smoothed loss 5.23389, grad norm 3.26544, param norm 102.76950
INFO:root:epoch 2, iter 1740, loss 5.36062, smoothed loss 5.22885, grad norm 2.95100, param norm 102.86717
Adding batches start...
Added  144  batches
INFO:root:epoch 2, iter 1745, loss 4.92557, smoothed loss 5.21484, grad norm 3.02020, param norm 102.95847
INFO:root:epoch 2, iter 1750, loss 5.57746, smoothed loss 5.21762, grad norm 2.95971, param norm 103.05486
INFO:root:epoch 2, iter 1755, loss 5.07253, smoothed loss 5.20007, grad norm 2.67131, param norm 103.14249
INFO:root:epoch 2, iter 1760, loss 5.38928, smoothed loss 5.18579, grad norm 3.10419, param norm 103.22881
INFO:root:epoch 2, iter 1765, loss 5.65714, smoothed loss 5.18379, grad norm 2.94397, param norm 103.31342
INFO:root:epoch 2, iter 1770, loss 4.93137, smoothed loss 5.19316, grad norm 2.85947, param norm 103.39680
INFO:root:epoch 2, iter 1775, loss 4.96022, smoothed loss 5.20188, grad norm 2.66568, param norm 103.48618
INFO:root:epoch 2, iter 1780, loss 5.69782, smoothed loss 5.21281, grad norm 3.48351, param norm 103.57098
INFO:root:epoch 2, iter 1785, loss 4.80537, smoothed loss 5.19715, grad norm 2.91102, param norm 103.66042
INFO:root:epoch 2, iter 1790, loss 5.09933, smoothed loss 5.20956, grad norm 3.18983, param norm 103.75204
INFO:root:epoch 2, iter 1795, loss 5.28604, smoothed loss 5.19064, grad norm 3.12440, param norm 103.85081
INFO:root:epoch 2, iter 1800, loss 4.93954, smoothed loss 5.18859, grad norm 3.12040, param norm 103.95973
INFO:root:epoch 2, iter 1805, loss 4.88855, smoothed loss 5.16922, grad norm 2.98845, param norm 104.07409
INFO:root:epoch 2, iter 1810, loss 5.43408, smoothed loss 5.16392, grad norm 2.95052, param norm 104.17443
INFO:root:epoch 2, iter 1815, loss 5.60257, smoothed loss 5.17237, grad norm 3.83298, param norm 104.26703
INFO:root:epoch 2, iter 1820, loss 4.71903, smoothed loss 5.16761, grad norm 3.15886, param norm 104.35150
INFO:root:epoch 2, iter 1825, loss 4.54379, smoothed loss 5.16001, grad norm 3.00112, param norm 104.44750
INFO:root:epoch 2, iter 1830, loss 5.45728, smoothed loss 5.14947, grad norm 3.01942, param norm 104.55108
INFO:root:epoch 2, iter 1835, loss 4.81061, smoothed loss 5.14065, grad norm 3.13094, param norm 104.64993
INFO:root:epoch 2, iter 1840, loss 5.44757, smoothed loss 5.14451, grad norm 3.05956, param norm 104.74610
INFO:root:epoch 2, iter 1845, loss 5.28184, smoothed loss 5.14466, grad norm 3.04790, param norm 104.83607
INFO:root:epoch 2, iter 1850, loss 5.36944, smoothed loss 5.16627, grad norm 2.78021, param norm 104.92949
INFO:root:epoch 2, iter 1855, loss 5.25365, smoothed loss 5.16546, grad norm 2.99196, param norm 105.02322
INFO:root:epoch 2, iter 1860, loss 4.64682, smoothed loss 5.16038, grad norm 2.86708, param norm 105.11359
INFO:root:epoch 2, iter 1865, loss 5.17278, smoothed loss 5.15699, grad norm 3.02590, param norm 105.20564
INFO:root:epoch 2, iter 1870, loss 4.88360, smoothed loss 5.15702, grad norm 2.60295, param norm 105.29976
INFO:root:epoch 2, iter 1875, loss 4.75622, smoothed loss 5.13529, grad norm 3.06739, param norm 105.39751
INFO:root:epoch 2, iter 1880, loss 4.69254, smoothed loss 5.14234, grad norm 2.80585, param norm 105.49301
INFO:root:epoch 2, iter 1885, loss 4.58141, smoothed loss 5.12793, grad norm 2.74723, param norm 105.58717
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 1890, loss 4.90228, smoothed loss 5.11291, grad norm 2.90237, param norm 105.68134
INFO:root:epoch 3, iter 1895, loss 5.04826, smoothed loss 5.10945, grad norm 2.98145, param norm 105.77348
INFO:root:epoch 3, iter 1900, loss 4.32038, smoothed loss 5.09727, grad norm 2.73089, param norm 105.85033
INFO:root:epoch 3, iter 1905, loss 4.69766, smoothed loss 5.09889, grad norm 2.88260, param norm 105.93867
INFO:root:epoch 3, iter 1910, loss 5.24304, smoothed loss 5.08484, grad norm 3.20049, param norm 106.04169
INFO:root:epoch 3, iter 1915, loss 4.85286, smoothed loss 5.07286, grad norm 3.30731, param norm 106.14515
INFO:root:epoch 3, iter 1920, loss 4.45509, smoothed loss 5.07677, grad norm 3.01435, param norm 106.24134
INFO:root:epoch 3, iter 1925, loss 5.15597, smoothed loss 5.07170, grad norm 3.34328, param norm 106.32611
INFO:root:epoch 3, iter 1930, loss 5.06852, smoothed loss 5.06935, grad norm 3.06170, param norm 106.41605
INFO:root:epoch 3, iter 1935, loss 5.23654, smoothed loss 5.07309, grad norm 3.41597, param norm 106.51521
INFO:root:epoch 3, iter 1940, loss 5.36576, smoothed loss 5.06701, grad norm 3.11532, param norm 106.62563
INFO:root:epoch 3, iter 1945, loss 4.84688, smoothed loss 5.06093, grad norm 2.88301, param norm 106.72678
INFO:root:epoch 3, iter 1950, loss 4.89448, smoothed loss 5.05515, grad norm 2.86622, param norm 106.80996
INFO:root:epoch 3, iter 1955, loss 5.28145, smoothed loss 5.04689, grad norm 3.21585, param norm 106.88907
INFO:root:epoch 3, iter 1960, loss 4.94488, smoothed loss 5.04604, grad norm 3.16773, param norm 106.97299
INFO:root:epoch 3, iter 1965, loss 4.85612, smoothed loss 5.03935, grad norm 3.06357, param norm 107.06606
INFO:root:epoch 3, iter 1970, loss 5.49816, smoothed loss 5.03809, grad norm 3.05657, param norm 107.16190
INFO:root:epoch 3, iter 1975, loss 4.73404, smoothed loss 5.03859, grad norm 2.57903, param norm 107.25357
INFO:root:epoch 3, iter 1980, loss 4.64350, smoothed loss 5.02892, grad norm 2.95665, param norm 107.34753
INFO:root:epoch 3, iter 1985, loss 5.18838, smoothed loss 5.02082, grad norm 3.41794, param norm 107.43730
INFO:root:epoch 3, iter 1990, loss 4.81221, smoothed loss 5.02401, grad norm 2.86640, param norm 107.52887
INFO:root:epoch 3, iter 1995, loss 5.47818, smoothed loss 5.03255, grad norm 3.52979, param norm 107.61698
INFO:root:epoch 3, iter 2000, loss 5.19546, smoothed loss 5.02982, grad norm 2.58948, param norm 107.70059
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 3, Iter 2000, dev loss: 4.705225
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.81577 seconds [Score: 0.48081]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.38195 seconds [Score: 0.39700]
INFO:root:Epoch 3, Iter 2000, Train F1 score: 0.480814, Train EM score: 0.397000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.96058 seconds [Score: 0.42046]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.57959 seconds [Score: 0.29727]
INFO:root:Epoch 3, Iter 2000, Dev F1 score: 0.420462, Dev EM score: 0.297275
INFO:root:End of epoch 3
INFO:root:epoch 3, iter 2005, loss 5.39573, smoothed loss 5.04512, grad norm 2.96279, param norm 107.78817
INFO:root:epoch 3, iter 2010, loss 5.57284, smoothed loss 5.05813, grad norm 3.29094, param norm 107.87690
INFO:root:epoch 3, iter 2015, loss 4.99315, smoothed loss 5.04204, grad norm 2.88821, param norm 107.96976
INFO:root:epoch 3, iter 2020, loss 4.70516, smoothed loss 5.03344, grad norm 2.86106, param norm 108.05432
INFO:root:epoch 3, iter 2025, loss 3.89827, smoothed loss 5.00686, grad norm 2.98616, param norm 108.13962
INFO:root:epoch 3, iter 2030, loss 4.54609, smoothed loss 5.00926, grad norm 3.34885, param norm 108.22621
INFO:root:epoch 3, iter 2035, loss 4.68001, smoothed loss 4.98323, grad norm 3.39834, param norm 108.31978
INFO:root:epoch 3, iter 2040, loss 4.42120, smoothed loss 4.96535, grad norm 2.98692, param norm 108.41418
INFO:root:epoch 3, iter 2045, loss 4.43242, smoothed loss 4.95220, grad norm 2.82806, param norm 108.49865
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2050, loss 5.27437, smoothed loss 4.94779, grad norm 3.02776, param norm 108.57611
INFO:root:epoch 3, iter 2055, loss 3.76395, smoothed loss 4.92937, grad norm 3.16041, param norm 108.66391
INFO:root:epoch 3, iter 2060, loss 4.81260, smoothed loss 4.90887, grad norm 3.34307, param norm 108.77031
INFO:root:epoch 3, iter 2065, loss 5.07048, smoothed loss 4.91249, grad norm 3.29365, param norm 108.86917
INFO:root:epoch 3, iter 2070, loss 4.32818, smoothed loss 4.90660, grad norm 3.34249, param norm 108.95940
INFO:root:epoch 3, iter 2075, loss 4.27347, smoothed loss 4.88620, grad norm 3.01588, param norm 109.05512
INFO:root:epoch 3, iter 2080, loss 4.83157, smoothed loss 4.87911, grad norm 3.21984, param norm 109.15250
INFO:root:epoch 3, iter 2085, loss 4.21312, smoothed loss 4.86969, grad norm 3.16864, param norm 109.25225
INFO:root:epoch 3, iter 2090, loss 4.95367, smoothed loss 4.86373, grad norm 2.69930, param norm 109.34895
INFO:root:epoch 3, iter 2095, loss 4.99911, smoothed loss 4.87388, grad norm 2.97736, param norm 109.44301
INFO:root:epoch 3, iter 2100, loss 5.52465, smoothed loss 4.88496, grad norm 2.82808, param norm 109.53242
INFO:root:epoch 3, iter 2105, loss 5.11422, smoothed loss 4.88618, grad norm 3.11982, param norm 109.62347
INFO:root:epoch 3, iter 2110, loss 3.49680, smoothed loss 4.86083, grad norm 2.74618, param norm 109.71940
INFO:root:epoch 3, iter 2115, loss 4.96886, smoothed loss 4.86267, grad norm 3.32156, param norm 109.81193
INFO:root:epoch 3, iter 2120, loss 4.64679, smoothed loss 4.85993, grad norm 3.16204, param norm 109.90143
INFO:root:epoch 3, iter 2125, loss 5.12104, smoothed loss 4.85122, grad norm 3.14192, param norm 109.98343
INFO:root:epoch 3, iter 2130, loss 5.02817, smoothed loss 4.84507, grad norm 3.05585, param norm 110.07511
INFO:root:epoch 3, iter 2135, loss 4.25769, smoothed loss 4.83755, grad norm 2.87341, param norm 110.16274
INFO:root:epoch 3, iter 2140, loss 5.61159, smoothed loss 4.83982, grad norm 3.35782, param norm 110.23978
INFO:root:epoch 3, iter 2145, loss 4.75643, smoothed loss 4.84289, grad norm 3.08420, param norm 110.31808
INFO:root:epoch 3, iter 2150, loss 4.41207, smoothed loss 4.83090, grad norm 3.20054, param norm 110.39881
INFO:root:epoch 3, iter 2155, loss 4.22053, smoothed loss 4.81232, grad norm 2.78005, param norm 110.47874
INFO:root:epoch 3, iter 2160, loss 4.62618, smoothed loss 4.80249, grad norm 3.54602, param norm 110.55663
INFO:root:epoch 3, iter 2165, loss 4.77520, smoothed loss 4.79895, grad norm 3.07057, param norm 110.63814
INFO:root:epoch 3, iter 2170, loss 5.16250, smoothed loss 4.78929, grad norm 3.36967, param norm 110.72422
INFO:root:epoch 3, iter 2175, loss 3.99039, smoothed loss 4.77356, grad norm 3.38336, param norm 110.80730
INFO:root:epoch 3, iter 2180, loss 4.57547, smoothed loss 4.76643, grad norm 3.53535, param norm 110.89328
INFO:root:epoch 3, iter 2185, loss 4.36035, smoothed loss 4.75853, grad norm 3.20653, param norm 110.97094
INFO:root:epoch 3, iter 2190, loss 4.42813, smoothed loss 4.74722, grad norm 3.12135, param norm 111.05287
INFO:root:epoch 3, iter 2195, loss 4.12628, smoothed loss 4.75014, grad norm 2.95909, param norm 111.14694
INFO:root:epoch 3, iter 2200, loss 4.47953, smoothed loss 4.73887, grad norm 2.95949, param norm 111.23638
INFO:root:epoch 3, iter 2205, loss 4.35265, smoothed loss 4.74164, grad norm 2.98851, param norm 111.31836
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2210, loss 5.17697, smoothed loss 4.72451, grad norm 3.35497, param norm 111.40218
INFO:root:epoch 3, iter 2215, loss 4.26496, smoothed loss 4.71259, grad norm 2.95705, param norm 111.48328
INFO:root:epoch 3, iter 2220, loss 4.24360, smoothed loss 4.70222, grad norm 2.95627, param norm 111.56858
INFO:root:epoch 3, iter 2225, loss 4.34464, smoothed loss 4.70130, grad norm 2.94306, param norm 111.66745
INFO:root:epoch 3, iter 2230, loss 5.02211, smoothed loss 4.70730, grad norm 3.26791, param norm 111.76585
INFO:root:epoch 3, iter 2235, loss 4.12482, smoothed loss 4.69155, grad norm 3.07620, param norm 111.86057
INFO:root:epoch 3, iter 2240, loss 4.80820, smoothed loss 4.68019, grad norm 3.23238, param norm 111.94798
INFO:root:epoch 3, iter 2245, loss 5.28371, smoothed loss 4.67149, grad norm 3.63401, param norm 112.01963
INFO:root:epoch 3, iter 2250, loss 4.50725, smoothed loss 4.67258, grad norm 3.42301, param norm 112.09467
INFO:root:epoch 3, iter 2255, loss 4.48559, smoothed loss 4.67830, grad norm 2.98880, param norm 112.17514
INFO:root:epoch 3, iter 2260, loss 4.64311, smoothed loss 4.67416, grad norm 3.01456, param norm 112.25909
INFO:root:epoch 3, iter 2265, loss 4.99843, smoothed loss 4.68005, grad norm 3.41031, param norm 112.33632
INFO:root:epoch 3, iter 2270, loss 6.06649, smoothed loss 4.68976, grad norm 3.66557, param norm 112.41306
INFO:root:epoch 3, iter 2275, loss 4.74064, smoothed loss 4.68214, grad norm 2.90162, param norm 112.48827
INFO:root:epoch 3, iter 2280, loss 4.11716, smoothed loss 4.65476, grad norm 2.80402, param norm 112.57848
INFO:root:epoch 3, iter 2285, loss 5.14073, smoothed loss 4.64306, grad norm 3.36036, param norm 112.67540
INFO:root:epoch 3, iter 2290, loss 4.78105, smoothed loss 4.64031, grad norm 3.63192, param norm 112.76956
INFO:root:epoch 3, iter 2295, loss 4.47126, smoothed loss 4.63217, grad norm 3.10506, param norm 112.83736
INFO:root:epoch 3, iter 2300, loss 5.06431, smoothed loss 4.60945, grad norm 3.62363, param norm 112.90200
INFO:root:epoch 3, iter 2305, loss 5.10772, smoothed loss 4.61938, grad norm 3.53275, param norm 112.98186
INFO:root:epoch 3, iter 2310, loss 4.01885, smoothed loss 4.59718, grad norm 2.92622, param norm 113.06487
INFO:root:epoch 3, iter 2315, loss 4.77364, smoothed loss 4.59255, grad norm 3.20388, param norm 113.15263
INFO:root:epoch 3, iter 2320, loss 5.36806, smoothed loss 4.58630, grad norm 3.46078, param norm 113.23914
INFO:root:epoch 3, iter 2325, loss 4.80876, smoothed loss 4.57947, grad norm 3.19491, param norm 113.32879
INFO:root:epoch 3, iter 2330, loss 4.80518, smoothed loss 4.57530, grad norm 3.23352, param norm 113.41065
INFO:root:epoch 3, iter 2335, loss 5.24442, smoothed loss 4.55959, grad norm 3.57053, param norm 113.47612
INFO:root:epoch 3, iter 2340, loss 4.16986, smoothed loss 4.55460, grad norm 3.44484, param norm 113.54176
INFO:root:epoch 3, iter 2345, loss 4.53993, smoothed loss 4.55978, grad norm 3.22297, param norm 113.61233
INFO:root:epoch 3, iter 2350, loss 4.74594, smoothed loss 4.56241, grad norm 3.35993, param norm 113.68499
INFO:root:epoch 3, iter 2355, loss 5.03452, smoothed loss 4.56176, grad norm 3.43882, param norm 113.77204
INFO:root:epoch 3, iter 2360, loss 4.84699, smoothed loss 4.56894, grad norm 3.26837, param norm 113.86545
INFO:root:epoch 3, iter 2365, loss 4.21298, smoothed loss 4.56111, grad norm 2.96209, param norm 113.96534
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2370, loss 4.51056, smoothed loss 4.56519, grad norm 2.94353, param norm 114.05373
INFO:root:epoch 3, iter 2375, loss 4.26754, smoothed loss 4.53899, grad norm 3.25272, param norm 114.13365
INFO:root:epoch 3, iter 2380, loss 4.54375, smoothed loss 4.52451, grad norm 3.30619, param norm 114.21316
INFO:root:epoch 3, iter 2385, loss 3.81700, smoothed loss 4.50427, grad norm 3.40153, param norm 114.28410
INFO:root:epoch 3, iter 2390, loss 4.21883, smoothed loss 4.48033, grad norm 3.35424, param norm 114.34634
INFO:root:epoch 3, iter 2395, loss 3.75491, smoothed loss 4.47023, grad norm 2.97428, param norm 114.41463
INFO:root:epoch 3, iter 2400, loss 4.96280, smoothed loss 4.47577, grad norm 4.55957, param norm 114.49825
INFO:root:epoch 3, iter 2405, loss 4.93832, smoothed loss 4.47505, grad norm 3.11529, param norm 114.59803
INFO:root:epoch 3, iter 2410, loss 4.99435, smoothed loss 4.48393, grad norm 3.21230, param norm 114.69569
INFO:root:epoch 3, iter 2415, loss 4.45566, smoothed loss 4.48921, grad norm 2.95529, param norm 114.77589
INFO:root:epoch 3, iter 2420, loss 4.49412, smoothed loss 4.48232, grad norm 2.93244, param norm 114.85572
INFO:root:epoch 3, iter 2425, loss 3.37452, smoothed loss 4.45533, grad norm 2.92634, param norm 114.93745
INFO:root:epoch 3, iter 2430, loss 4.29832, smoothed loss 4.44525, grad norm 3.02624, param norm 115.01611
INFO:root:epoch 3, iter 2435, loss 4.96258, smoothed loss 4.45252, grad norm 3.71297, param norm 115.09734
INFO:root:epoch 3, iter 2440, loss 4.71784, smoothed loss 4.44826, grad norm 3.08254, param norm 115.17616
INFO:root:epoch 3, iter 2445, loss 4.14221, smoothed loss 4.44445, grad norm 3.02170, param norm 115.24507
INFO:root:epoch 3, iter 2450, loss 4.11006, smoothed loss 4.44353, grad norm 2.74601, param norm 115.31596
INFO:root:epoch 3, iter 2455, loss 5.10500, smoothed loss 4.45384, grad norm 3.82162, param norm 115.37920
INFO:root:epoch 3, iter 2460, loss 4.45255, smoothed loss 4.45398, grad norm 3.36321, param norm 115.43876
INFO:root:epoch 3, iter 2465, loss 4.74228, smoothed loss 4.43339, grad norm 3.35543, param norm 115.51487
INFO:root:epoch 3, iter 2470, loss 4.49964, smoothed loss 4.45081, grad norm 2.97449, param norm 115.58625
INFO:root:epoch 3, iter 2475, loss 3.94612, smoothed loss 4.43260, grad norm 2.86388, param norm 115.65042
INFO:root:epoch 3, iter 2480, loss 3.65053, smoothed loss 4.41230, grad norm 3.28325, param norm 115.72832
INFO:root:epoch 3, iter 2485, loss 4.33880, smoothed loss 4.40928, grad norm 3.51428, param norm 115.81645
INFO:root:epoch 3, iter 2490, loss 4.48759, smoothed loss 4.39972, grad norm 3.44485, param norm 115.90608
INFO:root:epoch 3, iter 2495, loss 5.30322, smoothed loss 4.41898, grad norm 3.42076, param norm 115.98376
INFO:root:epoch 3, iter 2500, loss 3.99921, smoothed loss 4.41760, grad norm 3.17330, param norm 116.05389
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 3, Iter 2500, dev loss: 4.070505
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.87931 seconds [Score: 0.51649]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.72208 seconds [Score: 0.41600]
INFO:root:Epoch 3, Iter 2500, Train F1 score: 0.516489, Train EM score: 0.416000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.41020 seconds [Score: 0.49880]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.97039 seconds [Score: 0.36611]
INFO:root:Epoch 3, Iter 2500, Dev F1 score: 0.498796, Dev EM score: 0.366114
INFO:root:End of epoch 3
INFO:root:epoch 3, iter 2505, loss 3.80890, smoothed loss 4.41017, grad norm 2.80032, param norm 116.13364
INFO:root:epoch 3, iter 2510, loss 4.34043, smoothed loss 4.41285, grad norm 3.17813, param norm 116.21926
INFO:root:epoch 3, iter 2515, loss 4.01857, smoothed loss 4.39122, grad norm 3.15352, param norm 116.30623
INFO:root:epoch 3, iter 2520, loss 3.18330, smoothed loss 4.38070, grad norm 3.28167, param norm 116.38615
INFO:root:epoch 3, iter 2525, loss 4.69504, smoothed loss 4.38217, grad norm 4.05303, param norm 116.46582
Adding batches start...
Added  160  batches
INFO:root:epoch 3, iter 2530, loss 4.43727, smoothed loss 4.38270, grad norm 3.43925, param norm 116.53714
INFO:root:epoch 3, iter 2535, loss 3.66924, smoothed loss 4.37253, grad norm 2.74317, param norm 116.60374
INFO:root:epoch 3, iter 2540, loss 4.29906, smoothed loss 4.35581, grad norm 2.85306, param norm 116.66757
INFO:root:epoch 3, iter 2545, loss 5.16457, smoothed loss 4.36251, grad norm 3.17402, param norm 116.73447
INFO:root:epoch 3, iter 2550, loss 4.56839, smoothed loss 4.35632, grad norm 3.31438, param norm 116.79988
INFO:root:epoch 3, iter 2555, loss 3.70914, smoothed loss 4.34636, grad norm 2.86765, param norm 116.87014
INFO:root:epoch 3, iter 2560, loss 4.09769, smoothed loss 4.36396, grad norm 3.37750, param norm 116.94223
INFO:root:epoch 3, iter 2565, loss 4.98685, smoothed loss 4.35004, grad norm 3.32151, param norm 117.02251
INFO:root:epoch 3, iter 2570, loss 3.98774, smoothed loss 4.33195, grad norm 3.00410, param norm 117.10490
INFO:root:epoch 3, iter 2575, loss 4.65414, smoothed loss 4.34683, grad norm 3.15164, param norm 117.19057
INFO:root:epoch 3, iter 2580, loss 4.40960, smoothed loss 4.33507, grad norm 3.15727, param norm 117.26635
INFO:root:epoch 3, iter 2585, loss 3.93721, smoothed loss 4.32264, grad norm 2.97719, param norm 117.33817
INFO:root:epoch 3, iter 2590, loss 4.07440, smoothed loss 4.30570, grad norm 2.72682, param norm 117.40863
INFO:root:epoch 3, iter 2595, loss 3.54266, smoothed loss 4.29984, grad norm 2.94166, param norm 117.47739
INFO:root:epoch 3, iter 2600, loss 4.23327, smoothed loss 4.29426, grad norm 3.22949, param norm 117.53996
INFO:root:epoch 3, iter 2605, loss 4.85465, smoothed loss 4.32968, grad norm 3.16282, param norm 117.59940
INFO:root:epoch 3, iter 2610, loss 4.07797, smoothed loss 4.33176, grad norm 2.92027, param norm 117.66436
INFO:root:epoch 3, iter 2615, loss 3.92766, smoothed loss 4.32524, grad norm 2.69770, param norm 117.73993
INFO:root:epoch 3, iter 2620, loss 4.79369, smoothed loss 4.32398, grad norm 3.79741, param norm 117.81252
INFO:root:epoch 3, iter 2625, loss 3.69684, smoothed loss 4.33019, grad norm 3.01953, param norm 117.87656
INFO:root:epoch 3, iter 2630, loss 3.97585, smoothed loss 4.32979, grad norm 3.09152, param norm 117.94524
INFO:root:epoch 3, iter 2635, loss 4.11787, smoothed loss 4.33217, grad norm 3.12886, param norm 118.02058
INFO:root:epoch 3, iter 2640, loss 3.82968, smoothed loss 4.32340, grad norm 3.07803, param norm 118.10244
INFO:root:epoch 3, iter 2645, loss 5.01393, smoothed loss 4.31989, grad norm 3.32935, param norm 118.17444
INFO:root:epoch 3, iter 2650, loss 4.55985, smoothed loss 4.31291, grad norm 3.35528, param norm 118.23982
INFO:root:epoch 3, iter 2655, loss 3.92985, smoothed loss 4.31437, grad norm 3.01589, param norm 118.30739
INFO:root:epoch 3, iter 2660, loss 4.34149, smoothed loss 4.30556, grad norm 3.22734, param norm 118.37190
INFO:root:epoch 3, iter 2665, loss 3.35495, smoothed loss 4.28777, grad norm 2.83764, param norm 118.44152
INFO:root:epoch 3, iter 2670, loss 4.87771, smoothed loss 4.28933, grad norm 3.47077, param norm 118.50754
INFO:root:epoch 3, iter 2675, loss 4.68301, smoothed loss 4.29812, grad norm 3.50376, param norm 118.57320
INFO:root:epoch 3, iter 2680, loss 4.03838, smoothed loss 4.28184, grad norm 3.03578, param norm 118.64797
INFO:root:epoch 3, iter 2685, loss 4.29755, smoothed loss 4.27787, grad norm 2.85909, param norm 118.73125
Adding batches start...
Added  144  batches
INFO:root:epoch 3, iter 2690, loss 3.86155, smoothed loss 4.26997, grad norm 3.26572, param norm 118.81109
INFO:root:epoch 3, iter 2695, loss 4.11972, smoothed loss 4.27749, grad norm 3.20783, param norm 118.88470
INFO:root:epoch 3, iter 2700, loss 3.87582, smoothed loss 4.26453, grad norm 2.98088, param norm 118.96191
INFO:root:epoch 3, iter 2705, loss 3.93138, smoothed loss 4.25497, grad norm 3.10639, param norm 119.03683
INFO:root:epoch 3, iter 2710, loss 3.85570, smoothed loss 4.25165, grad norm 3.08815, param norm 119.11380
INFO:root:epoch 3, iter 2715, loss 4.00978, smoothed loss 4.25557, grad norm 2.98366, param norm 119.18573
INFO:root:epoch 3, iter 2720, loss 3.68000, smoothed loss 4.24431, grad norm 3.13613, param norm 119.25473
INFO:root:epoch 3, iter 2725, loss 4.17366, smoothed loss 4.23664, grad norm 3.10949, param norm 119.32065
INFO:root:epoch 3, iter 2730, loss 5.15295, smoothed loss 4.24372, grad norm 3.50308, param norm 119.38805
INFO:root:epoch 3, iter 2735, loss 4.02194, smoothed loss 4.23941, grad norm 2.96909, param norm 119.44894
INFO:root:epoch 3, iter 2740, loss 4.47143, smoothed loss 4.24447, grad norm 3.44755, param norm 119.50581
INFO:root:epoch 3, iter 2745, loss 4.22105, smoothed loss 4.24283, grad norm 3.15465, param norm 119.56627
INFO:root:epoch 3, iter 2750, loss 5.04949, smoothed loss 4.23763, grad norm 3.59872, param norm 119.63484
INFO:root:epoch 3, iter 2755, loss 4.06757, smoothed loss 4.22430, grad norm 3.41917, param norm 119.70853
INFO:root:epoch 3, iter 2760, loss 4.32736, smoothed loss 4.20926, grad norm 3.32851, param norm 119.78317
INFO:root:epoch 3, iter 2765, loss 3.59012, smoothed loss 4.19927, grad norm 2.80892, param norm 119.85519
INFO:root:epoch 3, iter 2770, loss 3.39186, smoothed loss 4.19707, grad norm 2.87892, param norm 119.92774
INFO:root:epoch 3, iter 2775, loss 3.75632, smoothed loss 4.19601, grad norm 3.26210, param norm 120.00180
INFO:root:epoch 3, iter 2780, loss 4.15344, smoothed loss 4.19640, grad norm 2.95160, param norm 120.06893
INFO:root:epoch 3, iter 2785, loss 3.56387, smoothed loss 4.19194, grad norm 3.46031, param norm 120.13559
INFO:root:epoch 3, iter 2790, loss 3.46924, smoothed loss 4.17779, grad norm 3.08586, param norm 120.19173
INFO:root:epoch 3, iter 2795, loss 3.53129, smoothed loss 4.18126, grad norm 3.35196, param norm 120.24447
INFO:root:epoch 3, iter 2800, loss 4.25526, smoothed loss 4.18633, grad norm 3.58940, param norm 120.30687
INFO:root:epoch 3, iter 2805, loss 4.44111, smoothed loss 4.19078, grad norm 3.16968, param norm 120.37648
INFO:root:epoch 3, iter 2810, loss 4.79871, smoothed loss 4.19798, grad norm 3.14113, param norm 120.45656
INFO:root:epoch 3, iter 2815, loss 3.77962, smoothed loss 4.19289, grad norm 2.49643, param norm 120.53615
INFO:root:epoch 3, iter 2820, loss 3.70619, smoothed loss 4.18687, grad norm 3.04089, param norm 120.60723
INFO:root:epoch 3, iter 2825, loss 4.14823, smoothed loss 4.18428, grad norm 3.38828, param norm 120.67464
INFO:root:epoch 3, iter 2830, loss 3.91497, smoothed loss 4.18040, grad norm 3.06122, param norm 120.73157
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 2835, loss 3.67142, smoothed loss 4.15825, grad norm 2.91472, param norm 120.79446
INFO:root:epoch 4, iter 2840, loss 3.64464, smoothed loss 4.15735, grad norm 2.99754, param norm 120.84626
INFO:root:epoch 4, iter 2845, loss 3.97112, smoothed loss 4.15741, grad norm 3.03794, param norm 120.91160
INFO:root:epoch 4, iter 2850, loss 4.56684, smoothed loss 4.15344, grad norm 2.84844, param norm 120.98953
INFO:root:epoch 4, iter 2855, loss 3.97616, smoothed loss 4.14342, grad norm 3.06800, param norm 121.06338
INFO:root:epoch 4, iter 2860, loss 3.33714, smoothed loss 4.12621, grad norm 3.16161, param norm 121.13599
INFO:root:epoch 4, iter 2865, loss 4.02029, smoothed loss 4.10949, grad norm 3.26316, param norm 121.19769
INFO:root:epoch 4, iter 2870, loss 4.77169, smoothed loss 4.12131, grad norm 3.54175, param norm 121.24820
INFO:root:epoch 4, iter 2875, loss 3.21822, smoothed loss 4.10502, grad norm 2.87720, param norm 121.29014
INFO:root:epoch 4, iter 2880, loss 4.53722, smoothed loss 4.11778, grad norm 3.31574, param norm 121.34914
INFO:root:epoch 4, iter 2885, loss 4.31148, smoothed loss 4.11724, grad norm 2.67560, param norm 121.41604
INFO:root:epoch 4, iter 2890, loss 4.13383, smoothed loss 4.10484, grad norm 3.20875, param norm 121.49209
INFO:root:epoch 4, iter 2895, loss 3.88846, smoothed loss 4.09282, grad norm 3.20798, param norm 121.56672
INFO:root:epoch 4, iter 2900, loss 3.94972, smoothed loss 4.09966, grad norm 3.21964, param norm 121.63152
INFO:root:epoch 4, iter 2905, loss 4.06094, smoothed loss 4.10359, grad norm 3.05469, param norm 121.68850
INFO:root:epoch 4, iter 2910, loss 4.08820, smoothed loss 4.09936, grad norm 3.29589, param norm 121.75187
INFO:root:epoch 4, iter 2915, loss 4.59449, smoothed loss 4.09834, grad norm 2.83097, param norm 121.82454
INFO:root:epoch 4, iter 2920, loss 3.86066, smoothed loss 4.09973, grad norm 3.06587, param norm 121.89290
INFO:root:epoch 4, iter 2925, loss 4.18302, smoothed loss 4.10072, grad norm 3.59833, param norm 121.95979
INFO:root:epoch 4, iter 2930, loss 3.56258, smoothed loss 4.11385, grad norm 2.88455, param norm 122.02766
INFO:root:epoch 4, iter 2935, loss 3.40072, smoothed loss 4.10874, grad norm 2.95579, param norm 122.08852
INFO:root:epoch 4, iter 2940, loss 3.78726, smoothed loss 4.10735, grad norm 3.13357, param norm 122.14407
INFO:root:epoch 4, iter 2945, loss 3.68524, smoothed loss 4.09990, grad norm 3.01647, param norm 122.19456
INFO:root:epoch 4, iter 2950, loss 4.11445, smoothed loss 4.09262, grad norm 3.61371, param norm 122.25858
INFO:root:epoch 4, iter 2955, loss 4.06871, smoothed loss 4.07820, grad norm 3.47632, param norm 122.32153
INFO:root:epoch 4, iter 2960, loss 5.10197, smoothed loss 4.08268, grad norm 3.52621, param norm 122.38433
INFO:root:epoch 4, iter 2965, loss 3.96641, smoothed loss 4.08714, grad norm 2.98954, param norm 122.43824
INFO:root:epoch 4, iter 2970, loss 3.78287, smoothed loss 4.09244, grad norm 2.55675, param norm 122.49348
INFO:root:epoch 4, iter 2975, loss 4.57622, smoothed loss 4.09663, grad norm 3.00449, param norm 122.54639
INFO:root:epoch 4, iter 2980, loss 4.18042, smoothed loss 4.10040, grad norm 3.32477, param norm 122.60822
INFO:root:epoch 4, iter 2985, loss 4.00314, smoothed loss 4.10396, grad norm 3.07026, param norm 122.68075
INFO:root:epoch 4, iter 2990, loss 4.03812, smoothed loss 4.10738, grad norm 2.87834, param norm 122.75220
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 2995, loss 3.95901, smoothed loss 4.10433, grad norm 3.09890, param norm 122.81851
INFO:root:epoch 4, iter 3000, loss 4.55220, smoothed loss 4.11635, grad norm 3.26813, param norm 122.87830
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 4, Iter 3000, dev loss: 3.743335
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 18.16602 seconds [Score: 0.60428]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.40955 seconds [Score: 0.46000]
INFO:root:Epoch 4, Iter 3000, Train F1 score: 0.604275, Train EM score: 0.460000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.60907 seconds [Score: 0.54835]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.41269 seconds [Score: 0.40124]
INFO:root:Epoch 4, Iter 3000, Dev F1 score: 0.548350, Dev EM score: 0.401236
INFO:root:End of epoch 4
INFO:root:epoch 4, iter 3005, loss 4.04996, smoothed loss 4.11080, grad norm 3.13767, param norm 122.93823
INFO:root:epoch 4, iter 3010, loss 3.09396, smoothed loss 4.10017, grad norm 2.80476, param norm 123.00515
INFO:root:epoch 4, iter 3015, loss 4.31736, smoothed loss 4.09568, grad norm 3.11988, param norm 123.07043
INFO:root:epoch 4, iter 3020, loss 3.81252, smoothed loss 4.09777, grad norm 3.41699, param norm 123.12833
INFO:root:epoch 4, iter 3025, loss 3.72676, smoothed loss 4.09742, grad norm 3.22357, param norm 123.18683
INFO:root:epoch 4, iter 3030, loss 3.87728, smoothed loss 4.10130, grad norm 2.82319, param norm 123.23462
INFO:root:epoch 4, iter 3035, loss 4.26293, smoothed loss 4.09984, grad norm 3.19569, param norm 123.29314
INFO:root:epoch 4, iter 3040, loss 4.11083, smoothed loss 4.09511, grad norm 2.96046, param norm 123.36123
INFO:root:epoch 4, iter 3045, loss 4.68859, smoothed loss 4.10194, grad norm 3.16200, param norm 123.42724
INFO:root:epoch 4, iter 3050, loss 3.52767, smoothed loss 4.10165, grad norm 3.03772, param norm 123.49454
INFO:root:epoch 4, iter 3055, loss 3.92081, smoothed loss 4.08725, grad norm 2.82199, param norm 123.56798
INFO:root:epoch 4, iter 3060, loss 4.56213, smoothed loss 4.09605, grad norm 3.26596, param norm 123.64099
INFO:root:epoch 4, iter 3065, loss 3.85727, smoothed loss 4.07657, grad norm 3.12808, param norm 123.70880
INFO:root:epoch 4, iter 3070, loss 3.58403, smoothed loss 4.09206, grad norm 2.65956, param norm 123.76216
INFO:root:epoch 4, iter 3075, loss 4.39671, smoothed loss 4.09294, grad norm 2.75037, param norm 123.80856
INFO:root:epoch 4, iter 3080, loss 4.45990, smoothed loss 4.08953, grad norm 3.08090, param norm 123.86499
INFO:root:epoch 4, iter 3085, loss 3.82573, smoothed loss 4.07271, grad norm 2.81965, param norm 123.93391
INFO:root:epoch 4, iter 3090, loss 3.44429, smoothed loss 4.05415, grad norm 2.64429, param norm 123.99355
INFO:root:epoch 4, iter 3095, loss 3.75771, smoothed loss 4.04873, grad norm 3.21186, param norm 124.04915
INFO:root:epoch 4, iter 3100, loss 4.26204, smoothed loss 4.04957, grad norm 3.36410, param norm 124.09897
INFO:root:epoch 4, iter 3105, loss 3.55706, smoothed loss 4.05409, grad norm 3.19844, param norm 124.15510
INFO:root:epoch 4, iter 3110, loss 3.67228, smoothed loss 4.04421, grad norm 2.90499, param norm 124.21895
INFO:root:epoch 4, iter 3115, loss 4.17037, smoothed loss 4.04721, grad norm 3.23932, param norm 124.28251
INFO:root:epoch 4, iter 3120, loss 4.12959, smoothed loss 4.06552, grad norm 3.01653, param norm 124.33926
INFO:root:epoch 4, iter 3125, loss 3.72648, smoothed loss 4.05037, grad norm 2.78900, param norm 124.39133
INFO:root:epoch 4, iter 3130, loss 4.00146, smoothed loss 4.04273, grad norm 3.05480, param norm 124.44637
INFO:root:epoch 4, iter 3135, loss 4.26664, smoothed loss 4.02068, grad norm 3.18913, param norm 124.50877
INFO:root:epoch 4, iter 3140, loss 3.68084, smoothed loss 4.01194, grad norm 2.98268, param norm 124.57674
INFO:root:epoch 4, iter 3145, loss 3.62936, smoothed loss 3.99331, grad norm 3.25466, param norm 124.63686
INFO:root:epoch 4, iter 3150, loss 3.67889, smoothed loss 3.97895, grad norm 3.22087, param norm 124.69054
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3155, loss 5.07558, smoothed loss 3.97345, grad norm 3.96200, param norm 124.74712
INFO:root:epoch 4, iter 3160, loss 4.06190, smoothed loss 3.97109, grad norm 3.25678, param norm 124.80589
INFO:root:epoch 4, iter 3165, loss 3.88739, smoothed loss 3.98247, grad norm 3.11454, param norm 124.86559
INFO:root:epoch 4, iter 3170, loss 4.43332, smoothed loss 3.98642, grad norm 3.37783, param norm 124.92018
INFO:root:epoch 4, iter 3175, loss 3.99806, smoothed loss 3.97710, grad norm 3.24025, param norm 124.97248
INFO:root:epoch 4, iter 3180, loss 3.98956, smoothed loss 3.97051, grad norm 2.84729, param norm 125.02190
INFO:root:epoch 4, iter 3185, loss 3.40614, smoothed loss 3.97486, grad norm 2.68472, param norm 125.07488
INFO:root:epoch 4, iter 3190, loss 3.89639, smoothed loss 3.96503, grad norm 3.00448, param norm 125.13195
INFO:root:epoch 4, iter 3195, loss 3.83170, smoothed loss 3.96113, grad norm 3.22532, param norm 125.18882
INFO:root:epoch 4, iter 3200, loss 4.34365, smoothed loss 3.94819, grad norm 3.51754, param norm 125.24679
INFO:root:epoch 4, iter 3205, loss 4.33607, smoothed loss 3.95085, grad norm 3.88169, param norm 125.29637
INFO:root:epoch 4, iter 3210, loss 3.18914, smoothed loss 3.94353, grad norm 2.93608, param norm 125.35384
INFO:root:epoch 4, iter 3215, loss 3.59629, smoothed loss 3.93407, grad norm 3.54482, param norm 125.42377
INFO:root:epoch 4, iter 3220, loss 3.85121, smoothed loss 3.92458, grad norm 2.91759, param norm 125.49873
INFO:root:epoch 4, iter 3225, loss 4.65245, smoothed loss 3.92678, grad norm 3.57464, param norm 125.56386
INFO:root:epoch 4, iter 3230, loss 4.57542, smoothed loss 3.92503, grad norm 3.65495, param norm 125.62827
INFO:root:epoch 4, iter 3235, loss 4.13276, smoothed loss 3.93914, grad norm 2.88840, param norm 125.68620
INFO:root:epoch 4, iter 3240, loss 4.18067, smoothed loss 3.94239, grad norm 2.86547, param norm 125.74664
INFO:root:epoch 4, iter 3245, loss 4.24585, smoothed loss 3.93564, grad norm 3.26034, param norm 125.80679
INFO:root:epoch 4, iter 3250, loss 3.52921, smoothed loss 3.92954, grad norm 3.37200, param norm 125.86758
INFO:root:epoch 4, iter 3255, loss 3.18805, smoothed loss 3.91334, grad norm 3.18846, param norm 125.93461
INFO:root:epoch 4, iter 3260, loss 4.18236, smoothed loss 3.92418, grad norm 3.34993, param norm 126.00704
INFO:root:epoch 4, iter 3265, loss 3.72782, smoothed loss 3.92705, grad norm 2.74183, param norm 126.06863
INFO:root:epoch 4, iter 3270, loss 4.25976, smoothed loss 3.93470, grad norm 2.90697, param norm 126.12776
INFO:root:epoch 4, iter 3275, loss 3.68339, smoothed loss 3.94187, grad norm 3.00341, param norm 126.18779
INFO:root:epoch 4, iter 3280, loss 4.02132, smoothed loss 3.95094, grad norm 2.79674, param norm 126.24596
INFO:root:epoch 4, iter 3285, loss 3.79930, smoothed loss 3.95724, grad norm 2.94541, param norm 126.29592
INFO:root:epoch 4, iter 3290, loss 3.51587, smoothed loss 3.94676, grad norm 2.96399, param norm 126.34417
INFO:root:epoch 4, iter 3295, loss 3.96975, smoothed loss 3.93689, grad norm 3.13790, param norm 126.39837
INFO:root:epoch 4, iter 3300, loss 3.37156, smoothed loss 3.92935, grad norm 2.87558, param norm 126.45412
INFO:root:epoch 4, iter 3305, loss 3.51368, smoothed loss 3.91898, grad norm 2.97699, param norm 126.51298
INFO:root:epoch 4, iter 3310, loss 3.82167, smoothed loss 3.91392, grad norm 3.46387, param norm 126.57045
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3315, loss 3.53712, smoothed loss 3.90206, grad norm 3.29623, param norm 126.62945
INFO:root:epoch 4, iter 3320, loss 3.78616, smoothed loss 3.90257, grad norm 3.08609, param norm 126.68852
INFO:root:epoch 4, iter 3325, loss 4.43567, smoothed loss 3.89194, grad norm 2.88943, param norm 126.75590
INFO:root:epoch 4, iter 3330, loss 3.98480, smoothed loss 3.89221, grad norm 5.81360, param norm 126.81776
INFO:root:epoch 4, iter 3335, loss 3.51064, smoothed loss 3.90953, grad norm 3.05687, param norm 126.87735
INFO:root:epoch 4, iter 3340, loss 3.99748, smoothed loss 3.88510, grad norm 3.26425, param norm 126.93907
INFO:root:epoch 4, iter 3345, loss 3.29657, smoothed loss 3.88396, grad norm 2.83503, param norm 126.99476
INFO:root:epoch 4, iter 3350, loss 3.69472, smoothed loss 3.87423, grad norm 2.94589, param norm 127.05320
INFO:root:epoch 4, iter 3355, loss 3.53734, smoothed loss 3.86317, grad norm 3.37365, param norm 127.11335
INFO:root:epoch 4, iter 3360, loss 4.30129, smoothed loss 3.85182, grad norm 3.10315, param norm 127.17079
INFO:root:epoch 4, iter 3365, loss 3.57422, smoothed loss 3.85126, grad norm 3.01567, param norm 127.22876
INFO:root:epoch 4, iter 3370, loss 3.66866, smoothed loss 3.83640, grad norm 2.74570, param norm 127.28639
INFO:root:epoch 4, iter 3375, loss 4.11436, smoothed loss 3.84750, grad norm 3.28573, param norm 127.34727
INFO:root:epoch 4, iter 3380, loss 4.10882, smoothed loss 3.85887, grad norm 3.75545, param norm 127.40914
INFO:root:epoch 4, iter 3385, loss 3.92281, smoothed loss 3.85271, grad norm 3.01570, param norm 127.47134
INFO:root:epoch 4, iter 3390, loss 4.11496, smoothed loss 3.84464, grad norm 2.99481, param norm 127.53520
INFO:root:epoch 4, iter 3395, loss 3.93158, smoothed loss 3.84605, grad norm 3.21330, param norm 127.60269
INFO:root:epoch 4, iter 3400, loss 4.07579, smoothed loss 3.84149, grad norm 2.88937, param norm 127.66673
INFO:root:epoch 4, iter 3405, loss 4.26191, smoothed loss 3.84394, grad norm 3.04090, param norm 127.72459
INFO:root:epoch 4, iter 3410, loss 3.54333, smoothed loss 3.83646, grad norm 2.72883, param norm 127.77744
INFO:root:epoch 4, iter 3415, loss 3.78737, smoothed loss 3.84320, grad norm 2.72468, param norm 127.83482
INFO:root:epoch 4, iter 3420, loss 3.39683, smoothed loss 3.81642, grad norm 3.07631, param norm 127.88922
INFO:root:epoch 4, iter 3425, loss 2.91329, smoothed loss 3.79652, grad norm 3.16473, param norm 127.93919
INFO:root:epoch 4, iter 3430, loss 3.60557, smoothed loss 3.80021, grad norm 3.10486, param norm 127.99585
INFO:root:epoch 4, iter 3435, loss 4.22092, smoothed loss 3.81258, grad norm 3.20923, param norm 128.05815
INFO:root:epoch 4, iter 3440, loss 3.57544, smoothed loss 3.82085, grad norm 2.86882, param norm 128.11856
INFO:root:epoch 4, iter 3445, loss 3.96502, smoothed loss 3.82221, grad norm 3.16328, param norm 128.17494
INFO:root:epoch 4, iter 3450, loss 3.72545, smoothed loss 3.82460, grad norm 2.94727, param norm 128.23242
INFO:root:epoch 4, iter 3455, loss 3.56455, smoothed loss 3.81535, grad norm 3.11420, param norm 128.29146
INFO:root:epoch 4, iter 3460, loss 3.45156, smoothed loss 3.81671, grad norm 2.78160, param norm 128.34959
INFO:root:epoch 4, iter 3465, loss 3.55035, smoothed loss 3.81691, grad norm 2.80348, param norm 128.40790
INFO:root:epoch 4, iter 3470, loss 4.18468, smoothed loss 3.82882, grad norm 3.53497, param norm 128.46664
Adding batches start...
Added  160  batches
INFO:root:epoch 4, iter 3475, loss 3.82851, smoothed loss 3.82687, grad norm 2.87552, param norm 128.52109
INFO:root:epoch 4, iter 3480, loss 4.35449, smoothed loss 3.81359, grad norm 3.11069, param norm 128.58049
INFO:root:epoch 4, iter 3485, loss 4.57301, smoothed loss 3.82760, grad norm 3.25847, param norm 128.63805
INFO:root:epoch 4, iter 3490, loss 3.93419, smoothed loss 3.81751, grad norm 3.00803, param norm 128.69519
INFO:root:epoch 4, iter 3495, loss 3.63546, smoothed loss 3.81173, grad norm 3.03891, param norm 128.75464
INFO:root:epoch 4, iter 3500, loss 3.46455, smoothed loss 3.79897, grad norm 2.78989, param norm 128.81352
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 4, Iter 3500, dev loss: 3.552990
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.63814 seconds [Score: 0.63152]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.64976 seconds [Score: 0.50700]
INFO:root:Epoch 4, Iter 3500, Train F1 score: 0.631521, Train EM score: 0.507000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 117.05829 seconds [Score: 0.56540]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.40109 seconds [Score: 0.42147]
INFO:root:Epoch 4, Iter 3500, Dev F1 score: 0.565402, Dev EM score: 0.421467
INFO:root:End of epoch 4
INFO:root:epoch 4, iter 3505, loss 4.03850, smoothed loss 3.80148, grad norm 3.34130, param norm 128.87071
INFO:root:epoch 4, iter 3510, loss 4.05774, smoothed loss 3.80579, grad norm 3.22810, param norm 128.92322
INFO:root:epoch 4, iter 3515, loss 4.31199, smoothed loss 3.80345, grad norm 3.36349, param norm 128.97388
INFO:root:epoch 4, iter 3520, loss 4.43142, smoothed loss 3.81033, grad norm 3.66377, param norm 129.02554
INFO:root:epoch 4, iter 3525, loss 4.61654, smoothed loss 3.83482, grad norm 3.10768, param norm 129.07634
INFO:root:epoch 4, iter 3530, loss 3.56720, smoothed loss 3.82325, grad norm 3.01075, param norm 129.13336
INFO:root:epoch 4, iter 3535, loss 3.55899, smoothed loss 3.82098, grad norm 3.02722, param norm 129.19417
INFO:root:epoch 4, iter 3540, loss 3.49236, smoothed loss 3.80838, grad norm 2.98128, param norm 129.25558
INFO:root:epoch 4, iter 3545, loss 3.53111, smoothed loss 3.79671, grad norm 3.19566, param norm 129.31219
INFO:root:epoch 4, iter 3550, loss 3.48574, smoothed loss 3.80108, grad norm 3.17312, param norm 129.35864
INFO:root:epoch 4, iter 3555, loss 3.94237, smoothed loss 3.81367, grad norm 3.28230, param norm 129.40106
INFO:root:epoch 4, iter 3560, loss 3.95316, smoothed loss 3.80414, grad norm 3.00008, param norm 129.44893
INFO:root:epoch 4, iter 3565, loss 3.68158, smoothed loss 3.79994, grad norm 2.89118, param norm 129.50468
INFO:root:epoch 4, iter 3570, loss 3.16508, smoothed loss 3.78775, grad norm 3.00319, param norm 129.56000
INFO:root:epoch 4, iter 3575, loss 4.01025, smoothed loss 3.78504, grad norm 3.10377, param norm 129.61394
INFO:root:epoch 4, iter 3580, loss 3.84668, smoothed loss 3.79600, grad norm 2.87557, param norm 129.66331
INFO:root:epoch 4, iter 3585, loss 4.15228, smoothed loss 3.79264, grad norm 2.95512, param norm 129.71939
INFO:root:epoch 4, iter 3590, loss 3.57140, smoothed loss 3.79519, grad norm 2.93425, param norm 129.77919
INFO:root:epoch 4, iter 3595, loss 3.90521, smoothed loss 3.80630, grad norm 3.08685, param norm 129.83607
INFO:root:epoch 4, iter 3600, loss 3.10130, smoothed loss 3.80395, grad norm 2.71906, param norm 129.88675
INFO:root:epoch 4, iter 3605, loss 3.86021, smoothed loss 3.80725, grad norm 3.44548, param norm 129.93810
INFO:root:epoch 4, iter 3610, loss 3.30284, smoothed loss 3.78728, grad norm 3.15694, param norm 130.00294
INFO:root:epoch 4, iter 3615, loss 3.66099, smoothed loss 3.78663, grad norm 2.95194, param norm 130.06570
INFO:root:epoch 4, iter 3620, loss 3.54693, smoothed loss 3.79358, grad norm 2.96187, param norm 130.11455
INFO:root:epoch 4, iter 3625, loss 3.51344, smoothed loss 3.78219, grad norm 2.84813, param norm 130.15752
INFO:root:epoch 4, iter 3630, loss 3.35368, smoothed loss 3.78256, grad norm 3.31576, param norm 130.19548
Adding batches start...
Added  144  batches
INFO:root:epoch 4, iter 3635, loss 3.62589, smoothed loss 3.77873, grad norm 3.15050, param norm 130.23749
INFO:root:epoch 4, iter 3640, loss 3.54110, smoothed loss 3.77586, grad norm 3.19130, param norm 130.29102
INFO:root:epoch 4, iter 3645, loss 3.75864, smoothed loss 3.77017, grad norm 3.00749, param norm 130.34589
INFO:root:epoch 4, iter 3650, loss 3.74587, smoothed loss 3.77407, grad norm 3.25194, param norm 130.39598
INFO:root:epoch 4, iter 3655, loss 3.95914, smoothed loss 3.77064, grad norm 3.28144, param norm 130.46039
INFO:root:epoch 4, iter 3660, loss 3.51719, smoothed loss 3.76223, grad norm 3.30506, param norm 130.52800
INFO:root:epoch 4, iter 3665, loss 4.17531, smoothed loss 3.77262, grad norm 3.23058, param norm 130.59384
INFO:root:epoch 4, iter 3670, loss 3.91768, smoothed loss 3.77401, grad norm 3.54632, param norm 130.65033
INFO:root:epoch 4, iter 3675, loss 3.33266, smoothed loss 3.76697, grad norm 2.96316, param norm 130.70448
INFO:root:epoch 4, iter 3680, loss 4.26040, smoothed loss 3.78773, grad norm 3.10830, param norm 130.75771
INFO:root:epoch 4, iter 3685, loss 4.62483, smoothed loss 3.79893, grad norm 3.32145, param norm 130.80917
INFO:root:epoch 4, iter 3690, loss 3.40872, smoothed loss 3.79723, grad norm 2.88101, param norm 130.86246
INFO:root:epoch 4, iter 3695, loss 3.76457, smoothed loss 3.78526, grad norm 3.41257, param norm 130.91841
INFO:root:epoch 4, iter 3700, loss 3.74731, smoothed loss 3.77363, grad norm 3.49598, param norm 130.97754
INFO:root:epoch 4, iter 3705, loss 3.47950, smoothed loss 3.76604, grad norm 3.52594, param norm 131.04160
INFO:root:epoch 4, iter 3710, loss 4.28644, smoothed loss 3.76312, grad norm 3.40256, param norm 131.10219
INFO:root:epoch 4, iter 3715, loss 4.11686, smoothed loss 3.75165, grad norm 3.57923, param norm 131.15825
INFO:root:epoch 4, iter 3720, loss 3.25425, smoothed loss 3.75054, grad norm 2.75104, param norm 131.21490
INFO:root:epoch 4, iter 3725, loss 3.68904, smoothed loss 3.74645, grad norm 3.13722, param norm 131.26535
INFO:root:epoch 4, iter 3730, loss 4.12620, smoothed loss 3.74743, grad norm 3.21763, param norm 131.31168
INFO:root:epoch 4, iter 3735, loss 4.56875, smoothed loss 3.75366, grad norm 3.59047, param norm 131.35753
INFO:root:epoch 4, iter 3740, loss 3.75140, smoothed loss 3.75192, grad norm 2.88856, param norm 131.41174
INFO:root:epoch 4, iter 3745, loss 4.25752, smoothed loss 3.75163, grad norm 3.00353, param norm 131.45790
INFO:root:epoch 4, iter 3750, loss 4.65205, smoothed loss 3.75332, grad norm 3.68253, param norm 131.50653
INFO:root:epoch 4, iter 3755, loss 3.60541, smoothed loss 3.73754, grad norm 3.14788, param norm 131.56120
INFO:root:epoch 4, iter 3760, loss 3.74772, smoothed loss 3.73048, grad norm 3.01941, param norm 131.61833
INFO:root:epoch 4, iter 3765, loss 3.31629, smoothed loss 3.73005, grad norm 2.82914, param norm 131.67197
INFO:root:epoch 4, iter 3770, loss 3.86159, smoothed loss 3.71168, grad norm 3.21633, param norm 131.72665
INFO:root:epoch 4, iter 3775, loss 3.44479, smoothed loss 3.71165, grad norm 2.84988, param norm 131.78102
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 3780, loss 2.95215, smoothed loss 3.71445, grad norm 2.63218, param norm 131.83307
INFO:root:epoch 5, iter 3785, loss 3.80933, smoothed loss 3.73295, grad norm 2.88994, param norm 131.87939
INFO:root:epoch 5, iter 3790, loss 3.73953, smoothed loss 3.73703, grad norm 2.79929, param norm 131.92497
INFO:root:epoch 5, iter 3795, loss 3.36823, smoothed loss 3.73102, grad norm 2.75906, param norm 131.97891
INFO:root:epoch 5, iter 3800, loss 3.69029, smoothed loss 3.71471, grad norm 3.11981, param norm 132.03705
INFO:root:epoch 5, iter 3805, loss 4.09203, smoothed loss 3.71303, grad norm 3.14813, param norm 132.09633
INFO:root:epoch 5, iter 3810, loss 3.70737, smoothed loss 3.71371, grad norm 3.21025, param norm 132.14949
INFO:root:epoch 5, iter 3815, loss 3.71759, smoothed loss 3.70110, grad norm 3.07634, param norm 132.20448
INFO:root:epoch 5, iter 3820, loss 4.45251, smoothed loss 3.69703, grad norm 3.51475, param norm 132.25053
INFO:root:epoch 5, iter 3825, loss 3.47346, smoothed loss 3.68146, grad norm 2.68063, param norm 132.30318
INFO:root:epoch 5, iter 3830, loss 3.57218, smoothed loss 3.70158, grad norm 3.04263, param norm 132.35895
INFO:root:epoch 5, iter 3835, loss 4.30935, smoothed loss 3.70141, grad norm 3.01869, param norm 132.41902
INFO:root:epoch 5, iter 3840, loss 3.84059, smoothed loss 3.70039, grad norm 3.00991, param norm 132.48079
INFO:root:epoch 5, iter 3845, loss 3.23821, smoothed loss 3.69025, grad norm 2.91559, param norm 132.53166
INFO:root:epoch 5, iter 3850, loss 3.05128, smoothed loss 3.68575, grad norm 2.90418, param norm 132.58063
INFO:root:epoch 5, iter 3855, loss 3.45493, smoothed loss 3.68515, grad norm 3.21202, param norm 132.63701
INFO:root:epoch 5, iter 3860, loss 3.67778, smoothed loss 3.68608, grad norm 3.09593, param norm 132.69508
INFO:root:epoch 5, iter 3865, loss 2.92530, smoothed loss 3.68447, grad norm 2.71220, param norm 132.75035
INFO:root:epoch 5, iter 3870, loss 4.17178, smoothed loss 3.68547, grad norm 3.04545, param norm 132.80070
INFO:root:epoch 5, iter 3875, loss 3.83373, smoothed loss 3.68909, grad norm 3.05398, param norm 132.84799
INFO:root:epoch 5, iter 3880, loss 3.62017, smoothed loss 3.70223, grad norm 2.69972, param norm 132.90402
INFO:root:epoch 5, iter 3885, loss 3.42090, smoothed loss 3.70361, grad norm 2.81871, param norm 132.95824
INFO:root:epoch 5, iter 3890, loss 3.38470, smoothed loss 3.68592, grad norm 3.08841, param norm 133.01555
INFO:root:epoch 5, iter 3895, loss 3.58907, smoothed loss 3.67666, grad norm 3.01820, param norm 133.06580
INFO:root:epoch 5, iter 3900, loss 3.88421, smoothed loss 3.68867, grad norm 3.61749, param norm 133.10854
INFO:root:epoch 5, iter 3905, loss 3.94279, smoothed loss 3.69196, grad norm 3.16358, param norm 133.15385
INFO:root:epoch 5, iter 3910, loss 3.59343, smoothed loss 3.67509, grad norm 3.66866, param norm 133.20085
INFO:root:epoch 5, iter 3915, loss 3.72462, smoothed loss 3.70143, grad norm 3.06021, param norm 133.25539
INFO:root:epoch 5, iter 3920, loss 3.47614, smoothed loss 3.70039, grad norm 2.88948, param norm 133.31097
INFO:root:epoch 5, iter 3925, loss 3.26579, smoothed loss 3.69487, grad norm 2.66171, param norm 133.36937
INFO:root:epoch 5, iter 3930, loss 3.13070, smoothed loss 3.69483, grad norm 2.98660, param norm 133.42586
INFO:root:epoch 5, iter 3935, loss 3.60833, smoothed loss 3.69502, grad norm 3.02866, param norm 133.47997
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 3940, loss 3.72672, smoothed loss 3.69280, grad norm 3.13488, param norm 133.53406
INFO:root:epoch 5, iter 3945, loss 4.37252, smoothed loss 3.70101, grad norm 3.42148, param norm 133.58447
INFO:root:epoch 5, iter 3950, loss 2.85131, smoothed loss 3.69385, grad norm 3.07680, param norm 133.63298
INFO:root:epoch 5, iter 3955, loss 3.56430, smoothed loss 3.69989, grad norm 3.45844, param norm 133.68904
INFO:root:epoch 5, iter 3960, loss 3.83431, smoothed loss 3.69786, grad norm 2.92043, param norm 133.73834
INFO:root:epoch 5, iter 3965, loss 4.64579, smoothed loss 3.71236, grad norm 3.37707, param norm 133.78648
INFO:root:epoch 5, iter 3970, loss 3.17808, smoothed loss 3.70241, grad norm 2.98056, param norm 133.83270
INFO:root:epoch 5, iter 3975, loss 4.00947, smoothed loss 3.70584, grad norm 3.14816, param norm 133.87871
INFO:root:epoch 5, iter 3980, loss 3.43894, smoothed loss 3.69322, grad norm 3.10507, param norm 133.91960
INFO:root:epoch 5, iter 3985, loss 3.75890, smoothed loss 3.71889, grad norm 2.92619, param norm 133.95856
INFO:root:epoch 5, iter 3990, loss 3.11316, smoothed loss 3.72595, grad norm 2.62342, param norm 134.00081
INFO:root:epoch 5, iter 3995, loss 4.05097, smoothed loss 3.71955, grad norm 3.19925, param norm 134.05411
INFO:root:epoch 5, iter 4000, loss 3.51242, smoothed loss 3.71927, grad norm 2.93599, param norm 134.11047
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 5, Iter 4000, dev loss: 3.461439
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.92036 seconds [Score: 0.68089]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.17795 seconds [Score: 0.51600]
INFO:root:Epoch 5, Iter 4000, Train F1 score: 0.680889, Train EM score: 0.516000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.56896 seconds [Score: 0.57896]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.45104 seconds [Score: 0.42849]
INFO:root:Epoch 5, Iter 4000, Dev F1 score: 0.578965, Dev EM score: 0.428491
INFO:root:End of epoch 5
INFO:root:epoch 5, iter 4005, loss 3.23708, smoothed loss 3.70888, grad norm 2.99152, param norm 134.16293
INFO:root:epoch 5, iter 4010, loss 3.96325, smoothed loss 3.71380, grad norm 3.34880, param norm 134.21315
INFO:root:epoch 5, iter 4015, loss 3.82534, smoothed loss 3.71696, grad norm 2.92897, param norm 134.26469
INFO:root:epoch 5, iter 4020, loss 3.91667, smoothed loss 3.71685, grad norm 2.98753, param norm 134.31842
INFO:root:epoch 5, iter 4025, loss 3.74014, smoothed loss 3.70543, grad norm 3.13838, param norm 134.36749
INFO:root:epoch 5, iter 4030, loss 3.41806, smoothed loss 3.67746, grad norm 2.89200, param norm 134.42375
INFO:root:epoch 5, iter 4035, loss 4.30656, smoothed loss 3.67708, grad norm 3.27208, param norm 134.47496
INFO:root:epoch 5, iter 4040, loss 3.42971, smoothed loss 3.66278, grad norm 3.39196, param norm 134.52545
INFO:root:epoch 5, iter 4045, loss 3.37740, smoothed loss 3.64662, grad norm 3.15708, param norm 134.57845
INFO:root:epoch 5, iter 4050, loss 3.20372, smoothed loss 3.65366, grad norm 2.95752, param norm 134.62459
INFO:root:epoch 5, iter 4055, loss 3.66987, smoothed loss 3.66203, grad norm 3.40293, param norm 134.67650
INFO:root:epoch 5, iter 4060, loss 3.55735, smoothed loss 3.67248, grad norm 2.91229, param norm 134.73189
INFO:root:epoch 5, iter 4065, loss 3.53839, smoothed loss 3.68883, grad norm 2.93475, param norm 134.78481
INFO:root:epoch 5, iter 4070, loss 3.44469, smoothed loss 3.68404, grad norm 2.70045, param norm 134.84134
INFO:root:epoch 5, iter 4075, loss 3.66307, smoothed loss 3.69716, grad norm 3.31795, param norm 134.88960
INFO:root:epoch 5, iter 4080, loss 2.94254, smoothed loss 3.67743, grad norm 2.83964, param norm 134.93108
INFO:root:epoch 5, iter 4085, loss 2.93755, smoothed loss 3.66729, grad norm 3.01500, param norm 134.98257
INFO:root:epoch 5, iter 4090, loss 3.82285, smoothed loss 3.68063, grad norm 2.96614, param norm 135.03473
INFO:root:epoch 5, iter 4095, loss 3.50309, smoothed loss 3.65531, grad norm 2.87464, param norm 135.08777
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4100, loss 3.63459, smoothed loss 3.64124, grad norm 3.45653, param norm 135.14084
INFO:root:epoch 5, iter 4105, loss 3.28745, smoothed loss 3.64209, grad norm 2.86642, param norm 135.19206
INFO:root:epoch 5, iter 4110, loss 3.49812, smoothed loss 3.64375, grad norm 2.77159, param norm 135.24384
INFO:root:epoch 5, iter 4115, loss 4.56216, smoothed loss 3.63739, grad norm 3.57724, param norm 135.30106
INFO:root:epoch 5, iter 4120, loss 3.75997, smoothed loss 3.64995, grad norm 3.07068, param norm 135.35417
INFO:root:epoch 5, iter 4125, loss 4.28570, smoothed loss 3.64685, grad norm 3.26979, param norm 135.40759
INFO:root:epoch 5, iter 4130, loss 3.59356, smoothed loss 3.64595, grad norm 3.03745, param norm 135.45349
INFO:root:epoch 5, iter 4135, loss 5.24030, smoothed loss 3.64729, grad norm 3.52914, param norm 135.50267
INFO:root:epoch 5, iter 4140, loss 4.11506, smoothed loss 3.64772, grad norm 2.95171, param norm 135.54906
INFO:root:epoch 5, iter 4145, loss 3.74245, smoothed loss 3.64898, grad norm 3.00550, param norm 135.59567
INFO:root:epoch 5, iter 4150, loss 3.55775, smoothed loss 3.65111, grad norm 3.37305, param norm 135.64041
INFO:root:epoch 5, iter 4155, loss 3.55549, smoothed loss 3.64032, grad norm 2.68997, param norm 135.68555
INFO:root:epoch 5, iter 4160, loss 3.26622, smoothed loss 3.64758, grad norm 2.74683, param norm 135.72957
INFO:root:epoch 5, iter 4165, loss 3.10006, smoothed loss 3.64290, grad norm 3.15705, param norm 135.77567
INFO:root:epoch 5, iter 4170, loss 3.77411, smoothed loss 3.63350, grad norm 3.19192, param norm 135.82828
INFO:root:epoch 5, iter 4175, loss 4.11412, smoothed loss 3.63008, grad norm 3.31416, param norm 135.88052
INFO:root:epoch 5, iter 4180, loss 3.38491, smoothed loss 3.63404, grad norm 2.82722, param norm 135.92982
INFO:root:epoch 5, iter 4185, loss 3.79414, smoothed loss 3.63102, grad norm 2.69769, param norm 135.97816
INFO:root:epoch 5, iter 4190, loss 3.56165, smoothed loss 3.63344, grad norm 2.86024, param norm 136.02031
INFO:root:epoch 5, iter 4195, loss 3.65170, smoothed loss 3.63686, grad norm 2.84644, param norm 136.06694
INFO:root:epoch 5, iter 4200, loss 3.15140, smoothed loss 3.63115, grad norm 2.87330, param norm 136.11871
INFO:root:epoch 5, iter 4205, loss 4.15021, smoothed loss 3.62232, grad norm 3.19387, param norm 136.17482
INFO:root:epoch 5, iter 4210, loss 3.05631, smoothed loss 3.61144, grad norm 2.87774, param norm 136.23253
INFO:root:epoch 5, iter 4215, loss 3.79526, smoothed loss 3.60417, grad norm 3.56246, param norm 136.28589
INFO:root:epoch 5, iter 4220, loss 3.62186, smoothed loss 3.58792, grad norm 2.93240, param norm 136.33871
INFO:root:epoch 5, iter 4225, loss 2.55084, smoothed loss 3.58286, grad norm 2.71786, param norm 136.38826
INFO:root:epoch 5, iter 4230, loss 3.72377, smoothed loss 3.57952, grad norm 3.34376, param norm 136.43410
INFO:root:epoch 5, iter 4235, loss 3.46340, smoothed loss 3.57455, grad norm 3.01496, param norm 136.48000
INFO:root:epoch 5, iter 4240, loss 2.73160, smoothed loss 3.56553, grad norm 2.91372, param norm 136.53506
INFO:root:epoch 5, iter 4245, loss 4.81977, smoothed loss 3.58354, grad norm 3.81869, param norm 136.59505
INFO:root:epoch 5, iter 4250, loss 3.56975, smoothed loss 3.58409, grad norm 2.97355, param norm 136.64671
INFO:root:epoch 5, iter 4255, loss 3.55394, smoothed loss 3.57599, grad norm 2.93407, param norm 136.69522
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4260, loss 3.63186, smoothed loss 3.56758, grad norm 3.11514, param norm 136.74855
INFO:root:epoch 5, iter 4265, loss 3.02319, smoothed loss 3.55974, grad norm 2.77064, param norm 136.80188
INFO:root:epoch 5, iter 4270, loss 3.15618, smoothed loss 3.55065, grad norm 3.01876, param norm 136.85728
INFO:root:epoch 5, iter 4275, loss 2.79874, smoothed loss 3.53723, grad norm 2.78101, param norm 136.90543
INFO:root:epoch 5, iter 4280, loss 3.93729, smoothed loss 3.53903, grad norm 3.56592, param norm 136.95755
INFO:root:epoch 5, iter 4285, loss 3.52218, smoothed loss 3.54110, grad norm 3.09958, param norm 137.01790
INFO:root:epoch 5, iter 4290, loss 3.24231, smoothed loss 3.53782, grad norm 2.66136, param norm 137.07458
INFO:root:epoch 5, iter 4295, loss 3.41680, smoothed loss 3.53349, grad norm 2.93517, param norm 137.12245
INFO:root:epoch 5, iter 4300, loss 3.90125, smoothed loss 3.52886, grad norm 3.10226, param norm 137.16826
INFO:root:epoch 5, iter 4305, loss 3.94612, smoothed loss 3.52729, grad norm 3.03170, param norm 137.21359
INFO:root:epoch 5, iter 4310, loss 3.31786, smoothed loss 3.53084, grad norm 2.71358, param norm 137.26163
INFO:root:epoch 5, iter 4315, loss 5.03945, smoothed loss 3.54810, grad norm 3.57392, param norm 137.31430
INFO:root:epoch 5, iter 4320, loss 3.82826, smoothed loss 3.54377, grad norm 3.40886, param norm 137.36707
INFO:root:epoch 5, iter 4325, loss 4.12037, smoothed loss 3.55222, grad norm 3.11838, param norm 137.41914
INFO:root:epoch 5, iter 4330, loss 4.36527, smoothed loss 3.57145, grad norm 3.29683, param norm 137.46367
INFO:root:epoch 5, iter 4335, loss 3.18729, smoothed loss 3.54765, grad norm 3.15600, param norm 137.50284
INFO:root:epoch 5, iter 4340, loss 3.26968, smoothed loss 3.54572, grad norm 3.13206, param norm 137.54695
INFO:root:epoch 5, iter 4345, loss 3.33049, smoothed loss 3.53684, grad norm 2.90580, param norm 137.59431
INFO:root:epoch 5, iter 4350, loss 4.04930, smoothed loss 3.54329, grad norm 3.11536, param norm 137.64104
INFO:root:epoch 5, iter 4355, loss 3.14767, smoothed loss 3.53542, grad norm 2.71631, param norm 137.68810
INFO:root:epoch 5, iter 4360, loss 3.63449, smoothed loss 3.52452, grad norm 3.00382, param norm 137.74393
INFO:root:epoch 5, iter 4365, loss 2.57296, smoothed loss 3.51592, grad norm 2.66730, param norm 137.79591
INFO:root:epoch 5, iter 4370, loss 2.92766, smoothed loss 3.51250, grad norm 3.06767, param norm 137.84578
INFO:root:epoch 5, iter 4375, loss 3.57760, smoothed loss 3.49582, grad norm 3.03206, param norm 137.89679
INFO:root:epoch 5, iter 4380, loss 3.10026, smoothed loss 3.48663, grad norm 3.21964, param norm 137.94069
INFO:root:epoch 5, iter 4385, loss 3.63264, smoothed loss 3.49487, grad norm 3.06767, param norm 137.98611
INFO:root:epoch 5, iter 4390, loss 3.65454, smoothed loss 3.49921, grad norm 3.02332, param norm 138.03261
INFO:root:epoch 5, iter 4395, loss 4.07168, smoothed loss 3.52360, grad norm 3.23584, param norm 138.08061
INFO:root:epoch 5, iter 4400, loss 3.72203, smoothed loss 3.51728, grad norm 3.04223, param norm 138.12866
INFO:root:epoch 5, iter 4405, loss 3.11101, smoothed loss 3.52607, grad norm 2.72931, param norm 138.17781
INFO:root:epoch 5, iter 4410, loss 3.45882, smoothed loss 3.53076, grad norm 2.93342, param norm 138.22810
INFO:root:epoch 5, iter 4415, loss 4.07099, smoothed loss 3.52827, grad norm 3.09137, param norm 138.27512
Adding batches start...
Added  160  batches
INFO:root:epoch 5, iter 4420, loss 3.35912, smoothed loss 3.52559, grad norm 2.90732, param norm 138.32100
INFO:root:epoch 5, iter 4425, loss 3.17303, smoothed loss 3.52066, grad norm 3.06400, param norm 138.36996
INFO:root:epoch 5, iter 4430, loss 3.76346, smoothed loss 3.52009, grad norm 3.25423, param norm 138.42734
INFO:root:epoch 5, iter 4435, loss 3.20004, smoothed loss 3.51036, grad norm 2.59403, param norm 138.47691
INFO:root:epoch 5, iter 4440, loss 3.37031, smoothed loss 3.52080, grad norm 3.12314, param norm 138.51631
INFO:root:epoch 5, iter 4445, loss 3.67035, smoothed loss 3.50743, grad norm 3.21472, param norm 138.56082
INFO:root:epoch 5, iter 4450, loss 3.62326, smoothed loss 3.50191, grad norm 3.01494, param norm 138.60991
INFO:root:epoch 5, iter 4455, loss 3.57289, smoothed loss 3.50917, grad norm 2.74880, param norm 138.64890
INFO:root:epoch 5, iter 4460, loss 3.46016, smoothed loss 3.50782, grad norm 3.13397, param norm 138.69083
INFO:root:epoch 5, iter 4465, loss 4.17108, smoothed loss 3.51820, grad norm 3.36830, param norm 138.73477
INFO:root:epoch 5, iter 4470, loss 2.80883, smoothed loss 3.50181, grad norm 2.92862, param norm 138.77995
INFO:root:epoch 5, iter 4475, loss 3.39835, smoothed loss 3.49803, grad norm 2.96097, param norm 138.83211
INFO:root:epoch 5, iter 4480, loss 3.85920, smoothed loss 3.52294, grad norm 3.19039, param norm 138.88240
INFO:root:epoch 5, iter 4485, loss 4.63490, smoothed loss 3.55397, grad norm 3.17053, param norm 138.92749
INFO:root:epoch 5, iter 4490, loss 3.19605, smoothed loss 3.55756, grad norm 2.58334, param norm 138.96301
INFO:root:epoch 5, iter 4495, loss 3.48741, smoothed loss 3.55415, grad norm 3.01439, param norm 139.00551
INFO:root:epoch 5, iter 4500, loss 3.54810, smoothed loss 3.54946, grad norm 3.12831, param norm 139.05724
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 5, Iter 4500, dev loss: 3.351877
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 18.13700 seconds [Score: 0.66887]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.68403 seconds [Score: 0.51100]
INFO:root:Epoch 5, Iter 4500, Train F1 score: 0.668870, Train EM score: 0.511000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.12178 seconds [Score: 0.59436]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.90599 seconds [Score: 0.44956]
INFO:root:Epoch 5, Iter 4500, Dev F1 score: 0.594360, Dev EM score: 0.449564
INFO:root:End of epoch 5
INFO:root:epoch 5, iter 4505, loss 3.74861, smoothed loss 3.55274, grad norm 3.75484, param norm 139.11076
INFO:root:epoch 5, iter 4510, loss 2.75561, smoothed loss 3.53167, grad norm 2.83369, param norm 139.16737
INFO:root:epoch 5, iter 4515, loss 3.72717, smoothed loss 3.52877, grad norm 3.60560, param norm 139.22159
INFO:root:epoch 5, iter 4520, loss 2.63964, smoothed loss 3.50964, grad norm 2.99560, param norm 139.26736
INFO:root:epoch 5, iter 4525, loss 3.43495, smoothed loss 3.50983, grad norm 2.97627, param norm 139.30930
INFO:root:epoch 5, iter 4530, loss 3.24257, smoothed loss 3.50787, grad norm 3.32904, param norm 139.34953
INFO:root:epoch 5, iter 4535, loss 3.98443, smoothed loss 3.51961, grad norm 3.03370, param norm 139.38930
INFO:root:epoch 5, iter 4540, loss 3.49094, smoothed loss 3.50486, grad norm 3.07249, param norm 139.43272
INFO:root:epoch 5, iter 4545, loss 3.31555, smoothed loss 3.49901, grad norm 3.13708, param norm 139.48062
INFO:root:epoch 5, iter 4550, loss 3.51936, smoothed loss 3.50478, grad norm 3.13830, param norm 139.53300
INFO:root:epoch 5, iter 4555, loss 3.86253, smoothed loss 3.51656, grad norm 3.34756, param norm 139.58745
INFO:root:epoch 5, iter 4560, loss 3.57564, smoothed loss 3.52053, grad norm 3.28408, param norm 139.63736
INFO:root:epoch 5, iter 4565, loss 3.63342, smoothed loss 3.53555, grad norm 3.08187, param norm 139.68510
INFO:root:epoch 5, iter 4570, loss 4.07321, smoothed loss 3.54908, grad norm 3.12685, param norm 139.73741
INFO:root:epoch 5, iter 4575, loss 2.53830, smoothed loss 3.53166, grad norm 2.54100, param norm 139.79137
Adding batches start...
Added  144  batches
INFO:root:epoch 5, iter 4580, loss 3.37128, smoothed loss 3.53053, grad norm 2.97827, param norm 139.84306
INFO:root:epoch 5, iter 4585, loss 2.85435, smoothed loss 3.52278, grad norm 3.36789, param norm 139.89577
INFO:root:epoch 5, iter 4590, loss 4.09909, smoothed loss 3.53081, grad norm 3.33089, param norm 139.95001
INFO:root:epoch 5, iter 4595, loss 3.74346, smoothed loss 3.52228, grad norm 3.27271, param norm 140.00385
INFO:root:epoch 5, iter 4600, loss 3.12764, smoothed loss 3.53047, grad norm 3.18370, param norm 140.06503
INFO:root:epoch 5, iter 4605, loss 3.48081, smoothed loss 3.52991, grad norm 2.84121, param norm 140.12207
INFO:root:epoch 5, iter 4610, loss 3.07736, smoothed loss 3.53934, grad norm 3.05249, param norm 140.16820
INFO:root:epoch 5, iter 4615, loss 3.32786, smoothed loss 3.53019, grad norm 3.06071, param norm 140.21431
INFO:root:epoch 5, iter 4620, loss 3.73748, smoothed loss 3.52917, grad norm 2.79580, param norm 140.26418
INFO:root:epoch 5, iter 4625, loss 3.96432, smoothed loss 3.53224, grad norm 3.09008, param norm 140.31161
INFO:root:epoch 5, iter 4630, loss 2.91219, smoothed loss 3.52935, grad norm 3.04254, param norm 140.36043
INFO:root:epoch 5, iter 4635, loss 3.41639, smoothed loss 3.52616, grad norm 2.83578, param norm 140.40952
INFO:root:epoch 5, iter 4640, loss 3.93015, smoothed loss 3.51592, grad norm 3.46248, param norm 140.45348
INFO:root:epoch 5, iter 4645, loss 4.08477, smoothed loss 3.51060, grad norm 3.76889, param norm 140.50212
INFO:root:epoch 5, iter 4650, loss 3.82237, smoothed loss 3.50747, grad norm 2.94208, param norm 140.54456
INFO:root:epoch 5, iter 4655, loss 3.53796, smoothed loss 3.51702, grad norm 3.14417, param norm 140.58299
INFO:root:epoch 5, iter 4660, loss 3.33436, smoothed loss 3.50207, grad norm 2.88192, param norm 140.62857
INFO:root:epoch 5, iter 4665, loss 3.24860, smoothed loss 3.49881, grad norm 2.96761, param norm 140.67450
INFO:root:epoch 5, iter 4670, loss 3.06581, smoothed loss 3.49794, grad norm 2.94933, param norm 140.72247
INFO:root:epoch 5, iter 4675, loss 3.02869, smoothed loss 3.48922, grad norm 2.73675, param norm 140.77036
INFO:root:epoch 5, iter 4680, loss 3.40895, smoothed loss 3.46814, grad norm 3.49362, param norm 140.82085
INFO:root:epoch 5, iter 4685, loss 3.78189, smoothed loss 3.46256, grad norm 3.36405, param norm 140.87442
INFO:root:epoch 5, iter 4690, loss 3.55711, smoothed loss 3.47205, grad norm 3.25007, param norm 140.92122
INFO:root:epoch 5, iter 4695, loss 2.99081, smoothed loss 3.45886, grad norm 2.73321, param norm 140.96808
INFO:root:epoch 5, iter 4700, loss 2.89636, smoothed loss 3.46729, grad norm 2.68932, param norm 141.01741
INFO:root:epoch 5, iter 4705, loss 3.92426, smoothed loss 3.47077, grad norm 3.03528, param norm 141.07005
INFO:root:epoch 5, iter 4710, loss 3.50408, smoothed loss 3.47723, grad norm 2.96763, param norm 141.12093
INFO:root:epoch 5, iter 4715, loss 3.22927, smoothed loss 3.47437, grad norm 2.95265, param norm 141.17447
INFO:root:epoch 5, iter 4720, loss 3.51872, smoothed loss 3.47741, grad norm 3.11460, param norm 141.22366
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 4725, loss 3.43549, smoothed loss 3.46834, grad norm 3.19078, param norm 141.26913
INFO:root:epoch 6, iter 4730, loss 4.40940, smoothed loss 3.47062, grad norm 3.65337, param norm 141.31750
INFO:root:epoch 6, iter 4735, loss 3.43056, smoothed loss 3.47406, grad norm 2.88860, param norm 141.36549
INFO:root:epoch 6, iter 4740, loss 3.36517, smoothed loss 3.46425, grad norm 3.07840, param norm 141.41566
INFO:root:epoch 6, iter 4745, loss 3.52133, smoothed loss 3.45999, grad norm 2.89959, param norm 141.46365
INFO:root:epoch 6, iter 4750, loss 3.29049, smoothed loss 3.46426, grad norm 2.82354, param norm 141.50526
INFO:root:epoch 6, iter 4755, loss 3.02534, smoothed loss 3.45317, grad norm 2.77721, param norm 141.54491
INFO:root:epoch 6, iter 4760, loss 3.94784, smoothed loss 3.45878, grad norm 3.28266, param norm 141.59038
INFO:root:epoch 6, iter 4765, loss 3.39157, smoothed loss 3.46452, grad norm 2.93145, param norm 141.63499
INFO:root:epoch 6, iter 4770, loss 3.14241, smoothed loss 3.47461, grad norm 3.18070, param norm 141.67697
INFO:root:epoch 6, iter 4775, loss 3.84544, smoothed loss 3.45814, grad norm 3.29743, param norm 141.72374
INFO:root:epoch 6, iter 4780, loss 3.96925, smoothed loss 3.45171, grad norm 3.48024, param norm 141.77237
INFO:root:epoch 6, iter 4785, loss 3.36144, smoothed loss 3.45353, grad norm 2.87489, param norm 141.82375
INFO:root:epoch 6, iter 4790, loss 3.22381, smoothed loss 3.45117, grad norm 3.34254, param norm 141.87115
INFO:root:epoch 6, iter 4795, loss 2.78608, smoothed loss 3.44254, grad norm 3.14806, param norm 141.92097
INFO:root:epoch 6, iter 4800, loss 3.44052, smoothed loss 3.44320, grad norm 3.33786, param norm 141.97144
INFO:root:epoch 6, iter 4805, loss 3.61019, smoothed loss 3.45025, grad norm 3.02007, param norm 142.01833
INFO:root:epoch 6, iter 4810, loss 3.42898, smoothed loss 3.46539, grad norm 2.60496, param norm 142.05859
INFO:root:epoch 6, iter 4815, loss 3.14992, smoothed loss 3.46175, grad norm 2.79320, param norm 142.09660
INFO:root:epoch 6, iter 4820, loss 3.52087, smoothed loss 3.46437, grad norm 2.88966, param norm 142.13992
INFO:root:epoch 6, iter 4825, loss 3.53366, smoothed loss 3.46465, grad norm 3.05161, param norm 142.18437
INFO:root:epoch 6, iter 4830, loss 3.23492, smoothed loss 3.46066, grad norm 3.00255, param norm 142.23265
INFO:root:epoch 6, iter 4835, loss 3.41224, smoothed loss 3.47253, grad norm 2.68536, param norm 142.28215
INFO:root:epoch 6, iter 4840, loss 3.11604, smoothed loss 3.47279, grad norm 2.81457, param norm 142.33493
INFO:root:epoch 6, iter 4845, loss 3.35947, smoothed loss 3.48483, grad norm 3.05988, param norm 142.38336
INFO:root:epoch 6, iter 4850, loss 3.14976, smoothed loss 3.47503, grad norm 2.89764, param norm 142.43227
INFO:root:epoch 6, iter 4855, loss 3.08582, smoothed loss 3.46740, grad norm 2.71908, param norm 142.47845
INFO:root:epoch 6, iter 4860, loss 4.19585, smoothed loss 3.48585, grad norm 3.49165, param norm 142.52170
INFO:root:epoch 6, iter 4865, loss 3.43608, smoothed loss 3.49148, grad norm 3.05451, param norm 142.56294
INFO:root:epoch 6, iter 4870, loss 3.66995, smoothed loss 3.50062, grad norm 3.14112, param norm 142.60626
INFO:root:epoch 6, iter 4875, loss 2.83625, smoothed loss 3.48105, grad norm 2.76404, param norm 142.65715
INFO:root:epoch 6, iter 4880, loss 3.73579, smoothed loss 3.47541, grad norm 3.01884, param norm 142.70793
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 4885, loss 3.83873, smoothed loss 3.48677, grad norm 3.07236, param norm 142.75139
INFO:root:epoch 6, iter 4890, loss 4.13287, smoothed loss 3.49240, grad norm 3.33708, param norm 142.79765
INFO:root:epoch 6, iter 4895, loss 3.13147, smoothed loss 3.48795, grad norm 3.08242, param norm 142.84355
INFO:root:epoch 6, iter 4900, loss 3.53898, smoothed loss 3.47702, grad norm 3.18323, param norm 142.88928
INFO:root:epoch 6, iter 4905, loss 3.77446, smoothed loss 3.47896, grad norm 3.12797, param norm 142.93736
INFO:root:epoch 6, iter 4910, loss 3.36907, smoothed loss 3.46739, grad norm 3.04986, param norm 142.98381
INFO:root:epoch 6, iter 4915, loss 4.26358, smoothed loss 3.46498, grad norm 3.46578, param norm 143.02905
INFO:root:epoch 6, iter 4920, loss 3.48646, smoothed loss 3.46813, grad norm 2.75703, param norm 143.07207
INFO:root:epoch 6, iter 4925, loss 3.31382, smoothed loss 3.45389, grad norm 2.91557, param norm 143.11714
INFO:root:epoch 6, iter 4930, loss 4.13439, smoothed loss 3.45959, grad norm 3.18636, param norm 143.17000
INFO:root:epoch 6, iter 4935, loss 3.11587, smoothed loss 3.44871, grad norm 2.87636, param norm 143.22876
INFO:root:epoch 6, iter 4940, loss 4.22047, smoothed loss 3.46066, grad norm 3.12505, param norm 143.28056
INFO:root:epoch 6, iter 4945, loss 3.61154, smoothed loss 3.46255, grad norm 3.21077, param norm 143.32832
INFO:root:epoch 6, iter 4950, loss 3.98172, smoothed loss 3.46232, grad norm 3.27585, param norm 143.37390
INFO:root:epoch 6, iter 4955, loss 2.82273, smoothed loss 3.45444, grad norm 2.62792, param norm 143.41426
INFO:root:epoch 6, iter 4960, loss 3.42265, smoothed loss 3.44773, grad norm 3.39610, param norm 143.45950
INFO:root:epoch 6, iter 4965, loss 2.42302, smoothed loss 3.45205, grad norm 2.47630, param norm 143.50365
INFO:root:epoch 6, iter 4970, loss 3.10095, smoothed loss 3.44550, grad norm 2.68799, param norm 143.55229
INFO:root:epoch 6, iter 4975, loss 3.71180, smoothed loss 3.43830, grad norm 3.32662, param norm 143.59717
INFO:root:epoch 6, iter 4980, loss 2.90616, smoothed loss 3.43716, grad norm 3.09191, param norm 143.64001
INFO:root:epoch 6, iter 4985, loss 4.09996, smoothed loss 3.43942, grad norm 3.82913, param norm 143.68268
INFO:root:epoch 6, iter 4990, loss 3.11038, smoothed loss 3.43705, grad norm 3.17183, param norm 143.72372
INFO:root:epoch 6, iter 4995, loss 3.19073, smoothed loss 3.43474, grad norm 2.99791, param norm 143.77118
INFO:root:epoch 6, iter 5000, loss 4.08783, smoothed loss 3.44526, grad norm 3.01332, param norm 143.82153
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 6, Iter 5000, dev loss: 3.298174
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.93034 seconds [Score: 0.67790]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.70635 seconds [Score: 0.55100]
INFO:root:Epoch 6, Iter 5000, Train F1 score: 0.677902, Train EM score: 0.551000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 116.74829 seconds [Score: 0.60292]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.03803 seconds [Score: 0.45181]
INFO:root:Epoch 6, Iter 5000, Dev F1 score: 0.602923, Dev EM score: 0.451812
INFO:root:End of epoch 6
INFO:root:epoch 6, iter 5005, loss 2.89626, smoothed loss 3.43658, grad norm 2.57509, param norm 143.86627
INFO:root:epoch 6, iter 5010, loss 3.86038, smoothed loss 3.43878, grad norm 3.31723, param norm 143.91260
INFO:root:epoch 6, iter 5015, loss 3.13450, smoothed loss 3.43145, grad norm 2.86157, param norm 143.96060
INFO:root:epoch 6, iter 5020, loss 3.48346, smoothed loss 3.42919, grad norm 3.12412, param norm 144.00227
INFO:root:epoch 6, iter 5025, loss 4.01325, smoothed loss 3.44333, grad norm 3.52154, param norm 144.04129
INFO:root:epoch 6, iter 5030, loss 2.51161, smoothed loss 3.43853, grad norm 2.70276, param norm 144.07925
INFO:root:epoch 6, iter 5035, loss 3.49878, smoothed loss 3.43175, grad norm 2.63454, param norm 144.12932
INFO:root:epoch 6, iter 5040, loss 3.31950, smoothed loss 3.42276, grad norm 3.09199, param norm 144.18033
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5045, loss 3.40534, smoothed loss 3.40444, grad norm 2.85160, param norm 144.23210
INFO:root:epoch 6, iter 5050, loss 3.75936, smoothed loss 3.40417, grad norm 3.46814, param norm 144.28340
INFO:root:epoch 6, iter 5055, loss 3.81663, smoothed loss 3.40925, grad norm 3.23173, param norm 144.33354
INFO:root:epoch 6, iter 5060, loss 3.54939, smoothed loss 3.40157, grad norm 3.58379, param norm 144.37637
INFO:root:epoch 6, iter 5065, loss 3.58409, smoothed loss 3.40152, grad norm 3.27252, param norm 144.41830
INFO:root:epoch 6, iter 5070, loss 3.41372, smoothed loss 3.39226, grad norm 3.49822, param norm 144.46024
INFO:root:epoch 6, iter 5075, loss 2.46149, smoothed loss 3.38582, grad norm 3.00146, param norm 144.50479
INFO:root:epoch 6, iter 5080, loss 3.01546, smoothed loss 3.38624, grad norm 2.93022, param norm 144.55165
INFO:root:epoch 6, iter 5085, loss 3.32583, smoothed loss 3.38534, grad norm 2.62629, param norm 144.60068
INFO:root:epoch 6, iter 5090, loss 2.89207, smoothed loss 3.38490, grad norm 2.55206, param norm 144.64482
INFO:root:epoch 6, iter 5095, loss 3.43361, smoothed loss 3.40752, grad norm 3.10222, param norm 144.68030
INFO:root:epoch 6, iter 5100, loss 3.19610, smoothed loss 3.40205, grad norm 3.01891, param norm 144.72191
INFO:root:epoch 6, iter 5105, loss 3.54724, smoothed loss 3.39939, grad norm 2.90148, param norm 144.76697
INFO:root:epoch 6, iter 5110, loss 3.27397, smoothed loss 3.39412, grad norm 3.21190, param norm 144.81461
INFO:root:epoch 6, iter 5115, loss 3.69745, smoothed loss 3.38847, grad norm 3.25575, param norm 144.86490
INFO:root:epoch 6, iter 5120, loss 3.26315, smoothed loss 3.37808, grad norm 3.14532, param norm 144.91393
INFO:root:epoch 6, iter 5125, loss 3.28510, smoothed loss 3.39290, grad norm 2.67605, param norm 144.95712
INFO:root:epoch 6, iter 5130, loss 3.84552, smoothed loss 3.40360, grad norm 3.54790, param norm 144.99988
INFO:root:epoch 6, iter 5135, loss 3.34474, smoothed loss 3.41442, grad norm 3.19786, param norm 145.03992
INFO:root:epoch 6, iter 5140, loss 3.14685, smoothed loss 3.41374, grad norm 2.78762, param norm 145.08519
INFO:root:epoch 6, iter 5145, loss 4.09136, smoothed loss 3.40792, grad norm 3.56821, param norm 145.13638
INFO:root:epoch 6, iter 5150, loss 3.45701, smoothed loss 3.39969, grad norm 3.04157, param norm 145.18335
INFO:root:epoch 6, iter 5155, loss 3.80510, smoothed loss 3.40025, grad norm 3.36255, param norm 145.22678
INFO:root:epoch 6, iter 5160, loss 3.07812, smoothed loss 3.38970, grad norm 2.94912, param norm 145.26553
INFO:root:epoch 6, iter 5165, loss 3.41442, smoothed loss 3.39322, grad norm 2.81485, param norm 145.30937
INFO:root:epoch 6, iter 5170, loss 3.80120, smoothed loss 3.40680, grad norm 3.10230, param norm 145.35817
INFO:root:epoch 6, iter 5175, loss 2.65474, smoothed loss 3.39768, grad norm 2.49784, param norm 145.41048
INFO:root:epoch 6, iter 5180, loss 3.12308, smoothed loss 3.38378, grad norm 3.10117, param norm 145.46150
INFO:root:epoch 6, iter 5185, loss 3.49172, smoothed loss 3.38665, grad norm 3.21565, param norm 145.51028
INFO:root:epoch 6, iter 5190, loss 3.21983, smoothed loss 3.38368, grad norm 2.92259, param norm 145.55247
INFO:root:epoch 6, iter 5195, loss 3.54697, smoothed loss 3.38215, grad norm 3.32422, param norm 145.59633
INFO:root:epoch 6, iter 5200, loss 2.79095, smoothed loss 3.38174, grad norm 2.89930, param norm 145.63991
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5205, loss 3.78438, smoothed loss 3.37182, grad norm 3.33343, param norm 145.68149
INFO:root:epoch 6, iter 5210, loss 3.37435, smoothed loss 3.37536, grad norm 3.24174, param norm 145.72441
INFO:root:epoch 6, iter 5215, loss 2.98477, smoothed loss 3.36409, grad norm 2.87438, param norm 145.77496
INFO:root:epoch 6, iter 5220, loss 4.10067, smoothed loss 3.37713, grad norm 3.27644, param norm 145.82892
INFO:root:epoch 6, iter 5225, loss 4.42385, smoothed loss 3.38590, grad norm 3.45989, param norm 145.87616
INFO:root:epoch 6, iter 5230, loss 3.24050, smoothed loss 3.37397, grad norm 2.70593, param norm 145.91759
INFO:root:epoch 6, iter 5235, loss 3.62017, smoothed loss 3.36620, grad norm 3.09768, param norm 145.95772
INFO:root:epoch 6, iter 5240, loss 3.38897, smoothed loss 3.35954, grad norm 2.97419, param norm 145.99373
INFO:root:epoch 6, iter 5245, loss 3.05519, smoothed loss 3.33921, grad norm 2.96279, param norm 146.03259
INFO:root:epoch 6, iter 5250, loss 3.03805, smoothed loss 3.32320, grad norm 3.10144, param norm 146.07574
INFO:root:epoch 6, iter 5255, loss 2.52499, smoothed loss 3.31588, grad norm 2.82040, param norm 146.12140
INFO:root:epoch 6, iter 5260, loss 4.27709, smoothed loss 3.31576, grad norm 3.29190, param norm 146.16585
INFO:root:epoch 6, iter 5265, loss 3.03275, smoothed loss 3.32121, grad norm 2.57994, param norm 146.21130
INFO:root:epoch 6, iter 5270, loss 2.94273, smoothed loss 3.32120, grad norm 2.86446, param norm 146.25824
INFO:root:epoch 6, iter 5275, loss 3.79119, smoothed loss 3.32582, grad norm 3.28792, param norm 146.31023
INFO:root:epoch 6, iter 5280, loss 2.89298, smoothed loss 3.32671, grad norm 2.69717, param norm 146.35722
INFO:root:epoch 6, iter 5285, loss 3.76422, smoothed loss 3.32103, grad norm 3.31195, param norm 146.39832
INFO:root:epoch 6, iter 5290, loss 2.93910, smoothed loss 3.31095, grad norm 3.12786, param norm 146.44110
INFO:root:epoch 6, iter 5295, loss 2.53462, smoothed loss 3.28274, grad norm 2.83921, param norm 146.48421
INFO:root:epoch 6, iter 5300, loss 3.56021, smoothed loss 3.29458, grad norm 3.02469, param norm 146.52466
INFO:root:epoch 6, iter 5305, loss 2.95501, smoothed loss 3.28532, grad norm 2.94685, param norm 146.56685
INFO:root:epoch 6, iter 5310, loss 3.37621, smoothed loss 3.27763, grad norm 3.68552, param norm 146.61505
INFO:root:epoch 6, iter 5315, loss 3.88637, smoothed loss 3.27905, grad norm 3.18577, param norm 146.66487
INFO:root:epoch 6, iter 5320, loss 3.52904, smoothed loss 3.28454, grad norm 3.32215, param norm 146.70938
INFO:root:epoch 6, iter 5325, loss 3.08755, smoothed loss 3.28917, grad norm 2.98310, param norm 146.75542
INFO:root:epoch 6, iter 5330, loss 3.85545, smoothed loss 3.29767, grad norm 3.23309, param norm 146.80157
INFO:root:epoch 6, iter 5335, loss 3.74815, smoothed loss 3.31112, grad norm 2.90737, param norm 146.84526
INFO:root:epoch 6, iter 5340, loss 3.31733, smoothed loss 3.30809, grad norm 3.16369, param norm 146.88756
INFO:root:epoch 6, iter 5345, loss 3.36590, smoothed loss 3.31132, grad norm 2.87176, param norm 146.92772
INFO:root:epoch 6, iter 5350, loss 3.54804, smoothed loss 3.31401, grad norm 3.19307, param norm 146.97025
INFO:root:epoch 6, iter 5355, loss 2.94746, smoothed loss 3.31966, grad norm 2.94169, param norm 147.00993
INFO:root:epoch 6, iter 5360, loss 3.77642, smoothed loss 3.32656, grad norm 3.36350, param norm 147.04994
Adding batches start...
Added  160  batches
INFO:root:epoch 6, iter 5365, loss 3.09521, smoothed loss 3.32926, grad norm 3.31135, param norm 147.08865
INFO:root:epoch 6, iter 5370, loss 3.58075, smoothed loss 3.33022, grad norm 3.47075, param norm 147.13097
INFO:root:epoch 6, iter 5375, loss 2.74422, smoothed loss 3.32367, grad norm 2.89272, param norm 147.17322
INFO:root:epoch 6, iter 5380, loss 3.19197, smoothed loss 3.33078, grad norm 2.99002, param norm 147.21004
INFO:root:epoch 6, iter 5385, loss 3.39036, smoothed loss 3.33013, grad norm 3.13104, param norm 147.24988
INFO:root:epoch 6, iter 5390, loss 3.08626, smoothed loss 3.32982, grad norm 2.91697, param norm 147.29355
INFO:root:epoch 6, iter 5395, loss 3.01095, smoothed loss 3.33927, grad norm 2.48662, param norm 147.34286
INFO:root:epoch 6, iter 5400, loss 3.08026, smoothed loss 3.34482, grad norm 2.58523, param norm 147.39514
INFO:root:epoch 6, iter 5405, loss 3.16079, smoothed loss 3.33447, grad norm 3.16406, param norm 147.44739
INFO:root:epoch 6, iter 5410, loss 2.74924, smoothed loss 3.33013, grad norm 2.41781, param norm 147.49869
INFO:root:epoch 6, iter 5415, loss 3.27427, smoothed loss 3.32940, grad norm 3.09547, param norm 147.54971
INFO:root:epoch 6, iter 5420, loss 3.17758, smoothed loss 3.33390, grad norm 3.26357, param norm 147.59396
INFO:root:epoch 6, iter 5425, loss 3.24898, smoothed loss 3.33277, grad norm 2.81671, param norm 147.62794
INFO:root:epoch 6, iter 5430, loss 3.10023, smoothed loss 3.32014, grad norm 2.97982, param norm 147.66856
INFO:root:epoch 6, iter 5435, loss 3.06095, smoothed loss 3.33111, grad norm 2.97629, param norm 147.70845
INFO:root:epoch 6, iter 5440, loss 2.72435, smoothed loss 3.34655, grad norm 2.70075, param norm 147.74826
INFO:root:epoch 6, iter 5445, loss 3.23517, smoothed loss 3.33805, grad norm 3.49364, param norm 147.78809
INFO:root:epoch 6, iter 5450, loss 3.78188, smoothed loss 3.34388, grad norm 3.40777, param norm 147.83009
INFO:root:epoch 6, iter 5455, loss 3.38165, smoothed loss 3.35744, grad norm 3.12521, param norm 147.86732
INFO:root:epoch 6, iter 5460, loss 2.84214, smoothed loss 3.34779, grad norm 2.75336, param norm 147.90816
INFO:root:epoch 6, iter 5465, loss 3.15448, smoothed loss 3.35038, grad norm 3.02546, param norm 147.95624
INFO:root:epoch 6, iter 5470, loss 3.44176, smoothed loss 3.34977, grad norm 2.95411, param norm 147.99936
INFO:root:epoch 6, iter 5475, loss 3.17773, smoothed loss 3.35266, grad norm 2.78126, param norm 148.03915
INFO:root:epoch 6, iter 5480, loss 3.77348, smoothed loss 3.34201, grad norm 3.38300, param norm 148.07866
INFO:root:epoch 6, iter 5485, loss 3.80450, smoothed loss 3.34076, grad norm 3.57229, param norm 148.11742
INFO:root:epoch 6, iter 5490, loss 3.29650, smoothed loss 3.32918, grad norm 2.73323, param norm 148.16029
INFO:root:epoch 6, iter 5495, loss 3.20986, smoothed loss 3.33174, grad norm 3.13710, param norm 148.20776
INFO:root:epoch 6, iter 5500, loss 3.73334, smoothed loss 3.34379, grad norm 3.68029, param norm 148.24808
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 6, Iter 5500, dev loss: 3.217205
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 16.48126 seconds [Score: 0.66472]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.10317 seconds [Score: 0.52400]
INFO:root:Epoch 6, Iter 5500, Train F1 score: 0.664718, Train EM score: 0.524000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 115.53990 seconds [Score: 0.61407]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 115.23787 seconds [Score: 0.46951]
INFO:root:Epoch 6, Iter 5500, Dev F1 score: 0.614072, Dev EM score: 0.469514
INFO:root:End of epoch 6
INFO:root:epoch 6, iter 5505, loss 3.04648, smoothed loss 3.32908, grad norm 2.94143, param norm 148.28964
INFO:root:epoch 6, iter 5510, loss 3.40911, smoothed loss 3.33340, grad norm 2.58800, param norm 148.33305
INFO:root:epoch 6, iter 5515, loss 3.68869, smoothed loss 3.32820, grad norm 3.06403, param norm 148.37735
INFO:root:epoch 6, iter 5520, loss 3.61286, smoothed loss 3.33458, grad norm 2.98490, param norm 148.42308
Adding batches start...
Added  144  batches
INFO:root:epoch 6, iter 5525, loss 4.38280, smoothed loss 3.34372, grad norm 3.22153, param norm 148.46587
INFO:root:epoch 6, iter 5530, loss 3.02735, smoothed loss 3.33337, grad norm 3.17232, param norm 148.51218
INFO:root:epoch 6, iter 5535, loss 3.39462, smoothed loss 3.33699, grad norm 3.10413, param norm 148.55551
INFO:root:epoch 6, iter 5540, loss 3.34411, smoothed loss 3.34242, grad norm 2.69686, param norm 148.59750
INFO:root:epoch 6, iter 5545, loss 3.40408, smoothed loss 3.35268, grad norm 3.39787, param norm 148.63666
INFO:root:epoch 6, iter 5550, loss 3.25957, smoothed loss 3.34295, grad norm 3.06066, param norm 148.68491
INFO:root:epoch 6, iter 5555, loss 4.09091, smoothed loss 3.35546, grad norm 3.22513, param norm 148.73070
INFO:root:epoch 6, iter 5560, loss 2.86234, smoothed loss 3.35454, grad norm 2.37164, param norm 148.76775
INFO:root:epoch 6, iter 5565, loss 4.24005, smoothed loss 3.36288, grad norm 3.28671, param norm 148.81137
INFO:root:epoch 6, iter 5570, loss 3.01250, smoothed loss 3.34590, grad norm 2.87460, param norm 148.85632
INFO:root:epoch 6, iter 5575, loss 3.22174, smoothed loss 3.34811, grad norm 3.02673, param norm 148.90091
INFO:root:epoch 6, iter 5580, loss 3.72026, smoothed loss 3.34509, grad norm 3.01761, param norm 148.94278
INFO:root:epoch 6, iter 5585, loss 3.81422, smoothed loss 3.35606, grad norm 3.33459, param norm 148.98381
INFO:root:epoch 6, iter 5590, loss 3.81314, smoothed loss 3.35045, grad norm 3.28585, param norm 149.02640
INFO:root:epoch 6, iter 5595, loss 3.63776, smoothed loss 3.34746, grad norm 3.46333, param norm 149.07051
INFO:root:epoch 6, iter 5600, loss 3.29181, smoothed loss 3.33966, grad norm 3.08381, param norm 149.11340
INFO:root:epoch 6, iter 5605, loss 3.34761, smoothed loss 3.34042, grad norm 2.82246, param norm 149.15796
INFO:root:epoch 6, iter 5610, loss 2.89030, smoothed loss 3.31780, grad norm 3.09025, param norm 149.20285
INFO:root:epoch 6, iter 5615, loss 3.24993, smoothed loss 3.30855, grad norm 3.59266, param norm 149.24886
INFO:root:epoch 6, iter 5620, loss 3.24770, smoothed loss 3.31635, grad norm 2.69482, param norm 149.29301
INFO:root:epoch 6, iter 5625, loss 3.20333, smoothed loss 3.31220, grad norm 3.01327, param norm 149.33559
INFO:root:epoch 6, iter 5630, loss 3.45230, smoothed loss 3.30981, grad norm 2.65013, param norm 149.37512
INFO:root:epoch 6, iter 5635, loss 3.08902, smoothed loss 3.31102, grad norm 2.66695, param norm 149.40919
INFO:root:epoch 6, iter 5640, loss 3.42933, smoothed loss 3.30954, grad norm 2.83277, param norm 149.44865
INFO:root:epoch 6, iter 5645, loss 3.91187, smoothed loss 3.30404, grad norm 3.12389, param norm 149.49051
INFO:root:epoch 6, iter 5650, loss 3.61143, smoothed loss 3.31599, grad norm 3.00440, param norm 149.53127
INFO:root:epoch 6, iter 5655, loss 2.86255, smoothed loss 3.31694, grad norm 2.84194, param norm 149.57402
INFO:root:epoch 6, iter 5660, loss 3.65325, smoothed loss 3.31033, grad norm 3.19983, param norm 149.62335
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5665, loss 3.89711, smoothed loss 3.30178, grad norm 3.36945, param norm 149.67389
INFO:root:epoch 7, iter 5670, loss 3.12338, smoothed loss 3.30166, grad norm 2.96653, param norm 149.71762
INFO:root:epoch 7, iter 5675, loss 3.43972, smoothed loss 3.31393, grad norm 3.29929, param norm 149.75917
INFO:root:epoch 7, iter 5680, loss 3.15745, smoothed loss 3.32280, grad norm 3.01062, param norm 149.80147
INFO:root:epoch 7, iter 5685, loss 3.43914, smoothed loss 3.32726, grad norm 2.86535, param norm 149.84691
INFO:root:epoch 7, iter 5690, loss 2.83563, smoothed loss 3.31443, grad norm 3.02590, param norm 149.89700
INFO:root:epoch 7, iter 5695, loss 3.78587, smoothed loss 3.31473, grad norm 3.18748, param norm 149.94376
INFO:root:epoch 7, iter 5700, loss 3.53708, smoothed loss 3.30818, grad norm 3.22533, param norm 149.98369
INFO:root:epoch 7, iter 5705, loss 3.23385, smoothed loss 3.31524, grad norm 3.49858, param norm 150.02351
INFO:root:epoch 7, iter 5710, loss 2.95859, smoothed loss 3.30873, grad norm 2.99640, param norm 150.06892
INFO:root:epoch 7, iter 5715, loss 2.90108, smoothed loss 3.30023, grad norm 2.90803, param norm 150.11813
INFO:root:epoch 7, iter 5720, loss 3.13739, smoothed loss 3.30182, grad norm 3.11773, param norm 150.16347
INFO:root:epoch 7, iter 5725, loss 3.63027, smoothed loss 3.30414, grad norm 3.10332, param norm 150.20677
INFO:root:epoch 7, iter 5730, loss 3.15809, smoothed loss 3.28914, grad norm 2.97248, param norm 150.25006
INFO:root:epoch 7, iter 5735, loss 3.77323, smoothed loss 3.29829, grad norm 3.42737, param norm 150.29488
INFO:root:epoch 7, iter 5740, loss 3.23077, smoothed loss 3.29798, grad norm 2.63340, param norm 150.33844
INFO:root:epoch 7, iter 5745, loss 4.07075, smoothed loss 3.30489, grad norm 3.44894, param norm 150.37860
INFO:root:epoch 7, iter 5750, loss 3.49793, smoothed loss 3.31401, grad norm 2.90118, param norm 150.41959
INFO:root:epoch 7, iter 5755, loss 3.02833, smoothed loss 3.31375, grad norm 2.79503, param norm 150.46445
INFO:root:epoch 7, iter 5760, loss 4.14712, smoothed loss 3.32018, grad norm 3.42244, param norm 150.51041
INFO:root:epoch 7, iter 5765, loss 3.53035, smoothed loss 3.30171, grad norm 3.04849, param norm 150.55670
INFO:root:epoch 7, iter 5770, loss 2.96543, smoothed loss 3.30400, grad norm 2.81242, param norm 150.60217
INFO:root:epoch 7, iter 5775, loss 2.64945, smoothed loss 3.28184, grad norm 3.02395, param norm 150.64832
INFO:root:epoch 7, iter 5780, loss 2.73610, smoothed loss 3.28031, grad norm 3.05228, param norm 150.69437
INFO:root:epoch 7, iter 5785, loss 3.15747, smoothed loss 3.27206, grad norm 2.83893, param norm 150.74106
INFO:root:epoch 7, iter 5790, loss 4.02060, smoothed loss 3.28015, grad norm 3.14363, param norm 150.78146
INFO:root:epoch 7, iter 5795, loss 3.62844, smoothed loss 3.27765, grad norm 3.02864, param norm 150.81750
INFO:root:epoch 7, iter 5800, loss 3.64592, smoothed loss 3.28121, grad norm 3.03947, param norm 150.85657
INFO:root:epoch 7, iter 5805, loss 3.07599, smoothed loss 3.29084, grad norm 3.06210, param norm 150.89835
INFO:root:epoch 7, iter 5810, loss 2.83267, smoothed loss 3.26988, grad norm 2.59090, param norm 150.93797
INFO:root:epoch 7, iter 5815, loss 3.39170, smoothed loss 3.26713, grad norm 3.36917, param norm 150.97412
INFO:root:epoch 7, iter 5820, loss 2.95802, smoothed loss 3.25822, grad norm 3.21862, param norm 151.01102
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5825, loss 3.83719, smoothed loss 3.25413, grad norm 3.45732, param norm 151.05432
INFO:root:epoch 7, iter 5830, loss 3.57055, smoothed loss 3.25086, grad norm 3.37225, param norm 151.10124
INFO:root:epoch 7, iter 5835, loss 3.43693, smoothed loss 3.26395, grad norm 3.17204, param norm 151.14371
INFO:root:epoch 7, iter 5840, loss 2.68490, smoothed loss 3.26431, grad norm 2.91863, param norm 151.18391
INFO:root:epoch 7, iter 5845, loss 3.03548, smoothed loss 3.26412, grad norm 3.03889, param norm 151.22308
INFO:root:epoch 7, iter 5850, loss 3.35257, smoothed loss 3.24293, grad norm 3.20108, param norm 151.26701
INFO:root:epoch 7, iter 5855, loss 3.30266, smoothed loss 3.24537, grad norm 3.01258, param norm 151.31041
INFO:root:epoch 7, iter 5860, loss 3.84836, smoothed loss 3.25955, grad norm 3.16202, param norm 151.35905
INFO:root:epoch 7, iter 5865, loss 3.49756, smoothed loss 3.26459, grad norm 3.05758, param norm 151.40685
INFO:root:epoch 7, iter 5870, loss 4.07631, smoothed loss 3.27314, grad norm 3.38826, param norm 151.45808
INFO:root:epoch 7, iter 5875, loss 3.26639, smoothed loss 3.27141, grad norm 2.89051, param norm 151.50830
INFO:root:epoch 7, iter 5880, loss 3.65675, smoothed loss 3.27575, grad norm 3.05328, param norm 151.54994
INFO:root:epoch 7, iter 5885, loss 2.81997, smoothed loss 3.26179, grad norm 2.69108, param norm 151.58658
INFO:root:epoch 7, iter 5890, loss 3.94292, smoothed loss 3.27517, grad norm 3.12127, param norm 151.62450
INFO:root:epoch 7, iter 5895, loss 4.60426, smoothed loss 3.28765, grad norm 3.80154, param norm 151.66379
INFO:root:epoch 7, iter 5900, loss 3.16433, smoothed loss 3.28253, grad norm 2.91467, param norm 151.70776
INFO:root:epoch 7, iter 5905, loss 3.27470, smoothed loss 3.28558, grad norm 3.01519, param norm 151.75266
INFO:root:epoch 7, iter 5910, loss 3.11303, smoothed loss 3.28230, grad norm 2.96202, param norm 151.79613
INFO:root:epoch 7, iter 5915, loss 3.68035, smoothed loss 3.28001, grad norm 3.41721, param norm 151.83980
INFO:root:epoch 7, iter 5920, loss 3.20762, smoothed loss 3.27583, grad norm 2.80434, param norm 151.88608
INFO:root:epoch 7, iter 5925, loss 3.30638, smoothed loss 3.27210, grad norm 3.35850, param norm 151.92926
INFO:root:epoch 7, iter 5930, loss 2.98326, smoothed loss 3.26700, grad norm 3.36712, param norm 151.96991
INFO:root:epoch 7, iter 5935, loss 3.06549, smoothed loss 3.25041, grad norm 2.93577, param norm 152.01183
INFO:root:epoch 7, iter 5940, loss 3.34477, smoothed loss 3.25256, grad norm 3.00202, param norm 152.05597
INFO:root:epoch 7, iter 5945, loss 2.90577, smoothed loss 3.25552, grad norm 2.84777, param norm 152.09836
INFO:root:epoch 7, iter 5950, loss 3.13106, smoothed loss 3.24627, grad norm 2.91421, param norm 152.14020
INFO:root:epoch 7, iter 5955, loss 3.82447, smoothed loss 3.24520, grad norm 3.31317, param norm 152.18260
INFO:root:epoch 7, iter 5960, loss 3.85578, smoothed loss 3.24304, grad norm 3.36910, param norm 152.22438
INFO:root:epoch 7, iter 5965, loss 3.48176, smoothed loss 3.24911, grad norm 2.72606, param norm 152.25945
INFO:root:epoch 7, iter 5970, loss 3.62305, smoothed loss 3.25402, grad norm 2.87723, param norm 152.29445
INFO:root:epoch 7, iter 5975, loss 3.07110, smoothed loss 3.26495, grad norm 2.64859, param norm 152.33420
INFO:root:epoch 7, iter 5980, loss 3.43852, smoothed loss 3.28104, grad norm 2.91079, param norm 152.37827
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 5985, loss 3.35673, smoothed loss 3.27489, grad norm 2.84723, param norm 152.41965
INFO:root:epoch 7, iter 5990, loss 3.25590, smoothed loss 3.27885, grad norm 3.21726, param norm 152.45946
INFO:root:epoch 7, iter 5995, loss 3.35594, smoothed loss 3.27243, grad norm 3.38708, param norm 152.49805
INFO:root:epoch 7, iter 6000, loss 2.89318, smoothed loss 3.26751, grad norm 2.68066, param norm 152.53714
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 7, Iter 6000, dev loss: 3.189928
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.49135 seconds [Score: 0.69912]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.84725 seconds [Score: 0.58000]
INFO:root:Epoch 7, Iter 6000, Train F1 score: 0.699120, Train EM score: 0.580000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 114.81918 seconds [Score: 0.61968]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 114.35615 seconds [Score: 0.47373]
INFO:root:Epoch 7, Iter 6000, Dev F1 score: 0.619685, Dev EM score: 0.473729
INFO:root:End of epoch 7
INFO:root:epoch 7, iter 6005, loss 2.55693, smoothed loss 3.24545, grad norm 2.50500, param norm 152.57713
INFO:root:epoch 7, iter 6010, loss 3.17742, smoothed loss 3.23330, grad norm 3.10071, param norm 152.61806
INFO:root:epoch 7, iter 6015, loss 3.24570, smoothed loss 3.22493, grad norm 3.18819, param norm 152.65709
INFO:root:epoch 7, iter 6020, loss 2.74069, smoothed loss 3.21932, grad norm 2.79874, param norm 152.69618
INFO:root:epoch 7, iter 6025, loss 3.30135, smoothed loss 3.22219, grad norm 3.22248, param norm 152.73405
INFO:root:epoch 7, iter 6030, loss 3.02436, smoothed loss 3.21876, grad norm 2.72762, param norm 152.77632
INFO:root:epoch 7, iter 6035, loss 3.15060, smoothed loss 3.20403, grad norm 3.05577, param norm 152.81642
INFO:root:epoch 7, iter 6040, loss 3.37296, smoothed loss 3.19378, grad norm 3.14880, param norm 152.85574
INFO:root:epoch 7, iter 6045, loss 3.61548, smoothed loss 3.18818, grad norm 3.13238, param norm 152.89735
INFO:root:epoch 7, iter 6050, loss 3.43098, smoothed loss 3.18609, grad norm 3.55706, param norm 152.93710
INFO:root:epoch 7, iter 6055, loss 3.79543, smoothed loss 3.17486, grad norm 3.33380, param norm 152.97711
INFO:root:epoch 7, iter 6060, loss 3.42105, smoothed loss 3.18792, grad norm 3.13397, param norm 153.01570
INFO:root:epoch 7, iter 6065, loss 3.00266, smoothed loss 3.19350, grad norm 2.80566, param norm 153.04970
INFO:root:epoch 7, iter 6070, loss 2.53227, smoothed loss 3.18143, grad norm 2.95947, param norm 153.09023
INFO:root:epoch 7, iter 6075, loss 3.84742, smoothed loss 3.19004, grad norm 3.39733, param norm 153.13263
INFO:root:epoch 7, iter 6080, loss 3.00666, smoothed loss 3.18355, grad norm 3.22618, param norm 153.17242
INFO:root:epoch 7, iter 6085, loss 3.76249, smoothed loss 3.18849, grad norm 3.56501, param norm 153.21404
INFO:root:epoch 7, iter 6090, loss 3.49172, smoothed loss 3.19224, grad norm 3.34304, param norm 153.25334
INFO:root:epoch 7, iter 6095, loss 3.51150, smoothed loss 3.20037, grad norm 3.19984, param norm 153.29591
INFO:root:epoch 7, iter 6100, loss 3.30565, smoothed loss 3.20623, grad norm 2.98285, param norm 153.34427
INFO:root:epoch 7, iter 6105, loss 3.13351, smoothed loss 3.20773, grad norm 2.88996, param norm 153.39166
INFO:root:epoch 7, iter 6110, loss 3.70892, smoothed loss 3.21640, grad norm 3.50922, param norm 153.43504
INFO:root:epoch 7, iter 6115, loss 3.50951, smoothed loss 3.22898, grad norm 3.11992, param norm 153.47943
INFO:root:epoch 7, iter 6120, loss 2.54413, smoothed loss 3.23901, grad norm 2.55326, param norm 153.52472
INFO:root:epoch 7, iter 6125, loss 3.17173, smoothed loss 3.22768, grad norm 2.80031, param norm 153.57152
INFO:root:epoch 7, iter 6130, loss 3.23508, smoothed loss 3.24474, grad norm 3.33490, param norm 153.61490
INFO:root:epoch 7, iter 6135, loss 3.70537, smoothed loss 3.25288, grad norm 3.17599, param norm 153.65662
INFO:root:epoch 7, iter 6140, loss 3.66672, smoothed loss 3.25172, grad norm 3.00007, param norm 153.69345
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 6145, loss 3.97912, smoothed loss 3.25140, grad norm 3.68227, param norm 153.72813
INFO:root:epoch 7, iter 6150, loss 2.75695, smoothed loss 3.24730, grad norm 2.94146, param norm 153.77054
INFO:root:epoch 7, iter 6155, loss 3.29738, smoothed loss 3.23711, grad norm 2.92490, param norm 153.81937
INFO:root:epoch 7, iter 6160, loss 2.78272, smoothed loss 3.22901, grad norm 2.79388, param norm 153.86098
INFO:root:epoch 7, iter 6165, loss 2.56785, smoothed loss 3.23255, grad norm 2.82152, param norm 153.90067
INFO:root:epoch 7, iter 6170, loss 2.83153, smoothed loss 3.21703, grad norm 3.07765, param norm 153.93750
INFO:root:epoch 7, iter 6175, loss 3.33133, smoothed loss 3.22274, grad norm 2.90700, param norm 153.97815
INFO:root:epoch 7, iter 6180, loss 3.04879, smoothed loss 3.21891, grad norm 2.89010, param norm 154.02191
INFO:root:epoch 7, iter 6185, loss 3.37437, smoothed loss 3.21347, grad norm 2.88420, param norm 154.06723
INFO:root:epoch 7, iter 6190, loss 3.51296, smoothed loss 3.20792, grad norm 2.98295, param norm 154.11046
INFO:root:epoch 7, iter 6195, loss 3.50422, smoothed loss 3.21567, grad norm 3.00571, param norm 154.15173
INFO:root:epoch 7, iter 6200, loss 2.63186, smoothed loss 3.20502, grad norm 2.64998, param norm 154.19724
INFO:root:epoch 7, iter 6205, loss 3.24044, smoothed loss 3.20570, grad norm 2.80005, param norm 154.24277
INFO:root:epoch 7, iter 6210, loss 2.94814, smoothed loss 3.19505, grad norm 3.01287, param norm 154.28809
INFO:root:epoch 7, iter 6215, loss 2.72391, smoothed loss 3.18423, grad norm 3.23202, param norm 154.32953
INFO:root:epoch 7, iter 6220, loss 3.12075, smoothed loss 3.16967, grad norm 3.43872, param norm 154.37395
INFO:root:epoch 7, iter 6225, loss 2.79793, smoothed loss 3.16666, grad norm 3.02759, param norm 154.41528
INFO:root:epoch 7, iter 6230, loss 2.94336, smoothed loss 3.14475, grad norm 3.09497, param norm 154.45770
INFO:root:epoch 7, iter 6235, loss 2.61025, smoothed loss 3.14884, grad norm 2.82993, param norm 154.49869
INFO:root:epoch 7, iter 6240, loss 3.17053, smoothed loss 3.14536, grad norm 3.01775, param norm 154.53735
INFO:root:epoch 7, iter 6245, loss 2.84018, smoothed loss 3.15089, grad norm 2.65909, param norm 154.57622
INFO:root:epoch 7, iter 6250, loss 3.74393, smoothed loss 3.15059, grad norm 3.18652, param norm 154.61882
INFO:root:epoch 7, iter 6255, loss 3.83311, smoothed loss 3.16115, grad norm 3.12942, param norm 154.66219
INFO:root:epoch 7, iter 6260, loss 3.36559, smoothed loss 3.16134, grad norm 2.85591, param norm 154.70357
INFO:root:epoch 7, iter 6265, loss 3.58898, smoothed loss 3.17586, grad norm 3.19059, param norm 154.74791
INFO:root:epoch 7, iter 6270, loss 3.22814, smoothed loss 3.18527, grad norm 2.82276, param norm 154.79210
INFO:root:epoch 7, iter 6275, loss 3.32050, smoothed loss 3.17372, grad norm 3.40086, param norm 154.83620
INFO:root:epoch 7, iter 6280, loss 3.82711, smoothed loss 3.16978, grad norm 3.00348, param norm 154.87912
INFO:root:epoch 7, iter 6285, loss 2.92138, smoothed loss 3.15218, grad norm 3.56985, param norm 154.91675
INFO:root:epoch 7, iter 6290, loss 2.83497, smoothed loss 3.14869, grad norm 2.98704, param norm 154.95197
INFO:root:epoch 7, iter 6295, loss 3.12356, smoothed loss 3.14263, grad norm 2.98647, param norm 154.99413
INFO:root:epoch 7, iter 6300, loss 3.11048, smoothed loss 3.15710, grad norm 2.89862, param norm 155.03487
Adding batches start...
Added  160  batches
INFO:root:epoch 7, iter 6305, loss 3.37718, smoothed loss 3.16248, grad norm 2.84288, param norm 155.07957
INFO:root:epoch 7, iter 6310, loss 3.18226, smoothed loss 3.17628, grad norm 2.82958, param norm 155.12436
INFO:root:epoch 7, iter 6315, loss 3.04100, smoothed loss 3.17981, grad norm 2.83453, param norm 155.16463
INFO:root:epoch 7, iter 6320, loss 2.90088, smoothed loss 3.19147, grad norm 2.83089, param norm 155.20038
INFO:root:epoch 7, iter 6325, loss 3.06078, smoothed loss 3.19609, grad norm 2.65678, param norm 155.23434
INFO:root:epoch 7, iter 6330, loss 3.00208, smoothed loss 3.18984, grad norm 3.03926, param norm 155.26807
INFO:root:epoch 7, iter 6335, loss 3.07246, smoothed loss 3.18944, grad norm 3.08607, param norm 155.30908
INFO:root:epoch 7, iter 6340, loss 3.65828, smoothed loss 3.19831, grad norm 3.60050, param norm 155.35271
INFO:root:epoch 7, iter 6345, loss 3.12726, smoothed loss 3.18602, grad norm 3.17351, param norm 155.39771
INFO:root:epoch 7, iter 6350, loss 2.88391, smoothed loss 3.18788, grad norm 3.21371, param norm 155.44196
INFO:root:epoch 7, iter 6355, loss 3.16576, smoothed loss 3.18378, grad norm 3.12686, param norm 155.48071
INFO:root:epoch 7, iter 6360, loss 3.00832, smoothed loss 3.18382, grad norm 2.53053, param norm 155.51758
INFO:root:epoch 7, iter 6365, loss 3.65661, smoothed loss 3.17889, grad norm 3.08152, param norm 155.55336
INFO:root:epoch 7, iter 6370, loss 2.52407, smoothed loss 3.17824, grad norm 3.22196, param norm 155.58900
INFO:root:epoch 7, iter 6375, loss 2.78239, smoothed loss 3.17632, grad norm 3.03611, param norm 155.63147
INFO:root:epoch 7, iter 6380, loss 2.78008, smoothed loss 3.16455, grad norm 2.45304, param norm 155.68031
INFO:root:epoch 7, iter 6385, loss 3.21687, smoothed loss 3.16530, grad norm 3.15284, param norm 155.72394
INFO:root:epoch 7, iter 6390, loss 3.69034, smoothed loss 3.16987, grad norm 3.70595, param norm 155.75865
INFO:root:epoch 7, iter 6395, loss 3.04781, smoothed loss 3.16408, grad norm 2.62195, param norm 155.79282
INFO:root:epoch 7, iter 6400, loss 3.03414, smoothed loss 3.17551, grad norm 3.09888, param norm 155.82411
INFO:root:epoch 7, iter 6405, loss 3.56069, smoothed loss 3.17396, grad norm 3.20987, param norm 155.86017
INFO:root:epoch 7, iter 6410, loss 3.20907, smoothed loss 3.16254, grad norm 3.11120, param norm 155.89932
INFO:root:epoch 7, iter 6415, loss 3.01253, smoothed loss 3.16803, grad norm 2.91211, param norm 155.94479
INFO:root:epoch 7, iter 6420, loss 3.60357, smoothed loss 3.17510, grad norm 3.08328, param norm 155.98978
INFO:root:epoch 7, iter 6425, loss 3.73895, smoothed loss 3.19300, grad norm 3.29131, param norm 156.02829
INFO:root:epoch 7, iter 6430, loss 3.86927, smoothed loss 3.18979, grad norm 3.12453, param norm 156.06929
INFO:root:epoch 7, iter 6435, loss 2.52636, smoothed loss 3.16453, grad norm 3.04719, param norm 156.11272
INFO:root:epoch 7, iter 6440, loss 2.79806, smoothed loss 3.15914, grad norm 2.83865, param norm 156.15440
INFO:root:epoch 7, iter 6445, loss 3.39000, smoothed loss 3.16797, grad norm 3.32829, param norm 156.19846
INFO:root:epoch 7, iter 6450, loss 2.31414, smoothed loss 3.14927, grad norm 2.59822, param norm 156.24139
INFO:root:epoch 7, iter 6455, loss 3.38371, smoothed loss 3.15623, grad norm 3.38014, param norm 156.28078
INFO:root:epoch 7, iter 6460, loss 2.57172, smoothed loss 3.15938, grad norm 2.82953, param norm 156.31850
Adding batches start...
Added  144  batches
INFO:root:epoch 7, iter 6465, loss 2.99640, smoothed loss 3.16187, grad norm 3.59690, param norm 156.35614
INFO:root:epoch 7, iter 6470, loss 3.38303, smoothed loss 3.16205, grad norm 3.02882, param norm 156.39842
INFO:root:epoch 7, iter 6475, loss 2.98002, smoothed loss 3.17173, grad norm 2.76363, param norm 156.43912
INFO:root:epoch 7, iter 6480, loss 3.48904, smoothed loss 3.16661, grad norm 3.28098, param norm 156.48305
INFO:root:epoch 7, iter 6485, loss 3.27629, smoothed loss 3.17122, grad norm 3.36891, param norm 156.52460
INFO:root:epoch 7, iter 6490, loss 3.10239, smoothed loss 3.16751, grad norm 3.32838, param norm 156.56108
INFO:root:epoch 7, iter 6495, loss 3.42949, smoothed loss 3.16005, grad norm 3.09564, param norm 156.60121
INFO:root:epoch 7, iter 6500, loss 2.58704, smoothed loss 3.14876, grad norm 2.63915, param norm 156.64433
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 7, Iter 6500, dev loss: 3.188767
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.33801 seconds [Score: 0.69849]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.05798 seconds [Score: 0.54400]
INFO:root:Epoch 7, Iter 6500, Train F1 score: 0.698494, Train EM score: 0.544000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 115.98414 seconds [Score: 0.62181]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.43727 seconds [Score: 0.47780]
INFO:root:Epoch 7, Iter 6500, Dev F1 score: 0.621810, Dev EM score: 0.477803
INFO:root:End of epoch 7
INFO:root:epoch 7, iter 6505, loss 3.31652, smoothed loss 3.13927, grad norm 3.11249, param norm 156.68877
INFO:root:epoch 7, iter 6510, loss 3.35769, smoothed loss 3.14854, grad norm 3.44324, param norm 156.73332
INFO:root:epoch 7, iter 6515, loss 3.01733, smoothed loss 3.14229, grad norm 3.32580, param norm 156.77295
INFO:root:epoch 7, iter 6520, loss 2.86128, smoothed loss 3.14580, grad norm 2.75879, param norm 156.80933
INFO:root:epoch 7, iter 6525, loss 3.38567, smoothed loss 3.15122, grad norm 3.20042, param norm 156.84668
INFO:root:epoch 7, iter 6530, loss 2.91786, smoothed loss 3.13607, grad norm 3.08315, param norm 156.88437
INFO:root:epoch 7, iter 6535, loss 3.17031, smoothed loss 3.14070, grad norm 3.16416, param norm 156.92409
INFO:root:epoch 7, iter 6540, loss 2.71096, smoothed loss 3.13517, grad norm 2.83477, param norm 156.96815
INFO:root:epoch 7, iter 6545, loss 2.74687, smoothed loss 3.14186, grad norm 3.01270, param norm 157.01601
INFO:root:epoch 7, iter 6550, loss 3.28536, smoothed loss 3.13657, grad norm 3.06407, param norm 157.06540
INFO:root:epoch 7, iter 6555, loss 2.63471, smoothed loss 3.12794, grad norm 3.20670, param norm 157.11168
INFO:root:epoch 7, iter 6560, loss 3.32381, smoothed loss 3.13283, grad norm 2.89594, param norm 157.15390
INFO:root:epoch 7, iter 6565, loss 3.80686, smoothed loss 3.14016, grad norm 3.31210, param norm 157.19315
INFO:root:epoch 7, iter 6570, loss 3.11815, smoothed loss 3.15411, grad norm 2.99935, param norm 157.22893
INFO:root:epoch 7, iter 6575, loss 3.04826, smoothed loss 3.16102, grad norm 2.79313, param norm 157.26900
INFO:root:epoch 7, iter 6580, loss 3.42276, smoothed loss 3.16456, grad norm 2.98614, param norm 157.30930
INFO:root:epoch 7, iter 6585, loss 2.59657, smoothed loss 3.15990, grad norm 2.40197, param norm 157.34932
INFO:root:epoch 7, iter 6590, loss 2.81337, smoothed loss 3.16126, grad norm 2.43450, param norm 157.38773
INFO:root:epoch 7, iter 6595, loss 2.96801, smoothed loss 3.15232, grad norm 2.70306, param norm 157.42662
INFO:root:epoch 7, iter 6600, loss 3.02145, smoothed loss 3.15547, grad norm 3.12144, param norm 157.46927
INFO:root:epoch 7, iter 6605, loss 2.55777, smoothed loss 3.15113, grad norm 2.89909, param norm 157.50699
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6610, loss 3.17327, smoothed loss 3.14610, grad norm 3.28082, param norm 157.55075
INFO:root:epoch 8, iter 6615, loss 3.52523, smoothed loss 3.14619, grad norm 3.33841, param norm 157.59644
INFO:root:epoch 8, iter 6620, loss 3.17764, smoothed loss 3.14044, grad norm 3.31278, param norm 157.63983
INFO:root:epoch 8, iter 6625, loss 3.13154, smoothed loss 3.13326, grad norm 3.14250, param norm 157.67757
INFO:root:epoch 8, iter 6630, loss 2.77520, smoothed loss 3.14408, grad norm 2.76140, param norm 157.71652
INFO:root:epoch 8, iter 6635, loss 2.82896, smoothed loss 3.12888, grad norm 2.95100, param norm 157.75676
INFO:root:epoch 8, iter 6640, loss 2.99504, smoothed loss 3.11985, grad norm 3.01493, param norm 157.79732
INFO:root:epoch 8, iter 6645, loss 3.38890, smoothed loss 3.12942, grad norm 3.30622, param norm 157.83510
INFO:root:epoch 8, iter 6650, loss 2.81583, smoothed loss 3.12943, grad norm 2.89125, param norm 157.87454
INFO:root:epoch 8, iter 6655, loss 2.77494, smoothed loss 3.13089, grad norm 2.94878, param norm 157.91321
INFO:root:epoch 8, iter 6660, loss 3.37482, smoothed loss 3.13910, grad norm 3.17694, param norm 157.95273
INFO:root:epoch 8, iter 6665, loss 3.22895, smoothed loss 3.14783, grad norm 3.12305, param norm 157.99013
INFO:root:epoch 8, iter 6670, loss 3.56271, smoothed loss 3.14976, grad norm 3.07263, param norm 158.03680
INFO:root:epoch 8, iter 6675, loss 2.97691, smoothed loss 3.15302, grad norm 2.86424, param norm 158.08131
INFO:root:epoch 8, iter 6680, loss 3.62803, smoothed loss 3.16265, grad norm 3.41737, param norm 158.12192
INFO:root:epoch 8, iter 6685, loss 3.19319, smoothed loss 3.16255, grad norm 2.61341, param norm 158.16284
INFO:root:epoch 8, iter 6690, loss 3.30049, smoothed loss 3.15702, grad norm 3.11206, param norm 158.20699
INFO:root:epoch 8, iter 6695, loss 2.85350, smoothed loss 3.14273, grad norm 2.70990, param norm 158.25092
INFO:root:epoch 8, iter 6700, loss 3.18275, smoothed loss 3.14357, grad norm 2.86002, param norm 158.29094
INFO:root:epoch 8, iter 6705, loss 3.45359, smoothed loss 3.14429, grad norm 3.57393, param norm 158.32930
INFO:root:epoch 8, iter 6710, loss 3.93191, smoothed loss 3.15053, grad norm 3.31529, param norm 158.36775
INFO:root:epoch 8, iter 6715, loss 3.59075, smoothed loss 3.15430, grad norm 3.06114, param norm 158.40190
INFO:root:epoch 8, iter 6720, loss 2.97977, smoothed loss 3.15184, grad norm 3.02997, param norm 158.44504
INFO:root:epoch 8, iter 6725, loss 3.11125, smoothed loss 3.15047, grad norm 2.67883, param norm 158.49290
INFO:root:epoch 8, iter 6730, loss 2.35429, smoothed loss 3.12904, grad norm 2.70622, param norm 158.53630
INFO:root:epoch 8, iter 6735, loss 3.46777, smoothed loss 3.14879, grad norm 2.84452, param norm 158.57274
INFO:root:epoch 8, iter 6740, loss 2.98070, smoothed loss 3.13951, grad norm 2.96394, param norm 158.60728
INFO:root:epoch 8, iter 6745, loss 3.66911, smoothed loss 3.14663, grad norm 2.96122, param norm 158.64677
INFO:root:epoch 8, iter 6750, loss 3.79889, smoothed loss 3.16217, grad norm 3.20206, param norm 158.67993
INFO:root:epoch 8, iter 6755, loss 3.04396, smoothed loss 3.15701, grad norm 2.65658, param norm 158.70734
INFO:root:epoch 8, iter 6760, loss 2.54632, smoothed loss 3.16022, grad norm 2.94451, param norm 158.74063
INFO:root:epoch 8, iter 6765, loss 3.68059, smoothed loss 3.15222, grad norm 3.13108, param norm 158.77930
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6770, loss 3.27627, smoothed loss 3.15614, grad norm 3.10553, param norm 158.81602
INFO:root:epoch 8, iter 6775, loss 3.86719, smoothed loss 3.16601, grad norm 3.30860, param norm 158.85182
INFO:root:epoch 8, iter 6780, loss 3.26219, smoothed loss 3.17125, grad norm 3.00011, param norm 158.88535
INFO:root:epoch 8, iter 6785, loss 2.86024, smoothed loss 3.17595, grad norm 2.66151, param norm 158.92191
INFO:root:epoch 8, iter 6790, loss 3.16083, smoothed loss 3.17489, grad norm 3.17535, param norm 158.96649
INFO:root:epoch 8, iter 6795, loss 3.16317, smoothed loss 3.16571, grad norm 3.03455, param norm 159.01370
INFO:root:epoch 8, iter 6800, loss 2.68865, smoothed loss 3.16211, grad norm 2.80973, param norm 159.05664
INFO:root:epoch 8, iter 6805, loss 3.33023, smoothed loss 3.15019, grad norm 3.53129, param norm 159.09781
INFO:root:epoch 8, iter 6810, loss 2.13707, smoothed loss 3.13217, grad norm 2.62972, param norm 159.13239
INFO:root:epoch 8, iter 6815, loss 2.69736, smoothed loss 3.10937, grad norm 2.59244, param norm 159.16898
INFO:root:epoch 8, iter 6820, loss 3.45952, smoothed loss 3.10591, grad norm 3.41003, param norm 159.20457
INFO:root:epoch 8, iter 6825, loss 2.60982, smoothed loss 3.11478, grad norm 2.73407, param norm 159.23802
INFO:root:epoch 8, iter 6830, loss 3.48593, smoothed loss 3.11779, grad norm 3.16770, param norm 159.27615
INFO:root:epoch 8, iter 6835, loss 3.45683, smoothed loss 3.11218, grad norm 2.99876, param norm 159.31271
INFO:root:epoch 8, iter 6840, loss 3.35759, smoothed loss 3.11173, grad norm 3.39599, param norm 159.35042
INFO:root:epoch 8, iter 6845, loss 3.08058, smoothed loss 3.10510, grad norm 3.18702, param norm 159.39452
INFO:root:epoch 8, iter 6850, loss 2.82347, smoothed loss 3.09718, grad norm 2.88078, param norm 159.43250
INFO:root:epoch 8, iter 6855, loss 3.81692, smoothed loss 3.12068, grad norm 3.36472, param norm 159.47244
INFO:root:epoch 8, iter 6860, loss 3.34948, smoothed loss 3.12868, grad norm 2.88719, param norm 159.51219
INFO:root:epoch 8, iter 6865, loss 3.19982, smoothed loss 3.12528, grad norm 3.14449, param norm 159.55786
INFO:root:epoch 8, iter 6870, loss 3.57530, smoothed loss 3.12797, grad norm 3.33820, param norm 159.59984
INFO:root:epoch 8, iter 6875, loss 2.28472, smoothed loss 3.11717, grad norm 2.79745, param norm 159.63680
INFO:root:epoch 8, iter 6880, loss 2.75994, smoothed loss 3.11640, grad norm 2.72402, param norm 159.67200
INFO:root:epoch 8, iter 6885, loss 3.36752, smoothed loss 3.10709, grad norm 3.14647, param norm 159.70866
INFO:root:epoch 8, iter 6890, loss 4.06760, smoothed loss 3.13132, grad norm 3.24072, param norm 159.74539
INFO:root:epoch 8, iter 6895, loss 3.37264, smoothed loss 3.14362, grad norm 3.08995, param norm 159.77524
INFO:root:epoch 8, iter 6900, loss 2.50618, smoothed loss 3.13293, grad norm 2.69750, param norm 159.80984
INFO:root:epoch 8, iter 6905, loss 2.96361, smoothed loss 3.14168, grad norm 3.25410, param norm 159.85242
INFO:root:epoch 8, iter 6910, loss 3.06322, smoothed loss 3.13025, grad norm 2.95361, param norm 159.89992
INFO:root:epoch 8, iter 6915, loss 3.52648, smoothed loss 3.13026, grad norm 3.22924, param norm 159.95010
INFO:root:epoch 8, iter 6920, loss 3.13694, smoothed loss 3.12885, grad norm 3.22583, param norm 159.99651
INFO:root:epoch 8, iter 6925, loss 3.18362, smoothed loss 3.13016, grad norm 3.08713, param norm 160.03789
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 6930, loss 3.10596, smoothed loss 3.14496, grad norm 3.04113, param norm 160.07590
INFO:root:epoch 8, iter 6935, loss 2.88929, smoothed loss 3.14217, grad norm 2.96585, param norm 160.11223
INFO:root:epoch 8, iter 6940, loss 3.76997, smoothed loss 3.16696, grad norm 2.84248, param norm 160.14613
INFO:root:epoch 8, iter 6945, loss 2.63091, smoothed loss 3.14346, grad norm 2.41210, param norm 160.17775
INFO:root:epoch 8, iter 6950, loss 2.95222, smoothed loss 3.13867, grad norm 3.09831, param norm 160.21407
INFO:root:epoch 8, iter 6955, loss 2.97873, smoothed loss 3.13818, grad norm 3.11364, param norm 160.25253
INFO:root:epoch 8, iter 6960, loss 3.02166, smoothed loss 3.14003, grad norm 2.91244, param norm 160.29086
INFO:root:epoch 8, iter 6965, loss 2.45854, smoothed loss 3.13025, grad norm 2.73228, param norm 160.33057
INFO:root:epoch 8, iter 6970, loss 3.05566, smoothed loss 3.12251, grad norm 3.17201, param norm 160.36716
INFO:root:epoch 8, iter 6975, loss 3.28129, smoothed loss 3.12042, grad norm 3.09871, param norm 160.40512
INFO:root:epoch 8, iter 6980, loss 3.17575, smoothed loss 3.11649, grad norm 2.80585, param norm 160.44470
INFO:root:epoch 8, iter 6985, loss 2.76456, smoothed loss 3.11642, grad norm 3.06586, param norm 160.48372
INFO:root:epoch 8, iter 6990, loss 3.55344, smoothed loss 3.11126, grad norm 3.01331, param norm 160.52153
INFO:root:epoch 8, iter 6995, loss 2.89670, smoothed loss 3.11410, grad norm 3.29566, param norm 160.56001
INFO:root:epoch 8, iter 7000, loss 3.11912, smoothed loss 3.11443, grad norm 3.17904, param norm 160.60405
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 8, Iter 7000, dev loss: 3.143654
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.73019 seconds [Score: 0.72091]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.34137 seconds [Score: 0.56100]
INFO:root:Epoch 8, Iter 7000, Train F1 score: 0.720908, Train EM score: 0.561000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 116.37539 seconds [Score: 0.62557]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.73902 seconds [Score: 0.48300]
INFO:root:Epoch 8, Iter 7000, Dev F1 score: 0.625571, Dev EM score: 0.483001
INFO:root:End of epoch 8
INFO:root:epoch 8, iter 7005, loss 3.32746, smoothed loss 3.10149, grad norm 3.24701, param norm 160.64760
INFO:root:epoch 8, iter 7010, loss 2.79355, smoothed loss 3.09141, grad norm 3.03476, param norm 160.69478
INFO:root:epoch 8, iter 7015, loss 2.83655, smoothed loss 3.08897, grad norm 2.84519, param norm 160.74104
INFO:root:epoch 8, iter 7020, loss 2.09635, smoothed loss 3.08452, grad norm 2.43615, param norm 160.78355
INFO:root:epoch 8, iter 7025, loss 3.16825, smoothed loss 3.09067, grad norm 3.29079, param norm 160.82219
INFO:root:epoch 8, iter 7030, loss 2.95053, smoothed loss 3.08103, grad norm 2.96101, param norm 160.85965
INFO:root:epoch 8, iter 7035, loss 2.83664, smoothed loss 3.08325, grad norm 3.05088, param norm 160.89935
INFO:root:epoch 8, iter 7040, loss 2.56713, smoothed loss 3.08124, grad norm 2.86275, param norm 160.93536
INFO:root:epoch 8, iter 7045, loss 3.66675, smoothed loss 3.09138, grad norm 3.00759, param norm 160.97247
INFO:root:epoch 8, iter 7050, loss 3.13129, smoothed loss 3.08444, grad norm 2.79320, param norm 161.01003
INFO:root:epoch 8, iter 7055, loss 2.70364, smoothed loss 3.08566, grad norm 2.74591, param norm 161.04550
INFO:root:epoch 8, iter 7060, loss 3.02680, smoothed loss 3.07797, grad norm 3.16059, param norm 161.08417
INFO:root:epoch 8, iter 7065, loss 2.44211, smoothed loss 3.07288, grad norm 2.60337, param norm 161.12537
INFO:root:epoch 8, iter 7070, loss 2.82979, smoothed loss 3.06874, grad norm 3.06800, param norm 161.16612
INFO:root:epoch 8, iter 7075, loss 2.81301, smoothed loss 3.06915, grad norm 3.01857, param norm 161.20340
INFO:root:epoch 8, iter 7080, loss 3.22776, smoothed loss 3.08757, grad norm 3.99127, param norm 161.23793
INFO:root:epoch 8, iter 7085, loss 2.95047, smoothed loss 3.09570, grad norm 2.66758, param norm 161.27505
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 7090, loss 2.92521, smoothed loss 3.08815, grad norm 2.73078, param norm 161.31482
INFO:root:epoch 8, iter 7095, loss 2.73084, smoothed loss 3.08151, grad norm 2.66243, param norm 161.35916
INFO:root:epoch 8, iter 7100, loss 3.34751, smoothed loss 3.08422, grad norm 2.87991, param norm 161.40294
INFO:root:epoch 8, iter 7105, loss 2.92848, smoothed loss 3.07457, grad norm 2.99989, param norm 161.44275
INFO:root:epoch 8, iter 7110, loss 2.64017, smoothed loss 3.07950, grad norm 2.99176, param norm 161.48297
INFO:root:epoch 8, iter 7115, loss 2.62796, smoothed loss 3.07265, grad norm 2.98917, param norm 161.52281
INFO:root:epoch 8, iter 7120, loss 2.47753, smoothed loss 3.05840, grad norm 3.20215, param norm 161.56108
INFO:root:epoch 8, iter 7125, loss 3.67154, smoothed loss 3.05520, grad norm 3.44392, param norm 161.59708
INFO:root:epoch 8, iter 7130, loss 2.69257, smoothed loss 3.05298, grad norm 2.93158, param norm 161.63802
INFO:root:epoch 8, iter 7135, loss 4.27540, smoothed loss 3.06856, grad norm 3.36018, param norm 161.67786
INFO:root:epoch 8, iter 7140, loss 3.39272, smoothed loss 3.05480, grad norm 2.90675, param norm 161.71852
INFO:root:epoch 8, iter 7145, loss 2.90127, smoothed loss 3.06394, grad norm 2.96353, param norm 161.75377
INFO:root:epoch 8, iter 7150, loss 2.97107, smoothed loss 3.06249, grad norm 3.62996, param norm 161.78682
INFO:root:epoch 8, iter 7155, loss 2.69635, smoothed loss 3.05381, grad norm 3.02105, param norm 161.82410
INFO:root:epoch 8, iter 7160, loss 2.70397, smoothed loss 3.04319, grad norm 3.07550, param norm 161.86340
INFO:root:epoch 8, iter 7165, loss 3.05311, smoothed loss 3.03920, grad norm 2.87211, param norm 161.90387
INFO:root:epoch 8, iter 7170, loss 2.83556, smoothed loss 3.03104, grad norm 3.09907, param norm 161.94633
INFO:root:epoch 8, iter 7175, loss 3.82479, smoothed loss 3.04115, grad norm 3.46114, param norm 161.98360
INFO:root:epoch 8, iter 7180, loss 3.78777, smoothed loss 3.04682, grad norm 3.56704, param norm 162.02196
INFO:root:epoch 8, iter 7185, loss 2.83596, smoothed loss 3.04701, grad norm 2.71885, param norm 162.06139
INFO:root:epoch 8, iter 7190, loss 3.00544, smoothed loss 3.04472, grad norm 3.00090, param norm 162.10207
INFO:root:epoch 8, iter 7195, loss 2.71837, smoothed loss 3.03158, grad norm 2.97649, param norm 162.14697
INFO:root:epoch 8, iter 7200, loss 2.37445, smoothed loss 3.02628, grad norm 2.97685, param norm 162.18779
INFO:root:epoch 8, iter 7205, loss 2.65534, smoothed loss 3.03093, grad norm 2.66249, param norm 162.22890
INFO:root:epoch 8, iter 7210, loss 2.64413, smoothed loss 3.03196, grad norm 2.98363, param norm 162.26978
INFO:root:epoch 8, iter 7215, loss 3.81632, smoothed loss 3.04139, grad norm 3.81305, param norm 162.30714
INFO:root:epoch 8, iter 7220, loss 2.72076, smoothed loss 3.03404, grad norm 2.61093, param norm 162.34552
INFO:root:epoch 8, iter 7225, loss 2.47416, smoothed loss 3.03822, grad norm 2.85626, param norm 162.38354
INFO:root:epoch 8, iter 7230, loss 3.34411, smoothed loss 3.02857, grad norm 3.05662, param norm 162.42627
INFO:root:epoch 8, iter 7235, loss 3.07516, smoothed loss 3.02866, grad norm 2.87502, param norm 162.46895
INFO:root:epoch 8, iter 7240, loss 3.28779, smoothed loss 3.03660, grad norm 3.03738, param norm 162.50592
INFO:root:epoch 8, iter 7245, loss 3.36846, smoothed loss 3.03997, grad norm 3.03379, param norm 162.54379
Adding batches start...
Added  160  batches
INFO:root:epoch 8, iter 7250, loss 2.98485, smoothed loss 3.05508, grad norm 2.87625, param norm 162.57709
INFO:root:epoch 8, iter 7255, loss 3.06837, smoothed loss 3.05906, grad norm 3.44638, param norm 162.61264
INFO:root:epoch 8, iter 7260, loss 4.15369, smoothed loss 3.07021, grad norm 3.36193, param norm 162.65224
INFO:root:epoch 8, iter 7265, loss 3.09602, smoothed loss 3.07172, grad norm 2.81540, param norm 162.69304
INFO:root:epoch 8, iter 7270, loss 3.38752, smoothed loss 3.07232, grad norm 2.77770, param norm 162.73027
INFO:root:epoch 8, iter 7275, loss 2.78366, smoothed loss 3.07492, grad norm 3.10708, param norm 162.76083
INFO:root:epoch 8, iter 7280, loss 3.30261, smoothed loss 3.07247, grad norm 3.05361, param norm 162.79202
INFO:root:epoch 8, iter 7285, loss 2.82535, smoothed loss 3.08170, grad norm 2.70832, param norm 162.82608
INFO:root:epoch 8, iter 7290, loss 2.77811, smoothed loss 3.07878, grad norm 2.78970, param norm 162.86223
INFO:root:epoch 8, iter 7295, loss 3.09834, smoothed loss 3.06817, grad norm 2.98073, param norm 162.90146
INFO:root:epoch 8, iter 7300, loss 2.73443, smoothed loss 3.05729, grad norm 2.49668, param norm 162.94624
INFO:root:epoch 8, iter 7305, loss 3.12658, smoothed loss 3.06427, grad norm 3.01017, param norm 162.98834
INFO:root:epoch 8, iter 7310, loss 2.64857, smoothed loss 3.05848, grad norm 2.67588, param norm 163.02866
INFO:root:epoch 8, iter 7315, loss 3.29593, smoothed loss 3.05702, grad norm 3.08586, param norm 163.06570
INFO:root:epoch 8, iter 7320, loss 3.12317, smoothed loss 3.05009, grad norm 3.30023, param norm 163.10069
INFO:root:epoch 8, iter 7325, loss 3.08765, smoothed loss 3.05551, grad norm 2.94563, param norm 163.14107
INFO:root:epoch 8, iter 7330, loss 2.83850, smoothed loss 3.05293, grad norm 2.88086, param norm 163.18292
INFO:root:epoch 8, iter 7335, loss 3.47365, smoothed loss 3.06230, grad norm 3.39760, param norm 163.22437
INFO:root:epoch 8, iter 7340, loss 3.27181, smoothed loss 3.07783, grad norm 3.22951, param norm 163.26309
INFO:root:epoch 8, iter 7345, loss 3.02060, smoothed loss 3.07440, grad norm 2.65283, param norm 163.30191
INFO:root:epoch 8, iter 7350, loss 2.94222, smoothed loss 3.07247, grad norm 2.80605, param norm 163.34393
INFO:root:epoch 8, iter 7355, loss 2.42553, smoothed loss 3.07274, grad norm 2.62608, param norm 163.38715
INFO:root:epoch 8, iter 7360, loss 2.92549, smoothed loss 3.07680, grad norm 2.86713, param norm 163.42918
INFO:root:epoch 8, iter 7365, loss 3.13328, smoothed loss 3.07685, grad norm 3.17648, param norm 163.47115
INFO:root:epoch 8, iter 7370, loss 2.12971, smoothed loss 3.06576, grad norm 2.68632, param norm 163.51460
INFO:root:epoch 8, iter 7375, loss 3.34341, smoothed loss 3.05140, grad norm 3.26793, param norm 163.56093
INFO:root:epoch 8, iter 7380, loss 2.75515, smoothed loss 3.04364, grad norm 2.99066, param norm 163.60585
INFO:root:epoch 8, iter 7385, loss 2.82769, smoothed loss 3.04378, grad norm 2.74512, param norm 163.64709
INFO:root:epoch 8, iter 7390, loss 3.34436, smoothed loss 3.04570, grad norm 3.03745, param norm 163.68425
INFO:root:epoch 8, iter 7395, loss 3.61645, smoothed loss 3.04313, grad norm 3.32501, param norm 163.72018
INFO:root:epoch 8, iter 7400, loss 3.15224, smoothed loss 3.05850, grad norm 3.21268, param norm 163.75774
INFO:root:epoch 8, iter 7405, loss 2.81445, smoothed loss 3.05717, grad norm 2.90976, param norm 163.79828
Adding batches start...
Added  144  batches
INFO:root:epoch 8, iter 7410, loss 3.64195, smoothed loss 3.06824, grad norm 2.85367, param norm 163.83275
INFO:root:epoch 8, iter 7415, loss 2.41401, smoothed loss 3.05958, grad norm 2.88350, param norm 163.86749
INFO:root:epoch 8, iter 7420, loss 3.29680, smoothed loss 3.05794, grad norm 3.32369, param norm 163.90607
INFO:root:epoch 8, iter 7425, loss 2.64354, smoothed loss 3.06143, grad norm 2.64244, param norm 163.94272
INFO:root:epoch 8, iter 7430, loss 2.85434, smoothed loss 3.07140, grad norm 2.71737, param norm 163.98022
INFO:root:epoch 8, iter 7435, loss 2.75940, smoothed loss 3.06467, grad norm 2.70398, param norm 164.01921
INFO:root:epoch 8, iter 7440, loss 3.08142, smoothed loss 3.05627, grad norm 2.74732, param norm 164.05836
INFO:root:epoch 8, iter 7445, loss 2.81116, smoothed loss 3.06165, grad norm 2.60836, param norm 164.09982
INFO:root:epoch 8, iter 7450, loss 2.43473, smoothed loss 3.05718, grad norm 2.59091, param norm 164.13771
INFO:root:epoch 8, iter 7455, loss 3.29433, smoothed loss 3.04772, grad norm 3.53878, param norm 164.17459
INFO:root:epoch 8, iter 7460, loss 2.92203, smoothed loss 3.04954, grad norm 2.93369, param norm 164.21445
INFO:root:epoch 8, iter 7465, loss 3.54518, smoothed loss 3.04869, grad norm 3.40659, param norm 164.25461
INFO:root:epoch 8, iter 7470, loss 3.05416, smoothed loss 3.03680, grad norm 3.09960, param norm 164.29395
INFO:root:epoch 8, iter 7475, loss 2.95240, smoothed loss 3.03875, grad norm 3.26079, param norm 164.33626
INFO:root:epoch 8, iter 7480, loss 3.27426, smoothed loss 3.03923, grad norm 3.16264, param norm 164.38042
INFO:root:epoch 8, iter 7485, loss 2.81290, smoothed loss 3.04568, grad norm 2.68221, param norm 164.42241
INFO:root:epoch 8, iter 7490, loss 2.75612, smoothed loss 3.04368, grad norm 2.80489, param norm 164.45760
INFO:root:epoch 8, iter 7495, loss 3.19504, smoothed loss 3.05872, grad norm 3.03106, param norm 164.49077
INFO:root:epoch 8, iter 7500, loss 3.32473, smoothed loss 3.05916, grad norm 3.03268, param norm 164.52448
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 8, Iter 7500, dev loss: 3.107548
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 16.98836 seconds [Score: 0.71579]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.13940 seconds [Score: 0.57600]
INFO:root:Epoch 8, Iter 7500, Train F1 score: 0.715789, Train EM score: 0.576000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 115.87795 seconds [Score: 0.62852]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 114.97855 seconds [Score: 0.48427]
INFO:root:Epoch 8, Iter 7500, Dev F1 score: 0.628516, Dev EM score: 0.484265
INFO:root:End of epoch 8
INFO:root:epoch 8, iter 7505, loss 3.39620, smoothed loss 3.06808, grad norm 3.05911, param norm 164.55693
INFO:root:epoch 8, iter 7510, loss 2.68319, smoothed loss 3.05517, grad norm 2.75432, param norm 164.59373
INFO:root:epoch 8, iter 7515, loss 3.42435, smoothed loss 3.06773, grad norm 3.51330, param norm 164.63487
INFO:root:epoch 8, iter 7520, loss 3.24151, smoothed loss 3.07322, grad norm 3.13289, param norm 164.67310
INFO:root:epoch 8, iter 7525, loss 3.20114, smoothed loss 3.07231, grad norm 2.89289, param norm 164.70869
INFO:root:epoch 8, iter 7530, loss 2.68378, smoothed loss 3.06846, grad norm 2.92037, param norm 164.74750
INFO:root:epoch 8, iter 7535, loss 2.70403, smoothed loss 3.06009, grad norm 3.12686, param norm 164.79155
INFO:root:epoch 8, iter 7540, loss 3.46275, smoothed loss 3.05973, grad norm 3.02162, param norm 164.83998
INFO:root:epoch 8, iter 7545, loss 3.07617, smoothed loss 3.05559, grad norm 3.31142, param norm 164.88312
INFO:root:epoch 8, iter 7550, loss 3.75765, smoothed loss 3.06468, grad norm 3.57633, param norm 164.92299
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7555, loss 3.46374, smoothed loss 3.06264, grad norm 3.41247, param norm 164.96078
INFO:root:epoch 9, iter 7560, loss 2.73534, smoothed loss 3.05811, grad norm 2.96785, param norm 164.99490
INFO:root:epoch 9, iter 7565, loss 3.54129, smoothed loss 3.05549, grad norm 3.42275, param norm 165.02626
INFO:root:epoch 9, iter 7570, loss 3.46914, smoothed loss 3.04647, grad norm 3.39635, param norm 165.05409
INFO:root:epoch 9, iter 7575, loss 2.86745, smoothed loss 3.05781, grad norm 2.86085, param norm 165.08437
INFO:root:epoch 9, iter 7580, loss 2.85364, smoothed loss 3.05905, grad norm 2.75893, param norm 165.12021
INFO:root:epoch 9, iter 7585, loss 3.05606, smoothed loss 3.06401, grad norm 2.86500, param norm 165.15742
INFO:root:epoch 9, iter 7590, loss 2.70347, smoothed loss 3.04616, grad norm 2.87327, param norm 165.19615
INFO:root:epoch 9, iter 7595, loss 3.43423, smoothed loss 3.05202, grad norm 3.47881, param norm 165.23409
INFO:root:epoch 9, iter 7600, loss 4.03803, smoothed loss 3.06114, grad norm 3.86830, param norm 165.26968
INFO:root:epoch 9, iter 7605, loss 2.99599, smoothed loss 3.06456, grad norm 2.75148, param norm 165.30513
INFO:root:epoch 9, iter 7610, loss 2.96684, smoothed loss 3.06433, grad norm 3.04161, param norm 165.34752
INFO:root:epoch 9, iter 7615, loss 2.75505, smoothed loss 3.06114, grad norm 2.50104, param norm 165.38887
INFO:root:epoch 9, iter 7620, loss 3.20850, smoothed loss 3.05987, grad norm 3.04656, param norm 165.42865
INFO:root:epoch 9, iter 7625, loss 3.14622, smoothed loss 3.05782, grad norm 3.13657, param norm 165.46674
INFO:root:epoch 9, iter 7630, loss 2.50167, smoothed loss 3.05387, grad norm 2.73040, param norm 165.50192
INFO:root:epoch 9, iter 7635, loss 3.45616, smoothed loss 3.06824, grad norm 2.79622, param norm 165.53806
INFO:root:epoch 9, iter 7640, loss 2.40333, smoothed loss 3.06429, grad norm 2.55937, param norm 165.57262
INFO:root:epoch 9, iter 7645, loss 2.66469, smoothed loss 3.05219, grad norm 2.59777, param norm 165.61163
INFO:root:epoch 9, iter 7650, loss 2.65384, smoothed loss 3.04080, grad norm 2.75630, param norm 165.64812
INFO:root:epoch 9, iter 7655, loss 2.78342, smoothed loss 3.04433, grad norm 3.01874, param norm 165.68240
INFO:root:epoch 9, iter 7660, loss 3.28602, smoothed loss 3.04568, grad norm 3.02273, param norm 165.71837
INFO:root:epoch 9, iter 7665, loss 3.00615, smoothed loss 3.04571, grad norm 2.96599, param norm 165.75053
INFO:root:epoch 9, iter 7670, loss 3.15824, smoothed loss 3.03175, grad norm 3.06346, param norm 165.78746
INFO:root:epoch 9, iter 7675, loss 2.57010, smoothed loss 3.04347, grad norm 2.76786, param norm 165.82639
INFO:root:epoch 9, iter 7680, loss 2.76620, smoothed loss 3.03472, grad norm 2.90082, param norm 165.86642
INFO:root:epoch 9, iter 7685, loss 2.84281, smoothed loss 3.03268, grad norm 2.83901, param norm 165.90750
INFO:root:epoch 9, iter 7690, loss 3.05798, smoothed loss 3.03782, grad norm 3.06781, param norm 165.94838
INFO:root:epoch 9, iter 7695, loss 3.45708, smoothed loss 3.03572, grad norm 3.47394, param norm 165.98836
INFO:root:epoch 9, iter 7700, loss 3.09743, smoothed loss 3.04498, grad norm 2.91027, param norm 166.02237
INFO:root:epoch 9, iter 7705, loss 2.59954, smoothed loss 3.04727, grad norm 2.97292, param norm 166.05336
INFO:root:epoch 9, iter 7710, loss 3.44987, smoothed loss 3.04870, grad norm 3.75564, param norm 166.09164
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7715, loss 2.75656, smoothed loss 3.04384, grad norm 2.90058, param norm 166.13422
INFO:root:epoch 9, iter 7720, loss 2.92325, smoothed loss 3.03534, grad norm 2.84077, param norm 166.17648
INFO:root:epoch 9, iter 7725, loss 2.96136, smoothed loss 3.03179, grad norm 3.11506, param norm 166.21262
INFO:root:epoch 9, iter 7730, loss 2.62436, smoothed loss 3.02417, grad norm 2.69508, param norm 166.24644
INFO:root:epoch 9, iter 7735, loss 3.53748, smoothed loss 3.02352, grad norm 3.44563, param norm 166.28429
INFO:root:epoch 9, iter 7740, loss 2.86845, smoothed loss 3.01071, grad norm 2.82986, param norm 166.32828
INFO:root:epoch 9, iter 7745, loss 3.41826, smoothed loss 3.01074, grad norm 3.08738, param norm 166.37419
INFO:root:epoch 9, iter 7750, loss 2.79220, smoothed loss 3.00632, grad norm 2.58673, param norm 166.41518
INFO:root:epoch 9, iter 7755, loss 3.06085, smoothed loss 3.00106, grad norm 2.97931, param norm 166.45195
INFO:root:epoch 9, iter 7760, loss 2.56179, smoothed loss 2.99337, grad norm 2.72492, param norm 166.48729
INFO:root:epoch 9, iter 7765, loss 2.72401, smoothed loss 2.98491, grad norm 2.92573, param norm 166.52255
INFO:root:epoch 9, iter 7770, loss 2.87870, smoothed loss 2.97397, grad norm 3.38774, param norm 166.55890
INFO:root:epoch 9, iter 7775, loss 2.95399, smoothed loss 2.96609, grad norm 3.38357, param norm 166.59581
INFO:root:epoch 9, iter 7780, loss 2.90372, smoothed loss 2.97288, grad norm 2.95317, param norm 166.63541
INFO:root:epoch 9, iter 7785, loss 3.56121, smoothed loss 2.97929, grad norm 3.39300, param norm 166.67345
INFO:root:epoch 9, iter 7790, loss 2.73481, smoothed loss 2.97552, grad norm 2.72360, param norm 166.71437
INFO:root:epoch 9, iter 7795, loss 3.20598, smoothed loss 2.98950, grad norm 3.37586, param norm 166.75348
INFO:root:epoch 9, iter 7800, loss 2.63578, smoothed loss 2.98904, grad norm 2.83345, param norm 166.79730
INFO:root:epoch 9, iter 7805, loss 2.95614, smoothed loss 2.98508, grad norm 3.07819, param norm 166.84007
INFO:root:epoch 9, iter 7810, loss 2.60483, smoothed loss 2.98936, grad norm 2.77395, param norm 166.88022
INFO:root:epoch 9, iter 7815, loss 2.88405, smoothed loss 2.99127, grad norm 2.95017, param norm 166.91971
INFO:root:epoch 9, iter 7820, loss 2.91860, smoothed loss 3.02168, grad norm 2.71182, param norm 166.95480
INFO:root:epoch 9, iter 7825, loss 2.73327, smoothed loss 3.01988, grad norm 2.62652, param norm 166.99138
INFO:root:epoch 9, iter 7830, loss 3.88987, smoothed loss 3.02826, grad norm 3.57875, param norm 167.03014
INFO:root:epoch 9, iter 7835, loss 2.43206, smoothed loss 3.03463, grad norm 3.34539, param norm 167.07237
INFO:root:epoch 9, iter 7840, loss 3.34051, smoothed loss 3.03603, grad norm 3.24451, param norm 167.11531
INFO:root:epoch 9, iter 7845, loss 3.01551, smoothed loss 3.03168, grad norm 3.17161, param norm 167.15643
INFO:root:epoch 9, iter 7850, loss 3.20067, smoothed loss 3.04622, grad norm 3.03437, param norm 167.19576
INFO:root:epoch 9, iter 7855, loss 2.66115, smoothed loss 3.03885, grad norm 2.84485, param norm 167.23456
INFO:root:epoch 9, iter 7860, loss 3.06895, smoothed loss 3.03473, grad norm 2.90324, param norm 167.27353
INFO:root:epoch 9, iter 7865, loss 3.02796, smoothed loss 3.03959, grad norm 2.76390, param norm 167.30954
INFO:root:epoch 9, iter 7870, loss 3.81292, smoothed loss 3.04076, grad norm 3.20556, param norm 167.34727
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 7875, loss 3.10800, smoothed loss 3.05999, grad norm 2.74007, param norm 167.38138
INFO:root:epoch 9, iter 7880, loss 3.16977, smoothed loss 3.06698, grad norm 2.81870, param norm 167.41562
INFO:root:epoch 9, iter 7885, loss 3.52239, smoothed loss 3.07491, grad norm 3.22075, param norm 167.45218
INFO:root:epoch 9, iter 7890, loss 3.53648, smoothed loss 3.08798, grad norm 3.41604, param norm 167.49077
INFO:root:epoch 9, iter 7895, loss 2.96325, smoothed loss 3.08953, grad norm 3.26339, param norm 167.52954
INFO:root:epoch 9, iter 7900, loss 3.25668, smoothed loss 3.07864, grad norm 3.42496, param norm 167.56799
INFO:root:epoch 9, iter 7905, loss 3.58428, smoothed loss 3.08533, grad norm 2.92571, param norm 167.60309
INFO:root:epoch 9, iter 7910, loss 2.88221, smoothed loss 3.07677, grad norm 2.68759, param norm 167.64186
INFO:root:epoch 9, iter 7915, loss 2.86476, smoothed loss 3.07287, grad norm 3.26108, param norm 167.68016
INFO:root:epoch 9, iter 7920, loss 2.19565, smoothed loss 3.07525, grad norm 2.37234, param norm 167.71487
INFO:root:epoch 9, iter 7925, loss 3.11207, smoothed loss 3.06650, grad norm 3.16996, param norm 167.74879
INFO:root:epoch 9, iter 7930, loss 3.53370, smoothed loss 3.06789, grad norm 3.07271, param norm 167.78578
INFO:root:epoch 9, iter 7935, loss 3.07633, smoothed loss 3.07035, grad norm 2.99978, param norm 167.81924
INFO:root:epoch 9, iter 7940, loss 2.47591, smoothed loss 3.06060, grad norm 3.14341, param norm 167.85194
INFO:root:epoch 9, iter 7945, loss 3.25848, smoothed loss 3.06338, grad norm 3.10643, param norm 167.88501
INFO:root:epoch 9, iter 7950, loss 3.61373, smoothed loss 3.06029, grad norm 3.17736, param norm 167.91852
INFO:root:epoch 9, iter 7955, loss 2.84985, smoothed loss 3.06491, grad norm 2.50525, param norm 167.94865
INFO:root:epoch 9, iter 7960, loss 3.37309, smoothed loss 3.04953, grad norm 2.80571, param norm 167.98035
INFO:root:epoch 9, iter 7965, loss 2.84253, smoothed loss 3.03532, grad norm 3.50899, param norm 168.01360
INFO:root:epoch 9, iter 7970, loss 2.43474, smoothed loss 3.02694, grad norm 2.82095, param norm 168.04713
INFO:root:epoch 9, iter 7975, loss 2.90779, smoothed loss 3.01671, grad norm 3.37793, param norm 168.07965
INFO:root:epoch 9, iter 7980, loss 2.55803, smoothed loss 3.01100, grad norm 2.89564, param norm 168.11119
INFO:root:epoch 9, iter 7985, loss 2.88315, smoothed loss 3.01680, grad norm 2.80789, param norm 168.14258
INFO:root:epoch 9, iter 7990, loss 2.94846, smoothed loss 3.01533, grad norm 3.20750, param norm 168.17421
INFO:root:epoch 9, iter 7995, loss 2.87440, smoothed loss 3.01984, grad norm 3.01359, param norm 168.21068
INFO:root:epoch 9, iter 8000, loss 2.88922, smoothed loss 3.01895, grad norm 3.25263, param norm 168.25000
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 9, Iter 8000, dev loss: 3.101855
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.56592 seconds [Score: 0.73683]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.43103 seconds [Score: 0.59400]
INFO:root:Epoch 9, Iter 8000, Train F1 score: 0.736832, Train EM score: 0.594000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 117.71055 seconds [Score: 0.63652]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.66673 seconds [Score: 0.48736]
INFO:root:Epoch 9, Iter 8000, Dev F1 score: 0.636522, Dev EM score: 0.487356
INFO:root:End of epoch 9
INFO:root:epoch 9, iter 8005, loss 3.36989, smoothed loss 3.01715, grad norm 3.16717, param norm 168.29440
INFO:root:epoch 9, iter 8010, loss 2.87418, smoothed loss 3.01641, grad norm 3.29269, param norm 168.33597
INFO:root:epoch 9, iter 8015, loss 3.18338, smoothed loss 3.01423, grad norm 2.94508, param norm 168.37187
INFO:root:epoch 9, iter 8020, loss 3.47491, smoothed loss 3.01426, grad norm 3.43876, param norm 168.40796
INFO:root:epoch 9, iter 8025, loss 2.01371, smoothed loss 2.99727, grad norm 2.45729, param norm 168.44720
INFO:root:epoch 9, iter 8030, loss 3.37511, smoothed loss 2.99232, grad norm 3.30254, param norm 168.48735
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 8035, loss 2.57803, smoothed loss 2.98525, grad norm 2.76500, param norm 168.52682
INFO:root:epoch 9, iter 8040, loss 2.86848, smoothed loss 2.98607, grad norm 2.80218, param norm 168.56111
INFO:root:epoch 9, iter 8045, loss 3.00475, smoothed loss 2.97675, grad norm 3.07228, param norm 168.59338
INFO:root:epoch 9, iter 8050, loss 3.07016, smoothed loss 2.96892, grad norm 2.91377, param norm 168.62761
INFO:root:epoch 9, iter 8055, loss 3.41116, smoothed loss 2.96988, grad norm 3.60113, param norm 168.66539
INFO:root:epoch 9, iter 8060, loss 2.78780, smoothed loss 2.97321, grad norm 3.14098, param norm 168.70235
INFO:root:epoch 9, iter 8065, loss 2.89155, smoothed loss 2.96720, grad norm 2.92318, param norm 168.73666
INFO:root:epoch 9, iter 8070, loss 3.68995, smoothed loss 2.96968, grad norm 3.20249, param norm 168.77153
INFO:root:epoch 9, iter 8075, loss 3.65369, smoothed loss 2.97547, grad norm 2.86924, param norm 168.80910
INFO:root:epoch 9, iter 8080, loss 2.07241, smoothed loss 2.96907, grad norm 2.84390, param norm 168.84135
INFO:root:epoch 9, iter 8085, loss 3.18140, smoothed loss 2.97542, grad norm 2.97387, param norm 168.87413
INFO:root:epoch 9, iter 8090, loss 3.05889, smoothed loss 2.98428, grad norm 3.11401, param norm 168.90965
INFO:root:epoch 9, iter 8095, loss 2.46242, smoothed loss 2.96697, grad norm 2.97891, param norm 168.94695
INFO:root:epoch 9, iter 8100, loss 3.00289, smoothed loss 2.97020, grad norm 2.74208, param norm 168.98679
INFO:root:epoch 9, iter 8105, loss 2.56080, smoothed loss 2.97385, grad norm 2.74221, param norm 169.02141
INFO:root:epoch 9, iter 8110, loss 3.07905, smoothed loss 2.98525, grad norm 3.14386, param norm 169.05930
INFO:root:epoch 9, iter 8115, loss 2.58118, smoothed loss 2.98553, grad norm 2.96535, param norm 169.09947
INFO:root:epoch 9, iter 8120, loss 3.18116, smoothed loss 2.97629, grad norm 2.96641, param norm 169.13692
INFO:root:epoch 9, iter 8125, loss 3.37817, smoothed loss 2.96795, grad norm 3.28152, param norm 169.17374
INFO:root:epoch 9, iter 8130, loss 2.88056, smoothed loss 2.96597, grad norm 3.05684, param norm 169.21289
INFO:root:epoch 9, iter 8135, loss 2.77261, smoothed loss 2.95798, grad norm 3.19313, param norm 169.25386
INFO:root:epoch 9, iter 8140, loss 2.98013, smoothed loss 2.96581, grad norm 3.10552, param norm 169.29166
INFO:root:epoch 9, iter 8145, loss 2.85877, smoothed loss 2.97675, grad norm 2.86968, param norm 169.32858
INFO:root:epoch 9, iter 8150, loss 2.91337, smoothed loss 2.97600, grad norm 2.70172, param norm 169.36794
INFO:root:epoch 9, iter 8155, loss 2.07559, smoothed loss 2.96877, grad norm 2.37720, param norm 169.40688
INFO:root:epoch 9, iter 8160, loss 3.01086, smoothed loss 2.96856, grad norm 3.05994, param norm 169.44308
INFO:root:epoch 9, iter 8165, loss 2.76287, smoothed loss 2.95531, grad norm 2.83841, param norm 169.48125
INFO:root:epoch 9, iter 8170, loss 2.76927, smoothed loss 2.94322, grad norm 3.21867, param norm 169.51913
INFO:root:epoch 9, iter 8175, loss 2.38387, smoothed loss 2.95146, grad norm 2.58254, param norm 169.55447
INFO:root:epoch 9, iter 8180, loss 2.14316, smoothed loss 2.93226, grad norm 2.97939, param norm 169.58672
INFO:root:epoch 9, iter 8185, loss 2.84467, smoothed loss 2.93266, grad norm 3.25320, param norm 169.62041
INFO:root:epoch 9, iter 8190, loss 2.39908, smoothed loss 2.92585, grad norm 2.95738, param norm 169.65776
Adding batches start...
Added  160  batches
INFO:root:epoch 9, iter 8195, loss 3.70720, smoothed loss 2.93887, grad norm 3.41192, param norm 169.69641
INFO:root:epoch 9, iter 8200, loss 2.74322, smoothed loss 2.94533, grad norm 3.04238, param norm 169.72957
INFO:root:epoch 9, iter 8205, loss 2.90761, smoothed loss 2.94529, grad norm 2.86561, param norm 169.76170
INFO:root:epoch 9, iter 8210, loss 2.46358, smoothed loss 2.94434, grad norm 2.61037, param norm 169.79453
INFO:root:epoch 9, iter 8215, loss 3.32829, smoothed loss 2.93448, grad norm 3.27439, param norm 169.82831
INFO:root:epoch 9, iter 8220, loss 3.17892, smoothed loss 2.93071, grad norm 3.40613, param norm 169.86475
INFO:root:epoch 9, iter 8225, loss 3.35223, smoothed loss 2.95008, grad norm 3.19624, param norm 169.89833
INFO:root:epoch 9, iter 8230, loss 2.46468, smoothed loss 2.93974, grad norm 2.64292, param norm 169.93523
INFO:root:epoch 9, iter 8235, loss 2.83608, smoothed loss 2.93756, grad norm 3.25619, param norm 169.97496
INFO:root:epoch 9, iter 8240, loss 3.17028, smoothed loss 2.94734, grad norm 3.13597, param norm 170.01350
INFO:root:epoch 9, iter 8245, loss 2.51649, smoothed loss 2.93238, grad norm 2.57521, param norm 170.05196
INFO:root:epoch 9, iter 8250, loss 3.08981, smoothed loss 2.92975, grad norm 2.77070, param norm 170.08856
INFO:root:epoch 9, iter 8255, loss 2.67700, smoothed loss 2.92512, grad norm 3.22548, param norm 170.12132
INFO:root:epoch 9, iter 8260, loss 2.63730, smoothed loss 2.92338, grad norm 2.57427, param norm 170.15619
INFO:root:epoch 9, iter 8265, loss 3.53527, smoothed loss 2.94485, grad norm 3.40457, param norm 170.19232
INFO:root:epoch 9, iter 8270, loss 3.03061, smoothed loss 2.95188, grad norm 3.12238, param norm 170.22739
INFO:root:epoch 9, iter 8275, loss 2.74892, smoothed loss 2.94435, grad norm 3.15885, param norm 170.26518
INFO:root:epoch 9, iter 8280, loss 3.17062, smoothed loss 2.93993, grad norm 2.99258, param norm 170.30197
INFO:root:epoch 9, iter 8285, loss 2.85393, smoothed loss 2.94249, grad norm 2.88727, param norm 170.33289
INFO:root:epoch 9, iter 8290, loss 3.79357, smoothed loss 2.94620, grad norm 3.46537, param norm 170.36487
INFO:root:epoch 9, iter 8295, loss 2.68386, smoothed loss 2.94484, grad norm 2.70737, param norm 170.39951
INFO:root:epoch 9, iter 8300, loss 2.77779, smoothed loss 2.95026, grad norm 2.62152, param norm 170.43721
INFO:root:epoch 9, iter 8305, loss 3.49688, smoothed loss 2.97035, grad norm 3.19321, param norm 170.47449
INFO:root:epoch 9, iter 8310, loss 2.68018, smoothed loss 2.96177, grad norm 3.27210, param norm 170.51033
INFO:root:epoch 9, iter 8315, loss 2.75335, smoothed loss 2.96427, grad norm 2.70044, param norm 170.54434
INFO:root:epoch 9, iter 8320, loss 2.79668, smoothed loss 2.95505, grad norm 2.76108, param norm 170.58026
INFO:root:epoch 9, iter 8325, loss 3.16166, smoothed loss 2.95910, grad norm 3.64213, param norm 170.61716
INFO:root:epoch 9, iter 8330, loss 2.72456, smoothed loss 2.96208, grad norm 2.72798, param norm 170.65407
INFO:root:epoch 9, iter 8335, loss 3.07567, smoothed loss 2.96410, grad norm 3.09321, param norm 170.69267
INFO:root:epoch 9, iter 8340, loss 2.91625, smoothed loss 2.96479, grad norm 3.13325, param norm 170.73259
INFO:root:epoch 9, iter 8345, loss 2.84969, smoothed loss 2.96049, grad norm 2.86614, param norm 170.77168
INFO:root:epoch 9, iter 8350, loss 2.88032, smoothed loss 2.96580, grad norm 3.30289, param norm 170.80899
Adding batches start...
Added  144  batches
INFO:root:epoch 9, iter 8355, loss 3.04303, smoothed loss 2.97230, grad norm 3.02489, param norm 170.84329
INFO:root:epoch 9, iter 8360, loss 2.73925, smoothed loss 2.98245, grad norm 2.72226, param norm 170.88043
INFO:root:epoch 9, iter 8365, loss 3.48829, smoothed loss 2.98437, grad norm 3.39230, param norm 170.91905
INFO:root:epoch 9, iter 8370, loss 2.76524, smoothed loss 2.98431, grad norm 3.04237, param norm 170.95583
INFO:root:epoch 9, iter 8375, loss 2.89147, smoothed loss 2.98064, grad norm 3.12588, param norm 170.98900
INFO:root:epoch 9, iter 8380, loss 3.12871, smoothed loss 2.98075, grad norm 2.73772, param norm 171.02939
INFO:root:epoch 9, iter 8385, loss 3.95935, smoothed loss 2.98653, grad norm 3.13760, param norm 171.07208
INFO:root:epoch 9, iter 8390, loss 3.01141, smoothed loss 2.98459, grad norm 3.08808, param norm 171.11276
INFO:root:epoch 9, iter 8395, loss 2.50972, smoothed loss 2.97685, grad norm 2.93103, param norm 171.14700
INFO:root:epoch 9, iter 8400, loss 2.83129, smoothed loss 2.97169, grad norm 2.87064, param norm 171.18109
INFO:root:epoch 9, iter 8405, loss 2.81531, smoothed loss 2.96288, grad norm 3.08339, param norm 171.21400
INFO:root:epoch 9, iter 8410, loss 3.10105, smoothed loss 2.97511, grad norm 2.86900, param norm 171.24756
INFO:root:epoch 9, iter 8415, loss 3.78117, smoothed loss 2.97475, grad norm 3.24150, param norm 171.27797
INFO:root:epoch 9, iter 8420, loss 2.68237, smoothed loss 2.96859, grad norm 2.98461, param norm 171.31313
INFO:root:epoch 9, iter 8425, loss 3.06377, smoothed loss 2.97030, grad norm 2.93672, param norm 171.35342
INFO:root:epoch 9, iter 8430, loss 3.45975, smoothed loss 2.97166, grad norm 3.59824, param norm 171.39842
INFO:root:epoch 9, iter 8435, loss 2.70562, smoothed loss 2.97142, grad norm 2.85510, param norm 171.43877
INFO:root:epoch 9, iter 8440, loss 2.98070, smoothed loss 2.96955, grad norm 3.14106, param norm 171.47653
INFO:root:epoch 9, iter 8445, loss 3.08884, smoothed loss 2.96710, grad norm 3.47903, param norm 171.51282
INFO:root:epoch 9, iter 8450, loss 3.21750, smoothed loss 2.95764, grad norm 3.40080, param norm 171.54709
INFO:root:epoch 9, iter 8455, loss 3.38454, smoothed loss 2.96895, grad norm 3.03031, param norm 171.58093
INFO:root:epoch 9, iter 8460, loss 2.91261, smoothed loss 2.98562, grad norm 2.80951, param norm 171.61743
INFO:root:epoch 9, iter 8465, loss 2.38750, smoothed loss 2.96956, grad norm 2.35997, param norm 171.65488
INFO:root:epoch 9, iter 8470, loss 3.02440, smoothed loss 2.96530, grad norm 3.14372, param norm 171.69043
INFO:root:epoch 9, iter 8475, loss 3.17734, smoothed loss 2.97336, grad norm 3.30281, param norm 171.72437
INFO:root:epoch 9, iter 8480, loss 2.67762, smoothed loss 2.96888, grad norm 2.58281, param norm 171.75533
INFO:root:epoch 9, iter 8485, loss 3.83819, smoothed loss 2.96661, grad norm 4.18793, param norm 171.78520
INFO:root:epoch 9, iter 8490, loss 2.80998, smoothed loss 2.96901, grad norm 2.79752, param norm 171.81886
INFO:root:epoch 9, iter 8495, loss 2.86816, smoothed loss 2.95847, grad norm 2.86424, param norm 171.85693
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8500, loss 3.55928, smoothed loss 2.95518, grad norm 3.61489, param norm 171.89674
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 10, Iter 8500, dev loss: 3.082552
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.81215 seconds [Score: 0.71285]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.17504 seconds [Score: 0.58500]
INFO:root:Epoch 10, Iter 8500, Train F1 score: 0.712855, Train EM score: 0.585000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.41028 seconds [Score: 0.63382]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.51272 seconds [Score: 0.48918]
INFO:root:Epoch 10, Iter 8500, Dev F1 score: 0.633816, Dev EM score: 0.489182
INFO:root:End of epoch 10
INFO:root:epoch 10, iter 8505, loss 3.21525, smoothed loss 2.95357, grad norm 3.44761, param norm 171.93065
INFO:root:epoch 10, iter 8510, loss 3.41032, smoothed loss 2.95481, grad norm 2.97704, param norm 171.96321
INFO:root:epoch 10, iter 8515, loss 3.17592, smoothed loss 2.96420, grad norm 3.15382, param norm 171.99217
INFO:root:epoch 10, iter 8520, loss 2.97744, smoothed loss 2.95963, grad norm 2.99790, param norm 172.02550
INFO:root:epoch 10, iter 8525, loss 3.11789, smoothed loss 2.96787, grad norm 3.22650, param norm 172.05774
INFO:root:epoch 10, iter 8530, loss 2.53594, smoothed loss 2.96281, grad norm 2.69652, param norm 172.08990
INFO:root:epoch 10, iter 8535, loss 2.48949, smoothed loss 2.96009, grad norm 3.18567, param norm 172.12639
INFO:root:epoch 10, iter 8540, loss 2.57409, smoothed loss 2.95611, grad norm 2.68413, param norm 172.16719
INFO:root:epoch 10, iter 8545, loss 2.82714, smoothed loss 2.95339, grad norm 3.01877, param norm 172.21010
INFO:root:epoch 10, iter 8550, loss 2.74158, smoothed loss 2.94795, grad norm 2.99374, param norm 172.25197
INFO:root:epoch 10, iter 8555, loss 2.83927, smoothed loss 2.94386, grad norm 3.08223, param norm 172.29065
INFO:root:epoch 10, iter 8560, loss 2.79884, smoothed loss 2.95259, grad norm 3.19120, param norm 172.32643
INFO:root:epoch 10, iter 8565, loss 2.97328, smoothed loss 2.94833, grad norm 3.05701, param norm 172.36343
INFO:root:epoch 10, iter 8570, loss 2.89648, smoothed loss 2.94814, grad norm 2.82738, param norm 172.40063
INFO:root:epoch 10, iter 8575, loss 2.90162, smoothed loss 2.95865, grad norm 3.02201, param norm 172.43526
INFO:root:epoch 10, iter 8580, loss 3.17532, smoothed loss 2.95252, grad norm 3.42399, param norm 172.47159
INFO:root:epoch 10, iter 8585, loss 2.58806, smoothed loss 2.95537, grad norm 2.99800, param norm 172.50861
INFO:root:epoch 10, iter 8590, loss 3.20902, smoothed loss 2.96037, grad norm 3.07956, param norm 172.54538
INFO:root:epoch 10, iter 8595, loss 2.98647, smoothed loss 2.95066, grad norm 3.07601, param norm 172.57948
INFO:root:epoch 10, iter 8600, loss 3.12355, smoothed loss 2.95015, grad norm 3.37412, param norm 172.61145
INFO:root:epoch 10, iter 8605, loss 2.74753, smoothed loss 2.93936, grad norm 3.10431, param norm 172.64212
INFO:root:epoch 10, iter 8610, loss 3.65873, smoothed loss 2.93961, grad norm 3.09763, param norm 172.67360
INFO:root:epoch 10, iter 8615, loss 2.06186, smoothed loss 2.92093, grad norm 2.89956, param norm 172.71082
INFO:root:epoch 10, iter 8620, loss 2.94930, smoothed loss 2.91212, grad norm 3.48682, param norm 172.75465
INFO:root:epoch 10, iter 8625, loss 2.94194, smoothed loss 2.91373, grad norm 3.49979, param norm 172.79617
INFO:root:epoch 10, iter 8630, loss 3.32995, smoothed loss 2.91635, grad norm 3.31051, param norm 172.82883
INFO:root:epoch 10, iter 8635, loss 3.02576, smoothed loss 2.91908, grad norm 3.38451, param norm 172.85857
INFO:root:epoch 10, iter 8640, loss 3.02532, smoothed loss 2.91654, grad norm 3.29541, param norm 172.89085
INFO:root:epoch 10, iter 8645, loss 3.35328, smoothed loss 2.93325, grad norm 3.43161, param norm 172.92442
INFO:root:epoch 10, iter 8650, loss 2.88271, smoothed loss 2.93979, grad norm 3.38265, param norm 172.96239
INFO:root:epoch 10, iter 8655, loss 2.59217, smoothed loss 2.93873, grad norm 2.68636, param norm 173.00076
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8660, loss 2.78608, smoothed loss 2.93418, grad norm 2.91498, param norm 173.03958
INFO:root:epoch 10, iter 8665, loss 3.01736, smoothed loss 2.93317, grad norm 3.06173, param norm 173.07524
INFO:root:epoch 10, iter 8670, loss 3.33085, smoothed loss 2.93550, grad norm 2.94868, param norm 173.11023
INFO:root:epoch 10, iter 8675, loss 2.67082, smoothed loss 2.93389, grad norm 2.93567, param norm 173.14452
INFO:root:epoch 10, iter 8680, loss 2.71755, smoothed loss 2.92597, grad norm 2.89827, param norm 173.18192
INFO:root:epoch 10, iter 8685, loss 2.96200, smoothed loss 2.91344, grad norm 2.84557, param norm 173.21812
INFO:root:epoch 10, iter 8690, loss 3.22509, smoothed loss 2.91730, grad norm 3.39058, param norm 173.25333
INFO:root:epoch 10, iter 8695, loss 3.15985, smoothed loss 2.90746, grad norm 3.55162, param norm 173.28654
INFO:root:epoch 10, iter 8700, loss 3.01010, smoothed loss 2.90524, grad norm 3.14217, param norm 173.31947
INFO:root:epoch 10, iter 8705, loss 2.20431, smoothed loss 2.89076, grad norm 2.67607, param norm 173.35008
INFO:root:epoch 10, iter 8710, loss 2.43181, smoothed loss 2.88777, grad norm 2.88816, param norm 173.38266
INFO:root:epoch 10, iter 8715, loss 3.30753, smoothed loss 2.89499, grad norm 3.37516, param norm 173.41975
INFO:root:epoch 10, iter 8720, loss 2.57047, smoothed loss 2.89708, grad norm 3.01682, param norm 173.45242
INFO:root:epoch 10, iter 8725, loss 3.28244, smoothed loss 2.89704, grad norm 3.32865, param norm 173.48904
INFO:root:epoch 10, iter 8730, loss 2.84080, smoothed loss 2.90168, grad norm 3.16053, param norm 173.52869
INFO:root:epoch 10, iter 8735, loss 3.00566, smoothed loss 2.88838, grad norm 3.10746, param norm 173.57167
INFO:root:epoch 10, iter 8740, loss 2.99233, smoothed loss 2.89745, grad norm 3.37987, param norm 173.61217
INFO:root:epoch 10, iter 8745, loss 2.96724, smoothed loss 2.90053, grad norm 2.77612, param norm 173.64758
INFO:root:epoch 10, iter 8750, loss 3.20187, smoothed loss 2.90069, grad norm 3.24658, param norm 173.68506
INFO:root:epoch 10, iter 8755, loss 2.70406, smoothed loss 2.91243, grad norm 2.76882, param norm 173.72096
INFO:root:epoch 10, iter 8760, loss 2.63698, smoothed loss 2.93123, grad norm 2.81865, param norm 173.75436
INFO:root:epoch 10, iter 8765, loss 4.00958, smoothed loss 2.93511, grad norm 3.51086, param norm 173.79182
INFO:root:epoch 10, iter 8770, loss 2.93205, smoothed loss 2.94054, grad norm 2.47345, param norm 173.82933
INFO:root:epoch 10, iter 8775, loss 2.63553, smoothed loss 2.93898, grad norm 2.83180, param norm 173.86858
INFO:root:epoch 10, iter 8780, loss 3.05967, smoothed loss 2.92488, grad norm 3.17223, param norm 173.90817
INFO:root:epoch 10, iter 8785, loss 3.64312, smoothed loss 2.93485, grad norm 3.58187, param norm 173.94339
INFO:root:epoch 10, iter 8790, loss 2.76772, smoothed loss 2.93396, grad norm 3.28392, param norm 173.97690
INFO:root:epoch 10, iter 8795, loss 2.92912, smoothed loss 2.92564, grad norm 3.01887, param norm 174.00650
INFO:root:epoch 10, iter 8800, loss 2.72248, smoothed loss 2.93277, grad norm 2.94513, param norm 174.03836
INFO:root:epoch 10, iter 8805, loss 3.97487, smoothed loss 2.95354, grad norm 3.61799, param norm 174.07445
INFO:root:epoch 10, iter 8810, loss 2.71675, smoothed loss 2.94511, grad norm 2.87230, param norm 174.11691
INFO:root:epoch 10, iter 8815, loss 2.53143, smoothed loss 2.93620, grad norm 2.95499, param norm 174.16165
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8820, loss 2.62027, smoothed loss 2.93681, grad norm 2.99300, param norm 174.20247
INFO:root:epoch 10, iter 8825, loss 2.45561, smoothed loss 2.93039, grad norm 2.67326, param norm 174.24043
INFO:root:epoch 10, iter 8830, loss 2.17872, smoothed loss 2.91989, grad norm 2.80674, param norm 174.27704
INFO:root:epoch 10, iter 8835, loss 2.27129, smoothed loss 2.91171, grad norm 2.81573, param norm 174.31770
INFO:root:epoch 10, iter 8840, loss 2.50768, smoothed loss 2.91986, grad norm 3.02087, param norm 174.35481
INFO:root:epoch 10, iter 8845, loss 2.70702, smoothed loss 2.90792, grad norm 3.51713, param norm 174.38690
INFO:root:epoch 10, iter 8850, loss 2.67567, smoothed loss 2.90031, grad norm 3.12083, param norm 174.41974
INFO:root:epoch 10, iter 8855, loss 2.03546, smoothed loss 2.88814, grad norm 2.89701, param norm 174.44914
INFO:root:epoch 10, iter 8860, loss 2.68542, smoothed loss 2.88473, grad norm 2.85202, param norm 174.48372
INFO:root:epoch 10, iter 8865, loss 3.31527, smoothed loss 2.90641, grad norm 3.02926, param norm 174.52328
INFO:root:epoch 10, iter 8870, loss 2.92910, smoothed loss 2.90706, grad norm 3.07750, param norm 174.56602
INFO:root:epoch 10, iter 8875, loss 2.95051, smoothed loss 2.92516, grad norm 3.29228, param norm 174.61136
INFO:root:epoch 10, iter 8880, loss 2.64438, smoothed loss 2.92126, grad norm 2.87768, param norm 174.65398
INFO:root:epoch 10, iter 8885, loss 2.92195, smoothed loss 2.91257, grad norm 3.34255, param norm 174.69200
INFO:root:epoch 10, iter 8890, loss 2.79403, smoothed loss 2.90403, grad norm 3.21932, param norm 174.72736
INFO:root:epoch 10, iter 8895, loss 2.88553, smoothed loss 2.90477, grad norm 3.14050, param norm 174.76097
INFO:root:epoch 10, iter 8900, loss 3.04025, smoothed loss 2.91246, grad norm 3.21018, param norm 174.79179
INFO:root:epoch 10, iter 8905, loss 2.74855, smoothed loss 2.90583, grad norm 3.00771, param norm 174.82584
INFO:root:epoch 10, iter 8910, loss 2.81503, smoothed loss 2.91450, grad norm 2.95625, param norm 174.86263
INFO:root:epoch 10, iter 8915, loss 3.19436, smoothed loss 2.92073, grad norm 3.02727, param norm 174.90102
INFO:root:epoch 10, iter 8920, loss 2.53025, smoothed loss 2.92414, grad norm 2.69916, param norm 174.93515
INFO:root:epoch 10, iter 8925, loss 2.77782, smoothed loss 2.92207, grad norm 2.70498, param norm 174.96761
INFO:root:epoch 10, iter 8930, loss 2.69236, smoothed loss 2.91664, grad norm 3.09897, param norm 175.00401
INFO:root:epoch 10, iter 8935, loss 2.98931, smoothed loss 2.91014, grad norm 2.84229, param norm 175.04320
INFO:root:epoch 10, iter 8940, loss 3.70321, smoothed loss 2.90952, grad norm 3.81592, param norm 175.08217
INFO:root:epoch 10, iter 8945, loss 2.17613, smoothed loss 2.90127, grad norm 2.87980, param norm 175.11888
INFO:root:epoch 10, iter 8950, loss 3.30031, smoothed loss 2.90720, grad norm 3.13118, param norm 175.15749
INFO:root:epoch 10, iter 8955, loss 3.23075, smoothed loss 2.91826, grad norm 3.38909, param norm 175.19310
INFO:root:epoch 10, iter 8960, loss 3.27171, smoothed loss 2.90771, grad norm 2.85437, param norm 175.23222
INFO:root:epoch 10, iter 8965, loss 2.85669, smoothed loss 2.90382, grad norm 3.00309, param norm 175.27371
INFO:root:epoch 10, iter 8970, loss 3.20356, smoothed loss 2.90793, grad norm 3.09757, param norm 175.31438
INFO:root:epoch 10, iter 8975, loss 2.40016, smoothed loss 2.90480, grad norm 3.03462, param norm 175.34950
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 8980, loss 2.94714, smoothed loss 2.91899, grad norm 2.94324, param norm 175.38322
INFO:root:epoch 10, iter 8985, loss 3.07361, smoothed loss 2.91739, grad norm 3.13237, param norm 175.41277
INFO:root:epoch 10, iter 8990, loss 2.73275, smoothed loss 2.91686, grad norm 2.76538, param norm 175.44563
INFO:root:epoch 10, iter 8995, loss 2.60338, smoothed loss 2.90402, grad norm 2.77179, param norm 175.48434
INFO:root:epoch 10, iter 9000, loss 2.55676, smoothed loss 2.90215, grad norm 2.81481, param norm 175.52388
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 10, Iter 9000, dev loss: 3.073013
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.54131 seconds [Score: 0.73171]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.79031 seconds [Score: 0.59100]
INFO:root:Epoch 10, Iter 9000, Train F1 score: 0.731711, Train EM score: 0.591000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.60234 seconds [Score: 0.64099]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.75670 seconds [Score: 0.49775]
INFO:root:Epoch 10, Iter 9000, Dev F1 score: 0.640994, Dev EM score: 0.497752
INFO:root:End of epoch 10
INFO:root:epoch 10, iter 9005, loss 2.95034, smoothed loss 2.89884, grad norm 3.45661, param norm 175.56599
INFO:root:epoch 10, iter 9010, loss 2.74263, smoothed loss 2.89462, grad norm 2.77245, param norm 175.60753
INFO:root:epoch 10, iter 9015, loss 2.65434, smoothed loss 2.88933, grad norm 3.01924, param norm 175.64529
INFO:root:epoch 10, iter 9020, loss 2.79042, smoothed loss 2.88197, grad norm 3.08933, param norm 175.67851
INFO:root:epoch 10, iter 9025, loss 3.08921, smoothed loss 2.87331, grad norm 3.12876, param norm 175.71092
INFO:root:epoch 10, iter 9030, loss 2.20921, smoothed loss 2.84905, grad norm 2.51558, param norm 175.74446
INFO:root:epoch 10, iter 9035, loss 2.91837, smoothed loss 2.85537, grad norm 3.04883, param norm 175.78030
INFO:root:epoch 10, iter 9040, loss 3.06732, smoothed loss 2.85749, grad norm 3.71948, param norm 175.81198
INFO:root:epoch 10, iter 9045, loss 2.93293, smoothed loss 2.85965, grad norm 3.32534, param norm 175.84518
INFO:root:epoch 10, iter 9050, loss 3.17694, smoothed loss 2.85788, grad norm 3.10709, param norm 175.88034
INFO:root:epoch 10, iter 9055, loss 3.25211, smoothed loss 2.85467, grad norm 3.45322, param norm 175.91499
INFO:root:epoch 10, iter 9060, loss 3.03543, smoothed loss 2.84320, grad norm 3.14237, param norm 175.94788
INFO:root:epoch 10, iter 9065, loss 2.74352, smoothed loss 2.84966, grad norm 2.70903, param norm 175.97937
INFO:root:epoch 10, iter 9070, loss 2.51460, smoothed loss 2.84852, grad norm 2.83429, param norm 176.01097
INFO:root:epoch 10, iter 9075, loss 3.25468, smoothed loss 2.84211, grad norm 3.28064, param norm 176.04610
INFO:root:epoch 10, iter 9080, loss 3.45741, smoothed loss 2.84982, grad norm 3.41621, param norm 176.08180
INFO:root:epoch 10, iter 9085, loss 3.45503, smoothed loss 2.84893, grad norm 2.98129, param norm 176.11938
INFO:root:epoch 10, iter 9090, loss 3.64614, smoothed loss 2.85166, grad norm 3.47058, param norm 176.15898
INFO:root:epoch 10, iter 9095, loss 3.15151, smoothed loss 2.86323, grad norm 3.18937, param norm 176.19199
INFO:root:epoch 10, iter 9100, loss 2.60796, smoothed loss 2.86559, grad norm 2.69657, param norm 176.22658
INFO:root:epoch 10, iter 9105, loss 2.21905, smoothed loss 2.85178, grad norm 2.93128, param norm 176.25815
INFO:root:epoch 10, iter 9110, loss 2.97161, smoothed loss 2.85434, grad norm 2.65731, param norm 176.28899
INFO:root:epoch 10, iter 9115, loss 3.04997, smoothed loss 2.85881, grad norm 3.39286, param norm 176.32166
INFO:root:epoch 10, iter 9120, loss 3.23557, smoothed loss 2.86144, grad norm 2.94834, param norm 176.35692
INFO:root:epoch 10, iter 9125, loss 2.39202, smoothed loss 2.86291, grad norm 3.35940, param norm 176.39465
INFO:root:epoch 10, iter 9130, loss 2.45709, smoothed loss 2.84895, grad norm 2.72876, param norm 176.43193
INFO:root:epoch 10, iter 9135, loss 2.84334, smoothed loss 2.85840, grad norm 2.91292, param norm 176.46884
Adding batches start...
Added  160  batches
INFO:root:epoch 10, iter 9140, loss 2.64351, smoothed loss 2.85280, grad norm 3.31689, param norm 176.50554
INFO:root:epoch 10, iter 9145, loss 2.91229, smoothed loss 2.85684, grad norm 2.88629, param norm 176.54051
INFO:root:epoch 10, iter 9150, loss 3.01821, smoothed loss 2.86219, grad norm 3.11849, param norm 176.57175
INFO:root:epoch 10, iter 9155, loss 2.73197, smoothed loss 2.85889, grad norm 2.96751, param norm 176.60033
INFO:root:epoch 10, iter 9160, loss 2.68919, smoothed loss 2.85045, grad norm 2.73279, param norm 176.63206
INFO:root:epoch 10, iter 9165, loss 2.48563, smoothed loss 2.84067, grad norm 2.65267, param norm 176.66792
INFO:root:epoch 10, iter 9170, loss 3.21567, smoothed loss 2.84050, grad norm 3.32830, param norm 176.70723
INFO:root:epoch 10, iter 9175, loss 2.69922, smoothed loss 2.83985, grad norm 2.71874, param norm 176.74550
INFO:root:epoch 10, iter 9180, loss 2.82101, smoothed loss 2.84800, grad norm 2.66907, param norm 176.78094
INFO:root:epoch 10, iter 9185, loss 2.81509, smoothed loss 2.85332, grad norm 3.12100, param norm 176.81625
INFO:root:epoch 10, iter 9190, loss 2.83160, smoothed loss 2.84842, grad norm 2.63973, param norm 176.85359
INFO:root:epoch 10, iter 9195, loss 3.88186, smoothed loss 2.86263, grad norm 3.47209, param norm 176.88867
INFO:root:epoch 10, iter 9200, loss 2.53667, smoothed loss 2.87238, grad norm 2.81115, param norm 176.91786
INFO:root:epoch 10, iter 9205, loss 3.18304, smoothed loss 2.87798, grad norm 3.26213, param norm 176.94919
INFO:root:epoch 10, iter 9210, loss 2.93801, smoothed loss 2.87902, grad norm 2.77667, param norm 176.98595
INFO:root:epoch 10, iter 9215, loss 3.31440, smoothed loss 2.88517, grad norm 3.04714, param norm 177.02344
INFO:root:epoch 10, iter 9220, loss 3.42885, smoothed loss 2.89192, grad norm 3.13408, param norm 177.05927
INFO:root:epoch 10, iter 9225, loss 2.92597, smoothed loss 2.89164, grad norm 2.83922, param norm 177.09238
INFO:root:epoch 10, iter 9230, loss 3.41729, smoothed loss 2.91899, grad norm 3.48497, param norm 177.12794
INFO:root:epoch 10, iter 9235, loss 2.61255, smoothed loss 2.92463, grad norm 2.47627, param norm 177.16289
INFO:root:epoch 10, iter 9240, loss 2.75285, smoothed loss 2.91278, grad norm 2.68791, param norm 177.19693
INFO:root:epoch 10, iter 9245, loss 3.48863, smoothed loss 2.91755, grad norm 3.26758, param norm 177.23053
INFO:root:epoch 10, iter 9250, loss 2.49588, smoothed loss 2.91075, grad norm 3.11313, param norm 177.26518
INFO:root:epoch 10, iter 9255, loss 2.31648, smoothed loss 2.91338, grad norm 2.68760, param norm 177.29884
INFO:root:epoch 10, iter 9260, loss 2.36010, smoothed loss 2.89493, grad norm 2.41661, param norm 177.32938
INFO:root:epoch 10, iter 9265, loss 3.07311, smoothed loss 2.89073, grad norm 3.07887, param norm 177.36201
INFO:root:epoch 10, iter 9270, loss 3.54669, smoothed loss 2.89498, grad norm 3.40087, param norm 177.39639
INFO:root:epoch 10, iter 9275, loss 2.79570, smoothed loss 2.90230, grad norm 2.86687, param norm 177.42867
INFO:root:epoch 10, iter 9280, loss 2.84878, smoothed loss 2.90379, grad norm 2.59473, param norm 177.45947
INFO:root:epoch 10, iter 9285, loss 3.20049, smoothed loss 2.91108, grad norm 3.01676, param norm 177.49272
INFO:root:epoch 10, iter 9290, loss 3.07991, smoothed loss 2.90838, grad norm 2.85852, param norm 177.52821
INFO:root:epoch 10, iter 9295, loss 2.69985, smoothed loss 2.90598, grad norm 2.81656, param norm 177.56790
Adding batches start...
Added  144  batches
INFO:root:epoch 10, iter 9300, loss 2.91342, smoothed loss 2.90486, grad norm 3.21973, param norm 177.60751
INFO:root:epoch 10, iter 9305, loss 3.07183, smoothed loss 2.89907, grad norm 2.97167, param norm 177.64432
INFO:root:epoch 10, iter 9310, loss 2.88876, smoothed loss 2.89992, grad norm 3.46463, param norm 177.67937
INFO:root:epoch 10, iter 9315, loss 3.49747, smoothed loss 2.89820, grad norm 3.33932, param norm 177.71301
INFO:root:epoch 10, iter 9320, loss 2.50305, smoothed loss 2.89065, grad norm 2.72088, param norm 177.74557
INFO:root:epoch 10, iter 9325, loss 3.31937, smoothed loss 2.89265, grad norm 3.30798, param norm 177.77826
INFO:root:epoch 10, iter 9330, loss 2.52577, smoothed loss 2.88854, grad norm 2.93114, param norm 177.80994
INFO:root:epoch 10, iter 9335, loss 2.37239, smoothed loss 2.87683, grad norm 2.96222, param norm 177.84738
INFO:root:epoch 10, iter 9340, loss 2.54039, smoothed loss 2.87170, grad norm 2.91566, param norm 177.88516
INFO:root:epoch 10, iter 9345, loss 2.77748, smoothed loss 2.87662, grad norm 3.33806, param norm 177.92009
INFO:root:epoch 10, iter 9350, loss 2.69441, smoothed loss 2.86903, grad norm 2.99921, param norm 177.95724
INFO:root:epoch 10, iter 9355, loss 3.20630, smoothed loss 2.87277, grad norm 3.33973, param norm 177.99139
INFO:root:epoch 10, iter 9360, loss 3.06188, smoothed loss 2.85629, grad norm 2.95410, param norm 178.02934
INFO:root:epoch 10, iter 9365, loss 3.00544, smoothed loss 2.87787, grad norm 3.08104, param norm 178.06114
INFO:root:epoch 10, iter 9370, loss 2.79385, smoothed loss 2.87862, grad norm 3.18871, param norm 178.09000
INFO:root:epoch 10, iter 9375, loss 2.86054, smoothed loss 2.87050, grad norm 2.97277, param norm 178.12000
INFO:root:epoch 10, iter 9380, loss 3.98145, smoothed loss 2.87497, grad norm 3.50308, param norm 178.15211
INFO:root:epoch 10, iter 9385, loss 2.72824, smoothed loss 2.86642, grad norm 3.28865, param norm 178.18401
INFO:root:epoch 10, iter 9390, loss 2.49267, smoothed loss 2.85427, grad norm 3.11394, param norm 178.21950
INFO:root:epoch 10, iter 9395, loss 3.26617, smoothed loss 2.86569, grad norm 3.84071, param norm 178.25774
INFO:root:epoch 10, iter 9400, loss 3.12006, smoothed loss 2.85975, grad norm 3.50701, param norm 178.29700
INFO:root:epoch 10, iter 9405, loss 2.42672, smoothed loss 2.85679, grad norm 2.83222, param norm 178.32996
INFO:root:epoch 10, iter 9410, loss 2.75017, smoothed loss 2.85736, grad norm 3.02147, param norm 178.36366
INFO:root:epoch 10, iter 9415, loss 3.32884, smoothed loss 2.86228, grad norm 3.39678, param norm 178.39931
INFO:root:epoch 10, iter 9420, loss 3.40887, smoothed loss 2.87883, grad norm 3.48963, param norm 178.43094
INFO:root:epoch 10, iter 9425, loss 2.17296, smoothed loss 2.87632, grad norm 2.87833, param norm 178.46143
INFO:root:epoch 10, iter 9430, loss 2.80266, smoothed loss 2.86521, grad norm 2.67938, param norm 178.49268
INFO:root:epoch 10, iter 9435, loss 2.87162, smoothed loss 2.85101, grad norm 2.99022, param norm 178.52734
INFO:root:epoch 10, iter 9440, loss 2.13595, smoothed loss 2.83737, grad norm 2.77664, param norm 178.55983
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9445, loss 2.89097, smoothed loss 2.83607, grad norm 3.26241, param norm 178.59489
INFO:root:epoch 11, iter 9450, loss 2.78994, smoothed loss 2.84898, grad norm 3.07945, param norm 178.63011
INFO:root:epoch 11, iter 9455, loss 2.52206, smoothed loss 2.84568, grad norm 2.55293, param norm 178.66493
INFO:root:epoch 11, iter 9460, loss 3.32374, smoothed loss 2.83874, grad norm 3.43952, param norm 178.70303
INFO:root:epoch 11, iter 9465, loss 2.58946, smoothed loss 2.84705, grad norm 2.92445, param norm 178.74162
INFO:root:epoch 11, iter 9470, loss 2.93274, smoothed loss 2.84743, grad norm 3.18616, param norm 178.78140
INFO:root:epoch 11, iter 9475, loss 3.08039, smoothed loss 2.85266, grad norm 2.96929, param norm 178.81802
INFO:root:epoch 11, iter 9480, loss 3.24952, smoothed loss 2.85377, grad norm 3.10819, param norm 178.85258
INFO:root:epoch 11, iter 9485, loss 2.82540, smoothed loss 2.85201, grad norm 2.91472, param norm 178.88887
INFO:root:epoch 11, iter 9490, loss 3.45461, smoothed loss 2.86141, grad norm 3.22833, param norm 178.92456
INFO:root:epoch 11, iter 9495, loss 2.59725, smoothed loss 2.85688, grad norm 2.56576, param norm 178.95885
INFO:root:epoch 11, iter 9500, loss 2.21942, smoothed loss 2.86000, grad norm 2.82446, param norm 178.99483
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 11, Iter 9500, dev loss: 3.064982
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.96997 seconds [Score: 0.74969]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.44721 seconds [Score: 0.62700]
INFO:root:Epoch 11, Iter 9500, Train F1 score: 0.749688, Train EM score: 0.627000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.18782 seconds [Score: 0.63840]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.55900 seconds [Score: 0.49298]
INFO:root:Epoch 11, Iter 9500, Dev F1 score: 0.638400, Dev EM score: 0.492976
INFO:root:End of epoch 11
INFO:root:epoch 11, iter 9505, loss 2.96765, smoothed loss 2.84947, grad norm 3.11769, param norm 179.02872
INFO:root:epoch 11, iter 9510, loss 3.03757, smoothed loss 2.84261, grad norm 3.36906, param norm 179.06187
INFO:root:epoch 11, iter 9515, loss 2.21372, smoothed loss 2.84452, grad norm 2.73913, param norm 179.09299
INFO:root:epoch 11, iter 9520, loss 3.24886, smoothed loss 2.84232, grad norm 3.28632, param norm 179.12746
INFO:root:epoch 11, iter 9525, loss 2.69951, smoothed loss 2.83264, grad norm 3.03963, param norm 179.15997
INFO:root:epoch 11, iter 9530, loss 2.46288, smoothed loss 2.82207, grad norm 2.72940, param norm 179.19199
INFO:root:epoch 11, iter 9535, loss 2.04878, smoothed loss 2.81637, grad norm 2.71347, param norm 179.22610
INFO:root:epoch 11, iter 9540, loss 2.65639, smoothed loss 2.81179, grad norm 3.16949, param norm 179.26161
INFO:root:epoch 11, iter 9545, loss 2.77777, smoothed loss 2.82026, grad norm 3.08408, param norm 179.30170
INFO:root:epoch 11, iter 9550, loss 2.46355, smoothed loss 2.82353, grad norm 3.55594, param norm 179.34074
INFO:root:epoch 11, iter 9555, loss 2.76185, smoothed loss 2.82196, grad norm 3.07067, param norm 179.37721
INFO:root:epoch 11, iter 9560, loss 3.16162, smoothed loss 2.82401, grad norm 3.41940, param norm 179.41125
INFO:root:epoch 11, iter 9565, loss 2.87597, smoothed loss 2.82990, grad norm 3.36156, param norm 179.44125
INFO:root:epoch 11, iter 9570, loss 3.21621, smoothed loss 2.84181, grad norm 2.96307, param norm 179.47453
INFO:root:epoch 11, iter 9575, loss 2.78100, smoothed loss 2.84916, grad norm 3.10527, param norm 179.51094
INFO:root:epoch 11, iter 9580, loss 2.62795, smoothed loss 2.84327, grad norm 2.83633, param norm 179.55540
INFO:root:epoch 11, iter 9585, loss 2.70716, smoothed loss 2.84309, grad norm 2.95025, param norm 179.60120
INFO:root:epoch 11, iter 9590, loss 2.33684, smoothed loss 2.83756, grad norm 2.87684, param norm 179.63980
INFO:root:epoch 11, iter 9595, loss 3.22040, smoothed loss 2.84285, grad norm 3.30793, param norm 179.67352
INFO:root:epoch 11, iter 9600, loss 3.08265, smoothed loss 2.85497, grad norm 2.97880, param norm 179.70294
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9605, loss 2.76250, smoothed loss 2.86450, grad norm 2.97578, param norm 179.72737
INFO:root:epoch 11, iter 9610, loss 2.73338, smoothed loss 2.85787, grad norm 2.74010, param norm 179.75314
INFO:root:epoch 11, iter 9615, loss 3.24387, smoothed loss 2.86403, grad norm 3.37634, param norm 179.78456
INFO:root:epoch 11, iter 9620, loss 3.03605, smoothed loss 2.86246, grad norm 3.00062, param norm 179.82083
INFO:root:epoch 11, iter 9625, loss 2.96962, smoothed loss 2.85862, grad norm 2.59246, param norm 179.85846
INFO:root:epoch 11, iter 9630, loss 3.04839, smoothed loss 2.85268, grad norm 2.89939, param norm 179.89667
INFO:root:epoch 11, iter 9635, loss 2.50239, smoothed loss 2.85893, grad norm 3.12783, param norm 179.93294
INFO:root:epoch 11, iter 9640, loss 3.14211, smoothed loss 2.85487, grad norm 4.08026, param norm 179.96951
INFO:root:epoch 11, iter 9645, loss 2.59691, smoothed loss 2.85593, grad norm 2.69113, param norm 180.00490
INFO:root:epoch 11, iter 9650, loss 3.63721, smoothed loss 2.85760, grad norm 3.25557, param norm 180.03897
INFO:root:epoch 11, iter 9655, loss 2.96485, smoothed loss 2.85916, grad norm 3.07018, param norm 180.06656
INFO:root:epoch 11, iter 9660, loss 3.28361, smoothed loss 2.87270, grad norm 2.84608, param norm 180.09041
INFO:root:epoch 11, iter 9665, loss 2.79435, smoothed loss 2.87287, grad norm 2.90126, param norm 180.12065
INFO:root:epoch 11, iter 9670, loss 2.95666, smoothed loss 2.87897, grad norm 2.89778, param norm 180.15121
INFO:root:epoch 11, iter 9675, loss 3.12074, smoothed loss 2.88661, grad norm 2.99827, param norm 180.18251
INFO:root:epoch 11, iter 9680, loss 2.37590, smoothed loss 2.89044, grad norm 2.93334, param norm 180.21455
INFO:root:epoch 11, iter 9685, loss 3.39697, smoothed loss 2.89909, grad norm 2.87124, param norm 180.24777
INFO:root:epoch 11, iter 9690, loss 2.74309, smoothed loss 2.89476, grad norm 2.94961, param norm 180.28207
INFO:root:epoch 11, iter 9695, loss 3.01233, smoothed loss 2.89796, grad norm 2.92303, param norm 180.31812
INFO:root:epoch 11, iter 9700, loss 2.02370, smoothed loss 2.88152, grad norm 2.62108, param norm 180.35405
INFO:root:epoch 11, iter 9705, loss 2.57927, smoothed loss 2.86706, grad norm 2.66878, param norm 180.38927
INFO:root:epoch 11, iter 9710, loss 3.23494, smoothed loss 2.87171, grad norm 3.49516, param norm 180.41805
INFO:root:epoch 11, iter 9715, loss 2.43763, smoothed loss 2.87306, grad norm 3.00430, param norm 180.44905
INFO:root:epoch 11, iter 9720, loss 2.25186, smoothed loss 2.86823, grad norm 3.02772, param norm 180.48096
INFO:root:epoch 11, iter 9725, loss 3.10085, smoothed loss 2.87325, grad norm 3.21524, param norm 180.51468
INFO:root:epoch 11, iter 9730, loss 2.41954, smoothed loss 2.87037, grad norm 2.63943, param norm 180.54810
INFO:root:epoch 11, iter 9735, loss 2.30983, smoothed loss 2.86267, grad norm 2.78420, param norm 180.58238
INFO:root:epoch 11, iter 9740, loss 2.57514, smoothed loss 2.85195, grad norm 2.90309, param norm 180.61890
INFO:root:epoch 11, iter 9745, loss 2.69254, smoothed loss 2.84465, grad norm 3.03787, param norm 180.65472
INFO:root:epoch 11, iter 9750, loss 2.91042, smoothed loss 2.85048, grad norm 2.77829, param norm 180.69058
INFO:root:epoch 11, iter 9755, loss 2.32631, smoothed loss 2.84268, grad norm 2.87271, param norm 180.72110
INFO:root:epoch 11, iter 9760, loss 2.64856, smoothed loss 2.84483, grad norm 3.23201, param norm 180.74905
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9765, loss 3.41296, smoothed loss 2.84375, grad norm 3.14528, param norm 180.78111
INFO:root:epoch 11, iter 9770, loss 3.14338, smoothed loss 2.86180, grad norm 3.05987, param norm 180.81184
INFO:root:epoch 11, iter 9775, loss 2.96211, smoothed loss 2.84868, grad norm 3.03201, param norm 180.84186
INFO:root:epoch 11, iter 9780, loss 3.32482, smoothed loss 2.84653, grad norm 3.32577, param norm 180.87369
INFO:root:epoch 11, iter 9785, loss 2.75863, smoothed loss 2.83935, grad norm 2.75501, param norm 180.90704
INFO:root:epoch 11, iter 9790, loss 2.71685, smoothed loss 2.83709, grad norm 3.21230, param norm 180.94011
INFO:root:epoch 11, iter 9795, loss 2.70069, smoothed loss 2.83468, grad norm 2.92711, param norm 180.97253
INFO:root:epoch 11, iter 9800, loss 3.04420, smoothed loss 2.83970, grad norm 3.11111, param norm 181.00725
INFO:root:epoch 11, iter 9805, loss 2.76378, smoothed loss 2.83878, grad norm 2.91676, param norm 181.04643
INFO:root:epoch 11, iter 9810, loss 2.64863, smoothed loss 2.82208, grad norm 3.09976, param norm 181.08420
INFO:root:epoch 11, iter 9815, loss 2.44420, smoothed loss 2.81044, grad norm 3.20274, param norm 181.12096
INFO:root:epoch 11, iter 9820, loss 2.15323, smoothed loss 2.80386, grad norm 2.42810, param norm 181.15855
INFO:root:epoch 11, iter 9825, loss 2.76178, smoothed loss 2.80545, grad norm 3.13545, param norm 181.19293
INFO:root:epoch 11, iter 9830, loss 2.62969, smoothed loss 2.80296, grad norm 3.16095, param norm 181.22351
INFO:root:epoch 11, iter 9835, loss 3.02704, smoothed loss 2.80675, grad norm 3.37696, param norm 181.25842
INFO:root:epoch 11, iter 9840, loss 2.58789, smoothed loss 2.80669, grad norm 2.74286, param norm 181.29344
INFO:root:epoch 11, iter 9845, loss 2.83138, smoothed loss 2.80747, grad norm 2.77868, param norm 181.33070
INFO:root:epoch 11, iter 9850, loss 2.20138, smoothed loss 2.81346, grad norm 2.95982, param norm 181.37143
INFO:root:epoch 11, iter 9855, loss 2.74179, smoothed loss 2.81260, grad norm 2.96572, param norm 181.40851
INFO:root:epoch 11, iter 9860, loss 2.75960, smoothed loss 2.81038, grad norm 2.81792, param norm 181.44217
INFO:root:epoch 11, iter 9865, loss 2.82300, smoothed loss 2.80878, grad norm 3.22393, param norm 181.47314
INFO:root:epoch 11, iter 9870, loss 3.04700, smoothed loss 2.82418, grad norm 3.54097, param norm 181.50214
INFO:root:epoch 11, iter 9875, loss 2.08065, smoothed loss 2.81677, grad norm 2.80947, param norm 181.53542
INFO:root:epoch 11, iter 9880, loss 2.67119, smoothed loss 2.80468, grad norm 2.72343, param norm 181.57010
INFO:root:epoch 11, iter 9885, loss 2.21399, smoothed loss 2.79680, grad norm 2.58497, param norm 181.60312
INFO:root:epoch 11, iter 9890, loss 3.06542, smoothed loss 2.79801, grad norm 2.94771, param norm 181.63641
INFO:root:epoch 11, iter 9895, loss 2.95244, smoothed loss 2.82147, grad norm 3.10630, param norm 181.67055
INFO:root:epoch 11, iter 9900, loss 2.64447, smoothed loss 2.81829, grad norm 3.55727, param norm 181.70331
INFO:root:epoch 11, iter 9905, loss 3.05371, smoothed loss 2.81481, grad norm 3.49814, param norm 181.73892
INFO:root:epoch 11, iter 9910, loss 2.32801, smoothed loss 2.81771, grad norm 3.02844, param norm 181.77132
INFO:root:epoch 11, iter 9915, loss 3.65726, smoothed loss 2.81086, grad norm 3.42621, param norm 181.80435
INFO:root:epoch 11, iter 9920, loss 2.58348, smoothed loss 2.81064, grad norm 2.52320, param norm 181.83783
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 9925, loss 2.82346, smoothed loss 2.80033, grad norm 2.95877, param norm 181.87021
INFO:root:epoch 11, iter 9930, loss 2.93321, smoothed loss 2.79867, grad norm 3.16691, param norm 181.90479
INFO:root:epoch 11, iter 9935, loss 3.16717, smoothed loss 2.79549, grad norm 3.46540, param norm 181.94128
INFO:root:epoch 11, iter 9940, loss 2.10992, smoothed loss 2.79671, grad norm 2.51281, param norm 181.97911
INFO:root:epoch 11, iter 9945, loss 2.84960, smoothed loss 2.79972, grad norm 3.53920, param norm 182.01466
INFO:root:epoch 11, iter 9950, loss 2.70952, smoothed loss 2.78856, grad norm 2.97572, param norm 182.05020
INFO:root:epoch 11, iter 9955, loss 1.97594, smoothed loss 2.77641, grad norm 3.04915, param norm 182.08492
INFO:root:epoch 11, iter 9960, loss 1.95087, smoothed loss 2.76154, grad norm 3.22110, param norm 182.11966
INFO:root:epoch 11, iter 9965, loss 2.51482, smoothed loss 2.76312, grad norm 2.91283, param norm 182.15593
INFO:root:epoch 11, iter 9970, loss 2.76195, smoothed loss 2.77153, grad norm 2.96392, param norm 182.18837
INFO:root:epoch 11, iter 9975, loss 3.06682, smoothed loss 2.77676, grad norm 3.47939, param norm 182.22044
INFO:root:epoch 11, iter 9980, loss 1.81050, smoothed loss 2.76681, grad norm 3.05652, param norm 182.25809
INFO:root:epoch 11, iter 9985, loss 2.85858, smoothed loss 2.77035, grad norm 3.14368, param norm 182.29745
INFO:root:epoch 11, iter 9990, loss 2.95498, smoothed loss 2.77584, grad norm 2.82224, param norm 182.33203
INFO:root:epoch 11, iter 9995, loss 2.69049, smoothed loss 2.77090, grad norm 3.07503, param norm 182.36125
INFO:root:epoch 11, iter 10000, loss 2.52928, smoothed loss 2.77081, grad norm 2.66392, param norm 182.38802
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 11, Iter 10000, dev loss: 3.066006
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.96359 seconds [Score: 0.74565]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.74120 seconds [Score: 0.60800]
INFO:root:Epoch 11, Iter 10000, Train F1 score: 0.745654, Train EM score: 0.608000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 116.95378 seconds [Score: 0.64126]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.33614 seconds [Score: 0.49382]
INFO:root:Epoch 11, Iter 10000, Dev F1 score: 0.641265, Dev EM score: 0.493818
INFO:root:End of epoch 11
INFO:root:epoch 11, iter 10005, loss 2.71759, smoothed loss 2.77317, grad norm 3.31627, param norm 182.41469
INFO:root:epoch 11, iter 10010, loss 3.02199, smoothed loss 2.77811, grad norm 3.29142, param norm 182.44386
INFO:root:epoch 11, iter 10015, loss 2.38811, smoothed loss 2.77922, grad norm 2.63412, param norm 182.47493
INFO:root:epoch 11, iter 10020, loss 3.03964, smoothed loss 2.80166, grad norm 3.21954, param norm 182.50711
INFO:root:epoch 11, iter 10025, loss 2.49924, smoothed loss 2.80234, grad norm 2.61995, param norm 182.54147
INFO:root:epoch 11, iter 10030, loss 2.46191, smoothed loss 2.79362, grad norm 2.92461, param norm 182.57741
INFO:root:epoch 11, iter 10035, loss 2.28882, smoothed loss 2.78969, grad norm 2.75073, param norm 182.61140
INFO:root:epoch 11, iter 10040, loss 2.25773, smoothed loss 2.78427, grad norm 2.92150, param norm 182.64227
INFO:root:epoch 11, iter 10045, loss 3.13046, smoothed loss 2.77754, grad norm 3.10825, param norm 182.67667
INFO:root:epoch 11, iter 10050, loss 2.49527, smoothed loss 2.77688, grad norm 3.27273, param norm 182.71436
INFO:root:epoch 11, iter 10055, loss 2.93188, smoothed loss 2.77467, grad norm 3.08412, param norm 182.74966
INFO:root:epoch 11, iter 10060, loss 2.59003, smoothed loss 2.76508, grad norm 2.96513, param norm 182.78300
INFO:root:epoch 11, iter 10065, loss 2.14081, smoothed loss 2.76187, grad norm 3.23287, param norm 182.81750
INFO:root:epoch 11, iter 10070, loss 2.92972, smoothed loss 2.75055, grad norm 3.50045, param norm 182.85237
INFO:root:epoch 11, iter 10075, loss 2.43422, smoothed loss 2.74347, grad norm 2.80729, param norm 182.88666
INFO:root:epoch 11, iter 10080, loss 2.61328, smoothed loss 2.75318, grad norm 2.83886, param norm 182.91920
Adding batches start...
Added  160  batches
INFO:root:epoch 11, iter 10085, loss 2.61311, smoothed loss 2.75311, grad norm 2.79261, param norm 182.94949
INFO:root:epoch 11, iter 10090, loss 2.54266, smoothed loss 2.75483, grad norm 2.98530, param norm 182.98215
INFO:root:epoch 11, iter 10095, loss 2.53483, smoothed loss 2.76103, grad norm 3.00673, param norm 183.01729
INFO:root:epoch 11, iter 10100, loss 3.13559, smoothed loss 2.76655, grad norm 3.15437, param norm 183.05443
INFO:root:epoch 11, iter 10105, loss 2.49571, smoothed loss 2.76107, grad norm 2.74893, param norm 183.08792
INFO:root:epoch 11, iter 10110, loss 2.50527, smoothed loss 2.76381, grad norm 2.81164, param norm 183.11719
INFO:root:epoch 11, iter 10115, loss 2.87418, smoothed loss 2.75852, grad norm 3.52993, param norm 183.14839
INFO:root:epoch 11, iter 10120, loss 2.66175, smoothed loss 2.76389, grad norm 2.73793, param norm 183.18134
INFO:root:epoch 11, iter 10125, loss 3.42876, smoothed loss 2.78005, grad norm 3.11299, param norm 183.21562
INFO:root:epoch 11, iter 10130, loss 3.40287, smoothed loss 2.79161, grad norm 3.32567, param norm 183.24808
INFO:root:epoch 11, iter 10135, loss 2.90712, smoothed loss 2.79597, grad norm 3.13276, param norm 183.27910
INFO:root:epoch 11, iter 10140, loss 3.23278, smoothed loss 2.79461, grad norm 3.07384, param norm 183.31178
INFO:root:epoch 11, iter 10145, loss 2.76761, smoothed loss 2.80085, grad norm 2.98746, param norm 183.34622
INFO:root:epoch 11, iter 10150, loss 2.83082, smoothed loss 2.80932, grad norm 3.46912, param norm 183.38098
INFO:root:epoch 11, iter 10155, loss 3.27287, smoothed loss 2.81604, grad norm 3.32076, param norm 183.41557
INFO:root:epoch 11, iter 10160, loss 3.61441, smoothed loss 2.83061, grad norm 3.03843, param norm 183.45242
INFO:root:epoch 11, iter 10165, loss 2.89665, smoothed loss 2.82549, grad norm 3.14240, param norm 183.48615
INFO:root:epoch 11, iter 10170, loss 3.22721, smoothed loss 2.82862, grad norm 2.95127, param norm 183.52271
INFO:root:epoch 11, iter 10175, loss 2.53023, smoothed loss 2.83245, grad norm 2.85409, param norm 183.55890
INFO:root:epoch 11, iter 10180, loss 2.51508, smoothed loss 2.84974, grad norm 2.91786, param norm 183.59204
INFO:root:epoch 11, iter 10185, loss 2.28798, smoothed loss 2.84551, grad norm 2.84900, param norm 183.62253
INFO:root:epoch 11, iter 10190, loss 2.48732, smoothed loss 2.83581, grad norm 3.32261, param norm 183.65173
INFO:root:epoch 11, iter 10195, loss 2.95229, smoothed loss 2.83146, grad norm 3.12364, param norm 183.68134
INFO:root:epoch 11, iter 10200, loss 2.43278, smoothed loss 2.83923, grad norm 2.71889, param norm 183.71417
INFO:root:epoch 11, iter 10205, loss 2.80912, smoothed loss 2.82744, grad norm 3.08471, param norm 183.74536
INFO:root:epoch 11, iter 10210, loss 3.45354, smoothed loss 2.82729, grad norm 3.72998, param norm 183.77953
INFO:root:epoch 11, iter 10215, loss 3.23666, smoothed loss 2.82645, grad norm 3.18467, param norm 183.81700
INFO:root:epoch 11, iter 10220, loss 2.79903, smoothed loss 2.82092, grad norm 2.92072, param norm 183.85312
INFO:root:epoch 11, iter 10225, loss 2.82980, smoothed loss 2.81471, grad norm 3.20041, param norm 183.88860
INFO:root:epoch 11, iter 10230, loss 2.27471, smoothed loss 2.80308, grad norm 2.35201, param norm 183.92377
INFO:root:epoch 11, iter 10235, loss 2.26492, smoothed loss 2.79815, grad norm 2.54163, param norm 183.95859
INFO:root:epoch 11, iter 10240, loss 3.01591, smoothed loss 2.80198, grad norm 3.01375, param norm 183.99051
Adding batches start...
Added  144  batches
INFO:root:epoch 11, iter 10245, loss 2.58371, smoothed loss 2.78832, grad norm 2.83137, param norm 184.02180
INFO:root:epoch 11, iter 10250, loss 2.62533, smoothed loss 2.78478, grad norm 2.47630, param norm 184.05119
INFO:root:epoch 11, iter 10255, loss 2.55405, smoothed loss 2.78993, grad norm 3.20313, param norm 184.07870
INFO:root:epoch 11, iter 10260, loss 2.65898, smoothed loss 2.79231, grad norm 3.45485, param norm 184.10837
INFO:root:epoch 11, iter 10265, loss 3.15884, smoothed loss 2.78971, grad norm 3.21948, param norm 184.14337
INFO:root:epoch 11, iter 10270, loss 3.11017, smoothed loss 2.78856, grad norm 3.38449, param norm 184.17830
INFO:root:epoch 11, iter 10275, loss 3.09866, smoothed loss 2.80109, grad norm 3.21110, param norm 184.21451
INFO:root:epoch 11, iter 10280, loss 2.88099, smoothed loss 2.80650, grad norm 3.33681, param norm 184.25034
INFO:root:epoch 11, iter 10285, loss 3.03256, smoothed loss 2.79913, grad norm 3.24546, param norm 184.28609
INFO:root:epoch 11, iter 10290, loss 3.17618, smoothed loss 2.81118, grad norm 2.79922, param norm 184.32481
INFO:root:epoch 11, iter 10295, loss 2.81685, smoothed loss 2.79277, grad norm 3.37012, param norm 184.36330
INFO:root:epoch 11, iter 10300, loss 3.61174, smoothed loss 2.80018, grad norm 4.19709, param norm 184.40166
INFO:root:epoch 11, iter 10305, loss 2.85236, smoothed loss 2.80127, grad norm 2.76334, param norm 184.43451
INFO:root:epoch 11, iter 10310, loss 3.09780, smoothed loss 2.81111, grad norm 3.12310, param norm 184.46527
INFO:root:epoch 11, iter 10315, loss 2.80234, smoothed loss 2.80932, grad norm 2.97611, param norm 184.49911
INFO:root:epoch 11, iter 10320, loss 2.37071, smoothed loss 2.79679, grad norm 2.97706, param norm 184.53418
INFO:root:epoch 11, iter 10325, loss 2.61317, smoothed loss 2.79535, grad norm 2.91933, param norm 184.57117
INFO:root:epoch 11, iter 10330, loss 3.36360, smoothed loss 2.80769, grad norm 3.36389, param norm 184.60553
INFO:root:epoch 11, iter 10335, loss 2.52897, smoothed loss 2.79618, grad norm 2.69939, param norm 184.63609
INFO:root:epoch 11, iter 10340, loss 2.79709, smoothed loss 2.79655, grad norm 3.08223, param norm 184.66533
INFO:root:epoch 11, iter 10345, loss 2.00488, smoothed loss 2.78217, grad norm 3.21445, param norm 184.69644
INFO:root:epoch 11, iter 10350, loss 2.43374, smoothed loss 2.78464, grad norm 2.87158, param norm 184.72534
INFO:root:epoch 11, iter 10355, loss 2.97749, smoothed loss 2.79251, grad norm 3.27438, param norm 184.75380
INFO:root:epoch 11, iter 10360, loss 3.07976, smoothed loss 2.79517, grad norm 3.01948, param norm 184.78255
INFO:root:epoch 11, iter 10365, loss 2.93782, smoothed loss 2.79433, grad norm 2.90423, param norm 184.81444
INFO:root:epoch 11, iter 10370, loss 2.27555, smoothed loss 2.79383, grad norm 2.89791, param norm 184.84276
INFO:root:epoch 11, iter 10375, loss 3.11650, smoothed loss 2.79009, grad norm 3.38174, param norm 184.87253
INFO:root:epoch 11, iter 10380, loss 2.85780, smoothed loss 2.78708, grad norm 2.98804, param norm 184.90764
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 12, iter 10385, loss 3.58274, smoothed loss 2.80003, grad norm 3.69888, param norm 184.94467
INFO:root:epoch 12, iter 10390, loss 2.58146, smoothed loss 2.79609, grad norm 3.28345, param norm 184.98105
INFO:root:epoch 12, iter 10395, loss 2.98577, smoothed loss 2.80979, grad norm 2.85911, param norm 185.01541
INFO:root:epoch 12, iter 10400, loss 2.44103, smoothed loss 2.80218, grad norm 2.82519, param norm 185.04578
INFO:root:epoch 12, iter 10405, loss 3.15310, smoothed loss 2.80482, grad norm 2.95287, param norm 185.07452
INFO:root:epoch 12, iter 10410, loss 3.33126, smoothed loss 2.80131, grad norm 3.71522, param norm 185.10535
INFO:root:epoch 12, iter 10415, loss 2.92887, smoothed loss 2.80708, grad norm 2.83970, param norm 185.13953
INFO:root:epoch 12, iter 10420, loss 2.87664, smoothed loss 2.81538, grad norm 3.00637, param norm 185.17233
INFO:root:epoch 12, iter 10425, loss 2.08341, smoothed loss 2.80230, grad norm 2.77452, param norm 185.20779
INFO:root:epoch 12, iter 10430, loss 2.97469, smoothed loss 2.80401, grad norm 3.62404, param norm 185.24098
INFO:root:epoch 12, iter 10435, loss 2.83542, smoothed loss 2.80368, grad norm 3.34125, param norm 185.27092
INFO:root:epoch 12, iter 10440, loss 3.06441, smoothed loss 2.80370, grad norm 3.27779, param norm 185.30243
INFO:root:epoch 12, iter 10445, loss 2.70315, smoothed loss 2.80173, grad norm 2.95537, param norm 185.33179
INFO:root:epoch 12, iter 10450, loss 2.49888, smoothed loss 2.81422, grad norm 2.80680, param norm 185.36105
INFO:root:epoch 12, iter 10455, loss 2.41789, smoothed loss 2.81027, grad norm 3.05109, param norm 185.39282
INFO:root:epoch 12, iter 10460, loss 2.98196, smoothed loss 2.80038, grad norm 3.19336, param norm 185.42761
INFO:root:epoch 12, iter 10465, loss 2.44390, smoothed loss 2.78123, grad norm 2.76976, param norm 185.46443
INFO:root:epoch 12, iter 10470, loss 2.83218, smoothed loss 2.78498, grad norm 3.52320, param norm 185.50122
INFO:root:epoch 12, iter 10475, loss 2.20937, smoothed loss 2.77282, grad norm 2.60844, param norm 185.53651
INFO:root:epoch 12, iter 10480, loss 2.67134, smoothed loss 2.77117, grad norm 2.91285, param norm 185.57050
INFO:root:epoch 12, iter 10485, loss 3.05565, smoothed loss 2.77189, grad norm 3.16648, param norm 185.60423
INFO:root:epoch 12, iter 10490, loss 2.90029, smoothed loss 2.78015, grad norm 3.25589, param norm 185.63684
INFO:root:epoch 12, iter 10495, loss 3.03782, smoothed loss 2.78217, grad norm 3.10101, param norm 185.67206
INFO:root:epoch 12, iter 10500, loss 3.01253, smoothed loss 2.78091, grad norm 3.28174, param norm 185.70816
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 12, Iter 10500, dev loss: 3.082630
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.75002 seconds [Score: 0.76451]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.84656 seconds [Score: 0.66000]
INFO:root:Epoch 12, Iter 10500, Train F1 score: 0.764514, Train EM score: 0.660000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 116.18358 seconds [Score: 0.63872]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.97913 seconds [Score: 0.49185]
INFO:root:Epoch 12, Iter 10500, Dev F1 score: 0.638718, Dev EM score: 0.491852
INFO:root:End of epoch 12
INFO:root:epoch 12, iter 10505, loss 2.28129, smoothed loss 2.76822, grad norm 2.80123, param norm 185.74173
INFO:root:epoch 12, iter 10510, loss 2.73658, smoothed loss 2.77432, grad norm 2.81545, param norm 185.77142
INFO:root:epoch 12, iter 10515, loss 2.99916, smoothed loss 2.78468, grad norm 3.28846, param norm 185.80006
INFO:root:epoch 12, iter 10520, loss 2.16016, smoothed loss 2.77382, grad norm 3.17470, param norm 185.83127
INFO:root:epoch 12, iter 10525, loss 2.49653, smoothed loss 2.77841, grad norm 2.82812, param norm 185.86121
INFO:root:epoch 12, iter 10530, loss 2.76934, smoothed loss 2.77163, grad norm 2.92828, param norm 185.88957
INFO:root:epoch 12, iter 10535, loss 2.73540, smoothed loss 2.76984, grad norm 3.12009, param norm 185.91740
INFO:root:epoch 12, iter 10540, loss 2.73175, smoothed loss 2.77237, grad norm 2.73756, param norm 185.94684
Adding batches start...
Added  160  batches
INFO:root:epoch 12, iter 10545, loss 2.20001, smoothed loss 2.75827, grad norm 3.07093, param norm 185.97800
INFO:root:epoch 12, iter 10550, loss 2.87702, smoothed loss 2.75009, grad norm 2.88938, param norm 186.01332
INFO:root:epoch 12, iter 10555, loss 2.70721, smoothed loss 2.75205, grad norm 3.13662, param norm 186.04663
INFO:root:epoch 12, iter 10560, loss 3.14749, smoothed loss 2.74495, grad norm 3.22304, param norm 186.08092
INFO:root:epoch 12, iter 10565, loss 3.31151, smoothed loss 2.74262, grad norm 3.51307, param norm 186.11470
INFO:root:epoch 12, iter 10570, loss 2.12889, smoothed loss 2.74289, grad norm 2.58637, param norm 186.14429
INFO:root:epoch 12, iter 10575, loss 2.52752, smoothed loss 2.72945, grad norm 3.25111, param norm 186.17764
INFO:root:epoch 12, iter 10580, loss 2.82794, smoothed loss 2.73027, grad norm 3.10144, param norm 186.21275
INFO:root:epoch 12, iter 10585, loss 2.45668, smoothed loss 2.73479, grad norm 3.22621, param norm 186.24858
INFO:root:epoch 12, iter 10590, loss 3.27338, smoothed loss 2.73967, grad norm 3.20562, param norm 186.28165
INFO:root:epoch 12, iter 10595, loss 2.54098, smoothed loss 2.73186, grad norm 2.90417, param norm 186.30997
INFO:root:epoch 12, iter 10600, loss 3.13188, smoothed loss 2.74150, grad norm 3.41625, param norm 186.34244
INFO:root:epoch 12, iter 10605, loss 2.74568, smoothed loss 2.74460, grad norm 3.21246, param norm 186.37633
INFO:root:epoch 12, iter 10610, loss 3.25222, smoothed loss 2.73713, grad norm 3.40118, param norm 186.41203
INFO:root:epoch 12, iter 10615, loss 2.81216, smoothed loss 2.74296, grad norm 3.16502, param norm 186.44879
INFO:root:epoch 12, iter 10620, loss 2.34621, smoothed loss 2.74182, grad norm 3.04097, param norm 186.48256
INFO:root:epoch 12, iter 10625, loss 2.86685, smoothed loss 2.74157, grad norm 3.13303, param norm 186.51952
INFO:root:epoch 12, iter 10630, loss 3.21061, smoothed loss 2.76705, grad norm 3.43453, param norm 186.55289
INFO:root:epoch 12, iter 10635, loss 2.06309, smoothed loss 2.76474, grad norm 2.63839, param norm 186.58643
INFO:root:epoch 12, iter 10640, loss 2.85885, smoothed loss 2.76814, grad norm 2.67726, param norm 186.62276
INFO:root:epoch 12, iter 10645, loss 3.43730, smoothed loss 2.76783, grad norm 3.25243, param norm 186.65773
INFO:root:epoch 12, iter 10650, loss 2.74755, smoothed loss 2.77019, grad norm 2.95353, param norm 186.68857
INFO:root:epoch 12, iter 10655, loss 2.32139, smoothed loss 2.76821, grad norm 2.65313, param norm 186.71922
INFO:root:epoch 12, iter 10660, loss 2.73474, smoothed loss 2.76277, grad norm 3.28765, param norm 186.74890
INFO:root:epoch 12, iter 10665, loss 2.60940, smoothed loss 2.76428, grad norm 2.94533, param norm 186.78166
INFO:root:epoch 12, iter 10670, loss 2.67797, smoothed loss 2.75686, grad norm 3.07598, param norm 186.81540
INFO:root:epoch 12, iter 10675, loss 2.63102, smoothed loss 2.75326, grad norm 2.82791, param norm 186.85167
INFO:root:epoch 12, iter 10680, loss 2.48572, smoothed loss 2.75728, grad norm 2.91736, param norm 186.88464
INFO:root:epoch 12, iter 10685, loss 2.84801, smoothed loss 2.75762, grad norm 3.49781, param norm 186.91992
INFO:root:epoch 12, iter 10690, loss 2.52660, smoothed loss 2.75640, grad norm 3.05440, param norm 186.95508
INFO:root:epoch 12, iter 10695, loss 2.52332, smoothed loss 2.75525, grad norm 2.60958, param norm 186.98984
INFO:root:epoch 12, iter 10700, loss 2.78258, smoothed loss 2.75434, grad norm 3.49984, param norm 187.02354
Adding batches start...
Added  160  batches
INFO:root:epoch 12, iter 10705, loss 2.56144, smoothed loss 2.76547, grad norm 3.04842, param norm 187.05634
INFO:root:epoch 12, iter 10710, loss 2.58808, smoothed loss 2.76217, grad norm 2.70829, param norm 187.08888
INFO:root:epoch 12, iter 10715, loss 1.74701, smoothed loss 2.75911, grad norm 2.58562, param norm 187.12396
INFO:root:epoch 12, iter 10720, loss 2.40705, smoothed loss 2.74365, grad norm 2.94696, param norm 187.16458
INFO:root:epoch 12, iter 10725, loss 3.10781, smoothed loss 2.74246, grad norm 3.71881, param norm 187.20294
INFO:root:epoch 12, iter 10730, loss 2.99837, smoothed loss 2.75571, grad norm 3.13133, param norm 187.23692
INFO:root:epoch 12, iter 10735, loss 2.81440, smoothed loss 2.74746, grad norm 3.05541, param norm 187.27197
INFO:root:epoch 12, iter 10740, loss 2.99704, smoothed loss 2.75126, grad norm 3.20812, param norm 187.30586
INFO:root:epoch 12, iter 10745, loss 2.27367, smoothed loss 2.75818, grad norm 2.82496, param norm 187.33553
INFO:root:epoch 12, iter 10750, loss 2.27063, smoothed loss 2.74374, grad norm 2.65012, param norm 187.36636
INFO:root:epoch 12, iter 10755, loss 2.64019, smoothed loss 2.74082, grad norm 2.87525, param norm 187.39627
INFO:root:epoch 12, iter 10760, loss 2.70817, smoothed loss 2.75143, grad norm 3.11143, param norm 187.42519
INFO:root:epoch 12, iter 10765, loss 2.91542, smoothed loss 2.75748, grad norm 3.31591, param norm 187.45667
INFO:root:epoch 12, iter 10770, loss 3.15849, smoothed loss 2.75608, grad norm 2.93993, param norm 187.48871
INFO:root:epoch 12, iter 10775, loss 2.65359, smoothed loss 2.76576, grad norm 2.92927, param norm 187.51805
INFO:root:epoch 12, iter 10780, loss 2.65725, smoothed loss 2.76675, grad norm 2.67529, param norm 187.54582
INFO:root:epoch 12, iter 10785, loss 2.83894, smoothed loss 2.77579, grad norm 2.93101, param norm 187.57521
INFO:root:epoch 12, iter 10790, loss 2.94095, smoothed loss 2.78988, grad norm 3.16354, param norm 187.60530
INFO:root:epoch 12, iter 10795, loss 3.25306, smoothed loss 2.79300, grad norm 3.24663, param norm 187.63927
INFO:root:epoch 12, iter 10800, loss 2.33845, smoothed loss 2.80038, grad norm 2.92559, param norm 187.67284
INFO:root:epoch 12, iter 10805, loss 3.01828, smoothed loss 2.79538, grad norm 3.37283, param norm 187.70750
INFO:root:epoch 12, iter 10810, loss 2.15956, smoothed loss 2.79944, grad norm 2.66090, param norm 187.74066
INFO:root:epoch 12, iter 10815, loss 3.19400, smoothed loss 2.79612, grad norm 3.58579, param norm 187.77342
INFO:root:epoch 12, iter 10820, loss 2.02347, smoothed loss 2.77362, grad norm 2.66381, param norm 187.80510
INFO:root:epoch 12, iter 10825, loss 3.11338, smoothed loss 2.77174, grad norm 3.74092, param norm 187.83673
INFO:root:epoch 12, iter 10830, loss 2.51847, smoothed loss 2.79169, grad norm 2.94109, param norm 187.86670
INFO:root:epoch 12, iter 10835, loss 2.45488, smoothed loss 2.77801, grad norm 2.94787, param norm 187.89589
INFO:root:epoch 12, iter 10840, loss 2.68687, smoothed loss 2.77013, grad norm 3.08572, param norm 187.92511
INFO:root:epoch 12, iter 10845, loss 2.78099, smoothed loss 2.76031, grad norm 3.68956, param norm 187.95390
INFO:root:epoch 12, iter 10850, loss 2.66970, smoothed loss 2.75457, grad norm 3.32349, param norm 187.98279
INFO:root:epoch 12, iter 10855, loss 3.59662, smoothed loss 2.76546, grad norm 3.45798, param norm 188.01373
INFO:root:epoch 12, iter 10860, loss 2.61720, smoothed loss 2.75872, grad norm 2.89932, param norm 188.04712
Adding batches start...
Added  160  batches
INFO:root:epoch 12, iter 10865, loss 2.04500, smoothed loss 2.75078, grad norm 2.66066, param norm 188.08394
INFO:root:epoch 12, iter 10870, loss 2.41964, smoothed loss 2.75398, grad norm 2.79954, param norm 188.11964
INFO:root:epoch 12, iter 10875, loss 3.10485, smoothed loss 2.75931, grad norm 3.06764, param norm 188.15463
INFO:root:epoch 12, iter 10880, loss 3.14532, smoothed loss 2.76579, grad norm 3.58477, param norm 188.18816
INFO:root:epoch 12, iter 10885, loss 2.52018, smoothed loss 2.75837, grad norm 2.84517, param norm 188.22328
INFO:root:epoch 12, iter 10890, loss 2.83375, smoothed loss 2.75900, grad norm 3.21483, param norm 188.25803
INFO:root:epoch 12, iter 10895, loss 2.34132, smoothed loss 2.74398, grad norm 3.18528, param norm 188.29218
INFO:root:epoch 12, iter 10900, loss 2.31413, smoothed loss 2.74145, grad norm 2.59863, param norm 188.32819
INFO:root:epoch 12, iter 10905, loss 3.38937, smoothed loss 2.74982, grad norm 3.46458, param norm 188.35962
INFO:root:epoch 12, iter 10910, loss 2.21043, smoothed loss 2.74716, grad norm 2.98944, param norm 188.38991
INFO:root:epoch 12, iter 10915, loss 2.93767, smoothed loss 2.75881, grad norm 3.01506, param norm 188.42169
INFO:root:epoch 12, iter 10920, loss 1.77474, smoothed loss 2.72621, grad norm 3.05107, param norm 188.45345
INFO:root:epoch 12, iter 10925, loss 3.30477, smoothed loss 2.72999, grad norm 3.67286, param norm 188.48700
INFO:root:epoch 12, iter 10930, loss 2.90639, smoothed loss 2.72003, grad norm 3.69734, param norm 188.51793
INFO:root:epoch 12, iter 10935, loss 2.74843, smoothed loss 2.70812, grad norm 3.33179, param norm 188.54770
INFO:root:epoch 12, iter 10940, loss 3.21625, smoothed loss 2.70881, grad norm 3.44464, param norm 188.58163
INFO:root:epoch 12, iter 10945, loss 2.58911, smoothed loss 2.69347, grad norm 3.13074, param norm 188.61194
INFO:root:epoch 12, iter 10950, loss 3.21447, smoothed loss 2.70202, grad norm 3.66985, param norm 188.64229
INFO:root:epoch 12, iter 10955, loss 2.48631, smoothed loss 2.69750, grad norm 3.01798, param norm 188.67136
INFO:root:epoch 12, iter 10960, loss 2.45879, smoothed loss 2.69882, grad norm 2.95901, param norm 188.70082
INFO:root:epoch 12, iter 10965, loss 2.69692, smoothed loss 2.69276, grad norm 3.06349, param norm 188.73128
INFO:root:epoch 12, iter 10970, loss 2.47180, smoothed loss 2.69760, grad norm 3.31726, param norm 188.76335
INFO:root:epoch 12, iter 10975, loss 3.10482, smoothed loss 2.70660, grad norm 3.57511, param norm 188.79622
INFO:root:epoch 12, iter 10980, loss 2.69431, smoothed loss 2.71105, grad norm 3.58505, param norm 188.83141
INFO:root:epoch 12, iter 10985, loss 2.28194, smoothed loss 2.71899, grad norm 2.54306, param norm 188.86102
INFO:root:epoch 12, iter 10990, loss 2.34429, smoothed loss 2.70308, grad norm 2.68359, param norm 188.88977
INFO:root:epoch 12, iter 10995, loss 2.89295, smoothed loss 2.70506, grad norm 3.44480, param norm 188.92044
INFO:root:epoch 12, iter 11000, loss 3.25616, smoothed loss 2.70657, grad norm 3.44026, param norm 188.94974
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 12, Iter 11000, dev loss: 3.075534
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.49074 seconds [Score: 0.75561]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 16.86603 seconds [Score: 0.60200]
INFO:root:Epoch 12, Iter 11000, Train F1 score: 0.755610, Train EM score: 0.602000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 113.11833 seconds [Score: 0.64404]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 116.62820 seconds [Score: 0.49944]
INFO:root:Epoch 12, Iter 11000, Dev F1 score: 0.644036, Dev EM score: 0.499438
INFO:root:End of epoch 12
INFO:root:epoch 12, iter 11005, loss 3.39067, smoothed loss 2.72316, grad norm 3.41363, param norm 188.97578
INFO:root:epoch 12, iter 11010, loss 2.60226, smoothed loss 2.71528, grad norm 2.92235, param norm 189.00269
INFO:root:epoch 12, iter 11015, loss 2.65952, smoothed loss 2.71170, grad norm 2.85422, param norm 189.03751
INFO:root:epoch 12, iter 11020, loss 2.40956, smoothed loss 2.71845, grad norm 2.95487, param norm 189.07567
Adding batches start...
Added  160  batches
INFO:root:epoch 12, iter 11025, loss 2.38383, smoothed loss 2.70944, grad norm 2.98234, param norm 189.10968
INFO:root:epoch 12, iter 11030, loss 2.57578, smoothed loss 2.70734, grad norm 3.63769, param norm 189.14256
INFO:root:epoch 12, iter 11035, loss 2.96198, smoothed loss 2.70765, grad norm 3.52446, param norm 189.17484
INFO:root:epoch 12, iter 11040, loss 3.22576, smoothed loss 2.71201, grad norm 3.30069, param norm 189.20975
INFO:root:epoch 12, iter 11045, loss 2.35983, smoothed loss 2.72081, grad norm 3.01704, param norm 189.24417
INFO:root:epoch 12, iter 11050, loss 2.44096, smoothed loss 2.71930, grad norm 3.11767, param norm 189.27591
INFO:root:epoch 12, iter 11055, loss 3.27853, smoothed loss 2.71854, grad norm 3.26280, param norm 189.30603
INFO:root:epoch 12, iter 11060, loss 2.81781, smoothed loss 2.71882, grad norm 3.25168, param norm 189.33452
INFO:root:epoch 12, iter 11065, loss 2.79040, smoothed loss 2.72115, grad norm 3.15436, param norm 189.36397
INFO:root:epoch 12, iter 11070, loss 2.47536, smoothed loss 2.73834, grad norm 3.03783, param norm 189.39296
INFO:root:epoch 12, iter 11075, loss 2.44569, smoothed loss 2.73101, grad norm 3.11343, param norm 189.42184
INFO:root:epoch 12, iter 11080, loss 3.39872, smoothed loss 2.72109, grad norm 3.59653, param norm 189.44994
INFO:root:epoch 12, iter 11085, loss 2.97011, smoothed loss 2.73094, grad norm 3.07993, param norm 189.47951
INFO:root:epoch 12, iter 11090, loss 2.97344, smoothed loss 2.74097, grad norm 3.08699, param norm 189.50449
INFO:root:epoch 12, iter 11095, loss 2.75540, smoothed loss 2.73943, grad norm 3.49542, param norm 189.53102
INFO:root:epoch 12, iter 11100, loss 2.77323, smoothed loss 2.73386, grad norm 3.17437, param norm 189.56555
INFO:root:epoch 12, iter 11105, loss 2.68633, smoothed loss 2.73326, grad norm 3.08900, param norm 189.60045
INFO:root:epoch 12, iter 11110, loss 2.45385, smoothed loss 2.74118, grad norm 2.96183, param norm 189.63472
INFO:root:epoch 12, iter 11115, loss 3.11619, smoothed loss 2.75848, grad norm 3.05960, param norm 189.66846
INFO:root:epoch 12, iter 11120, loss 2.45046, smoothed loss 2.76363, grad norm 2.97089, param norm 189.70084
INFO:root:epoch 12, iter 11125, loss 3.30991, smoothed loss 2.78230, grad norm 3.21864, param norm 189.73109
INFO:root:epoch 12, iter 11130, loss 2.11948, smoothed loss 2.75989, grad norm 3.05279, param norm 189.76079
INFO:root:epoch 12, iter 11135, loss 2.23037, smoothed loss 2.74925, grad norm 3.07040, param norm 189.79437
INFO:root:epoch 12, iter 11140, loss 3.98225, smoothed loss 2.75482, grad norm 3.19299, param norm 189.82739
INFO:root:epoch 12, iter 11145, loss 3.19073, smoothed loss 2.75075, grad norm 3.54702, param norm 189.85887
INFO:root:epoch 12, iter 11150, loss 2.05224, smoothed loss 2.74092, grad norm 2.45995, param norm 189.89307
INFO:root:epoch 12, iter 11155, loss 2.62995, smoothed loss 2.74264, grad norm 3.54848, param norm 189.92822
INFO:root:epoch 12, iter 11160, loss 2.54402, smoothed loss 2.73066, grad norm 3.35312, param norm 189.96265
INFO:root:epoch 12, iter 11165, loss 3.54655, smoothed loss 2.73534, grad norm 3.54813, param norm 189.99956
INFO:root:epoch 12, iter 11170, loss 2.66185, smoothed loss 2.72725, grad norm 3.03228, param norm 190.03415
INFO:root:epoch 12, iter 11175, loss 3.24470, smoothed loss 2.73295, grad norm 3.73867, param norm 190.06458
INFO:root:epoch 12, iter 11180, loss 2.83201, smoothed loss 2.73245, grad norm 3.33442, param norm 190.09229
Adding batches start...
Added  144  batches
INFO:root:epoch 12, iter 11185, loss 2.68555, smoothed loss 2.73491, grad norm 2.64495, param norm 190.11809
INFO:root:epoch 12, iter 11190, loss 2.47132, smoothed loss 2.73496, grad norm 2.53842, param norm 190.14497
INFO:root:epoch 12, iter 11195, loss 2.38682, smoothed loss 2.72727, grad norm 2.77622, param norm 190.17532
INFO:root:epoch 12, iter 11200, loss 2.58078, smoothed loss 2.72489, grad norm 2.61577, param norm 190.20903
INFO:root:epoch 12, iter 11205, loss 2.24158, smoothed loss 2.72727, grad norm 2.56286, param norm 190.24139
INFO:root:epoch 12, iter 11210, loss 3.33456, smoothed loss 2.73726, grad norm 3.45734, param norm 190.27502
INFO:root:epoch 12, iter 11215, loss 2.85387, smoothed loss 2.73977, grad norm 3.40739, param norm 190.30949
INFO:root:epoch 12, iter 11220, loss 3.33781, smoothed loss 2.72860, grad norm 4.00393, param norm 190.34225
INFO:root:epoch 12, iter 11225, loss 2.28593, smoothed loss 2.72185, grad norm 2.72258, param norm 190.37372
INFO:root:epoch 12, iter 11230, loss 2.50189, smoothed loss 2.71407, grad norm 2.66985, param norm 190.40532
INFO:root:epoch 12, iter 11235, loss 3.19373, smoothed loss 2.72992, grad norm 3.37683, param norm 190.43414
INFO:root:epoch 12, iter 11240, loss 3.00213, smoothed loss 2.74250, grad norm 3.15182, param norm 190.45938
INFO:root:epoch 12, iter 11245, loss 2.15488, smoothed loss 2.73889, grad norm 2.46642, param norm 190.48915
INFO:root:epoch 12, iter 11250, loss 2.72853, smoothed loss 2.73702, grad norm 2.94087, param norm 190.52289
INFO:root:epoch 12, iter 11255, loss 3.53755, smoothed loss 2.74555, grad norm 3.13026, param norm 190.55255
INFO:root:epoch 12, iter 11260, loss 2.90025, smoothed loss 2.74692, grad norm 3.62001, param norm 190.58340
INFO:root:epoch 12, iter 11265, loss 2.62341, smoothed loss 2.75053, grad norm 2.76089, param norm 190.61417
INFO:root:epoch 12, iter 11270, loss 2.52045, smoothed loss 2.74800, grad norm 2.96312, param norm 190.64665
INFO:root:epoch 12, iter 11275, loss 2.92440, smoothed loss 2.74914, grad norm 3.04479, param norm 190.67911
INFO:root:epoch 12, iter 11280, loss 2.82292, smoothed loss 2.74256, grad norm 3.63548, param norm 190.70938
INFO:root:epoch 12, iter 11285, loss 3.10890, smoothed loss 2.75162, grad norm 3.27031, param norm 190.73930
INFO:root:epoch 12, iter 11290, loss 2.54622, smoothed loss 2.74213, grad norm 2.81537, param norm 190.77339
INFO:root:epoch 12, iter 11295, loss 2.64634, smoothed loss 2.73571, grad norm 2.77270, param norm 190.80521
INFO:root:epoch 12, iter 11300, loss 2.95448, smoothed loss 2.73973, grad norm 3.19461, param norm 190.83626
INFO:root:epoch 12, iter 11305, loss 2.62453, smoothed loss 2.73914, grad norm 3.15052, param norm 190.86775
INFO:root:epoch 12, iter 11310, loss 1.99337, smoothed loss 2.72850, grad norm 2.83558, param norm 190.90099
INFO:root:epoch 12, iter 11315, loss 2.44566, smoothed loss 2.71897, grad norm 3.22307, param norm 190.93669
INFO:root:epoch 12, iter 11320, loss 3.09886, smoothed loss 2.72623, grad norm 3.42649, param norm 190.97015
INFO:root:epoch 12, iter 11325, loss 2.65917, smoothed loss 2.73981, grad norm 3.20976, param norm 191.00296
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 13, iter 11330, loss 2.91177, smoothed loss 2.74822, grad norm 3.09744, param norm 191.03287
INFO:root:epoch 13, iter 11335, loss 2.02732, smoothed loss 2.72666, grad norm 3.06320, param norm 191.06310
INFO:root:epoch 13, iter 11340, loss 2.85739, smoothed loss 2.72454, grad norm 3.04341, param norm 191.09979
INFO:root:epoch 13, iter 11345, loss 2.98261, smoothed loss 2.71497, grad norm 2.98504, param norm 191.13214
INFO:root:epoch 13, iter 11350, loss 2.63796, smoothed loss 2.70747, grad norm 2.55217, param norm 191.16383
INFO:root:epoch 13, iter 11355, loss 2.67614, smoothed loss 2.71557, grad norm 2.93105, param norm 191.18774
INFO:root:epoch 13, iter 11360, loss 2.62331, smoothed loss 2.71630, grad norm 3.02139, param norm 191.21442
INFO:root:epoch 13, iter 11365, loss 2.44531, smoothed loss 2.71127, grad norm 2.98356, param norm 191.24750
INFO:root:epoch 13, iter 11370, loss 3.65202, smoothed loss 2.70963, grad norm 3.79494, param norm 191.28310
INFO:root:epoch 13, iter 11375, loss 2.19330, smoothed loss 2.70391, grad norm 2.96259, param norm 191.31837
INFO:root:epoch 13, iter 11380, loss 2.92814, smoothed loss 2.71963, grad norm 3.52454, param norm 191.35597
INFO:root:epoch 13, iter 11385, loss 2.68814, smoothed loss 2.70336, grad norm 3.17029, param norm 191.39107
INFO:root:epoch 13, iter 11390, loss 2.99305, smoothed loss 2.70390, grad norm 3.75996, param norm 191.42741
INFO:root:epoch 13, iter 11395, loss 3.05328, smoothed loss 2.70457, grad norm 3.19267, param norm 191.46474
INFO:root:epoch 13, iter 11400, loss 3.19242, smoothed loss 2.71182, grad norm 3.30413, param norm 191.49774
INFO:root:epoch 13, iter 11405, loss 2.98722, smoothed loss 2.70009, grad norm 3.45138, param norm 191.52806
INFO:root:epoch 13, iter 11410, loss 2.54303, smoothed loss 2.70487, grad norm 2.92451, param norm 191.55515
INFO:root:epoch 13, iter 11415, loss 2.76620, smoothed loss 2.70526, grad norm 2.93449, param norm 191.58609
INFO:root:epoch 13, iter 11420, loss 2.79507, smoothed loss 2.71049, grad norm 3.50204, param norm 191.61502
INFO:root:epoch 13, iter 11425, loss 3.50945, smoothed loss 2.72293, grad norm 3.72868, param norm 191.64265
INFO:root:epoch 13, iter 11430, loss 2.12403, smoothed loss 2.72345, grad norm 2.77514, param norm 191.67586
INFO:root:epoch 13, iter 11435, loss 3.62429, smoothed loss 2.74545, grad norm 3.99828, param norm 191.71153
INFO:root:epoch 13, iter 11440, loss 2.46098, smoothed loss 2.73972, grad norm 2.73483, param norm 191.74034
INFO:root:epoch 13, iter 11445, loss 3.05276, smoothed loss 2.74588, grad norm 3.36880, param norm 191.77168
INFO:root:epoch 13, iter 11450, loss 2.56728, smoothed loss 2.74434, grad norm 2.80187, param norm 191.80232
INFO:root:epoch 13, iter 11455, loss 2.77989, smoothed loss 2.73735, grad norm 2.99089, param norm 191.83327
INFO:root:epoch 13, iter 11460, loss 2.45245, smoothed loss 2.72772, grad norm 2.98061, param norm 191.86670
INFO:root:epoch 13, iter 11465, loss 2.65766, smoothed loss 2.73597, grad norm 3.05640, param norm 191.90125
INFO:root:epoch 13, iter 11470, loss 3.04799, smoothed loss 2.74461, grad norm 3.61897, param norm 191.93398
INFO:root:epoch 13, iter 11475, loss 2.69248, smoothed loss 2.74181, grad norm 3.34632, param norm 191.96466
INFO:root:epoch 13, iter 11480, loss 2.11180, smoothed loss 2.74464, grad norm 2.42964, param norm 191.99437
INFO:root:epoch 13, iter 11485, loss 3.13992, smoothed loss 2.74722, grad norm 3.24026, param norm 192.02538
Adding batches start...
Added  160  batches
INFO:root:epoch 13, iter 11490, loss 2.59009, smoothed loss 2.76240, grad norm 2.95683, param norm 192.05496
INFO:root:epoch 13, iter 11495, loss 2.17580, smoothed loss 2.74619, grad norm 2.83295, param norm 192.08928
INFO:root:epoch 13, iter 11500, loss 3.15734, smoothed loss 2.74922, grad norm 3.09349, param norm 192.12251
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 13, Iter 11500, dev loss: 3.067816
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.87590 seconds [Score: 0.79187]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.96625 seconds [Score: 0.66700]
INFO:root:Epoch 13, Iter 11500, Train F1 score: 0.791873, Train EM score: 0.667000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.66182 seconds [Score: 0.63814]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.70794 seconds [Score: 0.49129]
INFO:root:Epoch 13, Iter 11500, Dev F1 score: 0.638139, Dev EM score: 0.491290
INFO:root:End of epoch 13
INFO:root:epoch 13, iter 11505, loss 2.35651, smoothed loss 2.76015, grad norm 3.01864, param norm 192.15387
INFO:root:epoch 13, iter 11510, loss 2.43360, smoothed loss 2.76464, grad norm 2.41496, param norm 192.18384
INFO:root:epoch 13, iter 11515, loss 2.58411, smoothed loss 2.74563, grad norm 2.96423, param norm 192.21480
INFO:root:epoch 13, iter 11520, loss 2.01349, smoothed loss 2.72445, grad norm 2.72664, param norm 192.24551
INFO:root:epoch 13, iter 11525, loss 3.25550, smoothed loss 2.72636, grad norm 3.49761, param norm 192.27602
INFO:root:epoch 13, iter 11530, loss 2.04297, smoothed loss 2.71160, grad norm 2.75394, param norm 192.30647
INFO:root:epoch 13, iter 11535, loss 2.55171, smoothed loss 2.71496, grad norm 3.63827, param norm 192.33823
INFO:root:epoch 13, iter 11540, loss 2.32961, smoothed loss 2.71880, grad norm 2.84973, param norm 192.36847
INFO:root:epoch 13, iter 11545, loss 2.86851, smoothed loss 2.72189, grad norm 2.90048, param norm 192.40407
INFO:root:epoch 13, iter 11550, loss 2.14372, smoothed loss 2.70453, grad norm 2.76762, param norm 192.43753
INFO:root:epoch 13, iter 11555, loss 2.84727, smoothed loss 2.70285, grad norm 3.16851, param norm 192.47020
INFO:root:epoch 13, iter 11560, loss 2.82483, smoothed loss 2.71792, grad norm 2.92640, param norm 192.49835
INFO:root:epoch 13, iter 11565, loss 2.92235, smoothed loss 2.72403, grad norm 3.36005, param norm 192.52623
INFO:root:epoch 13, iter 11570, loss 2.39387, smoothed loss 2.72270, grad norm 2.66965, param norm 192.55687
INFO:root:epoch 13, iter 11575, loss 2.75117, smoothed loss 2.71461, grad norm 3.46693, param norm 192.58708
INFO:root:epoch 13, iter 11580, loss 3.08049, smoothed loss 2.72326, grad norm 3.06453, param norm 192.62094
INFO:root:epoch 13, iter 11585, loss 3.02277, smoothed loss 2.73067, grad norm 3.16115, param norm 192.65717
INFO:root:epoch 13, iter 11590, loss 2.23231, smoothed loss 2.71931, grad norm 2.79397, param norm 192.69040
INFO:root:epoch 13, iter 11595, loss 2.95400, smoothed loss 2.72155, grad norm 3.02470, param norm 192.72136
INFO:root:epoch 13, iter 11600, loss 2.81850, smoothed loss 2.71576, grad norm 2.98329, param norm 192.75311
INFO:root:epoch 13, iter 11605, loss 3.03810, smoothed loss 2.72470, grad norm 3.24758, param norm 192.78267
INFO:root:epoch 13, iter 11610, loss 2.77343, smoothed loss 2.72723, grad norm 3.30365, param norm 192.80983
INFO:root:epoch 13, iter 11615, loss 3.07109, smoothed loss 2.72367, grad norm 3.87120, param norm 192.84303
INFO:root:epoch 13, iter 11620, loss 2.72168, smoothed loss 2.72887, grad norm 3.09785, param norm 192.87752
INFO:root:epoch 13, iter 11625, loss 2.28184, smoothed loss 2.72895, grad norm 3.02263, param norm 192.90787
INFO:root:epoch 13, iter 11630, loss 2.62121, smoothed loss 2.72159, grad norm 3.16133, param norm 192.94031
INFO:root:epoch 13, iter 11635, loss 2.60113, smoothed loss 2.72089, grad norm 3.17976, param norm 192.97780
INFO:root:epoch 13, iter 11640, loss 3.51473, smoothed loss 2.71746, grad norm 3.29633, param norm 193.01335
INFO:root:epoch 13, iter 11645, loss 2.23357, smoothed loss 2.70362, grad norm 3.00466, param norm 193.04376
Adding batches start...
Added  160  batches
INFO:root:epoch 13, iter 11650, loss 2.92136, smoothed loss 2.71079, grad norm 3.33378, param norm 193.07376
INFO:root:epoch 13, iter 11655, loss 2.24603, smoothed loss 2.71012, grad norm 2.90589, param norm 193.10292
INFO:root:epoch 13, iter 11660, loss 2.42897, smoothed loss 2.71518, grad norm 3.25797, param norm 193.13263
INFO:root:epoch 13, iter 11665, loss 2.90281, smoothed loss 2.71890, grad norm 3.68325, param norm 193.16382
INFO:root:epoch 13, iter 11670, loss 2.40792, smoothed loss 2.71637, grad norm 2.96243, param norm 193.19441
INFO:root:epoch 13, iter 11675, loss 2.60275, smoothed loss 2.71526, grad norm 2.88593, param norm 193.22603
INFO:root:epoch 13, iter 11680, loss 2.48991, smoothed loss 2.71825, grad norm 2.90740, param norm 193.25783
INFO:root:epoch 13, iter 11685, loss 2.76277, smoothed loss 2.72366, grad norm 3.37530, param norm 193.28725
INFO:root:epoch 13, iter 11690, loss 3.11306, smoothed loss 2.70959, grad norm 3.37778, param norm 193.31758
INFO:root:epoch 13, iter 11695, loss 2.59018, smoothed loss 2.71911, grad norm 3.56975, param norm 193.34648
INFO:root:epoch 13, iter 11700, loss 2.95833, smoothed loss 2.70679, grad norm 3.48702, param norm 193.37711
INFO:root:epoch 13, iter 11705, loss 2.40080, smoothed loss 2.70969, grad norm 3.14699, param norm 193.40329
INFO:root:epoch 13, iter 11710, loss 2.56951, smoothed loss 2.70853, grad norm 3.69419, param norm 193.43056
INFO:root:epoch 13, iter 11715, loss 2.99025, smoothed loss 2.71038, grad norm 3.25926, param norm 193.45763
INFO:root:epoch 13, iter 11720, loss 2.78986, smoothed loss 2.70265, grad norm 2.93075, param norm 193.48718
INFO:root:epoch 13, iter 11725, loss 2.47744, smoothed loss 2.69121, grad norm 2.72292, param norm 193.51881
INFO:root:epoch 13, iter 11730, loss 2.82926, smoothed loss 2.68419, grad norm 3.04063, param norm 193.55208
INFO:root:epoch 13, iter 11735, loss 2.80812, smoothed loss 2.68499, grad norm 3.00134, param norm 193.58611
INFO:root:epoch 13, iter 11740, loss 2.43019, smoothed loss 2.69335, grad norm 2.75964, param norm 193.62350
INFO:root:epoch 13, iter 11745, loss 2.94374, smoothed loss 2.69602, grad norm 3.99407, param norm 193.66252
INFO:root:epoch 13, iter 11750, loss 2.76092, smoothed loss 2.69049, grad norm 2.87840, param norm 193.69974
INFO:root:epoch 13, iter 11755, loss 2.39774, smoothed loss 2.68382, grad norm 2.89451, param norm 193.73335
INFO:root:epoch 13, iter 11760, loss 2.82791, smoothed loss 2.69435, grad norm 3.41266, param norm 193.76872
INFO:root:epoch 13, iter 11765, loss 2.77376, smoothed loss 2.69064, grad norm 3.37475, param norm 193.80269
INFO:root:epoch 13, iter 11770, loss 2.77266, smoothed loss 2.68669, grad norm 2.99427, param norm 193.83418
INFO:root:epoch 13, iter 11775, loss 2.51192, smoothed loss 2.69308, grad norm 3.09229, param norm 193.86719
INFO:root:epoch 13, iter 11780, loss 2.68016, smoothed loss 2.67834, grad norm 2.97267, param norm 193.89639
INFO:root:epoch 13, iter 11785, loss 2.90090, smoothed loss 2.67850, grad norm 3.15862, param norm 193.92961
INFO:root:epoch 13, iter 11790, loss 3.59805, smoothed loss 2.68366, grad norm 3.95099, param norm 193.96532
INFO:root:epoch 13, iter 11795, loss 2.07893, smoothed loss 2.68830, grad norm 2.61467, param norm 194.00046
INFO:root:epoch 13, iter 11800, loss 2.35311, smoothed loss 2.69187, grad norm 3.13743, param norm 194.03505
INFO:root:epoch 13, iter 11805, loss 2.80746, smoothed loss 2.69500, grad norm 3.13303, param norm 194.06633
Adding batches start...
Added  160  batches
INFO:root:epoch 13, iter 11810, loss 2.48149, smoothed loss 2.69825, grad norm 3.19855, param norm 194.09451
INFO:root:epoch 13, iter 11815, loss 3.26238, smoothed loss 2.69343, grad norm 3.51484, param norm 194.12546
INFO:root:epoch 13, iter 11820, loss 2.45860, smoothed loss 2.68763, grad norm 2.77472, param norm 194.15782
INFO:root:epoch 13, iter 11825, loss 1.80573, smoothed loss 2.67346, grad norm 2.61703, param norm 194.19127
INFO:root:epoch 13, iter 11830, loss 2.50076, smoothed loss 2.67350, grad norm 3.22231, param norm 194.22377
INFO:root:epoch 13, iter 11835, loss 2.17440, smoothed loss 2.67679, grad norm 2.99563, param norm 194.25467
INFO:root:epoch 13, iter 11840, loss 2.16708, smoothed loss 2.67345, grad norm 2.64844, param norm 194.28769
INFO:root:epoch 13, iter 11845, loss 1.80365, smoothed loss 2.65502, grad norm 2.54833, param norm 194.31857
INFO:root:epoch 13, iter 11850, loss 2.18689, smoothed loss 2.65551, grad norm 3.26014, param norm 194.35110
INFO:root:epoch 13, iter 11855, loss 2.27878, smoothed loss 2.65634, grad norm 3.10183, param norm 194.37996
INFO:root:epoch 13, iter 11860, loss 1.95434, smoothed loss 2.64325, grad norm 3.22693, param norm 194.40796
INFO:root:epoch 13, iter 11865, loss 2.20516, smoothed loss 2.62491, grad norm 2.76495, param norm 194.43808
INFO:root:epoch 13, iter 11870, loss 2.81112, smoothed loss 2.64020, grad norm 3.83039, param norm 194.46922
INFO:root:epoch 13, iter 11875, loss 3.14619, smoothed loss 2.63898, grad norm 3.36556, param norm 194.50153
INFO:root:epoch 13, iter 11880, loss 2.72680, smoothed loss 2.62866, grad norm 3.50990, param norm 194.53413
INFO:root:epoch 13, iter 11885, loss 2.40776, smoothed loss 2.62766, grad norm 2.94230, param norm 194.56451
INFO:root:epoch 13, iter 11890, loss 2.60133, smoothed loss 2.64197, grad norm 3.18021, param norm 194.59444
INFO:root:epoch 13, iter 11895, loss 2.40009, smoothed loss 2.64013, grad norm 2.83311, param norm 194.62180
INFO:root:epoch 13, iter 11900, loss 2.54287, smoothed loss 2.63662, grad norm 3.15504, param norm 194.65178
INFO:root:epoch 13, iter 11905, loss 2.25167, smoothed loss 2.64314, grad norm 2.89389, param norm 194.68703
INFO:root:epoch 13, iter 11910, loss 3.20347, smoothed loss 2.63795, grad norm 3.88299, param norm 194.71950
INFO:root:epoch 13, iter 11915, loss 3.02773, smoothed loss 2.63706, grad norm 3.57392, param norm 194.75127
INFO:root:epoch 13, iter 11920, loss 2.92259, smoothed loss 2.65017, grad norm 3.48615, param norm 194.78336
INFO:root:epoch 13, iter 11925, loss 2.58920, smoothed loss 2.64030, grad norm 3.31658, param norm 194.81450
INFO:root:epoch 13, iter 11930, loss 2.24753, smoothed loss 2.63971, grad norm 3.17942, param norm 194.84534
INFO:root:epoch 13, iter 11935, loss 2.98911, smoothed loss 2.64513, grad norm 3.23754, param norm 194.87633
INFO:root:epoch 13, iter 11940, loss 2.50115, smoothed loss 2.65872, grad norm 2.70839, param norm 194.90656
INFO:root:epoch 13, iter 11945, loss 2.58323, smoothed loss 2.64964, grad norm 2.78438, param norm 194.93938
INFO:root:epoch 13, iter 11950, loss 2.36847, smoothed loss 2.64338, grad norm 2.63372, param norm 194.97554
INFO:root:epoch 13, iter 11955, loss 2.79195, smoothed loss 2.64189, grad norm 3.08038, param norm 195.00949
INFO:root:epoch 13, iter 11960, loss 2.74399, smoothed loss 2.64753, grad norm 3.11363, param norm 195.04152
INFO:root:epoch 13, iter 11965, loss 2.95471, smoothed loss 2.63253, grad norm 3.48847, param norm 195.07118
Adding batches start...
Added  160  batches
INFO:root:epoch 13, iter 11970, loss 3.40212, smoothed loss 2.64585, grad norm 3.64422, param norm 195.09943
INFO:root:epoch 13, iter 11975, loss 2.51465, smoothed loss 2.64152, grad norm 3.16326, param norm 195.12782
INFO:root:epoch 13, iter 11980, loss 2.67274, smoothed loss 2.64788, grad norm 3.88775, param norm 195.15727
INFO:root:epoch 13, iter 11985, loss 2.49304, smoothed loss 2.63546, grad norm 2.66308, param norm 195.18886
INFO:root:epoch 13, iter 11990, loss 2.96173, smoothed loss 2.64751, grad norm 3.02891, param norm 195.21860
INFO:root:epoch 13, iter 11995, loss 2.43680, smoothed loss 2.65813, grad norm 2.55752, param norm 195.24612
INFO:root:epoch 13, iter 12000, loss 2.58084, smoothed loss 2.65725, grad norm 2.52997, param norm 195.27571
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 13, Iter 12000, dev loss: 3.062715
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 18.10374 seconds [Score: 0.77395]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.07137 seconds [Score: 0.61100]
INFO:root:Epoch 13, Iter 12000, Train F1 score: 0.773951, Train EM score: 0.611000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.16877 seconds [Score: 0.64137]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.04639 seconds [Score: 0.50056]
INFO:root:Epoch 13, Iter 12000, Dev F1 score: 0.641373, Dev EM score: 0.500562
INFO:root:End of epoch 13
INFO:root:epoch 13, iter 12005, loss 2.56413, smoothed loss 2.65794, grad norm 2.90873, param norm 195.30846
INFO:root:epoch 13, iter 12010, loss 2.64563, smoothed loss 2.66143, grad norm 3.28590, param norm 195.33929
INFO:root:epoch 13, iter 12015, loss 2.71574, smoothed loss 2.67074, grad norm 3.22487, param norm 195.36804
INFO:root:epoch 13, iter 12020, loss 2.77688, smoothed loss 2.67112, grad norm 2.86814, param norm 195.39635
INFO:root:epoch 13, iter 12025, loss 3.29114, smoothed loss 2.67622, grad norm 3.69842, param norm 195.42473
INFO:root:epoch 13, iter 12030, loss 3.07715, smoothed loss 2.67918, grad norm 2.78384, param norm 195.45451
INFO:root:epoch 13, iter 12035, loss 2.98234, smoothed loss 2.68593, grad norm 3.50074, param norm 195.48630
INFO:root:epoch 13, iter 12040, loss 2.50056, smoothed loss 2.66393, grad norm 2.70987, param norm 195.51932
INFO:root:epoch 13, iter 12045, loss 2.30289, smoothed loss 2.64792, grad norm 2.82815, param norm 195.55309
INFO:root:epoch 13, iter 12050, loss 2.88715, smoothed loss 2.65136, grad norm 2.95865, param norm 195.58206
INFO:root:epoch 13, iter 12055, loss 3.05534, smoothed loss 2.66103, grad norm 3.03726, param norm 195.60469
INFO:root:epoch 13, iter 12060, loss 2.48763, smoothed loss 2.67135, grad norm 2.92645, param norm 195.62578
INFO:root:epoch 13, iter 12065, loss 2.57357, smoothed loss 2.67501, grad norm 2.98455, param norm 195.65053
INFO:root:epoch 13, iter 12070, loss 2.86193, smoothed loss 2.68340, grad norm 3.41919, param norm 195.68048
INFO:root:epoch 13, iter 12075, loss 3.13226, smoothed loss 2.69343, grad norm 3.98716, param norm 195.71280
INFO:root:epoch 13, iter 12080, loss 2.50886, smoothed loss 2.68895, grad norm 2.87629, param norm 195.74362
INFO:root:epoch 13, iter 12085, loss 3.09721, smoothed loss 2.68385, grad norm 3.12208, param norm 195.77269
INFO:root:epoch 13, iter 12090, loss 2.31213, smoothed loss 2.68455, grad norm 2.54446, param norm 195.80162
INFO:root:epoch 13, iter 12095, loss 3.28774, smoothed loss 2.68975, grad norm 3.79757, param norm 195.82993
INFO:root:epoch 13, iter 12100, loss 2.28187, smoothed loss 2.68184, grad norm 3.16955, param norm 195.86191
INFO:root:epoch 13, iter 12105, loss 2.51268, smoothed loss 2.67665, grad norm 3.15752, param norm 195.89919
INFO:root:epoch 13, iter 12110, loss 2.55841, smoothed loss 2.66710, grad norm 3.22424, param norm 195.93549
INFO:root:epoch 13, iter 12115, loss 2.05986, smoothed loss 2.66230, grad norm 2.95792, param norm 195.97038
INFO:root:epoch 13, iter 12120, loss 2.28788, smoothed loss 2.65892, grad norm 2.88503, param norm 196.00476
INFO:root:epoch 13, iter 12125, loss 2.83688, smoothed loss 2.67091, grad norm 3.42369, param norm 196.03714
Adding batches start...
Added  144  batches
INFO:root:epoch 13, iter 12130, loss 2.67581, smoothed loss 2.66588, grad norm 3.11883, param norm 196.06694
INFO:root:epoch 13, iter 12135, loss 3.11953, smoothed loss 2.68107, grad norm 3.59152, param norm 196.09482
INFO:root:epoch 13, iter 12140, loss 2.57575, smoothed loss 2.67033, grad norm 3.13565, param norm 196.12575
INFO:root:epoch 13, iter 12145, loss 3.07951, smoothed loss 2.66206, grad norm 3.63991, param norm 196.16083
INFO:root:epoch 13, iter 12150, loss 2.65858, smoothed loss 2.66496, grad norm 3.38946, param norm 196.19218
INFO:root:epoch 13, iter 12155, loss 2.59118, smoothed loss 2.67807, grad norm 3.19825, param norm 196.22122
INFO:root:epoch 13, iter 12160, loss 2.57505, smoothed loss 2.67013, grad norm 3.02835, param norm 196.25351
INFO:root:epoch 13, iter 12165, loss 2.07455, smoothed loss 2.66620, grad norm 2.91869, param norm 196.28363
INFO:root:epoch 13, iter 12170, loss 2.26366, smoothed loss 2.67532, grad norm 2.87651, param norm 196.31883
INFO:root:epoch 13, iter 12175, loss 2.54115, smoothed loss 2.68545, grad norm 3.41846, param norm 196.35587
INFO:root:epoch 13, iter 12180, loss 2.65882, smoothed loss 2.69126, grad norm 3.10360, param norm 196.39326
INFO:root:epoch 13, iter 12185, loss 2.53048, smoothed loss 2.69238, grad norm 3.08661, param norm 196.42909
INFO:root:epoch 13, iter 12190, loss 2.44003, smoothed loss 2.69040, grad norm 2.69546, param norm 196.46237
INFO:root:epoch 13, iter 12195, loss 2.99409, smoothed loss 2.69959, grad norm 3.06069, param norm 196.49358
INFO:root:epoch 13, iter 12200, loss 2.76036, smoothed loss 2.69540, grad norm 2.78385, param norm 196.52322
INFO:root:epoch 13, iter 12205, loss 2.48718, smoothed loss 2.69754, grad norm 2.78246, param norm 196.55086
INFO:root:epoch 13, iter 12210, loss 2.79693, smoothed loss 2.69950, grad norm 3.60355, param norm 196.57764
INFO:root:epoch 13, iter 12215, loss 2.41266, smoothed loss 2.68076, grad norm 3.24235, param norm 196.60349
INFO:root:epoch 13, iter 12220, loss 2.33134, smoothed loss 2.66829, grad norm 2.89142, param norm 196.63161
INFO:root:epoch 13, iter 12225, loss 2.47516, smoothed loss 2.67301, grad norm 3.34281, param norm 196.66199
INFO:root:epoch 13, iter 12230, loss 2.49600, smoothed loss 2.66310, grad norm 3.18955, param norm 196.69432
INFO:root:epoch 13, iter 12235, loss 2.73717, smoothed loss 2.67489, grad norm 3.48454, param norm 196.72523
INFO:root:epoch 13, iter 12240, loss 2.96659, smoothed loss 2.67689, grad norm 2.99896, param norm 196.75218
INFO:root:epoch 13, iter 12245, loss 2.68128, smoothed loss 2.68151, grad norm 2.79774, param norm 196.77991
INFO:root:epoch 13, iter 12250, loss 2.75490, smoothed loss 2.67611, grad norm 2.69532, param norm 196.80914
INFO:root:epoch 13, iter 12255, loss 2.54279, smoothed loss 2.66880, grad norm 3.03853, param norm 196.83987
INFO:root:epoch 13, iter 12260, loss 2.40394, smoothed loss 2.66368, grad norm 2.81225, param norm 196.87062
INFO:root:epoch 13, iter 12265, loss 2.86418, smoothed loss 2.66891, grad norm 3.27814, param norm 196.89980
INFO:root:epoch 13, iter 12270, loss 2.26587, smoothed loss 2.65369, grad norm 3.02556, param norm 196.92577
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 14, iter 12275, loss 2.55508, smoothed loss 2.65333, grad norm 3.46582, param norm 196.95157
INFO:root:epoch 14, iter 12280, loss 3.11532, smoothed loss 2.65618, grad norm 3.56707, param norm 196.97894
INFO:root:epoch 14, iter 12285, loss 2.48246, smoothed loss 2.65805, grad norm 3.01984, param norm 197.00923
INFO:root:epoch 14, iter 12290, loss 3.00998, smoothed loss 2.66532, grad norm 3.21999, param norm 197.04024
INFO:root:epoch 14, iter 12295, loss 3.22518, smoothed loss 2.66870, grad norm 3.40432, param norm 197.07066
INFO:root:epoch 14, iter 12300, loss 2.37147, smoothed loss 2.65421, grad norm 2.85336, param norm 197.09938
INFO:root:epoch 14, iter 12305, loss 2.50433, smoothed loss 2.63778, grad norm 3.40800, param norm 197.13371
INFO:root:epoch 14, iter 12310, loss 2.41796, smoothed loss 2.64005, grad norm 2.97709, param norm 197.16750
INFO:root:epoch 14, iter 12315, loss 2.13655, smoothed loss 2.62863, grad norm 3.54998, param norm 197.20113
INFO:root:epoch 14, iter 12320, loss 3.01874, smoothed loss 2.64333, grad norm 3.73138, param norm 197.23108
INFO:root:epoch 14, iter 12325, loss 3.15460, smoothed loss 2.64801, grad norm 3.64017, param norm 197.26175
INFO:root:epoch 14, iter 12330, loss 2.86061, smoothed loss 2.65200, grad norm 3.68974, param norm 197.29326
INFO:root:epoch 14, iter 12335, loss 2.70410, smoothed loss 2.64640, grad norm 2.92417, param norm 197.32487
INFO:root:epoch 14, iter 12340, loss 2.96498, smoothed loss 2.65278, grad norm 3.40421, param norm 197.35686
INFO:root:epoch 14, iter 12345, loss 2.70734, smoothed loss 2.64894, grad norm 2.70961, param norm 197.38942
INFO:root:epoch 14, iter 12350, loss 2.41656, smoothed loss 2.64802, grad norm 2.69825, param norm 197.41711
INFO:root:epoch 14, iter 12355, loss 3.39330, smoothed loss 2.64815, grad norm 3.29659, param norm 197.44691
INFO:root:epoch 14, iter 12360, loss 2.08868, smoothed loss 2.63676, grad norm 2.69701, param norm 197.47760
INFO:root:epoch 14, iter 12365, loss 2.72276, smoothed loss 2.64029, grad norm 3.16701, param norm 197.50732
INFO:root:epoch 14, iter 12370, loss 2.42532, smoothed loss 2.62934, grad norm 2.61667, param norm 197.53683
INFO:root:epoch 14, iter 12375, loss 2.96996, smoothed loss 2.63314, grad norm 3.24227, param norm 197.56622
INFO:root:epoch 14, iter 12380, loss 2.59826, smoothed loss 2.63223, grad norm 3.13930, param norm 197.59377
INFO:root:epoch 14, iter 12385, loss 2.72156, smoothed loss 2.62488, grad norm 2.78826, param norm 197.62318
INFO:root:epoch 14, iter 12390, loss 2.93501, smoothed loss 2.63494, grad norm 3.15166, param norm 197.65442
INFO:root:epoch 14, iter 12395, loss 3.37342, smoothed loss 2.64847, grad norm 3.70433, param norm 197.68808
INFO:root:epoch 14, iter 12400, loss 2.60103, smoothed loss 2.64836, grad norm 3.57398, param norm 197.72478
INFO:root:epoch 14, iter 12405, loss 2.17528, smoothed loss 2.64922, grad norm 2.91559, param norm 197.76122
INFO:root:epoch 14, iter 12410, loss 2.49336, smoothed loss 2.64593, grad norm 2.93280, param norm 197.79529
INFO:root:epoch 14, iter 12415, loss 2.93614, smoothed loss 2.64718, grad norm 3.05885, param norm 197.82805
INFO:root:epoch 14, iter 12420, loss 2.73386, smoothed loss 2.65599, grad norm 3.82701, param norm 197.85661
INFO:root:epoch 14, iter 12425, loss 2.43288, smoothed loss 2.65226, grad norm 2.54734, param norm 197.88727
INFO:root:epoch 14, iter 12430, loss 2.74752, smoothed loss 2.65827, grad norm 3.00991, param norm 197.91890
Adding batches start...
Added  160  batches
INFO:root:epoch 14, iter 12435, loss 2.61122, smoothed loss 2.65305, grad norm 3.01555, param norm 197.95296
INFO:root:epoch 14, iter 12440, loss 2.51407, smoothed loss 2.64956, grad norm 2.88236, param norm 197.98521
INFO:root:epoch 14, iter 12445, loss 2.60917, smoothed loss 2.64431, grad norm 3.14380, param norm 198.01807
INFO:root:epoch 14, iter 12450, loss 2.46507, smoothed loss 2.64022, grad norm 3.05133, param norm 198.05046
INFO:root:epoch 14, iter 12455, loss 3.02842, smoothed loss 2.65157, grad norm 3.72408, param norm 198.08093
INFO:root:epoch 14, iter 12460, loss 2.82714, smoothed loss 2.65512, grad norm 3.18796, param norm 198.10953
INFO:root:epoch 14, iter 12465, loss 2.50851, smoothed loss 2.66061, grad norm 3.35320, param norm 198.13860
INFO:root:epoch 14, iter 12470, loss 2.42063, smoothed loss 2.66068, grad norm 3.39321, param norm 198.17038
INFO:root:epoch 14, iter 12475, loss 2.58027, smoothed loss 2.65655, grad norm 3.04575, param norm 198.20084
INFO:root:epoch 14, iter 12480, loss 2.66993, smoothed loss 2.65490, grad norm 3.11061, param norm 198.22871
INFO:root:epoch 14, iter 12485, loss 2.88379, smoothed loss 2.65714, grad norm 3.31603, param norm 198.25983
INFO:root:epoch 14, iter 12490, loss 2.69598, smoothed loss 2.65911, grad norm 3.37585, param norm 198.28885
INFO:root:epoch 14, iter 12495, loss 2.56944, smoothed loss 2.64833, grad norm 2.92045, param norm 198.31984
INFO:root:epoch 14, iter 12500, loss 2.40098, smoothed loss 2.65166, grad norm 2.54311, param norm 198.35017
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 14, Iter 12500, dev loss: 3.042989
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 18.46404 seconds [Score: 0.79731]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.16515 seconds [Score: 0.65500]
INFO:root:Epoch 14, Iter 12500, Train F1 score: 0.797311, Train EM score: 0.655000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.50205 seconds [Score: 0.64384]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.04607 seconds [Score: 0.49916]
INFO:root:Epoch 14, Iter 12500, Dev F1 score: 0.643839, Dev EM score: 0.499157
INFO:root:End of epoch 14
INFO:root:epoch 14, iter 12505, loss 2.42026, smoothed loss 2.65616, grad norm 2.95021, param norm 198.38026
INFO:root:epoch 14, iter 12510, loss 2.25047, smoothed loss 2.65364, grad norm 2.91233, param norm 198.40953
INFO:root:epoch 14, iter 12515, loss 1.93374, smoothed loss 2.64825, grad norm 2.61407, param norm 198.43837
INFO:root:epoch 14, iter 12520, loss 3.16403, smoothed loss 2.66311, grad norm 3.50969, param norm 198.46948
INFO:root:epoch 14, iter 12525, loss 2.52324, smoothed loss 2.66600, grad norm 3.28470, param norm 198.50067
INFO:root:epoch 14, iter 12530, loss 2.68008, smoothed loss 2.67271, grad norm 3.18858, param norm 198.52974
INFO:root:epoch 14, iter 12535, loss 2.86385, smoothed loss 2.66821, grad norm 3.19777, param norm 198.55775
INFO:root:epoch 14, iter 12540, loss 2.45700, smoothed loss 2.67370, grad norm 3.03602, param norm 198.58592
INFO:root:epoch 14, iter 12545, loss 3.09016, smoothed loss 2.68715, grad norm 3.32101, param norm 198.61507
INFO:root:epoch 14, iter 12550, loss 2.53464, smoothed loss 2.67626, grad norm 3.40811, param norm 198.64636
INFO:root:epoch 14, iter 12555, loss 3.06911, smoothed loss 2.67788, grad norm 3.63824, param norm 198.67914
INFO:root:epoch 14, iter 12560, loss 2.96361, smoothed loss 2.68355, grad norm 3.43792, param norm 198.71049
INFO:root:epoch 14, iter 12565, loss 2.16697, smoothed loss 2.67832, grad norm 2.90007, param norm 198.74425
INFO:root:epoch 14, iter 12570, loss 2.63092, smoothed loss 2.67594, grad norm 3.11564, param norm 198.77887
INFO:root:epoch 14, iter 12575, loss 2.54133, smoothed loss 2.67470, grad norm 2.93217, param norm 198.81219
INFO:root:epoch 14, iter 12580, loss 3.14922, smoothed loss 2.66930, grad norm 3.26162, param norm 198.84464
INFO:root:epoch 14, iter 12585, loss 2.71215, smoothed loss 2.65425, grad norm 3.30824, param norm 198.87572
INFO:root:epoch 14, iter 12590, loss 2.74571, smoothed loss 2.64513, grad norm 3.07293, param norm 198.90810
Adding batches start...
Added  160  batches
INFO:root:epoch 14, iter 12595, loss 2.28075, smoothed loss 2.63523, grad norm 2.93372, param norm 198.93715
INFO:root:epoch 14, iter 12600, loss 1.98273, smoothed loss 2.61603, grad norm 2.42227, param norm 198.96364
INFO:root:epoch 14, iter 12605, loss 2.41590, smoothed loss 2.60460, grad norm 3.30554, param norm 198.99026
INFO:root:epoch 14, iter 12610, loss 2.91660, smoothed loss 2.60927, grad norm 3.53095, param norm 199.01895
INFO:root:epoch 14, iter 12615, loss 3.26210, smoothed loss 2.61794, grad norm 3.75064, param norm 199.05006
INFO:root:epoch 14, iter 12620, loss 2.73787, smoothed loss 2.62231, grad norm 3.08985, param norm 199.07999
INFO:root:epoch 14, iter 12625, loss 2.62899, smoothed loss 2.62059, grad norm 3.02344, param norm 199.11018
INFO:root:epoch 14, iter 12630, loss 2.69233, smoothed loss 2.62555, grad norm 3.24393, param norm 199.13953
INFO:root:epoch 14, iter 12635, loss 2.75879, smoothed loss 2.62963, grad norm 3.68482, param norm 199.16695
INFO:root:epoch 14, iter 12640, loss 3.06885, smoothed loss 2.62501, grad norm 3.27494, param norm 199.19893
INFO:root:epoch 14, iter 12645, loss 2.68190, smoothed loss 2.61610, grad norm 3.14471, param norm 199.23148
INFO:root:epoch 14, iter 12650, loss 2.82466, smoothed loss 2.62138, grad norm 3.35071, param norm 199.26204
INFO:root:epoch 14, iter 12655, loss 3.35677, smoothed loss 2.62548, grad norm 4.20702, param norm 199.29189
INFO:root:epoch 14, iter 12660, loss 2.59647, smoothed loss 2.62274, grad norm 2.90686, param norm 199.32399
INFO:root:epoch 14, iter 12665, loss 2.72954, smoothed loss 2.61539, grad norm 3.25290, param norm 199.35629
INFO:root:epoch 14, iter 12670, loss 2.29778, smoothed loss 2.61638, grad norm 3.09583, param norm 199.38654
INFO:root:epoch 14, iter 12675, loss 3.03756, smoothed loss 2.62177, grad norm 3.17196, param norm 199.41766
INFO:root:epoch 14, iter 12680, loss 2.87985, smoothed loss 2.62892, grad norm 2.90542, param norm 199.44749
INFO:root:epoch 14, iter 12685, loss 2.86383, smoothed loss 2.63773, grad norm 3.15417, param norm 199.47906
INFO:root:epoch 14, iter 12690, loss 2.59241, smoothed loss 2.64269, grad norm 3.02963, param norm 199.51115
INFO:root:epoch 14, iter 12695, loss 2.95285, smoothed loss 2.65197, grad norm 3.31264, param norm 199.54027
INFO:root:epoch 14, iter 12700, loss 2.70283, smoothed loss 2.64750, grad norm 3.52312, param norm 199.57051
INFO:root:epoch 14, iter 12705, loss 2.79613, smoothed loss 2.64516, grad norm 3.38403, param norm 199.60118
INFO:root:epoch 14, iter 12710, loss 2.62811, smoothed loss 2.63917, grad norm 3.14098, param norm 199.63130
INFO:root:epoch 14, iter 12715, loss 1.88780, smoothed loss 2.63309, grad norm 2.56764, param norm 199.65596
INFO:root:epoch 14, iter 12720, loss 2.24353, smoothed loss 2.62539, grad norm 2.90125, param norm 199.68228
INFO:root:epoch 14, iter 12725, loss 2.09927, smoothed loss 2.61702, grad norm 3.74306, param norm 199.71124
INFO:root:epoch 14, iter 12730, loss 2.89422, smoothed loss 2.62807, grad norm 3.12940, param norm 199.73859
INFO:root:epoch 14, iter 12735, loss 3.13960, smoothed loss 2.63308, grad norm 3.15421, param norm 199.76753
INFO:root:epoch 14, iter 12740, loss 3.36966, smoothed loss 2.65068, grad norm 3.76342, param norm 199.79657
INFO:root:epoch 14, iter 12745, loss 2.44598, smoothed loss 2.64875, grad norm 2.70625, param norm 199.82494
INFO:root:epoch 14, iter 12750, loss 3.15464, smoothed loss 2.65478, grad norm 3.74502, param norm 199.85315
Adding batches start...
Added  160  batches
INFO:root:epoch 14, iter 12755, loss 2.83261, smoothed loss 2.66611, grad norm 2.80931, param norm 199.88303
INFO:root:epoch 14, iter 12760, loss 2.30572, smoothed loss 2.66063, grad norm 2.73056, param norm 199.91046
INFO:root:epoch 14, iter 12765, loss 2.30808, smoothed loss 2.64642, grad norm 3.03761, param norm 199.93990
INFO:root:epoch 14, iter 12770, loss 2.62274, smoothed loss 2.65025, grad norm 3.56575, param norm 199.96802
INFO:root:epoch 14, iter 12775, loss 2.68117, smoothed loss 2.65138, grad norm 3.55902, param norm 199.99846
INFO:root:epoch 14, iter 12780, loss 2.22863, smoothed loss 2.63589, grad norm 2.90978, param norm 200.03099
INFO:root:epoch 14, iter 12785, loss 2.63079, smoothed loss 2.63155, grad norm 3.25366, param norm 200.05980
INFO:root:epoch 14, iter 12790, loss 2.45611, smoothed loss 2.62868, grad norm 3.01822, param norm 200.08798
INFO:root:epoch 14, iter 12795, loss 2.46609, smoothed loss 2.62729, grad norm 3.32418, param norm 200.11607
INFO:root:epoch 14, iter 12800, loss 2.31991, smoothed loss 2.60433, grad norm 2.90728, param norm 200.14555
INFO:root:epoch 14, iter 12805, loss 2.29110, smoothed loss 2.60584, grad norm 3.40025, param norm 200.17426
INFO:root:epoch 14, iter 12810, loss 2.12956, smoothed loss 2.60375, grad norm 2.85869, param norm 200.20288
INFO:root:epoch 14, iter 12815, loss 2.29446, smoothed loss 2.59953, grad norm 2.88310, param norm 200.23499
INFO:root:epoch 14, iter 12820, loss 2.41353, smoothed loss 2.58911, grad norm 2.97493, param norm 200.26837
INFO:root:epoch 14, iter 12825, loss 2.24004, smoothed loss 2.58648, grad norm 3.07568, param norm 200.30165
INFO:root:epoch 14, iter 12830, loss 2.47436, smoothed loss 2.59168, grad norm 3.17394, param norm 200.33209
INFO:root:epoch 14, iter 12835, loss 2.70185, smoothed loss 2.59971, grad norm 3.24412, param norm 200.36304
INFO:root:epoch 14, iter 12840, loss 1.99254, smoothed loss 2.59613, grad norm 2.49342, param norm 200.39432
INFO:root:epoch 14, iter 12845, loss 2.95924, smoothed loss 2.59238, grad norm 3.44646, param norm 200.42624
INFO:root:epoch 14, iter 12850, loss 2.56380, smoothed loss 2.59783, grad norm 3.43042, param norm 200.45741
INFO:root:epoch 14, iter 12855, loss 3.03111, smoothed loss 2.61936, grad norm 3.09471, param norm 200.48547
INFO:root:epoch 14, iter 12860, loss 2.43256, smoothed loss 2.61869, grad norm 3.46296, param norm 200.51207
INFO:root:epoch 14, iter 12865, loss 3.01218, smoothed loss 2.62610, grad norm 3.03437, param norm 200.54179
INFO:root:epoch 14, iter 12870, loss 2.30453, smoothed loss 2.61159, grad norm 2.68845, param norm 200.57224
INFO:root:epoch 14, iter 12875, loss 2.77468, smoothed loss 2.61164, grad norm 2.86564, param norm 200.60338
INFO:root:epoch 14, iter 12880, loss 2.36198, smoothed loss 2.60742, grad norm 2.92521, param norm 200.63319
INFO:root:epoch 14, iter 12885, loss 1.72162, smoothed loss 2.60108, grad norm 3.02633, param norm 200.66299
INFO:root:epoch 14, iter 12890, loss 2.36573, smoothed loss 2.59909, grad norm 3.26839, param norm 200.69514
INFO:root:epoch 14, iter 12895, loss 1.73695, smoothed loss 2.59291, grad norm 2.52015, param norm 200.72655
INFO:root:epoch 14, iter 12900, loss 2.71975, smoothed loss 2.60360, grad norm 2.98192, param norm 200.75493
INFO:root:epoch 14, iter 12905, loss 3.15021, smoothed loss 2.61667, grad norm 3.01996, param norm 200.77859
INFO:root:epoch 14, iter 12910, loss 2.58480, smoothed loss 2.60463, grad norm 2.94775, param norm 200.80292
Adding batches start...
Added  160  batches
INFO:root:epoch 14, iter 12915, loss 2.39720, smoothed loss 2.60208, grad norm 2.54031, param norm 200.82986
INFO:root:epoch 14, iter 12920, loss 3.69566, smoothed loss 2.60924, grad norm 3.37553, param norm 200.85855
INFO:root:epoch 14, iter 12925, loss 2.55577, smoothed loss 2.61092, grad norm 2.84141, param norm 200.88518
INFO:root:epoch 14, iter 12930, loss 3.04726, smoothed loss 2.60972, grad norm 3.61341, param norm 200.91397
INFO:root:epoch 14, iter 12935, loss 2.15355, smoothed loss 2.61428, grad norm 2.58469, param norm 200.94170
INFO:root:epoch 14, iter 12940, loss 1.73398, smoothed loss 2.62362, grad norm 2.56436, param norm 200.97482
INFO:root:epoch 14, iter 12945, loss 2.71089, smoothed loss 2.61762, grad norm 3.20463, param norm 201.00650
INFO:root:epoch 14, iter 12950, loss 2.96897, smoothed loss 2.61961, grad norm 3.96049, param norm 201.03590
INFO:root:epoch 14, iter 12955, loss 2.36498, smoothed loss 2.61366, grad norm 2.91171, param norm 201.06717
INFO:root:epoch 14, iter 12960, loss 2.29544, smoothed loss 2.60996, grad norm 2.98978, param norm 201.09598
INFO:root:epoch 14, iter 12965, loss 2.71417, smoothed loss 2.60131, grad norm 3.24450, param norm 201.12544
INFO:root:epoch 14, iter 12970, loss 2.25453, smoothed loss 2.60658, grad norm 3.39447, param norm 201.15764
INFO:root:epoch 14, iter 12975, loss 2.61272, smoothed loss 2.60651, grad norm 3.50037, param norm 201.19084
INFO:root:epoch 14, iter 12980, loss 2.55589, smoothed loss 2.60354, grad norm 3.14656, param norm 201.22752
INFO:root:epoch 14, iter 12985, loss 2.10359, smoothed loss 2.60109, grad norm 2.80295, param norm 201.26311
INFO:root:epoch 14, iter 12990, loss 2.63784, smoothed loss 2.59393, grad norm 3.09279, param norm 201.29962
INFO:root:epoch 14, iter 12995, loss 2.65985, smoothed loss 2.60132, grad norm 3.24167, param norm 201.33435
INFO:root:epoch 14, iter 13000, loss 2.56897, smoothed loss 2.60208, grad norm 3.11137, param norm 201.36348
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 14, Iter 13000, dev loss: 3.085029
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 16.93656 seconds [Score: 0.76058]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.11427 seconds [Score: 0.64500]
INFO:root:Epoch 14, Iter 13000, Train F1 score: 0.760577, Train EM score: 0.645000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 119.45133 seconds [Score: 0.64407]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 118.39311 seconds [Score: 0.50000]
INFO:root:Epoch 14, Iter 13000, Dev F1 score: 0.644069, Dev EM score: 0.500000
INFO:root:End of epoch 14
INFO:root:epoch 14, iter 13005, loss 3.01381, smoothed loss 2.61978, grad norm 3.54841, param norm 201.38675
INFO:root:epoch 14, iter 13010, loss 2.25462, smoothed loss 2.61569, grad norm 2.70091, param norm 201.40805
INFO:root:epoch 14, iter 13015, loss 2.50689, smoothed loss 2.61836, grad norm 2.75747, param norm 201.43297
INFO:root:epoch 14, iter 13020, loss 2.89989, smoothed loss 2.60949, grad norm 2.82330, param norm 201.46132
INFO:root:epoch 14, iter 13025, loss 2.48113, smoothed loss 2.60694, grad norm 3.10266, param norm 201.49213
INFO:root:epoch 14, iter 13030, loss 2.34346, smoothed loss 2.60793, grad norm 2.76665, param norm 201.52484
INFO:root:epoch 14, iter 13035, loss 2.80296, smoothed loss 2.60638, grad norm 3.16600, param norm 201.56140
INFO:root:epoch 14, iter 13040, loss 2.54471, smoothed loss 2.61037, grad norm 2.79924, param norm 201.59808
INFO:root:epoch 14, iter 13045, loss 2.86739, smoothed loss 2.62038, grad norm 3.32823, param norm 201.63063
INFO:root:epoch 14, iter 13050, loss 3.14726, smoothed loss 2.62676, grad norm 4.14152, param norm 201.66064
INFO:root:epoch 14, iter 13055, loss 2.67923, smoothed loss 2.60562, grad norm 3.29021, param norm 201.69034
INFO:root:epoch 14, iter 13060, loss 2.52533, smoothed loss 2.60939, grad norm 2.81129, param norm 201.71907
INFO:root:epoch 14, iter 13065, loss 2.56061, smoothed loss 2.60870, grad norm 3.06967, param norm 201.74904
INFO:root:epoch 14, iter 13070, loss 2.15401, smoothed loss 2.61091, grad norm 2.93309, param norm 201.77592
Adding batches start...
Added  144  batches
INFO:root:epoch 14, iter 13075, loss 2.66529, smoothed loss 2.61185, grad norm 3.09705, param norm 201.80347
INFO:root:epoch 14, iter 13080, loss 2.32967, smoothed loss 2.60533, grad norm 2.87507, param norm 201.83493
INFO:root:epoch 14, iter 13085, loss 2.45301, smoothed loss 2.60432, grad norm 3.28041, param norm 201.86525
INFO:root:epoch 14, iter 13090, loss 2.17525, smoothed loss 2.60317, grad norm 3.23955, param norm 201.89119
INFO:root:epoch 14, iter 13095, loss 2.19296, smoothed loss 2.59686, grad norm 3.35265, param norm 201.92096
INFO:root:epoch 14, iter 13100, loss 2.45393, smoothed loss 2.60186, grad norm 3.34965, param norm 201.95164
INFO:root:epoch 14, iter 13105, loss 2.79687, smoothed loss 2.60500, grad norm 3.49027, param norm 201.98090
INFO:root:epoch 14, iter 13110, loss 2.51039, smoothed loss 2.59997, grad norm 3.44497, param norm 202.01382
INFO:root:epoch 14, iter 13115, loss 1.92112, smoothed loss 2.60053, grad norm 2.82952, param norm 202.04616
INFO:root:epoch 14, iter 13120, loss 2.80271, smoothed loss 2.59569, grad norm 3.20282, param norm 202.07758
INFO:root:epoch 14, iter 13125, loss 2.48942, smoothed loss 2.58929, grad norm 3.05692, param norm 202.10713
INFO:root:epoch 14, iter 13130, loss 2.74994, smoothed loss 2.58864, grad norm 3.45177, param norm 202.13869
INFO:root:epoch 14, iter 13135, loss 2.55438, smoothed loss 2.58205, grad norm 3.46397, param norm 202.17241
INFO:root:epoch 14, iter 13140, loss 3.18293, smoothed loss 2.59295, grad norm 3.62824, param norm 202.20337
INFO:root:epoch 14, iter 13145, loss 2.00408, smoothed loss 2.59125, grad norm 2.81604, param norm 202.23392
INFO:root:epoch 14, iter 13150, loss 2.36325, smoothed loss 2.58354, grad norm 2.70589, param norm 202.26692
INFO:root:epoch 14, iter 13155, loss 2.28812, smoothed loss 2.57695, grad norm 2.80480, param norm 202.29820
INFO:root:epoch 14, iter 13160, loss 2.99843, smoothed loss 2.59272, grad norm 3.20028, param norm 202.32889
INFO:root:epoch 14, iter 13165, loss 2.86931, smoothed loss 2.59527, grad norm 3.14126, param norm 202.35771
INFO:root:epoch 14, iter 13170, loss 2.22174, smoothed loss 2.59829, grad norm 2.94542, param norm 202.38467
INFO:root:epoch 14, iter 13175, loss 2.96175, smoothed loss 2.61644, grad norm 3.12783, param norm 202.41110
INFO:root:epoch 14, iter 13180, loss 2.85694, smoothed loss 2.61585, grad norm 3.37511, param norm 202.44022
INFO:root:epoch 14, iter 13185, loss 2.60747, smoothed loss 2.61116, grad norm 3.29335, param norm 202.46999
INFO:root:epoch 14, iter 13190, loss 2.25080, smoothed loss 2.60803, grad norm 2.73622, param norm 202.50182
INFO:root:epoch 14, iter 13195, loss 2.89935, smoothed loss 2.61106, grad norm 3.25455, param norm 202.53223
INFO:root:epoch 14, iter 13200, loss 2.56016, smoothed loss 2.61365, grad norm 3.11489, param norm 202.56058
INFO:root:epoch 14, iter 13205, loss 2.43512, smoothed loss 2.61403, grad norm 3.28033, param norm 202.58882
INFO:root:epoch 14, iter 13210, loss 2.26519, smoothed loss 2.60969, grad norm 2.85206, param norm 202.61606
INFO:root:epoch 14, iter 13215, loss 2.77296, smoothed loss 2.61358, grad norm 2.94947, param norm 202.64288
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 15, iter 13220, loss 2.35915, smoothed loss 2.60640, grad norm 3.13962, param norm 202.67320
INFO:root:epoch 15, iter 13225, loss 2.06431, smoothed loss 2.59795, grad norm 3.02509, param norm 202.70270
INFO:root:epoch 15, iter 13230, loss 3.08257, smoothed loss 2.61320, grad norm 3.08420, param norm 202.73431
INFO:root:epoch 15, iter 13235, loss 2.65074, smoothed loss 2.60327, grad norm 3.35081, param norm 202.76411
INFO:root:epoch 15, iter 13240, loss 2.79937, smoothed loss 2.59453, grad norm 3.36374, param norm 202.79459
INFO:root:epoch 15, iter 13245, loss 2.70171, smoothed loss 2.58991, grad norm 2.88922, param norm 202.82574
INFO:root:epoch 15, iter 13250, loss 2.88159, smoothed loss 2.58597, grad norm 3.25158, param norm 202.85735
INFO:root:epoch 15, iter 13255, loss 2.28425, smoothed loss 2.57813, grad norm 3.24140, param norm 202.88622
INFO:root:epoch 15, iter 13260, loss 2.34459, smoothed loss 2.57556, grad norm 3.28058, param norm 202.91426
INFO:root:epoch 15, iter 13265, loss 2.76059, smoothed loss 2.58680, grad norm 3.28316, param norm 202.94504
INFO:root:epoch 15, iter 13270, loss 3.04810, smoothed loss 2.59519, grad norm 3.17878, param norm 202.97748
INFO:root:epoch 15, iter 13275, loss 2.50637, smoothed loss 2.58861, grad norm 2.91403, param norm 203.01083
INFO:root:epoch 15, iter 13280, loss 2.33335, smoothed loss 2.59514, grad norm 2.92774, param norm 203.04434
INFO:root:epoch 15, iter 13285, loss 2.03243, smoothed loss 2.58711, grad norm 3.08691, param norm 203.07715
INFO:root:epoch 15, iter 13290, loss 2.35391, smoothed loss 2.58760, grad norm 3.06583, param norm 203.10640
INFO:root:epoch 15, iter 13295, loss 3.26186, smoothed loss 2.59104, grad norm 3.96137, param norm 203.13263
INFO:root:epoch 15, iter 13300, loss 1.77287, smoothed loss 2.58089, grad norm 2.59977, param norm 203.15584
INFO:root:epoch 15, iter 13305, loss 2.55116, smoothed loss 2.58827, grad norm 3.11110, param norm 203.18562
INFO:root:epoch 15, iter 13310, loss 3.14019, smoothed loss 2.59435, grad norm 3.55379, param norm 203.21718
INFO:root:epoch 15, iter 13315, loss 2.44683, smoothed loss 2.59964, grad norm 2.71190, param norm 203.24748
INFO:root:epoch 15, iter 13320, loss 2.93925, smoothed loss 2.61145, grad norm 3.82419, param norm 203.27950
INFO:root:epoch 15, iter 13325, loss 1.68828, smoothed loss 2.60611, grad norm 2.26010, param norm 203.31734
INFO:root:epoch 15, iter 13330, loss 2.19011, smoothed loss 2.59536, grad norm 2.85828, param norm 203.35373
INFO:root:epoch 15, iter 13335, loss 2.72101, smoothed loss 2.59868, grad norm 3.12640, param norm 203.38774
INFO:root:epoch 15, iter 13340, loss 2.23532, smoothed loss 2.59593, grad norm 2.76441, param norm 203.41714
INFO:root:epoch 15, iter 13345, loss 2.77736, smoothed loss 2.59994, grad norm 3.34001, param norm 203.44423
INFO:root:epoch 15, iter 13350, loss 2.40030, smoothed loss 2.60457, grad norm 3.00962, param norm 203.47232
INFO:root:epoch 15, iter 13355, loss 2.52219, smoothed loss 2.60832, grad norm 3.11245, param norm 203.50114
INFO:root:epoch 15, iter 13360, loss 2.34966, smoothed loss 2.60722, grad norm 3.49321, param norm 203.53218
INFO:root:epoch 15, iter 13365, loss 2.55990, smoothed loss 2.60718, grad norm 3.23865, param norm 203.56378
INFO:root:epoch 15, iter 13370, loss 2.66371, smoothed loss 2.60917, grad norm 3.25203, param norm 203.59418
INFO:root:epoch 15, iter 13375, loss 2.79451, smoothed loss 2.61023, grad norm 2.94190, param norm 203.62521
Adding batches start...
Added  160  batches
INFO:root:epoch 15, iter 13380, loss 2.65518, smoothed loss 2.60603, grad norm 3.19841, param norm 203.65485
INFO:root:epoch 15, iter 13385, loss 3.24607, smoothed loss 2.61696, grad norm 3.57957, param norm 203.68390
INFO:root:epoch 15, iter 13390, loss 2.51145, smoothed loss 2.59904, grad norm 3.05708, param norm 203.71454
INFO:root:epoch 15, iter 13395, loss 2.27413, smoothed loss 2.59618, grad norm 2.49667, param norm 203.74330
INFO:root:epoch 15, iter 13400, loss 2.76084, smoothed loss 2.60004, grad norm 3.40753, param norm 203.76793
INFO:root:epoch 15, iter 13405, loss 2.57131, smoothed loss 2.60517, grad norm 3.16224, param norm 203.79852
INFO:root:epoch 15, iter 13410, loss 2.53800, smoothed loss 2.61378, grad norm 2.81842, param norm 203.83174
INFO:root:epoch 15, iter 13415, loss 2.95810, smoothed loss 2.61339, grad norm 3.29209, param norm 203.86621
INFO:root:epoch 15, iter 13420, loss 2.83759, smoothed loss 2.62830, grad norm 3.37749, param norm 203.89569
INFO:root:epoch 15, iter 13425, loss 2.80903, smoothed loss 2.63360, grad norm 3.58354, param norm 203.92630
INFO:root:epoch 15, iter 13430, loss 3.06924, smoothed loss 2.63790, grad norm 3.50022, param norm 203.95807
INFO:root:epoch 15, iter 13435, loss 2.52909, smoothed loss 2.63626, grad norm 2.98881, param norm 203.98776
INFO:root:epoch 15, iter 13440, loss 3.71585, smoothed loss 2.63287, grad norm 3.94676, param norm 204.01671
INFO:root:epoch 15, iter 13445, loss 2.42443, smoothed loss 2.63230, grad norm 3.59053, param norm 204.04778
INFO:root:epoch 15, iter 13450, loss 2.71351, smoothed loss 2.62903, grad norm 2.75412, param norm 204.08003
INFO:root:epoch 15, iter 13455, loss 2.01653, smoothed loss 2.62537, grad norm 2.69713, param norm 204.11237
INFO:root:epoch 15, iter 13460, loss 3.34402, smoothed loss 2.63751, grad norm 3.47910, param norm 204.14369
INFO:root:epoch 15, iter 13465, loss 3.01647, smoothed loss 2.63926, grad norm 3.55628, param norm 204.17441
INFO:root:epoch 15, iter 13470, loss 2.57002, smoothed loss 2.63092, grad norm 2.70330, param norm 204.20700
INFO:root:epoch 15, iter 13475, loss 2.31764, smoothed loss 2.63199, grad norm 2.82163, param norm 204.23979
INFO:root:epoch 15, iter 13480, loss 1.93007, smoothed loss 2.62535, grad norm 2.78377, param norm 204.27158
INFO:root:epoch 15, iter 13485, loss 2.09812, smoothed loss 2.63204, grad norm 2.96301, param norm 204.30124
INFO:root:epoch 15, iter 13490, loss 2.64950, smoothed loss 2.63857, grad norm 3.37948, param norm 204.32843
INFO:root:epoch 15, iter 13495, loss 2.63565, smoothed loss 2.63392, grad norm 3.01518, param norm 204.35573
INFO:root:epoch 15, iter 13500, loss 2.26419, smoothed loss 2.62343, grad norm 2.72001, param norm 204.38515
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 15, Iter 13500, dev loss: 3.040193
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.66512 seconds [Score: 0.81289]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 18.37488 seconds [Score: 0.64200]
INFO:root:Epoch 15, Iter 13500, Train F1 score: 0.812890, Train EM score: 0.642000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.95655 seconds [Score: 0.64826]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 119.28444 seconds [Score: 0.50183]
INFO:root:Epoch 15, Iter 13500, Dev F1 score: 0.648258, Dev EM score: 0.501826
INFO:root:End of epoch 15
INFO:root:epoch 15, iter 13505, loss 2.76875, smoothed loss 2.63373, grad norm 2.95974, param norm 204.41998
INFO:root:epoch 15, iter 13510, loss 2.00203, smoothed loss 2.61632, grad norm 2.72950, param norm 204.45329
INFO:root:epoch 15, iter 13515, loss 2.18386, smoothed loss 2.60049, grad norm 2.92942, param norm 204.48523
INFO:root:epoch 15, iter 13520, loss 2.76150, smoothed loss 2.60388, grad norm 2.88871, param norm 204.51645
INFO:root:epoch 15, iter 13525, loss 2.57049, smoothed loss 2.59239, grad norm 3.34003, param norm 204.54735
INFO:root:epoch 15, iter 13530, loss 2.78021, smoothed loss 2.60825, grad norm 3.10383, param norm 204.57759
INFO:root:epoch 15, iter 13535, loss 2.74956, smoothed loss 2.61212, grad norm 3.37346, param norm 204.60263
Adding batches start...
Added  160  batches
INFO:root:epoch 15, iter 13540, loss 2.18923, smoothed loss 2.61327, grad norm 2.62886, param norm 204.62875
INFO:root:epoch 15, iter 13545, loss 3.13360, smoothed loss 2.62339, grad norm 3.48563, param norm 204.65453
INFO:root:epoch 15, iter 13550, loss 1.72027, smoothed loss 2.60932, grad norm 2.64064, param norm 204.68188
INFO:root:epoch 15, iter 13555, loss 2.86048, smoothed loss 2.61135, grad norm 3.44094, param norm 204.71376
INFO:root:epoch 15, iter 13560, loss 3.11495, smoothed loss 2.62375, grad norm 3.28176, param norm 204.74348
INFO:root:epoch 15, iter 13565, loss 2.59985, smoothed loss 2.62361, grad norm 3.17173, param norm 204.77199
INFO:root:epoch 15, iter 13570, loss 2.04669, smoothed loss 2.61675, grad norm 2.79642, param norm 204.80292
INFO:root:epoch 15, iter 13575, loss 2.88706, smoothed loss 2.61708, grad norm 3.13920, param norm 204.83540
INFO:root:epoch 15, iter 13580, loss 2.89291, smoothed loss 2.62174, grad norm 3.80252, param norm 204.86699
INFO:root:epoch 15, iter 13585, loss 2.53946, smoothed loss 2.61674, grad norm 3.42311, param norm 204.89476
INFO:root:epoch 15, iter 13590, loss 2.60862, smoothed loss 2.62115, grad norm 3.20442, param norm 204.92235
INFO:root:epoch 15, iter 13595, loss 2.80447, smoothed loss 2.61380, grad norm 3.12642, param norm 204.95107
INFO:root:epoch 15, iter 13600, loss 3.42214, smoothed loss 2.61826, grad norm 3.83647, param norm 204.97914
INFO:root:epoch 15, iter 13605, loss 3.03166, smoothed loss 2.61568, grad norm 3.27878, param norm 205.00719
INFO:root:epoch 15, iter 13610, loss 3.01843, smoothed loss 2.62127, grad norm 3.04234, param norm 205.03549
INFO:root:epoch 15, iter 13615, loss 2.52315, smoothed loss 2.60808, grad norm 3.14818, param norm 205.07060
INFO:root:epoch 15, iter 13620, loss 2.45868, smoothed loss 2.60799, grad norm 2.92316, param norm 205.10211
INFO:root:epoch 15, iter 13625, loss 2.64408, smoothed loss 2.60368, grad norm 3.09977, param norm 205.13754
INFO:root:epoch 15, iter 13630, loss 2.50855, smoothed loss 2.60056, grad norm 2.98265, param norm 205.17183
INFO:root:epoch 15, iter 13635, loss 2.93419, smoothed loss 2.59801, grad norm 2.80462, param norm 205.20053
INFO:root:epoch 15, iter 13640, loss 2.95318, smoothed loss 2.59283, grad norm 3.45364, param norm 205.22694
INFO:root:epoch 15, iter 13645, loss 2.19494, smoothed loss 2.60124, grad norm 2.74483, param norm 205.25117
INFO:root:epoch 15, iter 13650, loss 2.36075, smoothed loss 2.58335, grad norm 3.56028, param norm 205.27426
INFO:root:epoch 15, iter 13655, loss 2.31663, smoothed loss 2.57472, grad norm 3.21838, param norm 205.30273
INFO:root:epoch 15, iter 13660, loss 1.61836, smoothed loss 2.55620, grad norm 2.56194, param norm 205.33218
INFO:root:epoch 15, iter 13665, loss 2.53470, smoothed loss 2.54847, grad norm 3.30438, param norm 205.36278
INFO:root:epoch 15, iter 13670, loss 2.91303, smoothed loss 2.54874, grad norm 3.66841, param norm 205.39081
INFO:root:epoch 15, iter 13675, loss 3.10878, smoothed loss 2.55414, grad norm 3.41650, param norm 205.41917
INFO:root:epoch 15, iter 13680, loss 2.28611, smoothed loss 2.54625, grad norm 2.96896, param norm 205.44792
INFO:root:epoch 15, iter 13685, loss 1.61351, smoothed loss 2.54631, grad norm 2.47886, param norm 205.47479
INFO:root:epoch 15, iter 13690, loss 2.16271, smoothed loss 2.54802, grad norm 2.94858, param norm 205.50308
INFO:root:epoch 15, iter 13695, loss 2.54958, smoothed loss 2.56191, grad norm 3.49160, param norm 205.53093
Adding batches start...
Added  160  batches
INFO:root:epoch 15, iter 13700, loss 2.70108, smoothed loss 2.55178, grad norm 3.37222, param norm 205.56248
INFO:root:epoch 15, iter 13705, loss 2.37248, smoothed loss 2.54458, grad norm 3.13229, param norm 205.59776
INFO:root:epoch 15, iter 13710, loss 2.04142, smoothed loss 2.53301, grad norm 3.12265, param norm 205.63350
INFO:root:epoch 15, iter 13715, loss 3.09061, smoothed loss 2.52560, grad norm 3.79061, param norm 205.66991
INFO:root:epoch 15, iter 13720, loss 2.16374, smoothed loss 2.51702, grad norm 3.19760, param norm 205.70248
INFO:root:epoch 15, iter 13725, loss 1.98331, smoothed loss 2.50554, grad norm 2.91040, param norm 205.73526
INFO:root:epoch 15, iter 13730, loss 2.81412, smoothed loss 2.50861, grad norm 3.60858, param norm 205.76607
INFO:root:epoch 15, iter 13735, loss 2.75041, smoothed loss 2.50213, grad norm 3.08229, param norm 205.79617
INFO:root:epoch 15, iter 13740, loss 2.30072, smoothed loss 2.50365, grad norm 2.86745, param norm 205.82582
INFO:root:epoch 15, iter 13745, loss 2.52848, smoothed loss 2.51228, grad norm 3.25851, param norm 205.85294
INFO:root:epoch 15, iter 13750, loss 2.89194, smoothed loss 2.51172, grad norm 3.13784, param norm 205.87933
INFO:root:epoch 15, iter 13755, loss 2.17161, smoothed loss 2.50192, grad norm 2.95037, param norm 205.90732
INFO:root:epoch 15, iter 13760, loss 2.81360, smoothed loss 2.49939, grad norm 3.55164, param norm 205.93524
INFO:root:epoch 15, iter 13765, loss 2.32333, smoothed loss 2.49941, grad norm 3.09860, param norm 205.96179
INFO:root:epoch 15, iter 13770, loss 2.46049, smoothed loss 2.49249, grad norm 3.24913, param norm 205.98857
INFO:root:epoch 15, iter 13775, loss 2.95824, smoothed loss 2.50041, grad norm 3.29577, param norm 206.01657
INFO:root:epoch 15, iter 13780, loss 2.31050, smoothed loss 2.49837, grad norm 3.28129, param norm 206.04224
INFO:root:epoch 15, iter 13785, loss 2.80728, smoothed loss 2.50829, grad norm 3.69707, param norm 206.06816
INFO:root:epoch 15, iter 13790, loss 2.81602, smoothed loss 2.50260, grad norm 3.21969, param norm 206.09386
INFO:root:epoch 15, iter 13795, loss 2.65806, smoothed loss 2.51915, grad norm 3.31826, param norm 206.12050
INFO:root:epoch 15, iter 13800, loss 3.69163, smoothed loss 2.53706, grad norm 4.48494, param norm 206.15004
INFO:root:epoch 15, iter 13805, loss 2.53769, smoothed loss 2.53981, grad norm 2.81034, param norm 206.17877
INFO:root:epoch 15, iter 13810, loss 2.64405, smoothed loss 2.55328, grad norm 3.16200, param norm 206.20445
INFO:root:epoch 15, iter 13815, loss 2.51639, smoothed loss 2.54285, grad norm 2.60152, param norm 206.23216
INFO:root:epoch 15, iter 13820, loss 2.54388, smoothed loss 2.54013, grad norm 3.13980, param norm 206.26093
INFO:root:epoch 15, iter 13825, loss 2.31309, smoothed loss 2.54742, grad norm 2.80087, param norm 206.29074
INFO:root:epoch 15, iter 13830, loss 2.94414, smoothed loss 2.54755, grad norm 3.20926, param norm 206.32019
INFO:root:epoch 15, iter 13835, loss 2.86332, smoothed loss 2.54515, grad norm 3.52894, param norm 206.34781
INFO:root:epoch 15, iter 13840, loss 2.20128, smoothed loss 2.54217, grad norm 2.99668, param norm 206.37460
INFO:root:epoch 15, iter 13845, loss 2.50020, smoothed loss 2.54460, grad norm 3.06129, param norm 206.40286
INFO:root:epoch 15, iter 13850, loss 2.30679, smoothed loss 2.54615, grad norm 3.26314, param norm 206.43083
INFO:root:epoch 15, iter 13855, loss 2.52431, smoothed loss 2.54671, grad norm 3.08106, param norm 206.46106
Adding batches start...
Added  160  batches
INFO:root:epoch 15, iter 13860, loss 2.26945, smoothed loss 2.54653, grad norm 2.97065, param norm 206.48795
INFO:root:epoch 15, iter 13865, loss 2.26068, smoothed loss 2.53365, grad norm 2.78169, param norm 206.51198
INFO:root:epoch 15, iter 13870, loss 2.37370, smoothed loss 2.53808, grad norm 2.63902, param norm 206.53735
INFO:root:epoch 15, iter 13875, loss 2.41828, smoothed loss 2.54999, grad norm 2.84397, param norm 206.56564
INFO:root:epoch 15, iter 13880, loss 1.98604, smoothed loss 2.53530, grad norm 2.56181, param norm 206.59502
INFO:root:epoch 15, iter 13885, loss 2.86008, smoothed loss 2.54463, grad norm 4.20116, param norm 206.62323
INFO:root:epoch 15, iter 13890, loss 2.97729, smoothed loss 2.54373, grad norm 3.32530, param norm 206.64984
INFO:root:epoch 15, iter 13895, loss 3.14920, smoothed loss 2.54529, grad norm 3.23572, param norm 206.67867
INFO:root:epoch 15, iter 13900, loss 2.53324, smoothed loss 2.54310, grad norm 3.32664, param norm 206.70880
INFO:root:epoch 15, iter 13905, loss 2.36325, smoothed loss 2.54010, grad norm 3.19195, param norm 206.74109
INFO:root:epoch 15, iter 13910, loss 2.11009, smoothed loss 2.53754, grad norm 2.75643, param norm 206.77405
INFO:root:epoch 15, iter 13915, loss 2.18379, smoothed loss 2.52927, grad norm 3.18604, param norm 206.80392
INFO:root:epoch 15, iter 13920, loss 2.83013, smoothed loss 2.52850, grad norm 3.18154, param norm 206.83298
INFO:root:epoch 15, iter 13925, loss 1.99009, smoothed loss 2.52549, grad norm 3.09304, param norm 206.86443
INFO:root:epoch 15, iter 13930, loss 2.71145, smoothed loss 2.52750, grad norm 3.19307, param norm 206.89690
INFO:root:epoch 15, iter 13935, loss 2.81184, smoothed loss 2.53400, grad norm 3.44622, param norm 206.92863
INFO:root:epoch 15, iter 13940, loss 2.04291, smoothed loss 2.52742, grad norm 2.54963, param norm 206.96086
INFO:root:epoch 15, iter 13945, loss 2.90179, smoothed loss 2.52970, grad norm 3.41538, param norm 206.99193
INFO:root:epoch 15, iter 13950, loss 2.62379, smoothed loss 2.53316, grad norm 2.97537, param norm 207.01878
INFO:root:epoch 15, iter 13955, loss 2.54196, smoothed loss 2.52982, grad norm 3.05506, param norm 207.04201
INFO:root:epoch 15, iter 13960, loss 2.22605, smoothed loss 2.52804, grad norm 3.12042, param norm 207.06770
INFO:root:epoch 15, iter 13965, loss 2.48716, smoothed loss 2.53221, grad norm 2.94415, param norm 207.09615
INFO:root:epoch 15, iter 13970, loss 2.10835, smoothed loss 2.53489, grad norm 2.91891, param norm 207.12526
INFO:root:epoch 15, iter 13975, loss 2.44981, smoothed loss 2.53925, grad norm 3.01042, param norm 207.15387
INFO:root:epoch 15, iter 13980, loss 2.81711, smoothed loss 2.54835, grad norm 3.40718, param norm 207.17958
INFO:root:epoch 15, iter 13985, loss 2.59529, smoothed loss 2.55790, grad norm 2.97690, param norm 207.20726
INFO:root:epoch 15, iter 13990, loss 2.70401, smoothed loss 2.55608, grad norm 3.15664, param norm 207.23741
INFO:root:epoch 15, iter 13995, loss 2.45841, smoothed loss 2.55885, grad norm 3.33101, param norm 207.26805
INFO:root:epoch 15, iter 14000, loss 3.29298, smoothed loss 2.56562, grad norm 3.43045, param norm 207.29706
INFO:root:Saving to ./train/qa.ckpt...
INFO:root:Calculating dev loss...
Adding batches start...
Added  118  batches
Adding batches start...
Added  0  batches
INFO:root:Epoch 15, Iter 14000, dev loss: 3.077887
INFO:root:Calculating Train F1/EM...
Adding batches start...
Added  160  batches
INFO:root:F1 train: 1000 examples took 17.58227 seconds [Score: 0.77770]
Adding batches start...
Added  160  batches
INFO:root:Exact Match train: 1000 examples took 17.71842 seconds [Score: 0.65200]
INFO:root:Epoch 15, Iter 14000, Train F1 score: 0.777695, Train EM score: 0.652000
INFO:root:Calculating Dev F1/EM...
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:F1 dev: 7118 examples took 118.34399 seconds [Score: 0.64237]
Adding batches start...
Added  119  batches
Adding batches start...
Added  0  batches
INFO:root:Exact Match dev: 7118 examples took 117.69556 seconds [Score: 0.49635]
INFO:root:Epoch 15, Iter 14000, Dev F1 score: 0.642366, Dev EM score: 0.496347
INFO:root:End of epoch 15
INFO:root:epoch 15, iter 14005, loss 2.52766, smoothed loss 2.56662, grad norm 3.12145, param norm 207.32492
INFO:root:epoch 15, iter 14010, loss 2.51062, smoothed loss 2.56511, grad norm 3.34422, param norm 207.35553
INFO:root:epoch 15, iter 14015, loss 2.72929, smoothed loss 2.56947, grad norm 3.00803, param norm 207.38844
Adding batches start...
Added  144  batches
INFO:root:epoch 15, iter 14020, loss 3.05120, smoothed loss 2.56824, grad norm 3.83875, param norm 207.41997
INFO:root:epoch 15, iter 14025, loss 3.05528, smoothed loss 2.56019, grad norm 4.20522, param norm 207.44939
INFO:root:epoch 15, iter 14030, loss 2.80113, smoothed loss 2.55302, grad norm 3.15914, param norm 207.47928
INFO:root:epoch 15, iter 14035, loss 2.23086, smoothed loss 2.55291, grad norm 3.18505, param norm 207.50687
INFO:root:epoch 15, iter 14040, loss 2.47154, smoothed loss 2.54366, grad norm 2.96391, param norm 207.53781
INFO:root:epoch 15, iter 14045, loss 2.61754, smoothed loss 2.55283, grad norm 3.23243, param norm 207.57146
INFO:root:epoch 15, iter 14050, loss 2.56015, smoothed loss 2.54523, grad norm 2.90260, param norm 207.60439
INFO:root:epoch 15, iter 14055, loss 3.07317, smoothed loss 2.54970, grad norm 3.61901, param norm 207.63489
INFO:root:epoch 15, iter 14060, loss 3.12252, smoothed loss 2.55916, grad norm 3.87005, param norm 207.66251
INFO:root:epoch 15, iter 14065, loss 2.70730, smoothed loss 2.56991, grad norm 3.08026, param norm 207.69389
INFO:root:epoch 15, iter 14070, loss 2.33521, smoothed loss 2.56815, grad norm 2.46310, param norm 207.72334
INFO:root:epoch 15, iter 14075, loss 1.85814, smoothed loss 2.56029, grad norm 2.92870, param norm 207.75137
INFO:root:epoch 15, iter 14080, loss 2.69580, smoothed loss 2.56721, grad norm 3.40622, param norm 207.77927
INFO:root:epoch 15, iter 14085, loss 3.19184, smoothed loss 2.57326, grad norm 4.11439, param norm 207.80965
INFO:root:epoch 15, iter 14090, loss 3.02644, smoothed loss 2.58470, grad norm 3.59412, param norm 207.84030
INFO:root:epoch 15, iter 14095, loss 2.51456, smoothed loss 2.58705, grad norm 2.91233, param norm 207.87039
INFO:root:epoch 15, iter 14100, loss 2.51069, smoothed loss 2.58684, grad norm 2.71509, param norm 207.89716
INFO:root:epoch 15, iter 14105, loss 2.43416, smoothed loss 2.58939, grad norm 2.91763, param norm 207.92378
INFO:root:epoch 15, iter 14110, loss 2.52934, smoothed loss 2.58718, grad norm 2.65960, param norm 207.95023
INFO:root:epoch 15, iter 14115, loss 2.35282, smoothed loss 2.57831, grad norm 3.18046, param norm 207.97659
INFO:root:epoch 15, iter 14120, loss 2.37438, smoothed loss 2.56640, grad norm 3.00884, param norm 208.00575
INFO:root:epoch 15, iter 14125, loss 1.87347, smoothed loss 2.55746, grad norm 2.43904, param norm 208.03648
INFO:root:epoch 15, iter 14130, loss 3.11105, smoothed loss 2.55866, grad norm 3.50068, param norm 208.06819
INFO:root:epoch 15, iter 14135, loss 2.67525, smoothed loss 2.55287, grad norm 3.09436, param norm 208.09496
INFO:root:epoch 15, iter 14140, loss 2.52436, smoothed loss 2.55416, grad norm 2.94977, param norm 208.12277
INFO:root:epoch 15, iter 14145, loss 2.80912, smoothed loss 2.55978, grad norm 3.17445, param norm 208.14833
INFO:root:epoch 15, iter 14150, loss 2.67622, smoothed loss 2.56502, grad norm 3.25397, param norm 208.17426
INFO:root:epoch 15, iter 14155, loss 2.79252, smoothed loss 2.55769, grad norm 3.05196, param norm 208.20123
INFO:root:epoch 15, iter 14160, loss 2.22327, smoothed loss 2.55599, grad norm 3.08920, param norm 208.22725
Adding batches start...
Added  0  batches
Adding batches start...
Added  160  batches
INFO:root:epoch 16, iter 14165, loss 3.02960, smoothed loss 2.56620, grad norm 3.12706, param norm 208.25220
INFO:root:epoch 16, iter 14170, loss 2.49008, smoothed loss 2.56615, grad norm 3.44943, param norm 208.27850
INFO:root:epoch 16, iter 14175, loss 2.60778, smoothed loss 2.55500, grad norm 2.92516, param norm 208.30406
INFO:root:epoch 16, iter 14180, loss 2.58085, smoothed loss 2.55483, grad norm 3.41942, param norm 208.33066
INFO:root:epoch 16, iter 14185, loss 2.41895, smoothed loss 2.54263, grad norm 2.89583, param norm 208.35799
INFO:root:epoch 16, iter 14190, loss 1.87921, smoothed loss 2.52711, grad norm 2.76887, param norm 208.38698
INFO:root:epoch 16, iter 14195, loss 2.38876, smoothed loss 2.52302, grad norm 2.90519, param norm 208.41658
INFO:root:epoch 16, iter 14200, loss 2.75279, smoothed loss 2.52055, grad norm 3.71309, param norm 208.44286
INFO:root:epoch 16, iter 14205, loss 2.63144, smoothed loss 2.52621, grad norm 3.09079, param norm 208.46915
INFO:root:epoch 16, iter 14210, loss 2.44992, smoothed loss 2.52382, grad norm 3.11510, param norm 208.49802
INFO:root:epoch 16, iter 14215, loss 2.84858, smoothed loss 2.53907, grad norm 3.29037, param norm 208.52591
INFO:root:epoch 16, iter 14220, loss 2.15653, smoothed loss 2.52888, grad norm 2.81351, param norm 208.55495
INFO:root:epoch 16, iter 14225, loss 2.69976, smoothed loss 2.53399, grad norm 3.14532, param norm 208.58614
INFO:root:epoch 16, iter 14230, loss 2.27828, smoothed loss 2.54043, grad norm 2.89552, param norm 208.61644
INFO:root:epoch 16, iter 14235, loss 2.20736, smoothed loss 2.53903, grad norm 2.98276, param norm 208.64680
INFO:root:epoch 16, iter 14240, loss 2.25411, smoothed loss 2.54280, grad norm 3.35824, param norm 208.67805
INFO:root:epoch 16, iter 14245, loss 2.19861, smoothed loss 2.53654, grad norm 3.54417, param norm 208.70950
INFO:root:epoch 16, iter 14250, loss 2.60435, smoothed loss 2.54398, grad norm 3.35831, param norm 208.74028
INFO:root:epoch 16, iter 14255, loss 2.35323, smoothed loss 2.54798, grad norm 3.33901, param norm 208.77293
INFO:root:epoch 16, iter 14260, loss 2.67543, smoothed loss 2.53998, grad norm 2.91763, param norm 208.80588
INFO:root:epoch 16, iter 14265, loss 2.80988, smoothed loss 2.54750, grad norm 3.34752, param norm 208.83974
INFO:root:epoch 16, iter 14270, loss 3.01672, smoothed loss 2.55035, grad norm 3.17680, param norm 208.87207
INFO:root:epoch 16, iter 14275, loss 2.90252, smoothed loss 2.55362, grad norm 3.38837, param norm 208.90582
INFO:root:epoch 16, iter 14280, loss 2.14335, smoothed loss 2.54943, grad norm 2.94022, param norm 208.93794
INFO:root:epoch 16, iter 14285, loss 2.69753, smoothed loss 2.56030, grad norm 3.25436, param norm 208.96913
INFO:root:epoch 16, iter 14290, loss 2.92957, smoothed loss 2.56353, grad norm 3.61593, param norm 209.00015
INFO:root:epoch 16, iter 14295, loss 2.53273, smoothed loss 2.55320, grad norm 3.52779, param norm 209.02827
INFO:root:epoch 16, iter 14300, loss 2.91090, smoothed loss 2.55965, grad norm 3.09215, param norm 209.05397
INFO:root:epoch 16, iter 14305, loss 2.45906, smoothed loss 2.56455, grad norm 2.72392, param norm 209.07953
INFO:root:epoch 16, iter 14310, loss 2.63958, smoothed loss 2.55460, grad norm 2.93298, param norm 209.10591
INFO:root:epoch 16, iter 14315, loss 2.40300, smoothed loss 2.55237, grad norm 2.96533, param norm 209.13322
INFO:root:epoch 16, iter 14320, loss 2.64308, smoothed loss 2.54390, grad norm 2.75125, param norm 209.15973
Adding batches start...
Added  160  batches
INFO:root:epoch 16, iter 14325, loss 2.70293, smoothed loss 2.53733, grad norm 3.55648, param norm 209.18465
INFO:root:epoch 16, iter 14330, loss 2.65099, smoothed loss 2.54485, grad norm 3.47088, param norm 209.20842
INFO:root:epoch 16, iter 14335, loss 2.58096, smoothed loss 2.53976, grad norm 3.18533, param norm 209.23297
INFO:root:epoch 16, iter 14340, loss 2.87073, smoothed loss 2.54601, grad norm 3.02154, param norm 209.25902
INFO:root:epoch 16, iter 14345, loss 2.11706, smoothed loss 2.53649, grad norm 3.43376, param norm 209.28722
INFO:root:epoch 16, iter 14350, loss 2.54439, smoothed loss 2.52750, grad norm 3.34624, param norm 209.31796
INFO:root:epoch 16, iter 14355, loss 3.00559, smoothed loss 2.52784, grad norm 3.31951, param norm 209.34717
INFO:root:epoch 16, iter 14360, loss 2.42563, smoothed loss 2.51472, grad norm 3.14160, param norm 209.37537
INFO:root:epoch 16, iter 14365, loss 2.47804, smoothed loss 2.51723, grad norm 2.52917, param norm 209.40274
INFO:root:epoch 16, iter 14370, loss 2.72746, smoothed loss 2.52160, grad norm 3.33500, param norm 209.43263
INFO:root:epoch 16, iter 14375, loss 3.07000, smoothed loss 2.52445, grad norm 3.35727, param norm 209.46330
INFO:root:epoch 16, iter 14380, loss 2.69533, smoothed loss 2.52476, grad norm 3.74693, param norm 209.49283
INFO:root:epoch 16, iter 14385, loss 2.09305, smoothed loss 2.52029, grad norm 3.08727, param norm 209.52122
INFO:root:epoch 16, iter 14390, loss 1.77274, smoothed loss 2.51179, grad norm 2.44542, param norm 209.55077

